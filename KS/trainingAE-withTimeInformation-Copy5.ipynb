{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666788634667,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3089,
     "status": "ok",
     "timestamp": 1666788637752,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666788637752,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "3AVrZNGlZu4Z"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666788637753,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "SxAd7iDL0Ami"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27766,
     "status": "ok",
     "timestamp": 1666788665512,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "JjNnPRuk0IIX",
    "outputId": "f93a8628-71fe-4d6d-b3b6-245dfcb8eb60"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666788665512,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "9REiGIIy0IzV",
    "outputId": "2b5b0b02-2f67-4635-a00c-82084a8d2ffb"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1381,
     "status": "ok",
     "timestamp": 1666788666890,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import mytimecallback, SaveLosses, plot_losses, readAndReturnLossHistories, plot_reconstructed_data_KS, plot_latent_states_KS \n",
    "from tools.ae_v6 import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666788666891,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "-mIQj_v4gzMh"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666788666892,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "QL5n-abCg0nI"
   },
   "outputs": [],
   "source": [
    "# setting seed for PRNGs\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    prng_seed = 42\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666788666892,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "f77bf689-c865-4a8d-8d40-37ec9c75f1ee"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 1\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print(tf.config.list_logical_devices())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2030,
     "status": "ok",
     "timestamp": 1666788668916,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "xcNgt4hqg6Xv",
    "outputId": "7735ac54-495c-493f-869b-7d15538ee30c"
   },
   "outputs": [],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # loading data directory\n",
    "    data_dir_idx = '005'\n",
    "\n",
    "    # making ae save directory\n",
    "    dir_name_ae = os.getcwd() + dir_sep + 'saved_ae'\n",
    "    if not os.path.isdir(dir_name_ae):\n",
    "        os.makedirs(dir_name_ae)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'ae_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_ae + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_ae = dir_name_ae + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_ae)\n",
    "    os.makedirs(dir_name_ae+dir_sep+'plots')\n",
    "else:\n",
    "    # some paramaters\n",
    "    dir_name_ae = os.getcwd()+'{ds}saved_ae{ds}ae_015'.format(ds=dir_sep)\n",
    "\n",
    "    with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_dict = eval(''.join(lines))\n",
    "    data_dir_idx = params_dict['data_dir_idx']\n",
    "    normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "    normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "    if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "        with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "            normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "\n",
    "print('dir_name_ae:', dir_name_ae)\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state = params_dict['init_state']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "numpoints_xgrid = params_dict['numpoints_xgrid']\n",
    "length = params_dict['length']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "xgrid = length*np.linspace(0, 1, numpoints_xgrid)\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data'].astype(FTYPE)\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666788668916,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "O7sl7i5H5Dqz"
   },
   "outputs": [],
   "source": [
    "def plot(\n",
    "        boundary_idx_arr,\n",
    "        delta_t,\n",
    "        all_data,\n",
    "        xgrid,\n",
    "        xticks_snapto=20,\n",
    "        num_yticks=11,\n",
    "        save_dir=None,\n",
    "        xlabel=r'Time',\n",
    "        ylabel=r'$x$',\n",
    "        ax_titles_list=None\n",
    "    ):\n",
    "\n",
    "    n = len(boundary_idx_arr)\n",
    "    num_digits_n = int(np.log10(n)+1)\n",
    "\n",
    "    # '''\n",
    "    num_cols = 1\n",
    "    num_rows = 1\n",
    "    factor = 1\n",
    "    # fig = plt.figure(figsize=(7.5*num_cols, 7.5*num_rows))\n",
    "\n",
    "    num_modes = xgrid.shape[0]\n",
    "\n",
    "    prev_idx = 0\n",
    "    for i in range(len(boundary_idx_arr)):\n",
    "        next_idx = boundary_idx_arr[i]\n",
    "        fig, ax = plt.subplots(figsize=(factor*7.5*num_cols, factor*5.0*num_rows))\n",
    "        N = next_idx - prev_idx\n",
    "        input_time = np.arange(0, N)*delta_t\n",
    "\n",
    "        im = ax.imshow(all_data[prev_idx:next_idx, 0:num_modes].transpose(), aspect='auto', origin='lower')\n",
    "        num_xticks = 1 + int((N*delta_t + 0.5*xticks_snapto) // xticks_snapto)\n",
    "        # xticks = np.linspace(0, N, num_xticks, dtype=np.int32)\n",
    "        xticks = np.arange(0, N, int((xticks_snapto+0.5*delta_t)//delta_t))\n",
    "        ax.set_xticks(ticks=xticks)\n",
    "        ax.set_xticklabels(np.round(xticks*delta_t, 1))\n",
    "        ax.tick_params(axis='x', rotation=270+45)\n",
    "\n",
    "        yticks = np.linspace(0, 1, 10+1)*(len(xgrid)-1)\n",
    "        yticklabels = np.round(np.linspace(0, 1, yticks.shape[0])*xgrid[-1], 2)\n",
    "        ax.set_yticks(ticks=yticks)\n",
    "        ax.set_yticklabels(yticklabels)\n",
    "\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel(r'$x$')\n",
    "        # ax.title.set_text(r'Latent States')\n",
    "        if ax_titles_list is not None:\n",
    "            ax.title.set_text(ax_titles_list[i])\n",
    "        else:\n",
    "            ax.title.set_text(r'Case '+str(i+1))\n",
    "\n",
    "        plt.colorbar(im)\n",
    "        if save_dir is not None:\n",
    "            fig.savefig(save_dir+'/Case_'+str(i+1).zfill(num_digits_n)+'.png', dpi=300, bbox_inches='tight')\n",
    "            fig.clear()\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "            print('')\n",
    "\n",
    "        prev_idx = next_idx\n",
    "\n",
    "    # '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2415,
     "status": "ok",
     "timestamp": 1666788671329,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "53f23b1d-fa61-4f27-bbc4-2e624421a866"
   },
   "outputs": [],
   "source": [
    "temp = boundary_idx_arr[0] + np.sum(boundary_idx_arr[1:] - boundary_idx_arr[0:-1])\n",
    "temp /= len(boundary_idx_arr)\n",
    "temp *= delta_t\n",
    "chosen_interval = temp//10\n",
    "sn = np.format_float_scientific(chosen_interval, exp_digits=4)\n",
    "snap = np.max([0.1, 25*np.round(10*float(sn[0:-6])/25)/10])\n",
    "snap *= float('1'+sn[-6:])\n",
    "xticks_snapto = int(snap*np.round(chosen_interval/snap))\n",
    "\n",
    "plot(\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    all_data,\n",
    "    xgrid,\n",
    "    xticks_snapto=xticks_snapto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1666788671330,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": [
    "# dealing with normalizing the data before feeding into autoencoder\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # normalize data before feeding into autoencoder?\n",
    "    normalizeforae_flag = True\n",
    "    normalization_type = 'stddev' # could be 'stddev' or 'minmax'\n",
    "    stddev_multiplier = 3\n",
    "    \n",
    "    ae_data_with_params = False # whether to feed in the parameters to the AE along with the data?\n",
    "\n",
    "    normalization_constant_arr_aedata = None\n",
    "    if normalizeforae_flag == True:\n",
    "        normalization_constant_arr_aedata = np.empty(shape=(2, numpoints_xgrid), dtype=FTYPE)\n",
    "        if normalization_type == 'stddev':\n",
    "            for i in range(numpoints_xgrid):\n",
    "                sample_mean = np.mean(all_data[:, i])\n",
    "                sample_std = np.std(all_data[:, i])\n",
    "                normalization_constant_arr_aedata[0, i] = sample_mean\n",
    "                normalization_constant_arr_aedata[1, i] = stddev_multiplier*sample_std\n",
    "        elif normalization_type == 'minmax':\n",
    "            for i in range(numpoints_xgrid):\n",
    "                sample_min = np.min(all_data[:, i])\n",
    "                sample_max = np.max(all_data[:, i])\n",
    "                if sample_max - sample_min == 0:\n",
    "                    sample_min = sample_min - 0.5\n",
    "                    sample_max = sample_min + 1\n",
    "                normalization_constant_arr_aedata[0, i] = sample_min\n",
    "                normalization_constant_arr_aedata[1, i] = sample_max - sample_min\n",
    "        for i in range(numpoints_xgrid):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "            \n",
    "    # saving sim data\n",
    "    ae_data = {\n",
    "        'data_dir_idx':data_dir_idx,\n",
    "        'normalizeforae_flag':normalizeforae_flag,\n",
    "        'normalization_constant_arr_aedata':normalization_constant_arr_aedata,\n",
    "        'normalization_type':normalization_type,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'ae_data_with_params':ae_data_with_params,\n",
    "        'module':Autoencoder.__module__,\n",
    "    }\n",
    "    with open(dir_name_ae+dir_sep+'ae_data.txt', 'w') as f:\n",
    "        f.write(str(ae_data))\n",
    "    np.savez(\n",
    "        dir_name_ae+dir_sep+'normalization_data',\n",
    "        normalization_constant_arr_aedata=[normalization_constant_arr_aedata],\n",
    "    )\n",
    "else:\n",
    "    if normalizeforae_flag == True:\n",
    "        for i in range(numpoints_xgrid):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "            \n",
    "time_stddev = np.std(all_data, axis=0)\n",
    "if ae_data_with_params == False:\n",
    "    all_data = all_data[:, 0:numpoints_xgrid]\n",
    "    time_stddev = time_stddev[0:numpoints_xgrid]\n",
    "else:\n",
    "    time_stddev[numpoints_xgrid:] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1020,
     "status": "ok",
     "timestamp": 1666788672340,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "uDhfYHU45IS8",
    "outputId": "982f534f-255c-41b9-f40a-327730ac89ae"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    all_data,\n",
    "    xgrid,\n",
    "    xticks_snapto=xticks_snapto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1666788672341,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "59kkrSP1GvzO"
   },
   "outputs": [],
   "source": [
    "num_tsteps_to_take = 100\n",
    "\n",
    "begin_idx = 0\n",
    "for i in range(boundary_idx_arr.shape[0]):\n",
    "    end_idx = boundary_idx_arr[i]\n",
    "    num_samples = end_idx - begin_idx\n",
    "    num_samples_new = int(num_samples) // num_tsteps_to_take\n",
    "    \n",
    "    end_idx_new = num_samples_new * num_tsteps_to_take\n",
    "    \n",
    "    difference_idx = end_idx - end_idx_new\n",
    "    temp_idx = begin_idx+end_idx_new\n",
    "    all_data[temp_idx:boundary_idx_arr[-1]-difference_idx] = all_data[begin_idx+end_idx:]\n",
    "    \n",
    "    boundary_idx_arr[i:] -= difference_idx\n",
    "    all_data = all_data[0:boundary_idx_arr[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1666788672342,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_data = np.reshape(\n",
    "    all_data,\n",
    "    newshape=(\n",
    "        int(all_data.shape[0]/num_tsteps_to_take),\n",
    "        num_tsteps_to_take,\n",
    "        all_data.shape[1]\n",
    "    ),\n",
    "    order='C'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_og = all_data\n",
    "all_data = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_idx_arr_og = boundary_idx_arr\n",
    "boundary_idx_arr = boundary_idx_arr // num_tsteps_to_take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1666788672765,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "c5cjQ1lnjcwt"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [1e-3, 1e-4, 1e-5]\n",
    "    epochs = 1000\n",
    "    patience = 25  # parameter for early stopping\n",
    "    min_delta = 1e-6  # parameter for early stopping\n",
    "    lambda_reg = 5e-7 # weight for regularizer\n",
    "    train_split = 0.8\n",
    "    val_split = 0.1\n",
    "    test_split = 1 - train_split - val_split\n",
    "    batch_size = 32\n",
    "    fRMS = 2/100\n",
    "    timeMeanofSpaceRMS = np.mean(np.mean(all_data**2, axis=1)**0.5)\n",
    "    \n",
    "    # stddev = fRMS*timeMeanofSpaceRMS\n",
    "    stddev = fRMS * np.mean(time_stddev[0:numpoints_xgrid])\n",
    "    dropout_rate = 0.0\n",
    "    cov_lmda = 1e-3\n",
    "\n",
    "\n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'timeMeanofSpaceRMS':timeMeanofSpaceRMS,\n",
    "        'stddev':stddev,\n",
    "        'dropout_rate':dropout_rate,\n",
    "        'cov_lmda':cov_lmda,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_ae+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "else:\n",
    "    with open(dir_name_ae + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "    try:\n",
    "        stddev = tparams_dict['stddev']\n",
    "    except:\n",
    "        print(\"'stddev' not in tparams_dict, set to 0\")\n",
    "        stddev = 0.0\n",
    "    try:\n",
    "        cov_lmda = tparams_dict['cov_lmda']\n",
    "    except:\n",
    "        print(\"'cov_lmda' not in tparams_dict, set to 0\")\n",
    "        cov_lmda = 0.0\n",
    "    try:\n",
    "        dropout_rate = tparams_dict['dropout_rate']\n",
    "    except:\n",
    "        print(\"'dropout_rate' not in tparams_dict, set to 0\")\n",
    "        dropout_rate = 0.0\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # setting seed for PRNGs\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1666788672769,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "lovTI3zuhlX0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1666788672770,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "IjsRi02g5ORG"
   },
   "outputs": [],
   "source": [
    "# # setting up data\n",
    "# idx = np.arange(all_data.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# boundary = int(np.round((1-test_split)*all_data.shape[0]))\n",
    "# training_data = all_data[idx[0:boundary], :]\n",
    "# testing_data = all_data[idx[boundary:], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1666788672771,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "Qwietg7eTG-s"
   },
   "outputs": [],
   "source": [
    "cum_samples = boundary_idx_arr[-1]\n",
    "# idx = np.arange(cum_samples)\n",
    "# np.random.shuffle(idx)\n",
    "num_train = 0\n",
    "num_val = 0\n",
    "begin_idx = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    num_samples = boundary_idx_arr[i] - begin_idx\n",
    "    num_train += int( (1-test_split-val_split)*num_samples )\n",
    "    num_val += int(val_split*num_samples)\n",
    "    begin_idx = boundary_idx_arr[i]\n",
    "\n",
    "# defining shapes\n",
    "training_shape = [num_train]\n",
    "training_shape.extend(all_data.shape[1:])\n",
    "\n",
    "val_shape = [num_val]\n",
    "val_shape.extend(all_data.shape[1:])\n",
    "\n",
    "testing_shape = [cum_samples-num_train-num_val]\n",
    "testing_shape.extend(all_data.shape[1:])\n",
    "\n",
    "# defining required arrays\n",
    "training_data = np.empty(shape=training_shape)\n",
    "\n",
    "val_data = np.empty(shape=val_shape)\n",
    "\n",
    "testing_data = np.empty(shape=testing_shape)\n",
    "\n",
    "begin_idx = 0\n",
    "training_data_rolling_count = 0\n",
    "val_data_rolling_count = 0\n",
    "testing_data_rolling_count = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    idx = np.arange(begin_idx, boundary_idx_arr[i])\n",
    "    # np.random.shuffle(idx)\n",
    "    num_samples = idx.shape[0]\n",
    "    num_train = int( (1-test_split-val_split)*num_samples )\n",
    "    num_val = int(val_split*num_samples)\n",
    "\n",
    "    training_data[training_data_rolling_count:training_data_rolling_count+num_train] = all_data[idx[0:num_train]]\n",
    "    training_data_rolling_count += num_train\n",
    "\n",
    "    val_data[val_data_rolling_count:val_data_rolling_count+num_val] = all_data[idx[num_train:num_train+num_val]]\n",
    "    val_data_rolling_count += num_val\n",
    "\n",
    "    num_test = num_samples-num_train-num_val+1\n",
    "    testing_data[testing_data_rolling_count:testing_data_rolling_count+num_test] = all_data[idx[num_train+num_val:]]\n",
    "    testing_data_rolling_count += num_test\n",
    "\n",
    "    begin_idx = boundary_idx_arr[i]\n",
    "\n",
    "# further shuffling\n",
    "# idx = np.arange(0, training_data.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# training_data_rnn_input = training_data_rnn_input[idx]\n",
    "np.random.shuffle(training_data)\n",
    "\n",
    "# idx = np.arange(0, val_data.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# val_data = val_data[idx]\n",
    "np.random.shuffle(val_data)\n",
    "\n",
    "# idx = np.arange(0, testing_data.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# testing_data = testing_data[idx]\n",
    "np.random.shuffle(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1666788672772,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "gJ-28EnzJ4Ur"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1666788672773,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "7xTsmS7lgpps"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1626,
     "status": "ok",
     "timestamp": 1666788674381,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "7l5kI1tfMszJ"
   },
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    latent_space_dim = 16\n",
    "    enc_layers = [128, 96, 64, 48, 32, 24, 26, 8, 4]\n",
    "    # enc_layers = [192, 96, 48, 24, 12, 6, 3]\n",
    "    # enc_layers = [96, 48, 24, 12, 6, 3]\n",
    "    # enc_layers = [128, 64, 32, 16, 8, 4]\n",
    "    # enc_layers = []\n",
    "#     enc_layers = [32, 16, 8, 4]\n",
    "    for i in range(len(enc_layers)):\n",
    "        if latent_space_dim < enc_layers[-i-1]:\n",
    "            enc_layers = enc_layers[0:-i]\n",
    "            break\n",
    "        if i == len(enc_layers)-1:\n",
    "            enc_layers = []\n",
    "    dec_layers = enc_layers[::-1]\n",
    "    \n",
    "    enc_layer_act_func = 'elu'\n",
    "    enc_final_layer_act_func = 'tanh'\n",
    "    dec_layer_act_func = 'elu'\n",
    "    dec_final_layer_act_func = 'tanh'\n",
    "    reg_name = 'L2'\n",
    "    use_weights_post_dense = False\n",
    "    \n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            ae_net = Autoencoder(\n",
    "                data_dim=all_data.shape[-1],\n",
    "                enc_layers=enc_layers,\n",
    "                dec_layers=dec_layers,\n",
    "                latent_space_dim=latent_space_dim,\n",
    "                lambda_reg=lambda_reg,\n",
    "                reg_name=reg_name,\n",
    "                enc_layer_act_func=enc_layer_act_func,\n",
    "                enc_final_layer_act_func=enc_final_layer_act_func,\n",
    "                dec_layer_act_func=dec_layer_act_func,\n",
    "                dec_final_layer_act_func=dec_final_layer_act_func,\n",
    "                load_file=None,\n",
    "                stddev=stddev,\n",
    "                dropout_rate=dropout_rate,\n",
    "                use_weights_post_dense=use_weights_post_dense,)\n",
    "    else:\n",
    "        ae_net = Autoencoder(\n",
    "            data_dim=all_data.shape[-1],\n",
    "            enc_layers=enc_layers,\n",
    "            dec_layers=dec_layers,\n",
    "            latent_space_dim=latent_space_dim,\n",
    "            lambda_reg=lambda_reg,\n",
    "            reg_name=reg_name,\n",
    "            enc_layer_act_func=enc_layer_act_func,\n",
    "            enc_final_layer_act_func=enc_final_layer_act_func,\n",
    "            dec_layer_act_func=dec_layer_act_func,\n",
    "            dec_final_layer_act_func=dec_final_layer_act_func,\n",
    "            load_file=None,\n",
    "            stddev=stddev,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_weights_post_dense=use_weights_post_dense,)\n",
    "    # saving the AE configuration\n",
    "    save_path = dir_name_ae+dir_sep+'final_net'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    ae_net.save_class_dict(save_path+dir_sep+'final_net_class_dict.txt')\n",
    "else:\n",
    "    load_file = dir_name_ae + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            ae_net = Autoencoder(data_dim=all_data.shape[-1], load_file=load_file)\n",
    "    else:\n",
    "        ae_net = Autoencoder(data_dim=all_data.shape[-1], load_file=load_file)\n",
    "    \n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        wt_file = tf.train.latest_checkpoint(dir_name_ae+dir_sep+'checkpoints')\n",
    "        # ae_net.load_weights(wt_file)\n",
    "    elif behaviour == 'loadFinalNetAndPlot':\n",
    "        wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'\n",
    "        ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1666788674637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "48tkgZxT0Amt"
   },
   "outputs": [],
   "source": [
    "from keras.engine import data_adapter\n",
    "\n",
    "class AE_withTime(tf.keras.models.Model):\n",
    "    def __init__(self, ae_net, cov_lmda):\n",
    "        super().__init__()\n",
    "        self.ae_net = ae_net\n",
    "        self.cov_lmda = cov_lmda\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        outputs = tf.keras.layers.TimeDistributed(self.ae_net.ae_net)(inputs, training=training)\n",
    "        return outputs\n",
    "        \n",
    "    def train_step(self, data):\n",
    "\n",
    "        # x, y = data\n",
    "        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            encoded = tf.keras.layers.TimeDistributed(self.ae_net.encoder_net)(x, training=True)\n",
    "            decoded = tf.keras.layers.TimeDistributed(self.ae_net.decoder_net)(encoded, training=True)\n",
    "            \n",
    "            y_mean = tf.reduce_mean(y, axis=-2, keepdims=True)\n",
    "            y_hat = y - y_mean\n",
    "            y_cov = (1 / y.shape[1]) * tf.linalg.matmul(y_hat, y_hat, transpose_a=True)\n",
    "            \n",
    "            decoded_mean = tf.reduce_mean(decoded, axis=-2, keepdims=True)\n",
    "            decoded_hat = decoded - decoded_mean\n",
    "            decoded_cov = (1 / decoded.shape[1]) * tf.linalg.matmul(decoded_hat, decoded_hat, transpose_a=True)\n",
    "            \n",
    "            cov_norm_diff = tf.reduce_mean(tf.norm(\n",
    "                y_cov - decoded_cov,\n",
    "                axis=[-2, -1]\n",
    "            ))\n",
    "            \n",
    "            loss = self.compiled_loss(\n",
    "                y,\n",
    "                decoded,\n",
    "                sample_weight,\n",
    "                regularization_losses=self.losses\n",
    "            ) + self.cov_lmda*cov_norm_diff\n",
    "\n",
    "        self._validate_target_and_loss(decoded, loss)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        return self.compute_metrics(x, y, decoded, sample_weight, cov_norm_diff)\n",
    "    \n",
    "    def compute_metrics(self, x, y, decoded, sample_weight, cov_norm_diff=0.0):\n",
    "        metric_results = super().compute_metrics(x, y, decoded, sample_weight)\n",
    "        metric_results['cov_norm_diff'] = cov_norm_diff\n",
    "        return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1666788674956,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "yUChBAKqIFtX"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    val_loss_hist = []\n",
    "    train_loss_hist = []\n",
    "    lr_change=[0, 0]\n",
    "    savelosses_cb_vallossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    savelosses_cb_trainlossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    starting_lr_idx = 0\n",
    "    num_epochs_left = epochs\n",
    "elif behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt = readAndReturnLossHistories(\n",
    "        dir_name_ae=dir_name_ae,\n",
    "        dir_sep=dir_sep,\n",
    "        epochs=epochs,\n",
    "        learning_rate_list=learning_rate_list)\n",
    "    savelosses_cb_vallossarr = val_loss_arr_fromckpt\n",
    "    savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n",
    "elif behaviour == 'loadFinalNetAndPlot':\n",
    "    with open(dir_name_ae+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    losses_dict = eval(''.join(lines))\n",
    "\n",
    "    val_loss_hist = losses_dict['val_loss_hist']\n",
    "    train_loss_hist = losses_dict['train_loss_hist']\n",
    "    lr_change = losses_dict['lr_change']\n",
    "    test_loss = losses_dict['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MSE_hist = []\n",
    "val_MSE_hist = []\n",
    "\n",
    "train_NMSE_hist = []\n",
    "val_NMSE_hist = []\n",
    "\n",
    "train_cov_loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666788674957,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "Z0oEGp6WKGu2"
   },
   "outputs": [],
   "source": [
    "class NMSE(tf.keras.metrics.MeanSquaredError):\n",
    "    def __init__(self, divisor_arr, name='NMSE', **kwargs):\n",
    "        super(NMSE, self).__init__(name, **kwargs)\n",
    "        self.divisor_arr = divisor_arr\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = y_true / self.divisor_arr\n",
    "        y_pred = y_pred / self.divisor_arr\n",
    "        return super(NMSE, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191278,
     "status": "ok",
     "timestamp": 1666788866231,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "gELga1WnQeMK",
    "outputId": "e923a97a-2d9d-4c74-c328-4793de05b919",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compiling the network\n",
    "ae_net.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "#     loss=losses.BinaryCrossentropy(from_logits=False),\n",
    "    run_eagerly=False,\n",
    "    metrics=['mse', NMSE(divisor_arr=tf.constant(time_stddev))]\n",
    ")\n",
    "\n",
    "if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # this loads the weights/attributes of the optimizer as well\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            ae_net.load_weights(wt_file)\n",
    "    else:\n",
    "        ae_net.load_weights(wt_file)\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    ae_t_net = AE_withTime(ae_net, cov_lmda)\n",
    "    ae_t_net.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "        loss=losses.MeanSquaredError(),\n",
    "    #     loss=losses.BinaryCrossentropy(from_logits=False),\n",
    "        run_eagerly=False,\n",
    "        metrics=['mse', NMSE(divisor_arr=tf.constant(time_stddev))]\n",
    "    )\n",
    "    \n",
    "    # implementing early stopping\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_NMSE',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=True,\n",
    "        min_delta=min_delta\n",
    "    )\n",
    "\n",
    "    # time callback for each epoch\n",
    "    timekeeper_cb = mytimecallback()\n",
    "\n",
    "    # model checkpoint callback\n",
    "    dir_name_ckpt = dir_name_ae+dir_sep+'checkpoints'\n",
    "    if not os.path.isdir(dir_name_ckpt):\n",
    "        os.makedirs(dir_name_ckpt)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dir_name_ckpt+dir_sep+'checkpoint',#+'/checkpoint--loss={loss:.4f}--vall_loss={val_loss:.4f}',\n",
    "        monitor='val_NMSE',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=2,\n",
    "        period=1  # saves every 5 epochs\n",
    "    )\n",
    "\n",
    "    # save losses callback\n",
    "    savelosses_cb = SaveLosses(\n",
    "        filepath=dir_name_ckpt+dir_sep+'LossHistoriesCheckpoint',\n",
    "        val_loss_arr=savelosses_cb_vallossarr,\n",
    "        train_loss_arr=savelosses_cb_trainlossarr,\n",
    "        total_epochs=epochs,\n",
    "        period=1)\n",
    "\n",
    "    # training the network\n",
    "    for i in range(starting_lr_idx, len(learning_rate_list)):\n",
    "        learning_rate = learning_rate_list[i]\n",
    "        K.set_value(ae_net.optimizer.lr, learning_rate)\n",
    "\n",
    "        savelosses_cb.update_lr_idx(i)\n",
    "\n",
    "        if i == starting_lr_idx:\n",
    "            EPOCHS = num_epochs_left\n",
    "            savelosses_cb.update_offset(epochs-num_epochs_left)\n",
    "        else:\n",
    "            EPOCHS = epochs\n",
    "            savelosses_cb.update_offset(0)\n",
    "\n",
    "        total_s_len = 80\n",
    "        sep_lr_s = ' LEARNING RATE : {} '.format(learning_rate)\n",
    "        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n",
    "        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n",
    "        print('\\n\\n' + '-'*len(sep_lr_s))\n",
    "        print('\\n' + sep_lr_s+'\\n')\n",
    "        print('-'*len(sep_lr_s) + '\\n\\n')\n",
    "        \n",
    "        history = ae_t_net.fit(training_data, training_data,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=batch_size,\n",
    "#             validation_split=val_split/train_split,\n",
    "            validation_data=(val_data, val_data),\n",
    "            callbacks=[early_stopping_cb, timekeeper_cb, checkpoint_cb, savelosses_cb],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        val_loss_hist.extend(history.history['val_loss'])\n",
    "        train_loss_hist.extend(history.history['loss'])\n",
    "        \n",
    "        val_MSE_hist.extend(history.history['val_mse'])\n",
    "        train_MSE_hist.extend(history.history['mse'])\n",
    "        \n",
    "        val_NMSE_hist.extend(history.history['val_NMSE'])\n",
    "        train_NMSE_hist.extend(history.history['NMSE'])\n",
    "        \n",
    "        train_cov_loss_hist.append(history.history['cov_norm_diff'])\n",
    "        \n",
    "        if i == starting_lr_idx:\n",
    "            lr_change[i+1] += len(history.history['val_loss'])\n",
    "        else:\n",
    "            lr_change.append(lr_change[i]+len(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9705,
     "status": "ok",
     "timestamp": 1666788875924,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "d_Od0ul4P9bK",
    "outputId": "860e9f94-e593-4a74-fcff-6a6657d925de"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    test_metrics = ae_t_net.evaluate(\n",
    "        testing_data, testing_data,\n",
    "    )\n",
    "    train_metrics = ae_t_net.evaluate(training_data, training_data)\n",
    "    val_metrics = ae_t_net.evaluate(val_data, val_data)\n",
    "\n",
    "    save_path = dir_name_ae+dir_sep+'final_net'\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "\n",
    "    with open(save_path+dir_sep+'losses.txt', 'w') as f:\n",
    "        f.write(str({\n",
    "            'val_loss_hist':val_loss_hist,\n",
    "            'train_loss_hist':train_loss_hist,\n",
    "            'val_MSE_hist':val_MSE_hist,\n",
    "            'train_MSE_hist':train_MSE_hist,\n",
    "            'val_NMSE_hist':val_NMSE_hist,\n",
    "            'train_NMSE_hist':train_NMSE_hist,\n",
    "            'train_cov_loss_hist':train_cov_loss_hist,\n",
    "            'lr_change':lr_change,\n",
    "            'test_loss':test_metrics[0],\n",
    "            'test_mse':test_metrics[1],\n",
    "            'train_loss':train_metrics[0],\n",
    "            'train_mse':train_metrics[1],\n",
    "            'val_loss':val_metrics[0],\n",
    "            'val_mse':val_metrics[1],\n",
    "        }))\n",
    "\n",
    "    ae_net.save_everything(\n",
    "        file_name=save_path+dir_sep+'final_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666788875925,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "Dy8GNcgMVD4T",
    "outputId": "e50e8738-9da1-43de-e551-43f47b64135e"
   },
   "outputs": [],
   "source": [
    "print('lr_change : ', lr_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for lst in train_cov_loss_hist:\n",
    "    temp.extend(lst)\n",
    "train_cov_loss_hist_og = train_cov_loss_hist\n",
    "train_cov_loss_hist = np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1666788876686,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "ewTz1COFSocM",
    "outputId": "15bc2be5-d571-433e-b5cd-9c722f38b48b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting losses\n",
    "dir_name_plot = dir_name_ae + '/plots'\n",
    "if not os.path.isdir(dir_name_plot):\n",
    "    os.makedirs(dir_name_plot)\n",
    "\n",
    "# Visualize loss history\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_loss_hist,\n",
    "    val_loss=val_loss_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list\n",
    ")\n",
    "\n",
    "plt.savefig(dir_name_plot + '{ds}loss_history.png'.format(ds=dir_sep), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_MSE_hist,\n",
    "    val_loss=val_MSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training MSE', 'Validation MSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='MSE',\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/MSE_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_NMSE_hist,\n",
    "    val_loss=val_NMSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training NMSE', 'Validation NMSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='NMSE',\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/NMSE_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_cov_loss_hist,\n",
    "    val_loss=None,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=[r\"cov_norm_diff\"],\n",
    "    xlabel='Epoch',\n",
    "    ylabel=r\"cov_norm_diff\",\n",
    "    plot_type='plot',\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/train_cov_loss_hist.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1666788877562,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "wwt4brHcOaXi",
    "outputId": "7ba39105-aa8e-49e8-dbfd-619069123fdd"
   },
   "outputs": [],
   "source": [
    "reconstructed_data = ae_net.predict(all_data_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1kzRwBdNfA2o28NxHkc2fku7QnFPAI8Bo"
    },
    "executionInfo": {
     "elapsed": 4835,
     "status": "ok",
     "timestamp": 1666788882395,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "_6rNhThyQrKc",
    "outputId": "fdeeb3f5-009a-404e-8856-b7655d596a88"
   },
   "outputs": [],
   "source": [
    "plot_reconstructed_data_KS(\n",
    "    boundary_idx_arr_og,\n",
    "    dir_name_ae,\n",
    "    all_data_og,\n",
    "    reconstructed_data, delta_t, xgrid,\n",
    "    save_figs=False,\n",
    "    normalization_constant_arr=normalization_constant_arr_aedata,\n",
    "    xticks_snapto=xticks_snapto,\n",
    "    num_yticks=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17515,
     "status": "ok",
     "timestamp": 1666788899903,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "cAiMV0iU0EpK"
   },
   "outputs": [],
   "source": [
    "plot_reconstructed_data_KS(\n",
    "    boundary_idx_arr_og,\n",
    "    dir_name_ae,\n",
    "    all_data_og,\n",
    "    reconstructed_data, delta_t, xgrid,\n",
    "    save_figs=True,\n",
    "    normalization_constant_arr=normalization_constant_arr_aedata,\n",
    "    xticks_snapto=xticks_snapto,\n",
    "    num_yticks=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_to_plot = 20\n",
    "\n",
    "\n",
    "\n",
    "plot_reconstructed_data_KS(\n",
    "    boundary_idx_arr_og,\n",
    "    dir_name_ae,\n",
    "    all_data_og,\n",
    "    reconstructed_data, delta_t, xgrid,\n",
    "    save_figs=True,\n",
    "    normalization_constant_arr=normalization_constant_arr_aedata,\n",
    "    xticks_snapto=xticks_snapto,\n",
    "    num_yticks=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1666788900494,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "jVqsAwsY0Amw",
    "outputId": "95dd7bc9-f2b0-421e-a1b4-3e7e594cd44a"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "latent_states_all = ae_net.encoder_net.predict(all_data_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2127,
     "status": "ok",
     "timestamp": 1666788902619,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "wjgPNitSrt5p",
    "outputId": "d60c9340-28f4-479b-8b66-5ee2a5fc5cee"
   },
   "outputs": [],
   "source": [
    "plot_latent_states_KS(\n",
    "    boundary_idx_arr_og,\n",
    "    latent_states_all,\n",
    "    delta_t,\n",
    "    dir_name_ae,\n",
    "    xticks_snapto=xticks_snapto,\n",
    "    num_yticks=16,\n",
    "    save_figs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5242,
     "status": "ok",
     "timestamp": 1666788907858,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "Jv8PgBgzV1_s"
   },
   "outputs": [],
   "source": [
    "plot_latent_states_KS(\n",
    "    boundary_idx_arr_og,\n",
    "    latent_states_all,\n",
    "    delta_t,\n",
    "    dir_name_ae,\n",
    "    xticks_snapto=xticks_snapto,\n",
    "    num_yticks=11,\n",
    "    save_figs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1666788907859,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -120
    },
    "id": "wnLnqg0Jrt5t"
   },
   "outputs": [],
   "source": [
    "# ae_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(boundary_idx_arr_og)\n",
    "num_digits_n = int(np.log10(n)+1)\n",
    "\n",
    "# '''\n",
    "num_cols = 1\n",
    "num_rows = latent_states_all.shape[-1]\n",
    "factor = 1\n",
    "num_modes = xgrid.shape[0]\n",
    "\n",
    "prev_idx = 0\n",
    "for i in range(n):\n",
    "    lyap_time = lyapunov_time_arr[i]\n",
    "    start_time = 0\n",
    "    plot_time = 20*lyap_time\n",
    "    # next_idx = boundary_idx_arr_og[i]\n",
    "    prev_idx += int((start_time + 0.5*delta_t)//delta_t) \n",
    "    next_idx = prev_idx + int((plot_time+0.5*delta_t)//delta_t)\n",
    "    fig, ax = plt.subplots(num_rows, num_cols, sharex=True, figsize=(factor*7.5*num_cols, factor*5.0*num_rows))\n",
    "    N = next_idx - prev_idx\n",
    "    # print(N)\n",
    "    input_time = (start_time + np.arange(0, N)*delta_t)/lyap_time\n",
    "    \n",
    "    # temp = N*delta_t\n",
    "    # chosen_interval = temp//10\n",
    "    # sn = np.format_float_scientific(chosen_interval, exp_digits=4)\n",
    "    # # print(temp, sn)\n",
    "    # snap = 25*np.round(10*float(sn[0:-6])/25)/10\n",
    "    # if snap == 0:\n",
    "    #     snap = 0.1\n",
    "    # snap *= float('1'+sn[-6:])\n",
    "    # xticks_snapto = int(snap*np.round(chosen_interval/snap))\n",
    "\n",
    "    for j in range(latent_states_all.shape[-1]):\n",
    "        ax[j].plot(input_time, latent_states_all[prev_idx:next_idx, j])\n",
    "        # ax[j].plot(input_time, D_reconstructed_list[i][prev_idx:next_idx], color='C0', label='Reconstructed Data')\n",
    "        # num_xticks = 1 + int((N*delta_t + 0.5*xticks_snapto) // xticks_snapto)\n",
    "        # xticks = np.linspace(0, N, num_xticks, dtype=np.int32)\n",
    "        # xticks = int((start_time + 0.5*delta_t)//delta_t) + np.arange(0, N+1, int((xticks_snapto+0.5*delta_t)//delta_t))\n",
    "        # print(xticks)\n",
    "        # ax[j].set_xticks(ticks=np.round(xticks*delta_t, 1))\n",
    "        # ax[j].set_xticklabels('')\n",
    "        ax[j].tick_params(axis='x', rotation=270+45)\n",
    "        ax[j].set_ylabel('latent state {}'.format(j+1))\n",
    "        ax[j].set_xlabel(r'Time$^+$')\n",
    "        ax[j].grid(True)\n",
    "\n",
    "    # yticks = np.linspace(0, 1, 10+1)*(len(xgrid)-1)\n",
    "    # yticklabels = np.round(np.linspace(0, 1, yticks.shape[0])*xgrid[-1], 2)\n",
    "    # ax.set_yticks(ticks=yticks)\n",
    "    # ax.set_yticklabels(yticklabels)\n",
    "\n",
    "    # ax[-1].set_xlabel(r'Time$^+$')\n",
    "    # ax[0].legend()\n",
    "    # ax.title.set_text(r'Latent States')\n",
    "    # if ax_titles_list is not None:\n",
    "    #     ax.title.set_text(ax_titles_list[i])\n",
    "    # else:\n",
    "    ax[0].title.set_text(r'Case '+str(i+1))\n",
    "\n",
    "    fig.savefig(dir_name_plot+'/LatentSpace-Case_'+str(i+1).zfill(num_digits_n)+'.png', dpi=300, bbox_inches='tight')\n",
    "    fig.clear()\n",
    "    plt.close()\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    prev_idx = boundary_idx_arr_og[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KE and Dissipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = dir_name_ae+dir_sep+'plots'\n",
    "# save_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time evolution of Kinetic Energy\n",
    "KE_org_list = []\n",
    "KE_reconstructed_list = []\n",
    "KE_nrmse = []\n",
    "\n",
    "if normalizeforae_flag == True:\n",
    "    rescaled_org_data = all_data_og[:, 0:xgrid.shape[0]]#.copy()\n",
    "    rescaled_recon_data = reconstructed_data[:, 0:xgrid.shape[0]]#.copy()\n",
    "    for i in range(xgrid.shape[0]):\n",
    "        rescaled_org_data[:, i] *= normalization_constant_arr_aedata[1, i]\n",
    "        rescaled_org_data[:, i] += normalization_constant_arr_aedata[0, i]\n",
    "        rescaled_recon_data[:, i] *= normalization_constant_arr_aedata[1, i]\n",
    "        rescaled_recon_data[:, i] += normalization_constant_arr_aedata[0, i]\n",
    "else:\n",
    "    rescaled_org_data = all_data_og[:, 0:xgrid.shape[0]]\n",
    "    rescaled_recon_data = reconstructed_data[:, 0:xgrid.shape[0]]\n",
    "\n",
    "if normalize_flag_ogdata == True:\n",
    "    for i in range(xgrid.shape[0]):\n",
    "        rescaled_org_data[:, i] *= normalization_constant_arr_ogdata[1, i]\n",
    "        rescaled_org_data[:, i] += normalization_constant_arr_ogdata[0, i]\n",
    "        rescaled_recon_data[:, i] *= normalization_constant_arr_ogdata[1, i]\n",
    "        rescaled_recon_data[:, i] += normalization_constant_arr_ogdata[0, i]\n",
    "\n",
    "begin_idx = 0\n",
    "for i in range(len(boundary_idx_arr_og)):\n",
    "    KE_org = np.sum(rescaled_org_data[begin_idx:boundary_idx_arr_og[i], 1:-1]**2, axis=1)\n",
    "    KE_org += 0.5*(rescaled_org_data[begin_idx:boundary_idx_arr_og[i], 0]**2)\n",
    "    KE_org += 0.5*(rescaled_org_data[begin_idx:boundary_idx_arr_og[i], -1]**2)\n",
    "    KE_org /= xgrid.shape[0]-1\n",
    "    KE_org_list.append(KE_org)\n",
    "    \n",
    "    KE_recon = np.sum(rescaled_recon_data[begin_idx:boundary_idx_arr_og[i], 1:-1]**2, axis=1)\n",
    "    KE_recon += 0.5*(rescaled_recon_data[begin_idx:boundary_idx_arr_og[i], 0]**2)\n",
    "    KE_recon += 0.5*(rescaled_recon_data[begin_idx:boundary_idx_arr_og[i], -1]**2)\n",
    "    KE_recon /= xgrid.shape[0]-1\n",
    "    KE_reconstructed_list.append(KE_recon)\n",
    "\n",
    "    KE_rmse = np.mean((KE_recon - KE_org)**2)**0.5\n",
    "    KE_mean = np.mean(KE_org)\n",
    "    KE_rmse_normalized = KE_rmse/KE_mean\n",
    "    KE_nrmse.append(KE_rmse_normalized)\n",
    "    \n",
    "    print('Case {} - KE_nrmse : {}'.format(i+1, KE_rmse_normalized))\n",
    "\n",
    "    begin_idx = boundary_idx_arr_og[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(boundary_idx_arr_og)\n",
    "num_digits_n = int(np.log10(n)+1)\n",
    "\n",
    "# '''\n",
    "num_cols = 1\n",
    "num_rows = 1\n",
    "factor = 1\n",
    "\n",
    "num_modes = xgrid.shape[0]\n",
    "\n",
    "prev_idx = 0\n",
    "for i in range(n):\n",
    "    lyap_time = lyapunov_time_arr[i]\n",
    "    print(lyap_time)\n",
    "    start_time = 0\n",
    "    plot_time = 20*lyap_time\n",
    "    # next_idx = boundary_idx_arr_og[i]\n",
    "    prev_idx += int((start_time + 0.5*delta_t)//delta_t) \n",
    "    next_idx = prev_idx + int((plot_time+0.5*delta_t)//delta_t)\n",
    "    fig, ax = plt.subplots(figsize=(factor*7.5*num_cols, factor*5.0*num_rows))\n",
    "    N = next_idx - prev_idx\n",
    "    # print(N)\n",
    "    input_time = (start_time + np.arange(0, N)*delta_t)/lyap_time\n",
    "\n",
    "    # temp = N*delta_t\n",
    "    # chosen_interval = temp//10\n",
    "    # sn = np.format_float_scientific(chosen_interval, exp_digits=4)\n",
    "    # # print(temp, sn)\n",
    "    # snap = 25*np.round(10*float(sn[0:-6])/25)/10\n",
    "    # if snap == 0:\n",
    "    #     snap = 0.1\n",
    "    # snap *= float('1'+sn[-6:])\n",
    "    # xticks_snapto = int(snap*np.round(chosen_interval/snap))\n",
    "\n",
    "    # prev_idx = boundary_idx_arr_og[i] - (next_idx-prev_idx)\n",
    "    # next_idx = boundary_idx_arr_og[i]\n",
    "    \n",
    "    ax.plot(input_time, KE_org_list[i][prev_idx:next_idx], color='k', label='Original Data')\n",
    "    ax.plot(input_time, KE_reconstructed_list[i][prev_idx:next_idx], color='C0', label='Reconstructed Data')\n",
    "    # num_xticks = 1 + int((N*delta_t + 0.5*xticks_snapto) // xticks_snapto)\n",
    "    # xticks = np.linspace(0, N, num_xticks, dtype=np.int32)\n",
    "    # xticks = int((start_time + 0.5*delta_t)//delta_t) + np.arange(0, N+1, int((xticks_snapto+0.5*delta_t)//delta_t))\n",
    "    # print(xticks)\n",
    "    # ax.set_xticks(ticks=np.round(xticks*delta_t, 1))\n",
    "    # ax.set_xticklabels(np.round(xticks*delta_t, 1))\n",
    "    ax.tick_params(axis='x', rotation=270+45)\n",
    "\n",
    "    # yticks = np.linspace(0, 1, 10+1)*(len(xgrid)-1)\n",
    "    # yticklabels = np.round(np.linspace(0, 1, yticks.shape[0])*xgrid[-1], 2)\n",
    "    # ax.set_yticks(ticks=yticks)\n",
    "    # ax.set_yticklabels(yticklabels)\n",
    "\n",
    "    ax.set_xlabel(r'Time$^+$')\n",
    "    ax.set_ylabel(r'$KE$')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    # ax.title.set_text(r'Latent States')\n",
    "    # if ax_titles_list is not None:\n",
    "    # ax.title.set_text(ax_titles_list[i])\n",
    "    # else:\n",
    "    ax.title.set_text(r'Case '+str(i+1))\n",
    "\n",
    "    text_xy = [0.05, 0.05]\n",
    "    ax.text(\n",
    "        text_xy[0],\n",
    "        text_xy[1],\n",
    "        'latent space dimensions : {}'.format(\n",
    "            latent_states_all.shape[1],\n",
    "        ),\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round\",\n",
    "            ec=(0.6, 0.6, 1),\n",
    "            fc=(0.9, 0.9, 1),\n",
    "            alpha=0.8\n",
    "        ),\n",
    "        # bbox=dict(facecolor='C0', alpha=0.5, boxstyle='round,pad=0.2'),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='bottom'\n",
    "    )\n",
    "    \n",
    "    # plt.colorbar(im)\n",
    "    if save_dir is not None:\n",
    "        fig.savefig(save_dir+dir_sep+'KE-Case_'+str(i+1).zfill(num_digits_n)+'.png', dpi=300, bbox_inches='tight')\n",
    "        fig.clear()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        print('')\n",
    "\n",
    "    prev_idx = next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplace_operator(\n",
    "        data,\n",
    "        xgrid\n",
    "    ):\n",
    "    from scipy.fft import fft, ifft, fftfreq\n",
    "    M = xgrid.shape[0]\n",
    "    length = xgrid[-1]\n",
    "    k = fftfreq(M) * M * 2*np.pi/length\n",
    "    laplace_operated_data = data[:, 0:M].copy()\n",
    "    for i in range(data.shape[0]):\n",
    "        v = fft(data[i, 0:M])\n",
    "        v = k*k*v\n",
    "        laplace_operated_data[i, :] = np.real(ifft(v))\n",
    "    return laplace_operated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time evolution of Dissipation\n",
    "D_org_list = []\n",
    "D_reconstructed_list = []\n",
    "D_nrmse = []\n",
    "\n",
    "begin_idx = 0\n",
    "for i in range(len(boundary_idx_arr_og)):\n",
    "    temp = compute_laplace_operator(rescaled_org_data[begin_idx:boundary_idx_arr_og[i]], xgrid)\n",
    "    D_org = np.sum(temp[:, 1:-1]**2, axis=1)\n",
    "    D_org += 0.5*(temp[:, 0]**2)\n",
    "    D_org += 0.5*(temp[:, -1]**2)\n",
    "    D_org /= xgrid.shape[0]-1\n",
    "    D_org_list.append(D_org)\n",
    "    \n",
    "    temp = compute_laplace_operator(rescaled_recon_data[begin_idx:boundary_idx_arr_og[i]], xgrid)\n",
    "    D_recon = np.sum(temp[:, 1:-1]**2, axis=1)\n",
    "    D_recon += 0.5*(temp[:, 0]**2)\n",
    "    D_recon += 0.5*(temp[:, -1]**2)\n",
    "    D_recon /= xgrid.shape[0]-1\n",
    "    D_reconstructed_list.append(D_recon)\n",
    "    \n",
    "    D_rmse = np.mean((D_recon - D_org)**2)**0.5\n",
    "    D_mean = np.mean(D_org)\n",
    "    D_rmse_normalized = D_rmse/D_mean\n",
    "    D_nrmse.append(D_rmse_normalized)\n",
    "    \n",
    "    print('Case {} - D_nrmse : {}'.format(i+1, D_rmse_normalized))\n",
    "    \n",
    "    begin_idx = boundary_idx_arr_og[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(boundary_idx_arr_og)\n",
    "num_digits_n = int(np.log10(n)+1)\n",
    "\n",
    "# '''\n",
    "num_cols = 1\n",
    "num_rows = 1\n",
    "factor = 1\n",
    "num_modes = xgrid.shape[0]\n",
    "\n",
    "prev_idx = 0\n",
    "for i in range(n):\n",
    "    lyap_time = lyapunov_time_arr[i]\n",
    "    start_time = 0\n",
    "    plot_time = 20*lyap_time\n",
    "    # next_idx = boundary_idx_arr_og[i]\n",
    "    prev_idx += int((start_time + 0.5*delta_t)//delta_t) \n",
    "    next_idx = prev_idx + int((plot_time+0.5*delta_t)//delta_t)\n",
    "    fig, ax = plt.subplots(figsize=(factor*7.5*num_cols, factor*5.0*num_rows))\n",
    "    N = next_idx - prev_idx\n",
    "    # print(N)\n",
    "    input_time = (start_time + np.arange(0, N)*delta_t)/lyap_time\n",
    "    \n",
    "    # temp = N*delta_t\n",
    "    # chosen_interval = temp//10\n",
    "    # sn = np.format_float_scientific(chosen_interval, exp_digits=4)\n",
    "    # # print(temp, sn)\n",
    "    # snap = 25*np.round(10*float(sn[0:-6])/25)/10\n",
    "    # if snap == 0:\n",
    "    #     snap = 0.1\n",
    "    # snap *= float('1'+sn[-6:])\n",
    "    # xticks_snapto = int(snap*np.round(chosen_interval/snap))\n",
    "\n",
    "    ax.plot(input_time, D_org_list[i][prev_idx:next_idx], color='k', label='Original Data')\n",
    "    ax.plot(input_time, D_reconstructed_list[i][prev_idx:next_idx], color='C0', label='Reconstructed Data')\n",
    "    # num_xticks = 1 + int((N*delta_t + 0.5*xticks_snapto) // xticks_snapto)\n",
    "    # xticks = np.linspace(0, N, num_xticks, dtype=np.int32)\n",
    "    # xticks = int((start_time + 0.5*delta_t)//delta_t) + np.arange(0, N+1, int((xticks_snapto+0.5*delta_t)//delta_t))\n",
    "    # print(xticks)\n",
    "    # ax.set_xticks(ticks=np.round(xticks*delta_t, 1))\n",
    "    # ax.set_xticklabels(np.round(xticks*delta_t, 1))\n",
    "    ax.tick_params(axis='x', rotation=270+45)\n",
    "\n",
    "    # yticks = np.linspace(0, 1, 10+1)*(len(xgrid)-1)\n",
    "    # yticklabels = np.round(np.linspace(0, 1, yticks.shape[0])*xgrid[-1], 2)\n",
    "    # ax.set_yticks(ticks=yticks)\n",
    "    # ax.set_yticklabels(yticklabels)\n",
    "\n",
    "    ax.set_xlabel(r'Time$^+$')\n",
    "    ax.set_ylabel(r'$D$')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    # ax.title.set_text(r'Latent States')\n",
    "    # if ax_titles_list is not None:\n",
    "    #     ax.title.set_text(ax_titles_list[i])\n",
    "    # else:\n",
    "    ax.title.set_text(r'Case '+str(i+1))\n",
    "\n",
    "    text_xy = [0.05, 0.05]\n",
    "    ax.text(\n",
    "        text_xy[0],\n",
    "        text_xy[1],\n",
    "        'latent space dimensions : {}'.format(\n",
    "            latent_states_all.shape[1],\n",
    "        ),\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round\",\n",
    "            ec=(0.6, 0.6, 1),\n",
    "            fc=(0.9, 0.9, 1),\n",
    "            alpha=0.8\n",
    "        ),\n",
    "        # bbox=dict(facecolor='C0', alpha=0.5, boxstyle='round,pad=0.2'),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='bottom'\n",
    "    )\n",
    "\n",
    "    if save_dir is not None:\n",
    "        fig.savefig(save_dir+'/Dissipation-Case_'+str(i+1).zfill(num_digits_n)+'.png', dpi=300, bbox_inches='tight')\n",
    "        fig.clear()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        print('')\n",
    "\n",
    "    prev_idx = next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    dir_name_ae+dir_sep+'KE_D_data',\n",
    "    KE_org_list=KE_org_list,\n",
    "    KE_reconstructed_list=KE_reconstructed_list,\n",
    "    KE_nrmse=KE_nrmse,\n",
    "    D_org_list=D_org_list,\n",
    "    D_reconstructed_list=D_reconstructed_list,\n",
    "    D_nrmse=D_nrmse,\n",
    "    num_latent_states=[latent_states_all.shape[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# a = 1 #- np.exp(-2)\n",
    "\n",
    "# modified_relu = lambda x : tf.keras.activations.relu(x+a).numpy()-a\n",
    "# elu = lambda x : tf.keras.activations.elu(x)\n",
    "\n",
    "# y1 = modified_relu(x)\n",
    "# y2 = elu(x)\n",
    "\n",
    "# plt.plot(x, y1, label='modified_relu')\n",
    "# plt.plot(x, y2, label='elu')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_net.decoder_layers_list[-1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
