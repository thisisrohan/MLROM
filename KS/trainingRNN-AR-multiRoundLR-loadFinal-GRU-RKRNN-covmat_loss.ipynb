{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from keras.engine import data_adapter\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data_KS, plot_latent_states_KS , readAndReturnLossHistories, sigmoidWarmupAndDecayLRSchedule\n",
    "from tools.ae_v5 import Autoencoder\n",
    "from tools.GRU_AR_v6 import AR_RNN_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpoin?tAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 0\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print(tf.config.list_logical_devices())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_rnn/rnn_038'\n",
    "\n",
    "    # making AR-RNN save directory\n",
    "    dir_name_ARrnn = os.getcwd() + dir_sep + 'saved_AR_rnn'\n",
    "    if not os.path.isdir(dir_name_ARrnn):\n",
    "        os.makedirs(dir_name_ARrnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'AR_rnn_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_ARrnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_ARrnn = dir_name_ARrnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_ARrnn)\n",
    "    os.makedirs(dir_name_ARrnn+dir_sep+'plots')\n",
    "    \n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in rnn_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "    try:\n",
    "        use_ae_data = params_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "    try:\n",
    "        dense_layer_act_func = params_rnn_dict['dense_layer_act_func']\n",
    "    except:\n",
    "        print(\"'dense_layer_act_func' not present in RNN_specific_data, set to 'linear'.\")\n",
    "        dense_layer_act_func = 'linear'\n",
    "    try:\n",
    "        stateful = params_rnn_dict['stateful']\n",
    "    except:\n",
    "        print(\"'stateful' not present in RNN_specific_data, set to True.\")\n",
    "        stateful = True\n",
    "    try:\n",
    "        use_learnable_state = params_rnn_dict['use_learnable_state']\n",
    "    except:\n",
    "        print(\"'use_learnable_state' not present in RNN_specific_data, set to False.\")\n",
    "        use_learnable_state = False\n",
    "    try:\n",
    "        use_weights_post_dense = params_rnn_dict['use_weights_post_dense']\n",
    "    except:\n",
    "        print(\"'use_weights_post_dense' not present in RNN_specific_data, set to False.\")\n",
    "        use_weights_post_dense = False\n",
    "        \n",
    "    \n",
    "    # training params\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "    try:\n",
    "        fRMS = tparams_dict['fRMS']\n",
    "    except:\n",
    "        fRMS = 0.0\n",
    "\n",
    "    loss_weights = 0.98\n",
    "else:\n",
    "    # AR-RNN directory\n",
    "    dir_name_ARrnn = os.getcwd()+'/saved_AR_rnn/AR_rnn_000'\n",
    "\n",
    "    # reading AR-RNN parameters\n",
    "    with open(dir_name_ARrnn + '/AR_rnn_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    params_AR_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dir_name_rnn = params_AR_rnn_dict['dir_name_rnn']\n",
    "    rnn_idx = dir_name_rnn[-3:]\n",
    "    dir_name_rnn = os.getcwd()+'/saved_rnn/rnn_'+rnn_idx\n",
    "\n",
    "    dt_rnn = params_AR_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_AR_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_AR_rnn_dict['T_sample_output']\n",
    "    T_offset = params_AR_rnn_dict['T_offset']\n",
    "    return_params_arr = params_AR_rnn_dict['return_params_arr']\n",
    "    params = params_AR_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_AR_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in AR_rnn_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_AR_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_AR_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        use_ae_data = params_AR_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "\n",
    "    # training params\n",
    "    with open(dir_name_ARrnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "    try:\n",
    "        fRMS = tparams_dict['fRMS']\n",
    "    except:\n",
    "        fRMS = 0.0\n",
    "    try:\n",
    "        loss_weights = tparams_dict['loss_weights']\n",
    "    except:\n",
    "        loss_weights = None\n",
    "\n",
    "\n",
    "# reading stddev\n",
    "with open(dir_name_rnn + '/final_net/final_net_class_dict.txt') as f:\n",
    "    lines = f.readlines()\n",
    "finalnet_dict = eval(''.join(lines))\n",
    "stddev = finalnet_dict['stddev']\n",
    "# stddev = 0.0\n",
    "\n",
    "# reading RNN normalization constants\n",
    "normalization_arr_rnn = None\n",
    "if normalize_dataset == True:\n",
    "    with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    normarr_rnn_dict = eval(''.join(lines))\n",
    "    normalization_arr_rnn = normarr_rnn_dict['normalization_arr']\n",
    "\n",
    "if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_arr = fl['normalization_arr'][0]\n",
    "\n",
    "# reading AE directory\n",
    "with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "params_dict = eval(''.join(lines))\n",
    "\n",
    "dir_name_ae = params_dict['dir_name_ae']\n",
    "ae_idx = dir_name_ae[-3:]\n",
    "dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "try:\n",
    "    use_ae_data = params_dict['use_ae_data']\n",
    "except:\n",
    "    print(\"'use_ae_data' not present in sim_data_AE_params, set to True.\")\n",
    "    normalize_dataset = True\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in RNN_specific_data, set to True.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "print('dir_name_ARrnn:', dir_name_ARrnn)\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state_mat = params_dict['init_state_mat']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "numpoints_xgrid = params_dict['numpoints_xgrid']\n",
    "length = params_dict['length']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "xgrid = length*np.linspace(0, 1, numpoints_xgrid)\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data']\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']\n",
    "\n",
    "\n",
    "test_split = 1 - train_split - val_split\n",
    "\n",
    "# setting seed for PRNGs\n",
    "np.random.seed(prng_seed)\n",
    "tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": [
    "def plot(\n",
    "        boundary_idx_arr,\n",
    "        delta_t,\n",
    "        all_data,\n",
    "        xgrid,\n",
    "        xticks_snapto=20,\n",
    "        num_yticks=11,\n",
    "    ):\n",
    "\n",
    "    n = len(boundary_idx_arr)\n",
    "    # '''\n",
    "    num_cols = 1\n",
    "    num_rows = 1\n",
    "    factor = 1\n",
    "    # fig = plt.figure(figsize=(7.5*num_cols, 7.5*num_rows))\n",
    "\n",
    "    num_modes = xgrid.shape[0]\n",
    "\n",
    "    prev_idx = 0\n",
    "    for i in range(len(boundary_idx_arr)):\n",
    "        next_idx = boundary_idx_arr[i]\n",
    "        fig, ax = plt.subplots(figsize=(factor*7.5*num_cols, factor*5.0*num_rows))\n",
    "        N = next_idx - prev_idx\n",
    "        input_time = np.arange(0, N)*delta_t\n",
    "\n",
    "        im = ax.imshow(all_data[prev_idx:next_idx, 0:num_modes].transpose(), aspect='auto', origin='lower')\n",
    "        num_xticks = 1 + int((N*delta_t + 0.5*xticks_snapto) // xticks_snapto)\n",
    "        # xticks = np.linspace(0, N, num_xticks, dtype=np.int32)\n",
    "        xticks = np.arange(0, N, int((xticks_snapto+0.5*delta_t)//delta_t))\n",
    "        ax.set_xticks(ticks=xticks)\n",
    "        ax.set_xticklabels(np.round(xticks*delta_t, 1))\n",
    "        ax.tick_params(axis='x', rotation=270+45)\n",
    "\n",
    "        yticks = np.linspace(0, 1, num_yticks)*(len(xgrid)-1)\n",
    "        yticklabels = np.round(np.linspace(0, 1, yticks.shape[0])*xgrid[-1], 2)\n",
    "        ax.set_yticks(ticks=yticks)\n",
    "        ax.set_yticklabels(yticklabels)\n",
    "\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel(r'$x$')\n",
    "        # ax.title.set_text(r'Latent States')\n",
    "\n",
    "        plt.colorbar(im)\n",
    "        plt.show()\n",
    "        print('')\n",
    "\n",
    "        prev_idx = next_idx\n",
    "\n",
    "    # '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    all_data,\n",
    "    xgrid,\n",
    "    xticks_snapto=int(40*np.round((T//10)/40))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dealing with normalizing the data before feeding into autoencoder\n",
    "if use_ae_data == True:\n",
    "    if normalizeforae_flag == True:\n",
    "        for i in range(numpoints_xgrid):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "    if ae_data_with_params == False:\n",
    "        all_data = all_data[:, 0:numpoints_xgrid]\n",
    "else:\n",
    "    # using raw data, neglecting the params attached (if any)\n",
    "    all_data = all_data[:, 0:numpoints_xgrid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    all_data,\n",
    "    xgrid,\n",
    "    xticks_snapto=int(40*np.round((T//10)/40))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "    wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "    ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Zl6ZvgtNtA_u",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776554,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lXpoaKRIneQc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1667868777509,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Q3a8HHyvneQc",
    "outputId": "51084913-6faf-4bb5-db69-2cbea705dd28"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "if use_ae_data == True:\n",
    "    latent_states_all = ae_net.encoder_net.predict(all_data)\n",
    "    del(all_data)\n",
    "else:\n",
    "    latent_states_all = all_data\n",
    "num_latent_states = latent_states_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1667868778304,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wjgPNitSrt5p",
    "outputId": "0c916524-33ec-47bf-a16a-51e53d2e25f6"
   },
   "outputs": [],
   "source": [
    "plot_latent_states_KS(\n",
    "    boundary_idx_arr,\n",
    "    latent_states_all,\n",
    "    delta_t,\n",
    "    dir_name_ae,\n",
    "    xticks_snapto=int(40*np.round((T//10)/40)),\n",
    "    num_yticks=11,\n",
    "    save_figs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868778305,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wnLnqg0Jrt5t"
   },
   "outputs": [],
   "source": [
    "# for i in range(ae_net.layers):\n",
    "#     tf.keras.utils.plot_model(\n",
    "#         ae_net.layers[i],\n",
    "#         to_file=dir_name_ae+'/plots/netlayer_{}.png'.format(i),\n",
    "#         show_shapes=True,\n",
    "#         dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BOJE8vREtque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fwjcsAxKneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "aFd7XgwVneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    num_lyaptimesteps_totrain = 10*dt_rnn/np.mean(lyapunov_time_arr)\n",
    "    num_timesteps_warmup = 1*np.mean(lyapunov_time_arr)/dt_rnn\n",
    "    T_sample_input = num_timesteps_warmup*dt_rnn\n",
    "    T_sample_output = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_offset = T_sample_input\n",
    "    skip_intermediate = 'full sample'\n",
    "    stateful = True\n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "\n",
    "    # saving AR RNN specific data\n",
    "    AR_RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_lyaptimesteps_totrain':num_lyaptimesteps_totrain,\n",
    "        'num_timesteps_warmup':num_timesteps_warmup,\n",
    "        'dir_name_rnn':dir_name_rnn,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':AR_RNN_GRU.__module__,\n",
    "        'normalization_type':normalization_type,\n",
    "        'use_ae_data':use_ae_data,\n",
    "        'stateful':stateful,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_ARrnn+dir_sep+'AR_RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(AR_RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "UGnj8uQQ83-y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "0t2_8mzI1fhX"
   },
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    latent_states_all,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=normalize_dataset,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type)\n",
    "    \n",
    "data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "num_samples = rnn_res_dict['num_samples']\n",
    "normalization_arr = rnn_res_dict['normalization_arr']\n",
    "rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "pIsWCXkbr7ws"
   },
   "outputs": [],
   "source": [
    "temp = np.divide(latent_states_all-normalization_arr[0], normalization_arr[1])\n",
    "time_stddev = np.std(temp, axis=0)\n",
    "timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "del(org_data_idx_arr_input)\n",
    "del(org_data_idx_arr_output)\n",
    "del(latent_states_all)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Hem_9PUqneQi"
   },
   "outputs": [],
   "source": [
    "print(' data_rnn_input.shape :', data_rnn_input.shape)\n",
    "print('data_rnn_output.shape :', data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778791,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uskBAAXpneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [1e-5, 5e-6, 1e-6]\n",
    "    epochs = [200, 400, 400]\n",
    "    patience = [25, 50, 75] # parameter for early stopping\n",
    "    min_delta = 1e-6  # parameter for early stopping\n",
    "    lambda_reg = 1e-6  # weight for regularizer\n",
    "\n",
    "    if loss_weights is None:\n",
    "        loss_weights = [1.0]*data_rnn_output.shape[1]\n",
    "    elif isinstance(loss_weights, list) == False:\n",
    "        loss_weights = list(loss_weights**np.arange(data_rnn_output.shape[1]))\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'loss_weights':loss_weights,\n",
    "        'timeMeanofSpaceRMS':timeMeanofSpaceRMS,\n",
    "        'stddev':stddev,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_ARrnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_ARrnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "# idx = np.arange(data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# boundary = int(np.round(train_split*data_rnn_input.shape[0]))\n",
    "\n",
    "# training_data_rnn_input = data_rnn_input[idx[0:boundary]]\n",
    "# training_data_rnn_output = data_rnn_output[idx[0:boundary]]\n",
    "\n",
    "# testing_data_rnn_input = data_rnn_input[idx[boundary:]]\n",
    "# testing_data_rnn_output = data_rnn_output[idx[boundary:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1667868779601,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "EENXaWqcKW7j"
   },
   "outputs": [],
   "source": [
    "cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "# idx = np.arange(cum_samples)\n",
    "# np.random.shuffle(idx)\n",
    "num_train_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_val_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_test_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_samples_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "begin_idx = 0\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    num_samples = rnn_data_boundary_idx_arr[i] - begin_idx\n",
    "    num_train_arr[i] = batch_size * (int( (1-test_split-val_split)*num_samples )//batch_size)\n",
    "    num_val_arr[i] = batch_size * (int(val_split*num_samples)//batch_size)\n",
    "    num_test_arr[i] = batch_size * int((num_samples - num_train_arr[i] - num_val_arr[i])//batch_size)\n",
    "    num_samples_arr[i] = num_train_arr[i] + num_val_arr[i] + num_test_arr[i]\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# defining shapes\n",
    "training_input_shape = [np.sum(num_train_arr)]\n",
    "training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "training_output_shape = [np.sum(num_train_arr)]\n",
    "training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "val_input_shape = [np.sum(num_val_arr)]\n",
    "val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "val_output_shape = [np.sum(num_val_arr)]\n",
    "val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "testing_input_shape = [np.sum(num_test_arr)]\n",
    "testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "testing_output_shape = [np.sum(num_test_arr)]\n",
    "testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "# defining required arrays\n",
    "training_data_rnn_input = np.empty(shape=training_input_shape)\n",
    "training_data_rnn_output = np.empty(shape=training_output_shape)\n",
    "\n",
    "val_data_rnn_input = np.empty(shape=val_input_shape)\n",
    "val_data_rnn_output = np.empty(shape=val_output_shape)\n",
    "\n",
    "testing_data_rnn_input = np.empty(shape=testing_input_shape)\n",
    "testing_data_rnn_output = np.empty(shape=testing_output_shape)\n",
    "\n",
    "begin_idx = 0\n",
    "training_data_rolling_count = 0\n",
    "val_data_rolling_count = 0\n",
    "testing_data_rolling_count = 0\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    idx = np.arange(begin_idx, rnn_data_boundary_idx_arr[i])\n",
    "    # np.random.shuffle(idx)\n",
    "    # num_samples = idx.shape[0]\n",
    "    # num_train = int( (1-test_split-val_split)*num_samples )\n",
    "    # num_val = int(val_split*num_samples)\n",
    "    num_samples = num_samples_arr[i]\n",
    "    num_train = num_train_arr[i]\n",
    "    num_val = num_val_arr[i]\n",
    "    num_test = num_test_arr[i]\n",
    "\n",
    "    for j in range(batch_size):\n",
    "        training_data_rnn_input[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_input[idx[0:num_train]][j::batch_size]\n",
    "        training_data_rnn_output[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_output[idx[0:num_train]][j::batch_size]\n",
    "        \n",
    "        val_data_rnn_input[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_input[idx[num_train:num_train+num_val]][j::batch_size]\n",
    "        val_data_rnn_output[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_output[idx[num_train:num_train+num_val]][j::batch_size]\n",
    "\n",
    "        testing_data_rnn_input[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_input[idx[num_train+num_val:num_samples]][j::batch_size]\n",
    "        testing_data_rnn_output[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_output[idx[num_train+num_val:num_samples]][j::batch_size]\n",
    "\n",
    "\n",
    "    # training_data_rnn_input[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_input[idx[0:num_train]]\n",
    "    # training_data_rnn_output[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_output[idx[0:num_train]]\n",
    "    training_data_rolling_count += num_train\n",
    "\n",
    "    # val_data_rnn_input[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_input[idx[num_train:num_train+num_val]]\n",
    "    # val_data_rnn_output[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_output[idx[num_train:num_train+num_val]]\n",
    "    val_data_rolling_count += num_val\n",
    "\n",
    "    # num_test = num_samples-num_train-num_val+1\n",
    "    # testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_input[idx[num_train+num_val:]]\n",
    "    # testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_output[idx[num_train+num_val:]]\n",
    "    testing_data_rolling_count += num_test\n",
    "\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# cleaning up\n",
    "del(data_rnn_input)\n",
    "del(data_rnn_output)\n",
    "\n",
    "# further shuffling\n",
    "if stateful == False:\n",
    "    idx = np.arange(0, training_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    training_data_rnn_input = training_data_rnn_input[idx]\n",
    "    training_data_rnn_output = training_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, val_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    val_data_rnn_input = val_data_rnn_input[idx]\n",
    "    val_data_rnn_output = val_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, testing_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    testing_data_rnn_input = testing_data_rnn_input[idx]\n",
    "    testing_data_rnn_output = testing_data_rnn_output[idx]\n",
    "\n",
    "    del(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868779603,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8isZN1tYBifp"
   },
   "outputs": [],
   "source": [
    "print(' training_data_rnn_input.shape : ', training_data_rnn_input.shape)\n",
    "print('training_data_rnn_output.shape : ', training_data_rnn_output.shape)\n",
    "print('\\n  testing_data_rnn_input.shape : ', testing_data_rnn_input.shape)\n",
    "print(' testing_data_rnn_output.shape : ', testing_data_rnn_output.shape)\n",
    "print('\\n      val_data_rnn_input.shape : ', val_data_rnn_input.shape)\n",
    "print('     val_data_rnn_output.shape : ', val_data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779605,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "x3KglJsgneQj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ixetsZHjCMKO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_NSTtZuyneQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    load_file = dir_name_rnn + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = AR_RNN_GRU(\n",
    "                load_file=load_file,\n",
    "                T_input=T_sample_input,\n",
    "                T_output=T_sample_output,\n",
    "                stddev=stddev,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "    else:\n",
    "        rnn_net = AR_RNN_GRU(\n",
    "            load_file=load_file,\n",
    "            T_input=T_sample_input,\n",
    "            T_output=T_sample_output,\n",
    "            stddev=stddev,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    save_path = dir_name_ARrnn+dir_sep+'final_net'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    rnn_net.save_class_dict(save_path+dir_sep+'final_net_class_dict.txt')\n",
    "    wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'final_net_gru_weights.h5'\n",
    "    rnn_net.build(input_shape=(batch_size, training_data_rnn_input.shape[1], training_data_rnn_input.shape[2]))\n",
    "else:\n",
    "    load_file = dir_name_ARrnn + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = AR_RNN_GRU(\n",
    "                load_file=load_file,\n",
    "                stddev=stddev\n",
    "            )\n",
    "    else:\n",
    "        rnn_net = AR_RNN_GRU(\n",
    "            load_file=load_file,\n",
    "            stddev=stddev\n",
    "        )\n",
    "    \n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        wt_file = tf.train.latest_checkpoint(dir_name_ARrnn+dir_sep+'checkpoints')\n",
    "    elif behaviour == 'loadFinalNetAndPlot':\n",
    "        wt_file = dir_name_ARrnn+dir_sep+'final_net'+dir_sep+'final_net_gru_weights.h5'\n",
    "        # wt_file = dir_name_ARrnn+dir_sep+'final_net'+dir_sep+'f2'#+dir_sep+'saved_model.pb'\n",
    "rnn_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment this out if not residual RNN\n",
    "# rnn_net.rnn_cells_list[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the network\n",
    "rnn_net.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    run_eagerly=False,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics='mse'\n",
    ")\n",
    "\n",
    "if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # this loads the weights/attributes of the optimizer as well\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net.load_weights(wt_file)\n",
    "    else:\n",
    "        rnn_net.load_weights(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class covmatloss(tf.keras.losses.Loss):\n",
    "#     def __init__(self, ae_net, rnn_normalization_arr, ae_normalization_arr, covmatloss_lmda=1e-6, name='covmatloss')\n",
    "#         super().__init__(name=name)\n",
    "#         self.ae_net = ae_net\n",
    "#         self.ae_normalization_layer = normalization_layer(ae_normalization_arr)\n",
    "#         self.rnn_normalization_layer = rnn_normalization_layer(rnn_normalization_arr)\n",
    "#         self.covmatloss_lmda = covmatloss_lmda\n",
    "    \n",
    "#     def call(self, y_true, y_pred):\n",
    "#         mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "#         ypred_decoded = self.rnn_normalization_layer(y_pred)\n",
    "#         ypred_decoded = layers.TimeDistributed(self.ae_net.decoder_net)(ypred_decoded, training=True)\n",
    "#         ypred_decoded = self.ae_normalization_layer(ypred_decoded)\n",
    "\n",
    "#         ytrue_decoded = self.rnn_normalization_layer(y)\n",
    "#         ytrue_decoded = layers.TimeDistributed(self.ae_net.decoder_net)(ytrue_decoded, training=False)\n",
    "#         ytrue_decoded = self.ae_normalization_layer(ytrue_decoded)\n",
    "\n",
    "#         covmat_pred = tf.linalg.matmul(\n",
    "#             ypred_decoded,\n",
    "#             ypred_decoded,\n",
    "#             transpose_a=True,\n",
    "#         )\n",
    "#         covmat_true = tf.linalg.matmul(\n",
    "#             ytrue_decoded,\n",
    "#             ytrue_decoded,\n",
    "#             transpose_a=True,\n",
    "#         )\n",
    "#         covmat_fro_loss = tf.norm(covmat_true - covmat_pred, ord='fro', axis=[-2, -1])\n",
    "#         loss = mse + self.covmatloss_lmda * covmat_fro_loss\n",
    "#         return loss\n",
    "\n",
    "\n",
    "class normalization_layer(layers.Layer):\n",
    "    def __init__(self, normalization_arr):\n",
    "        super(normalization_layer, self).__init__()\n",
    "        self.normalization_arr = normalization_arr\n",
    "        self.alpha = tf.Variable(\n",
    "            initial_value=self.normalization_arr[0:1, :],\n",
    "            trainable=False,\n",
    "            name='alpha',\n",
    "            dtype='float32',\n",
    "            shape=(1, self.normalization_arr.shape[1])\n",
    "        )\n",
    "        self.beta = tf.Variable(\n",
    "            initial_value=self.normalization_arr[1:, :],\n",
    "            trainable=False,\n",
    "            name='beta',\n",
    "            dtype='float32',\n",
    "            shape=(1, self.normalization_arr.shape[1])\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # batch_size = inputs.shape[0]\n",
    "        # if batch_size == None:\n",
    "        #     batch_size = 1\n",
    "        return inputs * self.beta + self.alpha\n",
    "\n",
    "\n",
    "class cov_loss_model(Model):\n",
    "    def __init__(self, rnn_net, ae_net, rnn_data_normalization_arr, ae_data_normalization_arr, covmat_lmda=1e-5):\n",
    "        super(cov_loss_model, self).__init__()\n",
    "        self.rnn_net = rnn_net\n",
    "        self.ae_net = ae_net\n",
    "        self.covmat_lmda = covmat_lmda\n",
    "        self.rnn_normalization_layer = normalization_layer(rnn_data_normalization_arr)\n",
    "        self.ae_normalization_layer = normalization_layer(ae_data_normalization_arr)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return self.rnn_net(inputs, training=training)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # x, y = data\n",
    "        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "        sw_cov = 1.0 if sample_weight is None else sample_weight\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            ypred = self.rnn_net.call(x, training=True, usenoiseflag=True)\n",
    "            loss = self.compiled_loss(\n",
    "                    y,\n",
    "                    ypred,\n",
    "                    sample_weight,\n",
    "                    regularization_losses=self.losses\n",
    "                )\n",
    "            ypred_decoded = self.rnn_normalization_layer(ypred)\n",
    "            ypred_decoded = layers.TimeDistributed(self.ae_net.decoder_net)(ypred_decoded, training=True)\n",
    "            ypred_decoded = self.ae_normalization_layer(ypred_decoded)\n",
    "\n",
    "            ytrue_decoded = self.rnn_normalization_layer(y)\n",
    "            ytrue_decoded = layers.TimeDistributed(self.ae_net.decoder_net)(ytrue_decoded, training=False)\n",
    "            ytrue_decoded = self.ae_normalization_layer(ytrue_decoded)\n",
    "            \n",
    "            covmat_pred = tf.linalg.matmul(\n",
    "                ypred_decoded,\n",
    "                ypred_decoded,\n",
    "                transpose_a=True,\n",
    "            )\n",
    "            covmat_true = tf.linalg.matmul(\n",
    "                ytrue_decoded,\n",
    "                ytrue_decoded,\n",
    "                transpose_a=True,\n",
    "            )\n",
    "            covmat_fro_loss = self.covmat_lmda * sw_cov * tf.norm(covmat_true - covmat_pred, ord='fro', axis=[-2, -1])\n",
    "            loss = loss + covmat_fro_loss\n",
    "            # print(tf.norm(covmat_true - covmat_pred, ord='fro', axis=[-2, -1]))\n",
    "        self._validate_target_and_loss(ypred, loss)\n",
    "\n",
    "        trainable_vars = self.rnn_net.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        return self.compute_metrics(x, y, ypred, sample_weight, covmat_fro_loss)\n",
    "    \n",
    "    def compute_metrics(self, x, y, y_pred, sample_weight, covmat_fro_loss=0.0):\n",
    "        metric_results = super(cov_loss_model, self).compute_metrics(x, y, y_pred, sample_weight)\n",
    "        # return_dict = self.get_metrics_result()\n",
    "        metric_results['covmat_fro_loss'] = covmat_fro_loss\n",
    "        return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_loss_rnn = cov_loss_model(\n",
    "    rnn_net,\n",
    "    ae_net,\n",
    "    normalization_arr,\n",
    "    normalization_constant_arr_aedata,\n",
    "    covmat_lmda=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cov_loss_rnn.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    run_eagerly=False,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics='mse'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    val_loss_hist = []\n",
    "    train_loss_hist = []\n",
    "    lr_change=[0, 0]\n",
    "    arr_len = 0\n",
    "    if type(epochs) != type([]):\n",
    "        arr_len = epochs*len(learning_rate_list)\n",
    "    else:\n",
    "        for i in range(len(epochs)):\n",
    "            arr_len += epochs[i]\n",
    "    savelosses_cb_vallossarr = np.ones(shape=arr_len)*np.NaN\n",
    "    savelosses_cb_trainlossarr = np.ones(shape=arr_len)*np.NaN\n",
    "    starting_lr_idx = 0\n",
    "    num_epochs_left = epochs if type(epochs) != type([]) else epochs[0]\n",
    "    earlystopping_wait = 0\n",
    "elif behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt, earlystopping_wait = readAndReturnLossHistories(\n",
    "        dir_name_ae=dir_name_ARrnn,\n",
    "        dir_sep=dir_sep,\n",
    "        epochs=epochs,\n",
    "        learning_rate_list=learning_rate_list,\n",
    "        return_earlystopping_wait=True)\n",
    "    savelosses_cb_vallossarr = val_loss_arr_fromckpt\n",
    "    savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n",
    "elif behaviour == 'loadFinalNetAndPlot':\n",
    "    with open(dir_name_ARrnn+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    losses_dict = eval(''.join(lines))\n",
    "\n",
    "    val_loss_hist = losses_dict['val_loss_hist']\n",
    "    train_loss_hist = losses_dict['train_loss_hist']\n",
    "    lr_change = losses_dict['lr_change']\n",
    "    test_loss = losses_dict['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # implementing early stopping\n",
    "    baseline = None\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        baseline = np.min(val_loss_hist)\n",
    "\n",
    "    # time callback for each epoch\n",
    "    timekeeper_cb = mytimecallback()\n",
    "\n",
    "    # model checkpoint callback\n",
    "    dir_name_ckpt = dir_name_ARrnn+dir_sep+'checkpoints'\n",
    "    if not os.path.isdir(dir_name_ckpt):\n",
    "        os.makedirs(dir_name_ckpt)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dir_name_ckpt+dir_sep+'checkpoint',#+'/checkpoint--loss={loss:.4f}--vall_loss={val_loss:.4f}',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=2,\n",
    "        initial_value_threshold=baseline,\n",
    "        period=1  # saves every `period` epochs\n",
    "    )\n",
    "\n",
    "    epochs_so_far = 0\n",
    "    for i in range(0, starting_lr_idx):\n",
    "        if type(epochs) == type(list):\n",
    "            if i < len(epochs):\n",
    "                epochs_i = epochs[i]\n",
    "            epochs_so_far += epochs_i\n",
    "        else:\n",
    "            epochs_so_far += epochs\n",
    "\n",
    "    for i in range(starting_lr_idx, len(learning_rate_list)):\n",
    "        learning_rate = learning_rate_list[i]\n",
    "        if type(patience) == type([]):\n",
    "            if i < len(patience):\n",
    "                patience_thislr = patience[i]\n",
    "            else:\n",
    "                patience_thislr = patience[-1]\n",
    "        else:\n",
    "            patience_thislr = patience\n",
    "        if type(epochs) == type([]):\n",
    "            if i < len(epochs):\n",
    "                epochs_thislr = epochs[i]\n",
    "            else:\n",
    "                epochs_thislr = epochs[-1]\n",
    "        else:\n",
    "            epochs_thislr = epochs\n",
    "            \n",
    "\n",
    "        K.set_value(rnn_net.optimizer.lr, learning_rate)\n",
    "\n",
    "        # save losses callback\n",
    "        savelosses_cb = SaveLosses(\n",
    "            filepath=dir_name_ckpt+dir_sep+'LossHistoriesCheckpoint',\n",
    "            val_loss_arr=savelosses_cb_vallossarr,\n",
    "            train_loss_arr=savelosses_cb_trainlossarr,\n",
    "            total_epochs=epochs_thislr,\n",
    "            period=1)\n",
    "        # savelosses_cb.update_lr_idx(i)\n",
    "        \n",
    "        early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience_thislr,\n",
    "            restore_best_weights=True,\n",
    "            verbose=True,\n",
    "            min_delta=min_delta,\n",
    "            baseline=baseline\n",
    "        )\n",
    "        #** the two lines below are useless because wait is set to 0 in on_train_begin\n",
    "        # early_stopping_cb.wait = earlystopping_wait\n",
    "        # print('early_stopping_cb.wait : {}\\n'.format(early_stopping_cb.wait))\n",
    "\n",
    "        if i == starting_lr_idx:\n",
    "            EPOCHS = num_epochs_left\n",
    "            savelosses_cb.update_offset(epochs_so_far + epochs_thislr-num_epochs_left)\n",
    "        else:\n",
    "            EPOCHS = epochs_thislr\n",
    "            savelosses_cb.update_offset(epochs_so_far + 0)\n",
    "\n",
    "        total_s_len = 80\n",
    "        sep_lr_s = ' LEARNING RATE : {} '.format(learning_rate)\n",
    "        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n",
    "        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n",
    "        print('\\n\\n' + '-'*len(sep_lr_s))\n",
    "        print('\\n' + sep_lr_s+'\\n')\n",
    "        print('-'*len(sep_lr_s) + '\\n\\n')\n",
    "        \n",
    "        history = cov_loss_rnn.fit(training_data_rnn_input, training_data_rnn_output,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=batch_size,\n",
    "#             validation_split=val_split/train_split,\n",
    "            validation_data=(val_data_rnn_input, val_data_rnn_output),\n",
    "            callbacks=[early_stopping_cb, timekeeper_cb, checkpoint_cb, savelosses_cb],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        val_loss_hist.extend(history.history['val_loss'])\n",
    "        train_loss_hist.extend(history.history['loss'])\n",
    "        \n",
    "        if i == starting_lr_idx:\n",
    "            lr_change[i+1] += len(history.history['val_loss'])\n",
    "        else:\n",
    "            lr_change.append(lr_change[i]+len(history.history['val_loss']))\n",
    "        \n",
    "        epochs_so_far += epochs_thislr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # test_loss = rnn_net.evaluate(\n",
    "    #     testing_data_rnn_input, testing_data_rnn_output,\n",
    "    # )\n",
    "    for layer in rnn_net.rnn_list:\n",
    "        if layer.stateful == True:\n",
    "            layer.reset_states()\n",
    "    print(testing_data_rnn_input.shape, testing_data_rnn_output.shape)\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    for i in range(int(testing_data_rnn_input.shape[0]//batch_size)):\n",
    "        # i_test_loss = rnn_net.evaluate(\n",
    "        #     testing_data_rnn_input[i*batch_size:(i+1)*batch_size, :, :],\n",
    "        #     testing_data_rnn_output[i*batch_size:(i+1)*batch_size, :, :],\n",
    "        # )\n",
    "        temp = rnn_net.call(testing_data_rnn_input[i*batch_size:(i+1)*batch_size, :, :], training=False)\n",
    "        i_test_loss = np.mean((testing_data_rnn_output[i*batch_size:(i+1)*batch_size, :, :] - temp.numpy())**2)\n",
    "        test_loss = (i*test_loss + i_test_loss)/(i+1)\n",
    "\n",
    "    save_path = dir_name_ARrnn+dir_sep+'final_net'\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "\n",
    "    with open(save_path+dir_sep+'losses.txt', 'w') as f:\n",
    "        f.write(str({\n",
    "            'val_loss_hist':val_loss_hist,\n",
    "            'train_loss_hist':train_loss_hist,\n",
    "            'lr_change':lr_change,\n",
    "            'test_loss':test_loss\n",
    "        }))\n",
    "        \n",
    "    if normalize_dataset == True:\n",
    "        with open(save_path+dir_sep+'rnn_normalization.txt', 'w') as f:\n",
    "            f.write(str({\n",
    "                'normalization_arr':normalization_arr\n",
    "            }))\n",
    "\n",
    "    rnn_net.save_everything(\n",
    "        file_name=save_path+dir_sep+'final_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": [
    "# plotting losses\n",
    "dir_name_plot = dir_name_ARrnn+dir_sep+'plots'\n",
    "if not os.path.isdir(dir_name_plot):\n",
    "    os.makedirs(dir_name_plot)\n",
    "\n",
    "# Visualize loss history\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_loss_hist,\n",
    "    val_loss=val_loss_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list\n",
    ")\n",
    "\n",
    "plt.savefig(dir_name_plot+'{ds}loss_history.png'.format(ds=dir_sep), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": [
    "# data_type = 'training'\n",
    "data_type = 'testing'\n",
    "\n",
    "for layer in rnn_net.rnn_list:\n",
    "    if layer.stateful == True:\n",
    "        layer.reset_states()\n",
    "\n",
    "data_in = eval(data_type+'_data_rnn_input')\n",
    "data_out = eval(data_type+'_data_rnn_output')\n",
    "\n",
    "data_idx = np.arange(data_in.shape[0])\n",
    "np.random.shuffle(data_idx)\n",
    "data_idx = data_idx[0]\n",
    "# data_idx = 3788\n",
    "print('data_idx : {}'.format(data_idx))\n",
    "\n",
    "# data_in = data_in[data_idx]\n",
    "data_out = data_out[data_idx]\n",
    "\n",
    "prediction = rnn_net.predict(data_in[data_idx:data_idx+1, :, :])\n",
    "\n",
    "plot_reconstructed_data_KS(\n",
    "    [data_out.shape[0]],\n",
    "    dir_name_ARrnn,\n",
    "    data_out,\n",
    "    prediction[0], dt_rnn, 1+np.arange(0, data_out.shape[1]),\n",
    "    save_figs=False,\n",
    "    normalization_constant_arr=normalization_arr,\n",
    "    xticks_snapto=2,\n",
    "    num_yticks=data_out.shape[1],\n",
    "    ylabel=r'Latent State Index',\n",
    "    ax1_title=r'Original Data',\n",
    "    ax2_title=r'RNN (AR) Predicted Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [],
   "source": [
    "prediction.shape, data_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": [
    "def rescale_data(data, normalization_arr):\n",
    "    '''\n",
    "    data - [num_batches x num_timesteps x num_states]\n",
    "    normalization_arr = [2 x num_states]\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(data.shape[-1]):\n",
    "        new_data[:, i] -= normalization_arr[0, i]\n",
    "        new_data[:, i] /= normalization_arr[1, i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def norm_sq_time_average(data):\n",
    "    data_norm_sq = np.zeros(shape=data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_norm_sq[:] += data[:, i]**2\n",
    "    # integrating using the trapezoidal rule\n",
    "    norm_sq_time_avg = np.sum(data_norm_sq) - 0.5*(data_norm_sq[0]+data_norm_sq[-1])\n",
    "    norm_sq_time_avg /= data_norm_sq.shape[0]\n",
    "    return norm_sq_time_avg\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(shape[-1]):\n",
    "        if len(shape) == 2:\n",
    "            new_data[:, i] *= normalization_arr[1, i]\n",
    "            new_data[:, i] += normalization_arr[0, i]\n",
    "        elif len(shape) == 3:\n",
    "            new_data[:, :, i] *= normalization_arr[1, i]\n",
    "            new_data[:, :, i] += normalization_arr[0, i]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_type = 'training'\n",
    "data_type = 'testing'\n",
    "\n",
    "for layer in rnn_net.rnn_list:\n",
    "    if layer.stateful == True:\n",
    "        layer.reset_states()\n",
    "\n",
    "data_in = eval(data_type+'_data_rnn_input')\n",
    "data_out = eval(data_type+'_data_rnn_output')\n",
    "\n",
    "data_idx = np.arange(data_in.shape[0])\n",
    "np.random.shuffle(data_idx)\n",
    "data_idx = data_idx[0]\n",
    "# data_idx = 0\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    if data_idx < rnn_data_boundary_idx_arr[i]:\n",
    "        case_idx = i\n",
    "        break\n",
    "lyap_time = lyapunov_time_arr[case_idx]\n",
    "\n",
    "print('case {}, data_idx : {}'.format(case_idx+1, data_idx))\n",
    "\n",
    "# data_in = data_in[data_idx]\n",
    "data_out = data_out[data_idx]\n",
    "# data_out = rescale_data(data_out, normalization_arr)\n",
    "# data_out = invert_normalization(data_out, normalization_arr)\n",
    "\n",
    "prediction = rnn_net.predict(data_in[data_idx:data_idx+1, :, :])\n",
    "# prediction = rnn_net(data_in[data_idx:data_idx+1, :, :], training=False)\n",
    "# prediction = rnn_net.call(data_in[data_idx:data_idx+1, :, :], training=False)\n",
    "# prediction = invert_normalization(prediction, normalization_arr)\n",
    "\n",
    "#-- computing KE --#\n",
    "KE_og = ae_net.decoder_net.predict(invert_normalization(data_out, normalization_arr))\n",
    "KE_pred = ae_net.decoder_net.predict(invert_normalization(prediction[0, :, :], normalization_arr))\n",
    "\n",
    "KE_og = np.sum(KE_og**2, axis=1)\n",
    "KE_pred = np.sum(KE_pred**2, axis=1)\n",
    "#-- KE computed --#\n",
    "\n",
    "n = 1\n",
    "num_latent_states = data_out.shape[-1]\n",
    "N = data_out.shape[0]\n",
    "\n",
    "num_cols = 1\n",
    "num_rows = n*num_latent_states\n",
    "\n",
    "ax_ylabels = ['$x^*_{' +str(i)+'}$' for i in range(1, num_latent_states+1)]\n",
    "\n",
    "fig, ax = plt.subplots(num_latent_states+1, 1, sharex=True, figsize=(7.5*num_cols, 2.5*num_rows))\n",
    "if num_latent_states == 1:\n",
    "    ax = [ax]\n",
    "input_time = np.arange(0, N)*dt_rnn/lyap_time\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 2*n)]\n",
    "\n",
    "prev_idx = 0\n",
    "\n",
    "# mpl_ax_artist_list = []\n",
    "for j in range(num_latent_states):\n",
    "    for i in range(n):\n",
    "        obj_in = ax[j].plot(input_time, data_out[:, j], linewidth=1, color=colors[2*i], label='Case {} - actual data'.format(i+1))\n",
    "        obj_out = ax[j].plot(input_time, prediction[0, :, j], linewidth=1, color=colors[2*i+1], label='Case {} - predicted data'.format(i+1))\n",
    "        # mpl_ax_artist_list.append(obj_in[0])\n",
    "        # mpl_ax_artist_list.append(obj_out[0])\n",
    "    ax[j].set_ylabel(ax_ylabels[j])\n",
    "    # if xlim is not None:\n",
    "    #     ax[j].set_xlim(xlim)\n",
    "    # if ylim is not None:\n",
    "    #     ax[j].set_ylim(ylim)\n",
    "    ax[j].grid(True)\n",
    "    ax[j].set_axisbelow(True)\n",
    "    # ax[j].set_ylim([-1, 1])\n",
    "\n",
    "ax[-1].plot(input_time, KE_og, linewidth=1, color=colors[0], label='Case {} - actual KE'.format(i+1))\n",
    "ax[-1].plot(input_time, KE_pred, linewidth=1, color=colors[1], label='Case {} - predicted KE'.format(i+1))\n",
    "ax[-1].set_ylabel('$KE$')\n",
    "ax[-1].grid(True)\n",
    "ax[-1].set_axisbelow(True)\n",
    "\n",
    "ax[-1].set_xlabel('Time$^+$')\n",
    "\n",
    "max_rows = 10\n",
    "max_rows = float(max_rows)\n",
    "ncols = int(np.ceil(len(boundary_idx_arr) / max_rows))\n",
    "ax[0].legend(\n",
    "    loc='best',\n",
    "    ncol=ncols,\n",
    ")\n",
    "ax[0].set_title(r'Latent States', size=12)\n",
    "plt.show()\n",
    "# plt.savefig('AR-GRU.png', dpi=300, bbox_inches='tight')\n",
    "print('')\n",
    "\n",
    "### Error and prediction horizon\n",
    "# error = np.linalg.norm(data_out[:, :] - prediction[0, :, :], axis=1)\n",
    "error = (data_out[:, :] - prediction[0, :, :])**2\n",
    "# error /= norm_sq_time_average(data_out)**0.5\n",
    "error = np.mean(np.divide(error, time_stddev**2), axis=1)**0.5\n",
    "\n",
    "# print(norm_sq_time_average(data_out)**0.5)\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(7.5, 2.5))\n",
    "ax2.plot(input_time, error)\n",
    "ax2.grid(True)\n",
    "ax2.set_axisbelow(True)\n",
    "ax2.set_xlabel('Time$^+$')\n",
    "ax2.set_ylabel('Normalized Error')\n",
    "\n",
    "error_threshold = 0.5\n",
    "\n",
    "predhor_idx = np.where(error >= error_threshold)[0][0]\n",
    "ax2.plot(input_time[predhor_idx], error[predhor_idx], 'o', color='k')\n",
    "ax2.axhline(error[predhor_idx], linewidth=0.9, linestyle='--', color='k')\n",
    "ax2.axvline(input_time[predhor_idx], linewidth=0.9, linestyle='--', color='k')\n",
    "\n",
    "prediction_horizon = predhor_idx*dt_rnn/lyap_time\n",
    "print(prediction_horizon)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
