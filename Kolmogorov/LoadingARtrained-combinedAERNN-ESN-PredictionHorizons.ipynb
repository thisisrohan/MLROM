{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg, fft\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from keras.engine import data_adapter\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/Kolmogorov\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data_KS, plot_latent_states_KS , readAndReturnLossHistories, sigmoidWarmupAndDecayLRSchedule\n",
    "from tools.ae_v2 import Autoencoder\n",
    "from tools.ESN_v2_AR import ESN as AR_RNN\n",
    "from tools.AEESN_AR_v1 import AR_AERNN_ESN as AR_AERNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 02:57:50.759232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:50.759824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:50.833163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:50.833603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:50.833880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:50.834251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:50.835551: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-20 02:57:50.836318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:50.836703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:50.837074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:51.425415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:51.425630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:51.425811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 02:57:51.425964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3359 MB memory:  -> device: 0, name: Quadro K2200, pci bus id: 0000:02:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 0\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "2.8.1\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print('')\n",
    "print(tf.config.list_logical_devices())\n",
    "print('')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_name_AR_AErnn: /home/rkaushik/Documents/Thesis/MLROM/Kolmogorov/saved_AR_AERNN_rnn/AR_rnn_002\n",
      "data_dir_idx: 000\n",
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/Kolmogorov/saved_ESN/ESN_003\n",
      "use_ae_data : True, dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/Kolmogorov/saved_ae/ae_013\n",
      "24 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "### setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "### AR AE-RNN directory\n",
    "dir_name_AR_AErnn = os.getcwd()+'/saved_AR_AERNN_rnn/AR_rnn_002'\n",
    "\n",
    "### reading AR-RNN parameters\n",
    "with open(dir_name_AR_AErnn + '/AR_RNN_specific_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "params_AR_AErnn_dict = eval(''.join(lines))\n",
    "\n",
    "dir_name_rnn = params_AR_AErnn_dict['dir_name_rnn']\n",
    "# rnn_idx = dir_name_rnn[-3:]\n",
    "# dir_name_rnn = os.getcwd()+'/saved_rnn/rnn_'+rnn_idx\n",
    "idx1 = dir_name_rnn[::-1].find('/')\n",
    "idx2 = dir_name_rnn[:-idx1-1][::-1].find('/')\n",
    "dir_name_rnn = os.getcwd() + dir_name_rnn[-idx1-idx2-1-1:]\n",
    "\n",
    "dir_name_ae = params_AR_AErnn_dict['dir_name_ae']\n",
    "ae_idx = dir_name_ae[-3:]\n",
    "dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "\n",
    "dt_rnn = params_AR_AErnn_dict['dt_rnn']\n",
    "# T_sample_input = params_AR_AErnn_dict['T_sample_input']\n",
    "T_sample_output = params_AR_AErnn_dict['T_sample_output']\n",
    "if type(T_sample_output) != type(np.array([])):\n",
    "    if type(T_sample_output) != type([]):\n",
    "        T_sample_output = [T_sample_output]\n",
    "    T_sample_output = np.array(T_sample_output)\n",
    "num_outsteps = np.int32(np.round(T_sample_output/dt_rnn))\n",
    "# T_offset = params_AR_AErnn_dict['T_offset']\n",
    "try:\n",
    "    # this is the normalization flag for the data fed into the rnn\n",
    "    normalize_dataset = params_AR_AErnn_dict['normalize_dataset']\n",
    "except:\n",
    "    print(\"'normalize_dataset' not present in AR_rnn_specific_data, set to False.\")\n",
    "    normalize_dataset = False\n",
    "try:\n",
    "    use_ae_data = params_AR_AErnn_dict['use_ae_data']\n",
    "except:\n",
    "    print(\"'use_ae_data' not present in AR_rnn_specific_data, set to True.\")\n",
    "    use_ae_data = True\n",
    "\n",
    "### reading RNN normalization constants\n",
    "normalization_arr_rnn = None\n",
    "if os.path.exists(dir_name_AR_AErnn+'/normalization_data.npz'):\n",
    "    with np.load(dir_name_AR_AErnn+'/normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_arr_rnn = fl['normalization_arr'][0]\n",
    "\n",
    "### training params\n",
    "with open(dir_name_AR_AErnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tparams_dict = eval(''.join(lines))\n",
    "\n",
    "prng_seed = tparams_dict['prng_seed']\n",
    "train_split = tparams_dict['train_split']\n",
    "val_split = tparams_dict['val_split']\n",
    "batch_size = tparams_dict['batch_size']\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "\n",
    "with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "    normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "\n",
    "print('dir_name_AR_AErnn:', dir_name_AR_AErnn)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('use_ae_data : ' + str(use_ae_data) + ', dir_name_ae:', dir_name_ae)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "\n",
    "with h5py.File(dir_name_data + '/data.h5', 'r') as f:\n",
    "    t_recorded_samples = np.array(f['t'])\n",
    "    \n",
    "    N = int(0.5*(np.array(f['num_wavenumbers'])-1))\n",
    "    print(N, type(N))\n",
    "    \n",
    "    u_ref = np.array(f['u_reference'], dtype=FTYPE)\n",
    "    v_ref = np.array(f['v_reference'], dtype=FTYPE)\n",
    "\n",
    "\n",
    "test_split = 1 - train_split - val_split\n",
    "\n",
    "# setting seed for PRNGs\n",
    "np.random.seed(prng_seed)\n",
    "tf.random.set_seed(prng_seed)\n",
    "\n",
    "# set which data to use for plotting histogram\n",
    "data_to_consider = 'testing' # could be 'all', 'testing', 'training', 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [],
   "source": [
    "all_data = np.empty(shape=(u_ref.shape[0], 2, u_ref.shape[1], u_ref.shape[2]), dtype=FTYPE)\n",
    "all_data[:, 0, :, :] = u_ref\n",
    "del(u_ref)\n",
    "all_data[:, 1, :, :] = v_ref\n",
    "del(v_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [],
   "source": [
    "lyap_time = 1/0.065\n",
    "delta_t = 1.\n",
    "T = t_recorded_samples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data.shape : (100001, 2, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "print('all_data.shape : {}'.format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = all_data[0:int(all_data.shape[0]/3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data.shape : (33333, 2, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "print('all_data.shape : {}'.format(all_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_sample_input_cd = lyap_time\n",
    "T_sample_output_cd = 3*lyap_time#np.mean(lyapunov_time_arr)\n",
    "T_offset_cd = T_sample_input_cd\n",
    "\n",
    "skip_intermediate_cd = 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_mean_ogdata = np.mean(all_data, axis=0)\n",
    "time_stddev_ogdata = np.std(all_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_og_shape = all_data.shape\n",
    "all_data = all_data.reshape(all_data.shape[0], -1)\n",
    "normalization_constant_arr_aedata = normalization_constant_arr_aedata.reshape(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    all_data,\n",
    "    dt_rnn,\n",
    "    T_sample_input_cd,\n",
    "    T_sample_output_cd,\n",
    "    T_offset_cd,\n",
    "    None,\n",
    "    [all_data.shape[0]],\n",
    "    dt_rnn,# delta_t,\n",
    "    params=None,#params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=normalizeforae_flag,\n",
    "    stddev_multiplier=3.0,\n",
    "    skip_intermediate=skip_intermediate_cd,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_constant_arr_aedata,\n",
    "    normalization_type='stddev')\n",
    "\n",
    "data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "num_samples = rnn_res_dict['num_samples']\n",
    "normalization_arr = rnn_res_dict['normalization_arr']\n",
    "rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.divide(all_data-normalization_arr[0], normalization_arr[1])\n",
    "time_stddev = np.std(temp, axis=0)\n",
    "timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "del(org_data_idx_arr_input)\n",
    "del(org_data_idx_arr_output)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stddev = time_stddev.reshape(all_data_og_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_constant_arr_aedata = normalization_constant_arr_aedata.reshape((2,)+tuple(all_data_og_shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_rnn_input.shape : (543, 62, 5000)\n",
      "data_rnn_output.shape : (543, 185, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(' data_rnn_input.shape :', data_rnn_input.shape)\n",
    "print('data_rnn_output.shape :', data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rnn_input = data_rnn_input.reshape((-1, data_rnn_input.shape[1]) + tuple(all_data_og_shape[1:]))\n",
    "data_rnn_output = data_rnn_output.reshape((-1, data_rnn_output.shape[1]) + tuple(all_data_og_shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_rnn_input.shape : (543, 62, 2, 50, 50)\n",
      "data_rnn_output.shape : (543, 185, 2, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "print(' data_rnn_input.shape :', data_rnn_input.shape)\n",
    "print('data_rnn_output.shape :', data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_to_consider != 'all':\n",
    "    cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "    num_train = 0\n",
    "    num_val = 0\n",
    "    begin_idx = 0\n",
    "    for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "        num_samples = rnn_data_boundary_idx_arr[i] - begin_idx\n",
    "        num_train += int( (1-test_split-val_split)*num_samples )\n",
    "        num_val += int(val_split*num_samples)\n",
    "        begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "    # defining shapes\n",
    "    training_input_shape = [num_train]\n",
    "    training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "    training_output_shape = [num_train]\n",
    "    training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "    val_input_shape = [num_val]\n",
    "    val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "    val_output_shape = [num_train]\n",
    "    val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "    testing_input_shape = [cum_samples-num_train-num_val]\n",
    "    testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "    testing_output_shape = [cum_samples-num_train-num_val]\n",
    "    testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "    shape_to_use = eval(data_to_consider+'_input_shape')\n",
    "    rnn_data_idx = np.empty(shape=shape_to_use[0], dtype=np.int32)\n",
    "    \n",
    "    begin_idx = 0\n",
    "    training_data_rolling_count = 0\n",
    "    val_data_rolling_count = 0\n",
    "    testing_data_rolling_count = 0\n",
    "    for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "        num_samples = rnn_data_boundary_idx_arr[i] - begin_idx\n",
    "        num_train = int( (1-test_split-val_split)*num_samples )\n",
    "        num_val = int(val_split*num_samples)\n",
    "        num_test = num_samples-num_train-num_val+1\n",
    "\n",
    "        if data_to_consider == 'training':\n",
    "            rnn_data_idx[training_data_rolling_count:training_data_rolling_count+num_train] = np.arange(begin_idx, begin_idx+num_train)\n",
    "        elif data_to_consider == 'val':\n",
    "            rnn_data_idx[val_data_rolling_count:val_data_rolling_count+num_val] = np.arange(begin_idx+num_train, begin_idx+num_train+num_val)\n",
    "        elif data_to_consider == 'testing':\n",
    "            rnn_data_idx[testing_data_rolling_count:testing_data_rolling_count+num_test] = np.arange(begin_idx+num_train+num_val, rnn_data_boundary_idx_arr[i])\n",
    "\n",
    "        training_data_rolling_count += num_train\n",
    "        val_data_rolling_count += num_val\n",
    "        testing_data_rolling_count += num_test\n",
    "\n",
    "        begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "    # shuffling\n",
    "    np.random.shuffle(rnn_data_idx)\n",
    "    data_rnn_input = data_rnn_input[rnn_data_idx]\n",
    "    data_rnn_output = data_rnn_output[rnn_data_idx]\n",
    "    del(rnn_data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_rnn_input.shape : (55, 62, 2, 50, 50)\n",
      "data_rnn_output.shape : (55, 185, 2, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "print(' data_rnn_input.shape :', data_rnn_input.shape)\n",
    "print('data_rnn_output.shape :', data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Prediction Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_data(data, normalization_arr):\n",
    "    '''\n",
    "    data - [num_batches x num_timesteps x num_states]\n",
    "    normalization_arr = [2 x num_states]\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(data.shape[-1]):\n",
    "        new_data[:, i] -= normalization_arr[0, i]\n",
    "        new_data[:, i] /= normalization_arr[1, i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def norm_sq_time_average(data):\n",
    "    data_norm_sq = np.zeros(shape=data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_norm_sq[:] += data[:, i]**2\n",
    "    # integrating using the trapezoidal rule\n",
    "    norm_sq_time_avg = np.sum(data_norm_sq) - 0.5*(data_norm_sq[0]+data_norm_sq[-1])\n",
    "    norm_sq_time_avg /= data_norm_sq.shape[0]\n",
    "    return norm_sq_time_avg\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    # for i in range(shape[-1]):\n",
    "    #     if len(shape) == 2:\n",
    "    #         new_data[:, i] *= normalization_arr[1, i]\n",
    "    #         new_data[:, i] += normalization_arr[0, i]\n",
    "    #     elif len(shape) == 3:\n",
    "    #         new_data[:, :, i] *= normalization_arr[1, i]\n",
    "    #         new_data[:, :, i] += normalization_arr[0, i]\n",
    "    new_data *= normalization_arr[1]\n",
    "    new_data += normalization_arr[0]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_horizons(**kwargs):\n",
    "    num_outsteps = kwargs['num_outsteps']\n",
    "    dir_name_AR_AErnn = kwargs['dir_name_AR_AErnn']\n",
    "    Autoencoder = kwargs['Autoencoder']\n",
    "    # all_data = kwargs['all_data']\n",
    "    data_rnn_input = kwargs['data_rnn_input']\n",
    "    data_rnn_output = kwargs['data_rnn_output']\n",
    "    AR_RNN = kwargs['AR_RNN']\n",
    "    T_sample_input_rnn = kwargs['T_sample_input_rnn']\n",
    "    T_sample_output_rnn = kwargs['T_sample_output_rnn']\n",
    "    AR_AERNN = kwargs['AR_AERNN']\n",
    "    normalization_constant_arr_rnn = kwargs['normalization_constant_arr_rnn']\n",
    "    normalization_constant_arr_aedata = kwargs['normalization_constant_arr_aedata']\n",
    "    time_stddev_ogdata = kwargs['time_stddev_ogdata']\n",
    "    time_mean_ogdata = kwargs['time_mean_ogdata']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    num_runs = kwargs.pop('num_runs', 100)\n",
    "    error_threshold = kwargs.pop('error_threshold', 0.5)\n",
    "    rnn_data_boundary_idx_arr = kwargs['rnn_data_boundary_idx_arr']\n",
    "    lyapunov_time_arr = kwargs['lyapunov_time_arr']\n",
    "    savefig_fname = kwargs['savefig_fname']\n",
    "    data_to_consider = kwargs['data_to_consider']\n",
    "    bin_width = kwargs.pop('bin_width', 0.05)\n",
    "    bin_begin = kwargs.pop('bin_begin', 0.0)\n",
    "    density = kwargs.pop('hist_pdf_flag', True)\n",
    "    rnn_wt_extension = kwargs.pop('rnn_wt_extension', 'h5')\n",
    "    ae_load_file = kwargs.pop('ae_load_file', None)\n",
    "    ae_wt_file = kwargs.pop('ae_wt_file', None)\n",
    "    rnn_load_file = kwargs.pop('rnn_load_file', None)\n",
    "    rnn_wt_file = kwargs.pop('rnn_wt_file', None)\n",
    "    use_ae_data = kwargs.pop('use_ae_data', True)\n",
    "    \n",
    "    if ae_load_file == None:\n",
    "        ae_load_file = dir_name_AR_AErnn+'/final_net/final_net-{}_outsteps_ae_class_dict.txt'.format(num_outsteps)\n",
    "    if ae_wt_file == None:\n",
    "        ae_wt_file = dir_name_AR_AErnn+'/final_net/final_net-{}_outsteps_ae_weights.h5'.format(num_outsteps)\n",
    "\n",
    "    if rnn_load_file == None:\n",
    "        rnn_load_file = dir_name_AR_AErnn+'/final_net/final_net-{}_outsteps_rnn_class_dict.txt'.format(num_outsteps)\n",
    "    if rnn_wt_file == None:\n",
    "        rnn_wt_file = dir_name_AR_AErnn+'/final_net/final_net-{}_outsteps_rnn_weights.'.format(num_outsteps)+rnn_wt_extension\n",
    "    \n",
    "    if use_ae_data == True:\n",
    "        ae_net = Autoencoder(data_rnn_input.shape[-1], load_file=ae_load_file)\n",
    "        ae_net.load_weights_from_file(ae_wt_file)\n",
    "    else:\n",
    "        ae_net = None\n",
    "        normalization_constant_arr_aedata = normalization_constant_arr_rnn\n",
    "        normalization_constant_arr_rnn = None\n",
    "\n",
    "    rnn_net = AR_RNN(\n",
    "        load_file=rnn_load_file,\n",
    "        T_input=T_sample_input_rnn,\n",
    "        T_output=T_sample_output_rnn,\n",
    "        stddev=0.0,\n",
    "        batch_size=1,\n",
    "        # stateful=stateful,\n",
    "    )\n",
    "    rnn_dd = rnn_net.data_dim\n",
    "    try:\n",
    "        ndim1 = len(rnn_dd)\n",
    "    except:\n",
    "        rnn_dd = (rnn_dd,)\n",
    "    rnn_net.build(input_shape=(batch_size, data_rnn_input.shape[1]) + tuple(rnn_dd))\n",
    "    rnn_net.load_weights_from_file(rnn_wt_file)\n",
    "    \n",
    "    normalization_constant_arr_rnn = normalization_constant_arr_rnn.reshape((2,)+tuple(rnn_dd))\n",
    "    \n",
    "    AR_AERNN_net = AR_AERNN(\n",
    "        ae_net,\n",
    "        rnn_net,\n",
    "        normalization_constant_arr_rnn,\n",
    "        normalization_constant_arr_aedata,\n",
    "        0.0,\n",
    "        time_stddev_ogdata,\n",
    "        time_mean_ogdata,\n",
    "    )\n",
    "    AR_AERNN_net.build(input_shape=(batch_size,)+data_rnn_input.shape[1:])\n",
    "    \n",
    "    data_in_og = data_rnn_input\n",
    "    data_out_og = data_rnn_output\n",
    "\n",
    "    num_runs = np.min([num_runs, data_in_og.shape[0]])\n",
    "    print('num_runs :', num_runs)\n",
    "\n",
    "    # data_idx_arr = np.arange(data_in_og.shape[0])\n",
    "    # np.random.shuffle(data_idx_arr)\n",
    "    data_idx_arr = np.linspace(0, data_in_og.shape[0]-1, num_runs, dtype=np.int32)\n",
    "\n",
    "    prediction_horizon_arr = np.empty(shape=num_runs)\n",
    "    # pod_eigvals_dataout_arr = np.empty(shape=(num_runs, data_out_og.shape[-1]))\n",
    "    # pod_eigvals_pred_arr = np.empty(shape=(num_runs, data_out_og.shape[-1]))\n",
    "    # pod_covmat_dataout = np.zeros(shape=(data_out_og.shape[-1], data_out_og.shape[-1]))\n",
    "    # pod_covmat_pred = np.zeros(shape=(data_out_og.shape[-1], data_out_og.shape[-1]))\n",
    "\n",
    "    # prediction = rnn_net.predict(data_in_og[data_idx_arr[0:num_runs], :, :])\n",
    "    prediction = AR_AERNN_net(data_in_og[data_idx_arr[0:num_runs], :, :], training=False).numpy()\n",
    "    prediction = invert_normalization(prediction, normalization_constant_arr_aedata)\n",
    "\n",
    "    energySpectrum_dataout = 0.0\n",
    "    energySpectrum_pred = 0.0\n",
    "    \n",
    "    TKE_pred = np.zeros(shape=data_in_og.shape[-2:])\n",
    "    TKE_true = np.zeros_like(TKE_pred)\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        data_idx = data_idx_arr[i]\n",
    "\n",
    "        for j in range(len(rnn_data_boundary_idx_arr)):\n",
    "            if data_idx < rnn_data_boundary_idx_arr[j]:\n",
    "                case_idx = j\n",
    "                break\n",
    "        lyap_time = 1/0.065#lyapunov_time_arr[j]\n",
    "\n",
    "        data_out = data_out_og[data_idx]\n",
    "        # data_out = rescale_data(data_out, normalization_arr)\n",
    "        data_out = invert_normalization(data_out, normalization_constant_arr_aedata)\n",
    "        \n",
    "        TKE_pred += 0.5*np.mean(np.sum((prediction[i] - time_mean_ogdata)**2, axis=-3), axis=0)\n",
    "        TKE_true += 0.5*np.mean(np.sum((data_out - time_mean_ogdata)**2, axis=-3), axis=0)\n",
    "        \n",
    "        # pod_dataout = data_out - np.mean(data_out, axis=0)\n",
    "        # pod_dataout = np.matmul(pod_dataout.transpose(), pod_dataout) / (pod_dataout.shape[0] - 1)\n",
    "        # pod_covmat_dataout += pod_dataout\n",
    "        # pod_dataout = np.abs(np.linalg.eigvals(pod_dataout))\n",
    "        # pod_dataout = np.sort(pod_dataout)\n",
    "        # pod_dataout = pod_dataout[::-1]\n",
    "        # pod_eigvals_dataout_arr[i, :] = pod_dataout\n",
    "        \n",
    "        # pod_prediction = prediction[i, :, :] - np.mean(prediction[i, :, :], axis=0)\n",
    "        # pod_prediction = np.matmul(pod_prediction.transpose(), pod_prediction) / (pod_prediction.shape[0] - 1)\n",
    "        # pod_covmat_pred += pod_prediction\n",
    "        # pod_prediction = np.abs(np.linalg.eigvals(pod_prediction))\n",
    "        # pod_prediction = np.sort(pod_prediction)\n",
    "        # pod_prediction = pod_prediction[::-1]\n",
    "        # pod_eigvals_pred_arr[i, :] = pod_prediction\n",
    "        \n",
    "        # FourierCoeffs_dataout = fft.fft(data_out, axis=1)\n",
    "        # energySpectrum_dataout_i = FourierCoeffs_dataout.real**2 + FourierCoeffs_dataout.imag**2\n",
    "        # energySpectrum_dataout_i = np.mean(energySpectrum_dataout_i, axis=0)\n",
    "        # normalizer = np.sum(energySpectrum_dataout_i)\n",
    "        # energySpectrum_dataout_i = energySpectrum_dataout_i / normalizer\n",
    "        # energySpectrum_dataout = (i*energySpectrum_dataout + energySpectrum_dataout_i)/(i+1)\n",
    "        \n",
    "        # FourierCoeffs_pred = fft.fft(prediction[i, :, :], axis=1)\n",
    "        # energySpectrum_pred_i = FourierCoeffs_pred.real**2 + FourierCoeffs_pred.imag**2\n",
    "        # energySpectrum_pred_i = np.mean(energySpectrum_pred_i, axis=0)\n",
    "        # energySpectrum_pred_i = energySpectrum_pred_i / normalizer\n",
    "        # energySpectrum_pred = (i*energySpectrum_pred + energySpectrum_pred_i)/(i+1)\n",
    "        \n",
    "        # prediction = rnn_net.predict(data_in_og[data_idx:data_idx+1, :, :])\n",
    "\n",
    "        ### Error and prediction horizon\n",
    "        # error = np.linalg.norm(data_out[:, :] - prediction[i, :, :], axis=1)\n",
    "        error = (data_out - prediction[i])**2\n",
    "        # error /= norm_sq_time_average(data_out)**0.5\n",
    "        error = np.divide(error, time_stddev_ogdata**2)\n",
    "        error = error.reshape(error.shape[0], -1)\n",
    "        error = np.mean(error, axis=1)**0.5\n",
    "\n",
    "        predhor_idx = np.where(error >= error_threshold)[0]\n",
    "        if predhor_idx.shape[0] == 0:\n",
    "            predhor_idx = error.shape[0]\n",
    "        else:\n",
    "            predhor_idx = predhor_idx[0]\n",
    "\n",
    "        prediction_horizon_arr[i] = predhor_idx*dt_rnn/lyap_time\n",
    "\n",
    "    # pod_eigvals_dataout_arr_mean = np.mean(pod_eigvals_dataout_arr, axis=0)\n",
    "    # pod_eigvals_pred_arr_mean = np.mean(pod_eigvals_pred_arr, axis=0)\n",
    "    \n",
    "    # pod_covmat_dataout /= num_runs\n",
    "    # pod_covmat_pred /= num_runs\n",
    "\n",
    "    TKE_pred /= num_runs\n",
    "    TKE_true /= num_runs\n",
    "    \n",
    "    median_idx = int(np.round(0.5*num_runs-1))\n",
    "    quartile_1_idx = int(np.round(0.25*num_runs-1))\n",
    "    quartile_3_idx = int(np.round(0.75*num_runs-1))\n",
    "\n",
    "    prediction_horizon_arr.sort()\n",
    "\n",
    "    median = prediction_horizon_arr[median_idx]\n",
    "    quartile_1 = prediction_horizon_arr[quartile_1_idx]\n",
    "    quartile_3 = prediction_horizon_arr[quartile_3_idx]\n",
    "    IQR = quartile_3 - quartile_1\n",
    "\n",
    "    prediction_horizon = np.mean(prediction_horizon_arr)\n",
    "    stddev_ph = np.std(prediction_horizon_arr)\n",
    "\n",
    "    s = 'error_threshold = {}\\n'.format(error_threshold)\n",
    "    s += 'prediction_horizon : {}, median : {}\\n'.format(prediction_horizon, median)\n",
    "    s += 'ph_min : {}, ph_max : {}\\n'.format(prediction_horizon_arr.min(), prediction_horizon_arr.max())\n",
    "    s += 'stddev : {}, IQR : {}\\n'.format(stddev_ph, IQR)\n",
    "    s += '1st quartile : {}, 3rd quartile : {}'.format(quartile_1, quartile_3)\n",
    "\n",
    "    print('\\n'+s)\n",
    "    \n",
    "    if savefig_fname != None:\n",
    "        npsavedata_fname = '/prediction_horizons-'+data_to_consider+'data--{}outsteps'.format(num_outsteps)\n",
    "        np.savez(\n",
    "            dir_name_AR_AErnn+npsavedata_fname,\n",
    "            prediction_horizon_arr=prediction_horizon_arr,\n",
    "            error_threshold=error_threshold,\n",
    "        )\n",
    "\n",
    "        with open(dir_name_AR_AErnn+npsavedata_fname+'--statistics.txt', 'w') as fl:\n",
    "            fl.write(s)\n",
    "        # npsavepod_fname = '/pod_eigvals-'+data_to_consider+'data--{}outsteps'.format(num_outsteps)\n",
    "        # np.savez(\n",
    "        #     dir_name_AR_AErnn+npsavepod_fname,\n",
    "        #     pod_eigvals_dataout_arr=pod_eigvals_dataout_arr,\n",
    "        #     pod_eigvals_pred_arr=pod_eigvals_pred_arr,\n",
    "        # )\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    bin_end = bin_width*np.round((np.max(prediction_horizon_arr)+0.5*bin_width)//bin_width)\n",
    "    nbins = int(np.round(bin_end/bin_width))\n",
    "\n",
    "    ax.hist(prediction_horizon_arr, bins=nbins, range = [bin_begin, bin_end], density=density)\n",
    "    ax.axvline(prediction_horizon, linewidth=0.9, linestyle='--', color='k')\n",
    "\n",
    "    ax.set_xlabel('Prediction Horizon (Lyapunov times)')\n",
    "    ax.set_ylabel('PDF')\n",
    "\n",
    "    ax.grid(True)\n",
    "    # ax.set_axisbelow(True)\n",
    "\n",
    "    ax.text(\n",
    "        0.01 + ax.transAxes.inverted().transform(ax.transData.transform([prediction_horizon, 0]))[0],\n",
    "        0.8,\n",
    "        'mean',\n",
    "        rotation=90,\n",
    "        verticalalignment='bottom',\n",
    "        horizontalalignment='left',\n",
    "        bbox=dict(facecolor=np.array([255,255,153])/255, alpha=1, boxstyle='square,pad=0.2'),\n",
    "        transform=ax.transAxes\n",
    "    )\n",
    "\n",
    "    text_xy = [0.95, 0.95]\n",
    "    ax.text(\n",
    "        text_xy[0],\n",
    "        text_xy[1],\n",
    "        'mean : {:.4f}\\nmax : {:.4f}\\nmin : {:.4f}\\nstddev : {:.4f}'.format(\n",
    "            prediction_horizon,\n",
    "            np.max(prediction_horizon_arr),\n",
    "            np.min(prediction_horizon_arr),\n",
    "            stddev_ph,\n",
    "        ),\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round\",\n",
    "            ec=(0.6, 0.6, 1),\n",
    "            fc=(0.9, 0.9, 1),\n",
    "        ),\n",
    "        # bbox=dict(facecolor='C0', alpha=0.5, boxstyle='round,pad=0.2'),\n",
    "        horizontalalignment='right',\n",
    "        verticalalignment='top'\n",
    "    )\n",
    "\n",
    "    ax.set_title('nbins = {}'.format(nbins))\n",
    "\n",
    "    if savefig_fname is not None:\n",
    "        fig.savefig(\n",
    "            dir_name_AR_AErnn+'/plots/'+savefig_fname+'--{}outsteps.png'.format(num_outsteps),\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')\n",
    "        fig.clear()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        print('')\n",
    "\n",
    "    '''\n",
    "    fig_eigvals, ax_eigvals = plt.subplots()\n",
    "    ax_eigvals.semilogy(pod_eigvals_dataout_arr_mean, linestyle='--', marker='s', linewidth=0.9, markersize=2)\n",
    "    ax_eigvals.semilogy(pod_eigvals_pred_arr_mean, linestyle='--', marker='^', linewidth=0.9, markersize=2)\n",
    "    ax_eigvals.grid(True)\n",
    "    ax_eigvals.legend([r'True Data', r'Predicted Data'])\n",
    "    ax_eigvals.set_axisbelow(True)\n",
    "    ax_eigvals.set_title('Eigenvalues of the covariance matrix')\n",
    "    if savefig_fname is not None:\n",
    "        fig_eigvals.savefig(\n",
    "            dir_name_AR_AErnn+'/plots/'+savefig_fname+'--eigvals--{}outsteps.png'.format(num_outsteps),\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')\n",
    "        fig_eigvals.clear()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        print('')\n",
    "\n",
    "    '''\n",
    "\n",
    "    fig_TKE = plt.figure(figsize=(5.0*3, 5.0*1))\n",
    "    subplot1 = 1\n",
    "    subplot2 = subplot1 + 1\n",
    "    \n",
    "    normalizer = np.sum(TKE_true)\n",
    "    # TKE_pred /= normalizer\n",
    "    # TKE_true /= normalizer\n",
    "    \n",
    "    vmin_snap = 0.005\n",
    "    vmax_snap = 0.005\n",
    "    vmin = np.min([\n",
    "        TKE_pred.min(),\n",
    "        TKE_true.min()\n",
    "    ])\n",
    "    # vmin = min(vmin, -1.0)\n",
    "    # vmin = -vmin_snap*np.round(-vmin/vmin_snap + 0.5)\n",
    "    vmax = np.max([\n",
    "        TKE_pred.max(),\n",
    "        TKE_true.max()\n",
    "    ])\n",
    "    # vmax = max(vmax, 1.0)\n",
    "    # vmax = vmax_snap*np.round(vmax/vmax_snap + 0.5)\n",
    "\n",
    "    # plotting the original data\n",
    "    ax_TKE_orig = fig_TKE.add_subplot(1, 3, subplot1)\n",
    "    im_orig = ax_TKE_orig.imshow(\n",
    "        TKE_true,\n",
    "        aspect='equal',\n",
    "        origin='upper',\n",
    "        vmin=vmin,\n",
    "        vmax=vmax\n",
    "    )\n",
    "    ax_TKE_orig.title.set_text('TKE (True Data)')\n",
    "    # xticks = np.arange(0, N, int((xticks_snapto+0.5*delta_t)//delta_t))\n",
    "    # ax_covmat_orig.set_xticks(ticks=xticks)\n",
    "    # ax_covmat_orig.set_xticklabels(np.round(xticks*delta_t, 1))\n",
    "    # ax_covmat_orig.tick_params(axis='x', rotation=270+45)\n",
    "    # yticks = np.linspace(0, 1, num_yticks)*(len(xgrid)-1)\n",
    "    # yticklabels = np.round(xgrid[0]+np.linspace(0, 1, yticks.shape[0])*(xgrid[-1]-xgrid[0]), 2)\n",
    "    # ax_covmat_orig.set_yticks(ticks=yticks)\n",
    "    # ax_covmat_orig.set_yticklabels(yticklabels)\n",
    "    # ax_covmat_orig.set_xlabel(xlabel)\n",
    "    # ax_covmat_orig.set_ylabel(ylabel)\n",
    "\n",
    "    # plotting the predicted data\n",
    "    ax_TKE_predict = fig_TKE.add_subplot(1, 3, subplot2, sharey=ax_TKE_orig, sharex=ax_TKE_orig)\n",
    "    im_predict = ax_TKE_predict.imshow(\n",
    "        TKE_pred,\n",
    "        aspect='equal',\n",
    "        origin='upper',\n",
    "        vmin=vmin,\n",
    "        vmax=vmax\n",
    "    )\n",
    "    ax_TKE_predict.title.set_text('TKE (Predicted Data)')\n",
    "    # ax_covmat_predict.tick_params(axis='x', rotation=270+45)\n",
    "    # ax_covmat_predict.set_xlabel(xlabel)\n",
    "    # ax_covmat_predict.set_ylabel(ylabel)\n",
    "\n",
    "    # subplots adjustment to account for colorbars\n",
    "    fig_TKE.subplots_adjust(\n",
    "        bottom=0.2,\n",
    "        left=0.1,\n",
    "    )\n",
    "\n",
    "    # original data and recon data colorbar\n",
    "    cb_xbegin = ax_TKE_orig.transData.transform([0, 0])\n",
    "    cb_xbegin = fig_TKE.transFigure.inverted().transform(cb_xbegin)[0]\n",
    "    cb_xend = ax_TKE_predict.transData.transform([TKE_true.shape[-1], 0])\n",
    "    cb_xend = fig_TKE.transFigure.inverted().transform(cb_xend)[0]\n",
    "\n",
    "    cb_ax = fig_TKE.add_axes([cb_xbegin, 0.0, cb_xend-cb_xbegin, 0.025])\n",
    "    cbar = fig_TKE.colorbar(im_predict, cax=cb_ax, orientation='horizontal')\n",
    "\n",
    "    # computing the normalized error\n",
    "    subplot3 = subplot2+1\n",
    "    error = np.abs(TKE_pred-TKE_true)\n",
    "    vmax_error_snap = 0.8\n",
    "    vmax_error = np.max(error)\n",
    "    # vmax_error = vmax_error_snap*np.round(vmax_error/vmax_error_snap + 0.5)\n",
    "    # error = 100*error / np.abs(pod_covmat_dataout)\n",
    "    # plotting the normalized error\n",
    "    ax_TKE_error = fig_TKE.add_subplot(1, 3, subplot3, sharey=ax_TKE_orig, sharex=ax_TKE_orig)\n",
    "    im_error = ax_TKE_error.imshow(\n",
    "        error,\n",
    "        aspect='equal',\n",
    "        origin='upper',\n",
    "        vmin=0.0,\n",
    "        vmax=vmax_error,\n",
    "    )\n",
    "    ax_TKE_error.title.set_text(r'Error')\n",
    "    # ax_error.tick_params(axis='x', rotation=270+45)\n",
    "    # ax_error.set_xlabel(xlabel)\n",
    "    # ax_error.set_ylabel(ylabel)\n",
    "\n",
    "    # error colorbar\n",
    "    cbe_xbegin = ax_TKE_error.transData.transform([0, 0])\n",
    "    cbe_xbegin = fig_TKE.transFigure.inverted().transform(cbe_xbegin)[0]\n",
    "    cbe_xend = ax_TKE_error.transData.transform([TKE_true.shape[-1], 0])\n",
    "    cbe_xend = fig_TKE.transFigure.inverted().transform(cbe_xend)[0]\n",
    "    error_cb_ax = fig_TKE.add_axes([cbe_xbegin, 0.0, cbe_xend-cbe_xbegin, 0.025])\n",
    "    cbar_error = fig_TKE.colorbar(im_error, cax=error_cb_ax, orientation='horizontal')\n",
    "    \n",
    "    if savefig_fname is not None:\n",
    "        fig_TKE.savefig(\n",
    "            dir_name_AR_AErnn+'/plots/'+savefig_fname+'--TKE--{}outsteps.png'.format(num_outsteps),\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')\n",
    "        fig_TKE.clear()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        print('')\n",
    "    \n",
    "\n",
    "    '''\n",
    "    k = fft.fftfreq(numpoints_xgrid, d=1/numpoints_xgrid)\n",
    "    idx = np.where(k<0)[0]\n",
    "    k[idx] += numpoints_xgrid\n",
    "    \n",
    "    fig_Fourier, ax_Fourier = plt.subplots()\n",
    "    ax_Fourier.semilogy(k, energySpectrum_dataout, linestyle='--', marker='s', linewidth=0.9, markersize=2)\n",
    "    ax_Fourier.semilogy(k, energySpectrum_pred, linestyle='--', marker='^', linewidth=0.9, markersize=2)\n",
    "    ax_Fourier.grid(True)\n",
    "    ax_Fourier.legend([r'True Data', r'Predicted Data'])\n",
    "    ax_Fourier.set_axisbelow(True)\n",
    "    ax_Fourier.set_title(r'Squared magnitude of Fourier coefficients')\n",
    "    ax_Fourier.set_ylabel(r'$\\|a_k \\|^2 \\ / \\ \\left( \\sum_j \\|a_j^{(true)} \\|^2 \\right)$')\n",
    "    if savefig_fname is not None:\n",
    "        fig_Fourier.savefig(\n",
    "            dir_name_AR_AErnn+'/plots/'+savefig_fname+'--FourierCoeffs--{}outsteps.png'.format(num_outsteps),\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')\n",
    "        fig_Fourier.clear()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        print('')\n",
    "    '''\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : ZERO <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "num_runs : 55\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.33711363636363645, median : 0.325\n",
      "ph_min : 0.0, ph_max : 1.0725\n",
      "stddev : 0.20386449275182447, IQR : 0.24375\n",
      "1st quartile : 0.21125000000000002, 3rd quartile : 0.455\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 5 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "num_runs : 55\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.7049545454545456, median : 0.65\n",
      "ph_min : 0.0, ph_max : 2.3075\n",
      "stddev : 0.49159445844135385, IQR : 0.52\n",
      "1st quartile : 0.35750000000000004, 3rd quartile : 0.8775000000000001\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 10 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "num_runs : 55\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.6443863636363637, median : 0.63375\n",
      "ph_min : 0.0, ph_max : 2.015\n",
      "stddev : 0.43764029391670983, IQR : 0.4225\n",
      "1st quartile : 0.35750000000000004, 3rd quartile : 0.78\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 30 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Autoencoder.call at 0x7f6a01b11510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "num_runs : 55\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.6881136363636365, median : 0.65\n",
      "ph_min : 0.0, ph_max : 1.9500000000000002\n",
      "stddev : 0.4406250643279949, IQR : 0.6012500000000001\n",
      "1st quartile : 0.39, 3rd quartile : 0.9912500000000001\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 50 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Autoencoder.call at 0x7f69fbd37910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "num_runs : 55\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.7699545454545457, median : 0.65\n",
      "ph_min : 0.0, ph_max : 2.6\n",
      "stddev : 0.5602362003714252, IQR : 0.7150000000000002\n",
      "1st quartile : 0.39, 3rd quartile : 1.1050000000000002\n"
     ]
    }
   ],
   "source": [
    "for kk in range(num_outsteps.shape[0]+1):    \n",
    "    total_s_len = 80\n",
    "    \n",
    "    if kk == 0:\n",
    "        num_outsteps_kk = 'ZERO'\n",
    "        load_file_rnn = dir_name_rnn + '/final_net/final_net_class_dict.txt'\n",
    "        wt_file_rnn = dir_name_rnn+'/final_net/final_net_ESN_weights.hdf5'\n",
    "\n",
    "        load_file_ae = dir_name_ae+'/final_net/final_net_class_dict.txt'\n",
    "        wt_file_ae = dir_name_ae+'/final_net/final_net_ae_weights.h5'\n",
    "    else:\n",
    "        num_outsteps_kk = num_outsteps[kk-1]\n",
    "        load_file_rnn = None\n",
    "        wt_file_rnn = None\n",
    "\n",
    "        load_file_ae = None\n",
    "        wt_file_ae = None\n",
    "    \n",
    "    \n",
    "    sep_lr_s = ' num_outsteps : {} '.format(num_outsteps_kk)\n",
    "    \n",
    "    sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'>' + sep_lr_s\n",
    "    sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'<'\n",
    "    print('\\n\\n' + '*'*len(sep_lr_s))\n",
    "    print('' + sep_lr_s+'')\n",
    "    print('*'*len(sep_lr_s) + '\\n\\n')\n",
    "\n",
    "    prediction_horizons(\n",
    "        num_outsteps=num_outsteps_kk,\n",
    "        dir_name_AR_AErnn=dir_name_AR_AErnn,\n",
    "        Autoencoder=Autoencoder,\n",
    "        data_rnn_input=data_rnn_input,\n",
    "        data_rnn_output=data_rnn_output,\n",
    "        AR_RNN=AR_RNN,\n",
    "        T_sample_input_rnn=T_sample_input_cd,\n",
    "        T_sample_output_rnn=T_sample_output_cd,\n",
    "        AR_AERNN=AR_AERNN,\n",
    "        normalization_constant_arr_rnn=normalization_arr_rnn,\n",
    "        normalization_constant_arr_aedata=normalization_constant_arr_aedata,\n",
    "        time_stddev_ogdata=time_stddev_ogdata,\n",
    "        time_mean_ogdata=time_mean_ogdata,\n",
    "        batch_size=data_rnn_input.shape[0],\n",
    "        num_runs=100,\n",
    "        error_threshold=0.5,\n",
    "        rnn_data_boundary_idx_arr=rnn_data_boundary_idx_arr,\n",
    "        lyapunov_time_arr=[1/0.065],#lyapunov_time_arr,\n",
    "        savefig_fname='post-ARtraining'+'_'+data_to_consider+'data',\n",
    "        data_to_consider=data_to_consider,\n",
    "        bin_width=0.05,\n",
    "        bin_begin=0.0,\n",
    "        rnn_wt_extension='hdf5', # 'h5' for tf saved rnns, 'hdf5' for my ESNs\n",
    "        rnn_load_file=load_file_rnn,\n",
    "        rnn_wt_file=wt_file_rnn,\n",
    "        ae_load_file=load_file_ae,\n",
    "        ae_wt_file=wt_file_ae,\n",
    "        use_ae_data=use_ae_data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
