{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/new_ks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data_KS, plot_latent_states_KS , readAndReturnLossHistories, plot_histogram_and_save\n",
    "from tools.ae_v1 import Autoencoder\n",
    "from tools.GRU_SingleStep_v1 import RNN_GRU\n",
    "# from tools.LSTM_SingleStep_v2 import RNN_GRU\n",
    "# from tools.SimpleRNN_SingleStep_v2 import RNN_GRU\n",
    "from tools.GRU_AR_v1 import AR_RNN_GRU as AR_RNN\n",
    "from tools.AEGRU_AR_v1 import AR_AERNN_GRU as AR_AERNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": [
    "# setting seed for PRNGs\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    prng_seed = 42\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 15:15:30.294723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:30.295199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:30.364549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:30.364864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:30.365128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:30.365372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:30.366652: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 15:15:30.367147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:30.367361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:30.367574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:31.023435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:31.023705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:31.023902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-28 15:15:31.024060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3365 MB memory:  -> device: 1, name: Quadro K2200, pci bus id: 0000:03:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 1\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "2.8.1\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print('')\n",
    "print(tf.config.list_logical_devices())\n",
    "print('')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005\n",
      "dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_ae/ae_013\n",
      "data_dir_idx: 000\n",
      "normalize_flag_ogdata: False\n"
     ]
    }
   ],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # making RNN save directory\n",
    "    dir_name_rnn = os.getcwd() + dir_sep + 'saved_rnn'\n",
    "    if not os.path.isdir(dir_name_rnn):\n",
    "        os.makedirs(dir_name_rnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'rnn_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_rnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_rnn = dir_name_rnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_rnn)\n",
    "    os.makedirs(dir_name_rnn+dir_sep+'plots')\n",
    "\n",
    "    # whether to use AE data or just work on raw data\n",
    "    use_ae_data = True # if false, specifying ae_idx will only show which dataset to use\n",
    "\n",
    "    # autoencoder directory\n",
    "    ae_idx = '013'\n",
    "    dir_name_ae = os.getcwd()+'{ds}saved_ae{ds}ae_'.format(ds=dir_sep)+ae_idx\n",
    "else:\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_rnn/rnn_015'\n",
    "\n",
    "    # reading AE directory\n",
    "    with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_dict = eval(''.join(lines))\n",
    "\n",
    "    try:\n",
    "        use_ae_data = params_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in sim_data_AE_params, set to True.\")\n",
    "        normalize_dataset = True\n",
    "    \n",
    "    dir_name_ae = params_dict['dir_name_ae']\n",
    "    ae_idx = dir_name_ae[-3:]\n",
    "    dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "\n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_rnn_dict['T_sample_output']\n",
    "    T_offset = params_rnn_dict['T_offset']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in RNN_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "    try:\n",
    "        dense_layer_act_func = params_rnn_dict['dense_layer_act_func']\n",
    "    except:\n",
    "        print(\"'dense_layer_act_func' not present in RNN_specific_data, set to 'linear'.\")\n",
    "        dense_layer_act_func = 'linear'\n",
    "    try:\n",
    "        stateful = params_rnn_dict['stateful']\n",
    "    except:\n",
    "        print(\"'stateful' not present in RNN_specific_data, set to True.\")\n",
    "        stateful = True\n",
    "    try:\n",
    "        use_learnable_state = params_rnn_dict['use_learnable_state']\n",
    "    except:\n",
    "        print(\"'use_learnable_state' not present in RNN_specific_data, set to False.\")\n",
    "        use_learnable_state = False\n",
    "    try:\n",
    "        use_weights_post_dense = params_rnn_dict['use_weights_post_dense']\n",
    "    except:\n",
    "        print(\"'use_weights_post_dense' not present in RNN_specific_data, set to False.\")\n",
    "        use_weights_post_dense = False\n",
    "    try:\n",
    "        use_ae_data = params_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "\n",
    "    \n",
    "\n",
    "    normalization_arr = None\n",
    "    try:\n",
    "        with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        rnn_norm_arr_dict = eval(lines)\n",
    "        normalization_arr = rnn_norm_arr_dict['normalization_arr']\n",
    "    except:\n",
    "        pass\n",
    "    if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "        with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "            normalization_arr = fl['normalization_arr'][0]\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in ae_data, set to 'True'.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state = params_dict['init_state']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "numpoints_xgrid = params_dict['numpoints_xgrid']\n",
    "length = params_dict['length']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "xgrid = length*np.linspace(0, 1, numpoints_xgrid)\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data']\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case : 1, lyapunov exponent : 0.06465670311438651, lyapunov time : 15.466300964355469s\n"
     ]
    }
   ],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delaing with normalizing the data before feeding into autoencoder\n",
    "num_params = params_mat.shape[1]\n",
    "og_vars = all_data.shape[1]\n",
    "if alldata_withparams_flag == True:\n",
    "    og_vars -= num_params\n",
    "\n",
    "time_stddev_ogdata = np.std(all_data[:, 0:og_vars], axis=0)\n",
    "time_mean_ogdata = np.mean(all_data[:, 0:og_vars], axis=0)\n",
    "    \n",
    "if use_ae_data == True:\n",
    "    if ae_data_with_params == True and alldata_withparams_flag == False:\n",
    "        new_all_data = np.empty(shape=(all_data.shape[0], og_vars+num_params), dtype=FTYPE)\n",
    "        new_all_data[:, 0:og_vars] = all_data[:, 0:og_vars]\n",
    "        del(all_data)\n",
    "        all_data = new_all_data\n",
    "        prev_idx = 0\n",
    "        for i in range(boundary_idx_arr.shape[0]):\n",
    "            all_data[prev_idx:boundary_idx_arr[i], num_params:] = params_mat[i]\n",
    "            prev_idx = boundary_idx_arr[i]\n",
    "\n",
    "    if normalizeforae_flag == True:\n",
    "        for i in range(all_data.shape[1]):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "    if ae_data_with_params == False:\n",
    "        all_data = all_data[:, 0:og_vars]\n",
    "else:\n",
    "    # using raw data, neglecting the params attached (if any)\n",
    "    all_data = all_data[:, 0:og_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "    wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "    ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Zl6ZvgtNtA_u",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776554,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lXpoaKRIneQc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1667868777509,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Q3a8HHyvneQc",
    "outputId": "51084913-6faf-4bb5-db69-2cbea705dd28"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "if use_ae_data == True:\n",
    "    latent_states_all = ae_net.encoder_net.predict(all_data)\n",
    "    # del(all_data)\n",
    "else:\n",
    "    latent_states_all = all_data\n",
    "num_latent_states = latent_states_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1667868778304,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wjgPNitSrt5p",
    "outputId": "0c916524-33ec-47bf-a16a-51e53d2e25f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868778305,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wnLnqg0Jrt5t"
   },
   "outputs": [],
   "source": [
    "# for i in range(ae_net.layers):\n",
    "#     tf.keras.utils.plot_model(\n",
    "#         ae_net.layers[i],\n",
    "#         to_file=dir_name_ae+'/plots/netlayer_{}.png'.format(i),\n",
    "#         show_shapes=True,\n",
    "#         dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BOJE8vREtque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fwjcsAxKneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "aFd7XgwVneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    num_lyaptimesteps_totrain = 5 # int(5000/np.mean(lyapunov_time_arr))#\n",
    "    dt_rnn = 0.2\n",
    "    T_sample_input = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_sample_output = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_offset = dt_rnn\n",
    "    normalize_dataset = True # whether the data for the RNN should be normalized by the dataset's mean and std\n",
    "    normalization_arr = None\n",
    "    skip_intermediate = 'full sample'\n",
    "    noise_type = 'normal' # can be 'uniform' or 'normal'\n",
    "\n",
    "    # can be 'minmax', 'minmax2', 'stddev', or a list with\n",
    "    # sequential order of any of these; if it is 'minmax'\n",
    "    # then stddev_multiplier has no effect\n",
    "    normalization_type = 'stddev'\n",
    "    stddev_multiplier = 3\n",
    "\n",
    "    dense_layer_act_func = ['tanh']\n",
    "    use_weights_post_dense = True\n",
    "    stateful = True\n",
    "    use_learnable_state = False\n",
    "    use_trainable_weights_with_reslayers = False\n",
    "        \n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "        \n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "\n",
    "    # saving simulation data\n",
    "    sim_data = {\n",
    "        'params_mat':params_mat,\n",
    "        'init_state_mat':init_state_mat,\n",
    "        't0':t0,\n",
    "        'T':T,\n",
    "        'delta_t':delta_t,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'dir_name_ae':dir_name_ae,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'use_ae_data':use_ae_data,\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'sim_data_AE_params.txt', 'w') as f:\n",
    "        f.write(str(sim_data))\n",
    "        \n",
    "    # saving RNN specific data\n",
    "    RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_lyaptimesteps_totrain':num_lyaptimesteps_totrain,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':RNN_GRU.__module__,\n",
    "        'noise_type':noise_type,\n",
    "        'normalization_type':normalization_type,\n",
    "        'dense_layer_act_func':dense_layer_act_func,\n",
    "        'stateful':stateful,\n",
    "        'use_learnable_state':use_learnable_state,\n",
    "        'use_weights_post_dense':use_weights_post_dense,\n",
    "        'use_trainable_weights_with_reslayers':use_trainable_weights_with_reslayers,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "UGnj8uQQ83-y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "0t2_8mzI1fhX"
   },
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    latent_states_all,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=normalize_dataset,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type)\n",
    "    \n",
    "data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "num_samples = rnn_res_dict['num_samples']\n",
    "normalization_arr = rnn_res_dict['normalization_arr']\n",
    "rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "pIsWCXkbr7ws"
   },
   "outputs": [],
   "source": [
    "temp = np.divide(latent_states_all-normalization_arr[0], normalization_arr[1])\n",
    "time_stddev = np.std(temp, axis=0)\n",
    "timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "del(org_data_idx_arr_input)\n",
    "del(org_data_idx_arr_output)\n",
    "del(latent_states_all)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    all_data,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=False,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type,\n",
    "    FTYPE=FTYPE,\n",
    "    ITYPE=ITYPE)\n",
    "    \n",
    "AR_data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "AR_data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "AR_org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "AR_org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "AR_num_samples = rnn_res_dict['num_samples']\n",
    "AR_normalization_arr = rnn_res_dict['normalization_arr']\n",
    "AR_rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']\n",
    "\n",
    "del(all_data)\n",
    "del(AR_org_data_idx_arr_input)\n",
    "del(AR_org_data_idx_arr_output)\n",
    "del(AR_rnn_data_boundary_idx_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Hem_9PUqneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778791,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uskBAAXpneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "\n",
    "# ph computation parameters\n",
    "num_runs = 50\n",
    "T_sample_input_AR_ratio = 1\n",
    "T_sample_output_AR_ratio = 2\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [1e-3, 1e-4, 1e-5]\n",
    "    epochs = 200\n",
    "    patience = 10 # parameter for early stopping\n",
    "    min_delta = 1e-6  # parameter for early stopping\n",
    "    lambda_reg = 5.74802807e-07  # weight for regularizer\n",
    "    train_split = 0.8\n",
    "    val_split = 0.1\n",
    "    test_split = 1 - train_split - val_split\n",
    "    batch_size = 32\n",
    "    fRMS = 9.84468949e-02\n",
    "    zoneout_rate = 1.09893940e-02*2/8\n",
    "    rnncell_dropout_rate = 0.0\n",
    "    denselayer_dropout_rate = 0.0\n",
    "    \n",
    "\n",
    "    stddev = fRMS*timeMeanofSpaceRMS\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'timeMeanofSpaceRMS':timeMeanofSpaceRMS,\n",
    "        'stddev':stddev,\n",
    "        'zoneout_rate':zoneout_rate,\n",
    "        'rnncell_dropout_rate':rnncell_dropout_rate,\n",
    "        'denselayer_dropout_rate':denselayer_dropout_rate,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_rnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr],\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    # dir_name_rnn_og = dir_name_rnn\n",
    "    # dir_name_rnn_temp = '/home/rkaushik/Documents/Thesis/MLROM/CDV/saved_rnn/rnn_'+dir_name_rnn_og[-3:]\n",
    "    # dir_name_rnn = dir_name_rnn_temp\n",
    "\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # setting seed for PRNGs\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "# idx = np.arange(data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# boundary = int(np.round(train_split*data_rnn_input.shape[0]))\n",
    "\n",
    "# training_data_rnn_input = data_rnn_input[idx[0:boundary]]\n",
    "# training_data_rnn_output = data_rnn_output[idx[0:boundary]]\n",
    "\n",
    "# testing_data_rnn_input = data_rnn_input[idx[boundary:]]\n",
    "# testing_data_rnn_output = data_rnn_output[idx[boundary:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1667868779601,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "EENXaWqcKW7j"
   },
   "outputs": [],
   "source": [
    "cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "# idx = np.arange(cum_samples)\n",
    "# np.random.shuffle(idx)\n",
    "num_train_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_val_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_test_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_samples_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "begin_idx = 0\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    num_samples = rnn_data_boundary_idx_arr[i] - begin_idx\n",
    "    num_train_arr[i] = batch_size * (int( (1-test_split-val_split)*num_samples )//batch_size)\n",
    "    num_val_arr[i] = batch_size * (int(val_split*num_samples)//batch_size)\n",
    "    num_test_arr[i] = batch_size * int((num_samples - num_train_arr[i] - num_val_arr[i])//batch_size)\n",
    "    num_samples_arr[i] = num_train_arr[i] + num_val_arr[i] + num_test_arr[i]\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# defining shapes\n",
    "training_input_shape = [np.sum(num_train_arr)]\n",
    "training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "training_output_shape = [np.sum(num_train_arr)]\n",
    "training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "val_input_shape = [np.sum(num_val_arr)]\n",
    "val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "val_output_shape = [np.sum(num_val_arr)]\n",
    "val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "testing_input_shape = [np.sum(num_test_arr)]\n",
    "testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "testing_output_shape = [np.sum(num_test_arr)]\n",
    "testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "# defining required arrays\n",
    "training_data_rnn_input = np.empty(shape=training_input_shape, dtype=FTYPE)\n",
    "training_data_rnn_output = np.empty(shape=training_output_shape, dtype=FTYPE)\n",
    "\n",
    "val_data_rnn_input = np.empty(shape=val_input_shape, dtype=FTYPE)\n",
    "val_data_rnn_output = np.empty(shape=val_output_shape, dtype=FTYPE)\n",
    "\n",
    "testing_data_rnn_input = np.empty(shape=testing_input_shape, dtype=FTYPE)\n",
    "testing_data_rnn_output = np.empty(shape=testing_output_shape, dtype=FTYPE)\n",
    "\n",
    "AR_testing_data_rnn_input = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "AR_testing_data_rnn_output = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "\n",
    "begin_idx = 0\n",
    "training_data_rolling_count = 0\n",
    "val_data_rolling_count = 0\n",
    "testing_data_rolling_count = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    idx = np.arange(begin_idx, rnn_data_boundary_idx_arr[i])\n",
    "    # np.random.shuffle(idx)\n",
    "    # num_samples = idx.shape[0]\n",
    "    # num_train = int( np.round(train_split*num_samples/batch_size) )*batch_size\n",
    "    # num_val = int( np.round(val_split*num_samples/batch_size) )*batch_size\n",
    "    \n",
    "    num_samples = num_samples_arr[i]\n",
    "    num_train = num_train_arr[i]\n",
    "    num_val = num_val_arr[i]\n",
    "    num_test = num_test_arr[i]\n",
    "    \n",
    "    nbatches_train = num_train // batch_size\n",
    "    nbatches_val = num_val // batch_size\n",
    "    nbatches_test = num_test // batch_size\n",
    "\n",
    "    for j in range(batch_size):\n",
    "        training_data_rnn_input[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_input[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        training_data_rnn_output[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_output[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        \n",
    "        val_data_rnn_input[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_input[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "        val_data_rnn_output[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_output[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "\n",
    "        testing_data_rnn_input[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_input[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        testing_data_rnn_output[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_output[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        \n",
    "        AR_testing_data_rnn_input[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = AR_data_rnn_input[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        AR_testing_data_rnn_output[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = AR_data_rnn_output[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "\n",
    "\n",
    "    # training_data_rnn_input[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_input[idx[0:num_train]]\n",
    "    # training_data_rnn_output[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_output[idx[0:num_train]]\n",
    "    training_data_rolling_count += num_train\n",
    "\n",
    "    # val_data_rnn_input[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_input[idx[num_train:num_train+num_val]]\n",
    "    # val_data_rnn_output[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_output[idx[num_train:num_train+num_val]]\n",
    "    val_data_rolling_count += num_val\n",
    "\n",
    "    # num_test = num_samples-num_train-num_val+1\n",
    "    # testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_input[idx[num_train+num_val:]]\n",
    "    # testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_output[idx[num_train+num_val:]]\n",
    "    testing_data_rolling_count += num_test\n",
    "\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# cleaning up\n",
    "del(data_rnn_input)\n",
    "del(data_rnn_output)\n",
    "del(AR_data_rnn_input)\n",
    "del(AR_data_rnn_output)\n",
    "\n",
    "# further shuffling\n",
    "if stateful == False:\n",
    "    idx = np.arange(0, training_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    training_data_rnn_input = training_data_rnn_input[idx]\n",
    "    training_data_rnn_output = training_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, val_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    val_data_rnn_input = val_data_rnn_input[idx]\n",
    "    val_data_rnn_output = val_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, testing_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    testing_data_rnn_input = testing_data_rnn_input[idx]\n",
    "    testing_data_rnn_output = testing_data_rnn_output[idx]\n",
    "\n",
    "    del(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868779603,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8isZN1tYBifp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_runs :  50\n"
     ]
    }
   ],
   "source": [
    "s_in = AR_testing_data_rnn_input.shape\n",
    "AR_testing_data_rnn_input = AR_testing_data_rnn_input.reshape((1, s_in[0]*s_in[1]) + s_in[2:])\n",
    "\n",
    "s_out = AR_testing_data_rnn_output.shape\n",
    "AR_testing_data_rnn_output = AR_testing_data_rnn_output.reshape((1, s_out[0]*s_out[1]) + s_out[2:])\n",
    "\n",
    "T_sample_input_AR = T_sample_input_AR_ratio*np.mean(lyapunov_time_arr)#50.1*dt_rnn\n",
    "num_sample_input_AR = int((T_sample_input_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "T_sample_output_AR = T_sample_output_AR_ratio*np.mean(lyapunov_time_arr)\n",
    "num_sample_output_AR = int((T_sample_output_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "num_offset_AR = num_sample_input_AR\n",
    "T_offset_AR = num_offset_AR*dt_rnn\n",
    "\n",
    "batch_idx = np.random.randint(low=0, high=AR_testing_data_rnn_input.shape[0])\n",
    "maxpossible_num_runs = AR_testing_data_rnn_input.shape[1]-(num_sample_input_AR+num_sample_output_AR)\n",
    "\n",
    "num_runs = np.min([num_runs, maxpossible_num_runs])\n",
    "\n",
    "print('num_runs : ', num_runs)\n",
    "\n",
    "data_idx_arr = np.linspace(0, maxpossible_num_runs-1, num_runs, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779605,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "x3KglJsgneQj"
   },
   "outputs": [],
   "source": [
    "AR_data_in = np.empty(shape=(num_runs, num_sample_input_AR)+tuple(s_in[2:]))\n",
    "AR_data_out = np.empty(shape=(num_runs, num_sample_output_AR)+tuple(s_out[2:]))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    d_idx = data_idx_arr[i]\n",
    "    AR_data_in[i] = AR_testing_data_rnn_input[0, d_idx:d_idx+num_sample_input_AR]\n",
    "    AR_data_out[i] = AR_testing_data_rnn_input[0, d_idx+num_sample_input_AR:d_idx+num_sample_input_AR+num_sample_output_AR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ixetsZHjCMKO"
   },
   "outputs": [],
   "source": [
    "del(AR_testing_data_rnn_input)\n",
    "del(AR_testing_data_rnn_output)\n",
    "AR_testing_data_rnn_input = AR_data_in\n",
    "AR_testing_data_rnn_output = AR_data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   training_data_rnn_input.shape :  (512, 387, 16)\n",
      "  training_data_rnn_output.shape :  (512, 387, 16)\n",
      "    testing_data_rnn_input.shape :  (64, 387, 16)\n",
      "   testing_data_rnn_output.shape :  (64, 387, 16)\n",
      "        val_data_rnn_input.shape :  (64, 387, 16)\n",
      "       val_data_rnn_output.shape :  (64, 387, 16)\n",
      "\n",
      " AR_testing_data_rnn_input.shape :  (50, 77, 64)\n",
      "AR_testing_data_rnn_output.shape :  (50, 155, 64)\n"
     ]
    }
   ],
   "source": [
    "print('   training_data_rnn_input.shape : ', training_data_rnn_input.shape)\n",
    "print('  training_data_rnn_output.shape : ', training_data_rnn_output.shape)\n",
    "print('    testing_data_rnn_input.shape : ', testing_data_rnn_input.shape)\n",
    "print('   testing_data_rnn_output.shape : ', testing_data_rnn_output.shape)\n",
    "print('        val_data_rnn_input.shape : ', val_data_rnn_input.shape)\n",
    "print('       val_data_rnn_output.shape : ', val_data_rnn_output.shape)\n",
    "print('')\n",
    "print(' AR_testing_data_rnn_input.shape : ', AR_testing_data_rnn_input.shape)\n",
    "print('AR_testing_data_rnn_output.shape : ', AR_testing_data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_NSTtZuyneQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeMeanofSpaceRMS : 0.3310418\n",
      "stddev : 0.0325900385617497\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "#     rnn_layers_units = [500]*3\n",
    "    scalar_weights = None\n",
    "#     scalar_weights = [\n",
    "#         0.5, \n",
    "#         0.0, 0.5,\n",
    "#         0.0, 0.0, 1.0,\n",
    "#         1/6, 1/3, 1/3, 1/6\n",
    "#     ] # RK4\n",
    "    # scalar_weights = [\n",
    "    #     1.0,\n",
    "    #     0.25, 0.25,\n",
    "    #     1/6, 1/6, 2/3\n",
    "    # ] # TVD RK3\n",
    "#     scalar_weights = [\n",
    "#         1.0,\n",
    "#         0.5, 0.5\n",
    "#     ] # TVD RK2\n",
    "    num_rnn_layers = 1\n",
    "    if not isinstance(scalar_weights, type(None)):\n",
    "        num_rnn_layers += int( ((8*len(scalar_weights)+1)**0.5 - 1)/2 )\n",
    "    rnn_layers_units = [80*num_latent_states]*num_rnn_layers\n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "    print('timeMeanofSpaceRMS :', timeMeanofSpaceRMS)\n",
    "    print('stddev :', stddev)\n",
    "    if return_params_arr != False:\n",
    "        data_dim = num_latent_states + 3\n",
    "    else:\n",
    "        data_dim = num_latent_states\n",
    "\n",
    "    dense_dim = [rnn_layers_units[-1]]*(len(dense_layer_act_func)-1)\n",
    "    dense_dim.append(data_dim)\n",
    "        \n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_GRU(\n",
    "                data_dim=data_dim,\n",
    "            #     in_steps=int(T_sample_input // dt_rnn),\n",
    "            #     out_steps=int(T_sample_output // dt_rnn),\n",
    "                dt_rnn=dt_rnn,\n",
    "                lambda_reg=lambda_reg,\n",
    "                reg_name='L2',\n",
    "                rnn_layers_units=rnn_layers_units,\n",
    "                dense_layer_act_func=dense_layer_act_func,\n",
    "                load_file=None,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                stddev=stddev,\n",
    "                noise_type=noise_type,\n",
    "                dense_dim=dense_dim,\n",
    "                use_learnable_state=use_learnable_state,\n",
    "                stateful=stateful,\n",
    "                zoneout_rate=zoneout_rate,\n",
    "                batch_size=batch_size,\n",
    "                use_weights_post_dense=use_weights_post_dense,\n",
    "                rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "                denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "                scalar_weights=scalar_weights, # corresponding to RK4\n",
    "                use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_GRU(\n",
    "            data_dim=data_dim,\n",
    "        #     in_steps=int(T_sample_input // dt_rnn),\n",
    "        #     out_steps=int(T_sample_output // dt_rnn),\n",
    "            dt_rnn=dt_rnn,\n",
    "            lambda_reg=lambda_reg,\n",
    "            reg_name='L2',\n",
    "            rnn_layers_units=rnn_layers_units,\n",
    "            dense_layer_act_func=dense_layer_act_func,\n",
    "            load_file=None,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            stddev=stddev,\n",
    "            noise_type=noise_type,\n",
    "            dense_dim=dense_dim,\n",
    "            use_learnable_state=use_learnable_state,\n",
    "            stateful=stateful,\n",
    "            zoneout_rate=zoneout_rate,\n",
    "            batch_size=batch_size,\n",
    "            use_weights_post_dense=use_weights_post_dense,\n",
    "            rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "            denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "            scalar_weights=scalar_weights, # corresponding to RK4\n",
    "            use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "        )\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    rnn_net.save_class_dict(save_path+dir_sep+'final_net_class_dict.txt')\n",
    "else:\n",
    "    load_file = dir_name_rnn + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_GRU(\n",
    "                load_file=load_file,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                batch_size=batch_size,\n",
    "                \n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_GRU(\n",
    "            load_file=load_file,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    rnn_net.build(input_shape=(batch_size, None, num_latent_states))\n",
    "    \n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        wt_file = tf.train.latest_checkpoint(dir_name_rnn+dir_sep+'checkpoints')\n",
    "    elif behaviour == 'loadFinalNetAndPlot':\n",
    "        wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'final_net_gru_weights.h5'\n",
    "        # wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'f2'#+dir_sep+'saved_model.pb'\n",
    "        rnn_net.load_weights_from_file(wt_file)\n",
    "    \n",
    "    # this forces the model to initialize its kernel weights/biases\n",
    "    # temp = rnn_net.predict(tf.ones(shape=[batch_size, int(T_sample_input//dt_rnn), rnn_net.data_dim]))\n",
    "    # this loads just the kernel wieghts and biases of the model\n",
    "#     rnn_net.load_weights_from_file(wt_file)\n",
    "\n",
    "    # rnn_net = tf.keras.models.load_model(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    val_loss_hist = []\n",
    "    train_loss_hist = []\n",
    "    lr_change=[0, 0]\n",
    "    savelosses_cb_vallossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    savelosses_cb_trainlossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    starting_lr_idx = 0\n",
    "    num_epochs_left = epochs\n",
    "    earlystopping_wait = 0\n",
    "elif behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt, earlystopping_wait = readAndReturnLossHistories(\n",
    "        dir_name_ae=dir_name_rnn,\n",
    "        dir_sep=dir_sep,\n",
    "        epochs=epochs,\n",
    "        learning_rate_list=learning_rate_list,\n",
    "        return_earlystopping_wait=True)\n",
    "    savelosses_cb_vallossarr = val_loss_arr_fromckpt\n",
    "    savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n",
    "elif behaviour == 'loadFinalNetAndPlot':\n",
    "    with open(dir_name_rnn+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    losses_dict = eval(''.join(lines))\n",
    "\n",
    "    val_loss_hist = losses_dict['val_loss_hist']\n",
    "    train_loss_hist = losses_dict['train_loss_hist']\n",
    "    lr_change = losses_dict['lr_change']\n",
    "    test_loss = losses_dict['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NMSE_hist = []\n",
    "val_NMSE_hist = []\n",
    "\n",
    "train_MSE_hist = []\n",
    "val_MSE_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMSE(tf.keras.metrics.MeanSquaredError):\n",
    "    def __init__(self, divisor_arr, name='NMSE', **kwargs):\n",
    "        super(NMSE, self).__init__(name, **kwargs)\n",
    "        self.divisor_arr = divisor_arr\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = y_true / self.divisor_arr\n",
    "        y_pred = y_pred / self.divisor_arr\n",
    "        return super(NMSE, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.001 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0233 - mse: 0.0225 - NMSE: 0.2029 - tot_time: 0h 0m 23.6s\n",
      "\n",
      "Epoch 1: val_NMSE improved from inf to 0.04886, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 24s 1s/step - loss: 0.0233 - mse: 0.0225 - NMSE: 0.2029 - val_loss: 0.0062 - val_mse: 0.0054 - val_NMSE: 0.0489\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0046 - mse: 0.0038 - NMSE: 0.0346 - tot_time: 0h 0m 43.0s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.04886 to 0.02343, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0046 - mse: 0.0038 - NMSE: 0.0346 - val_loss: 0.0034 - val_mse: 0.0026 - val_NMSE: 0.0234\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0031 - mse: 0.0023 - NMSE: 0.0211 - tot_time: 0h 1m 2.9s\n",
      "\n",
      "Epoch 3: val_NMSE improved from 0.02343 to 0.01797, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0031 - mse: 0.0023 - NMSE: 0.0211 - val_loss: 0.0028 - val_mse: 0.0020 - val_NMSE: 0.0180\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0027 - mse: 0.0020 - NMSE: 0.0176 - tot_time: 0h 1m 22.6s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.01797 to 0.01540, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0027 - mse: 0.0020 - NMSE: 0.0176 - val_loss: 0.0024 - val_mse: 0.0017 - val_NMSE: 0.0154\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.0018 - NMSE: 0.0158 - tot_time: 0h 1m 42.4s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.01540 to 0.01382, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0025 - mse: 0.0018 - NMSE: 0.0158 - val_loss: 0.0022 - val_mse: 0.0015 - val_NMSE: 0.0138\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0016 - NMSE: 0.0146 - tot_time: 0h 2m 2.4s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.01382 to 0.01254, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0023 - mse: 0.0016 - NMSE: 0.0146 - val_loss: 0.0021 - val_mse: 0.0014 - val_NMSE: 0.0125\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0022 - mse: 0.0015 - NMSE: 0.0137 - tot_time: 0h 2m 22.2s\n",
      "\n",
      "Epoch 7: val_NMSE improved from 0.01254 to 0.01153, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0022 - mse: 0.0015 - NMSE: 0.0137 - val_loss: 0.0019 - val_mse: 0.0013 - val_NMSE: 0.0115\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0021 - mse: 0.0014 - NMSE: 0.0130 - tot_time: 0h 2m 42.1s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.01153 to 0.01073, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0021 - mse: 0.0014 - NMSE: 0.0130 - val_loss: 0.0018 - val_mse: 0.0012 - val_NMSE: 0.0107\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0020 - mse: 0.0014 - NMSE: 0.0124 - tot_time: 0h 3m 1.9s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.01073 to 0.01009, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0020 - mse: 0.0014 - NMSE: 0.0124 - val_loss: 0.0017 - val_mse: 0.0011 - val_NMSE: 0.0101\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0019 - mse: 0.0013 - NMSE: 0.0119 - tot_time: 0h 3m 21.7s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.01009 to 0.00958, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0019 - mse: 0.0013 - NMSE: 0.0119 - val_loss: 0.0016 - val_mse: 0.0011 - val_NMSE: 0.0096\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0018 - mse: 0.0013 - NMSE: 0.0115 - tot_time: 0h 3m 41.6s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00958 to 0.00911, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0018 - mse: 0.0013 - NMSE: 0.0115 - val_loss: 0.0015 - val_mse: 0.0010 - val_NMSE: 0.0091\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0017 - mse: 0.0012 - NMSE: 0.0112 - tot_time: 0h 4m 1.4s\n",
      "\n",
      "Epoch 12: val_NMSE improved from 0.00911 to 0.00872, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0017 - mse: 0.0012 - NMSE: 0.0112 - val_loss: 0.0015 - val_mse: 9.6847e-04 - val_NMSE: 0.0087\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0017 - mse: 0.0012 - NMSE: 0.0108 - tot_time: 0h 4m 21.3s\n",
      "\n",
      "Epoch 13: val_NMSE improved from 0.00872 to 0.00841, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0017 - mse: 0.0012 - NMSE: 0.0108 - val_loss: 0.0014 - val_mse: 9.3406e-04 - val_NMSE: 0.0084\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.0012 - NMSE: 0.0106 - tot_time: 0h 4m 41.2s\n",
      "\n",
      "Epoch 14: val_NMSE improved from 0.00841 to 0.00812, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0016 - mse: 0.0012 - NMSE: 0.0106 - val_loss: 0.0014 - val_mse: 9.0241e-04 - val_NMSE: 0.0081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.0011 - NMSE: 0.0103 - tot_time: 0h 5m 1.2s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00812 to 0.00789, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0016 - mse: 0.0011 - NMSE: 0.0103 - val_loss: 0.0013 - val_mse: 8.7648e-04 - val_NMSE: 0.0079\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0015 - mse: 0.0011 - NMSE: 0.0101 - tot_time: 0h 5m 21.1s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00789 to 0.00771, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0015 - mse: 0.0011 - NMSE: 0.0101 - val_loss: 0.0013 - val_mse: 8.5622e-04 - val_NMSE: 0.0077\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0015 - mse: 0.0011 - NMSE: 0.0099 - tot_time: 0h 5m 40.8s\n",
      "\n",
      "Epoch 17: val_NMSE improved from 0.00771 to 0.00752, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0015 - mse: 0.0011 - NMSE: 0.0099 - val_loss: 0.0012 - val_mse: 8.3554e-04 - val_NMSE: 0.0075\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0015 - mse: 0.0011 - NMSE: 0.0097 - tot_time: 0h 6m 0.7s\n",
      "\n",
      "Epoch 18: val_NMSE improved from 0.00752 to 0.00736, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0015 - mse: 0.0011 - NMSE: 0.0097 - val_loss: 0.0012 - val_mse: 8.1785e-04 - val_NMSE: 0.0074\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0011 - NMSE: 0.0095 - tot_time: 0h 6m 20.5s\n",
      "\n",
      "Epoch 19: val_NMSE improved from 0.00736 to 0.00718, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0014 - mse: 0.0011 - NMSE: 0.0095 - val_loss: 0.0012 - val_mse: 7.9719e-04 - val_NMSE: 0.0072\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0010 - NMSE: 0.0093 - tot_time: 0h 6m 40.3s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00718 to 0.00704, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0014 - mse: 0.0010 - NMSE: 0.0093 - val_loss: 0.0011 - val_mse: 7.8246e-04 - val_NMSE: 0.0070\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0010 - NMSE: 0.0092 - tot_time: 0h 7m 0.1s\n",
      "\n",
      "Epoch 21: val_NMSE improved from 0.00704 to 0.00690, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0014 - mse: 0.0010 - NMSE: 0.0092 - val_loss: 0.0011 - val_mse: 7.6617e-04 - val_NMSE: 0.0069\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.0010 - NMSE: 0.0091 - tot_time: 0h 7m 19.9s\n",
      "\n",
      "Epoch 22: val_NMSE improved from 0.00690 to 0.00680, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0014 - mse: 0.0010 - NMSE: 0.0091 - val_loss: 0.0011 - val_mse: 7.5591e-04 - val_NMSE: 0.0068\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0013 - mse: 9.9269e-04 - NMSE: 0.0089 - tot_time: 0h 7m 39.6s\n",
      "\n",
      "Epoch 23: val_NMSE improved from 0.00680 to 0.00673, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0013 - mse: 9.9269e-04 - NMSE: 0.0089 - val_loss: 0.0011 - val_mse: 7.4761e-04 - val_NMSE: 0.0067\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0013 - mse: 9.7878e-04 - NMSE: 0.0088 - tot_time: 0h 7m 59.4s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00673 to 0.00670, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0013 - mse: 9.7878e-04 - NMSE: 0.0088 - val_loss: 0.0011 - val_mse: 7.4396e-04 - val_NMSE: 0.0067\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0013 - mse: 9.6648e-04 - NMSE: 0.0087 - tot_time: 0h 8m 19.2s\n",
      "\n",
      "Epoch 25: val_NMSE improved from 0.00670 to 0.00655, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0013 - mse: 9.6648e-04 - NMSE: 0.0087 - val_loss: 0.0011 - val_mse: 7.2736e-04 - val_NMSE: 0.0065\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0013 - mse: 9.5629e-04 - NMSE: 0.0086 - tot_time: 0h 8m 38.9s\n",
      "\n",
      "Epoch 26: val_NMSE improved from 0.00655 to 0.00646, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0013 - mse: 9.5629e-04 - NMSE: 0.0086 - val_loss: 0.0010 - val_mse: 7.1788e-04 - val_NMSE: 0.0065\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0013 - mse: 9.4115e-04 - NMSE: 0.0085 - tot_time: 0h 8m 58.8s\n",
      "\n",
      "Epoch 27: val_NMSE improved from 0.00646 to 0.00640, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0013 - mse: 9.4115e-04 - NMSE: 0.0085 - val_loss: 0.0010 - val_mse: 7.1093e-04 - val_NMSE: 0.0064\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012 - mse: 9.2860e-04 - NMSE: 0.0084 - tot_time: 0h 9m 18.6s\n",
      "\n",
      "Epoch 28: val_NMSE improved from 0.00640 to 0.00634, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 9.2860e-04 - NMSE: 0.0084 - val_loss: 0.0010 - val_mse: 7.0449e-04 - val_NMSE: 0.0063\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012 - mse: 9.2036e-04 - NMSE: 0.0083 - tot_time: 0h 9m 38.5s\n",
      "\n",
      "Epoch 29: val_NMSE improved from 0.00634 to 0.00625, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 9.2036e-04 - NMSE: 0.0083 - val_loss: 0.0010 - val_mse: 6.9424e-04 - val_NMSE: 0.0062\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012 - mse: 9.1119e-04 - NMSE: 0.0082 - tot_time: 0h 9m 58.2s\n",
      "\n",
      "Epoch 30: val_NMSE improved from 0.00625 to 0.00623, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 9.1119e-04 - NMSE: 0.0082 - val_loss: 9.9404e-04 - val_mse: 6.9169e-04 - val_NMSE: 0.0062\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012 - mse: 9.0357e-04 - NMSE: 0.0081 - tot_time: 0h 10m 18.0s\n",
      "\n",
      "Epoch 31: val_NMSE improved from 0.00623 to 0.00616, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 9.0357e-04 - NMSE: 0.0081 - val_loss: 9.8331e-04 - val_mse: 6.8450e-04 - val_NMSE: 0.0062\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012 - mse: 8.9149e-04 - NMSE: 0.0080 - tot_time: 0h 10m 37.7s\n",
      "\n",
      "Epoch 32: val_NMSE improved from 0.00616 to 0.00606, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 8.9149e-04 - NMSE: 0.0080 - val_loss: 9.6862e-04 - val_mse: 6.7308e-04 - val_NMSE: 0.0061\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012 - mse: 8.8127e-04 - NMSE: 0.0079 - tot_time: 0h 10m 57.6s\n",
      "\n",
      "Epoch 33: val_NMSE improved from 0.00606 to 0.00594, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 8.8127e-04 - NMSE: 0.0079 - val_loss: 9.5228e-04 - val_mse: 6.5980e-04 - val_NMSE: 0.0059\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012 - mse: 8.7039e-04 - NMSE: 0.0078 - tot_time: 0h 11m 17.4s\n",
      "\n",
      "Epoch 34: val_NMSE improved from 0.00594 to 0.00594, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 8.7039e-04 - NMSE: 0.0078 - val_loss: 9.4912e-04 - val_mse: 6.5955e-04 - val_NMSE: 0.0059\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0012 - mse: 8.6237e-04 - NMSE: 0.0078 - tot_time: 0h 11m 37.1s\n",
      "\n",
      "Epoch 35: val_NMSE improved from 0.00594 to 0.00576, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 8.6237e-04 - NMSE: 0.0078 - val_loss: 9.2666e-04 - val_mse: 6.3989e-04 - val_NMSE: 0.0058\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 8.4815e-04 - NMSE: 0.0076 - tot_time: 0h 11m 57.0s\n",
      "\n",
      "Epoch 36: val_NMSE did not improve from 0.00576\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0011 - mse: 8.4815e-04 - NMSE: 0.0076 - val_loss: 9.2430e-04 - val_mse: 6.4016e-04 - val_NMSE: 0.0058\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 8.4212e-04 - NMSE: 0.0076 - tot_time: 0h 12m 16.5s\n",
      "\n",
      "Epoch 37: val_NMSE improved from 0.00576 to 0.00567, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0011 - mse: 8.4212e-04 - NMSE: 0.0076 - val_loss: 9.1166e-04 - val_mse: 6.3011e-04 - val_NMSE: 0.0057\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 8.2941e-04 - NMSE: 0.0075 - tot_time: 0h 12m 36.3s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00567 to 0.00559, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0011 - mse: 8.2941e-04 - NMSE: 0.0075 - val_loss: 8.9972e-04 - val_mse: 6.2064e-04 - val_NMSE: 0.0056\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 8.1970e-04 - NMSE: 0.0074 - tot_time: 0h 12m 56.1s\n",
      "\n",
      "Epoch 39: val_NMSE improved from 0.00559 to 0.00550, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0011 - mse: 8.1970e-04 - NMSE: 0.0074 - val_loss: 8.8769e-04 - val_mse: 6.1094e-04 - val_NMSE: 0.0055\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 8.1556e-04 - NMSE: 0.0073 - tot_time: 0h 13m 16.0s\n",
      "\n",
      "Epoch 40: val_NMSE did not improve from 0.00550\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0011 - mse: 8.1556e-04 - NMSE: 0.0073 - val_loss: 8.9101e-04 - val_mse: 6.1659e-04 - val_NMSE: 0.0056\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 8.1372e-04 - NMSE: 0.0073 - tot_time: 0h 13m 35.3s\n",
      "\n",
      "Epoch 41: val_NMSE improved from 0.00550 to 0.00549, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0011 - mse: 8.1372e-04 - NMSE: 0.0073 - val_loss: 8.8173e-04 - val_mse: 6.0946e-04 - val_NMSE: 0.0055\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 8.0626e-04 - NMSE: 0.0073 - tot_time: 0h 13m 55.1s\n",
      "\n",
      "Epoch 42: val_NMSE improved from 0.00549 to 0.00542, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0011 - mse: 8.0626e-04 - NMSE: 0.0073 - val_loss: 8.7187e-04 - val_mse: 6.0164e-04 - val_NMSE: 0.0054\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 7.9390e-04 - NMSE: 0.0071 - tot_time: 0h 14m 15.0s\n",
      "\n",
      "Epoch 43: val_NMSE improved from 0.00542 to 0.00536, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0011 - mse: 7.9390e-04 - NMSE: 0.0071 - val_loss: 8.6383e-04 - val_mse: 5.9549e-04 - val_NMSE: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0011 - mse: 7.8660e-04 - NMSE: 0.0071 - tot_time: 0h 14m 34.9s\n",
      "\n",
      "Epoch 44: val_NMSE improved from 0.00536 to 0.00536, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0011 - mse: 7.8660e-04 - NMSE: 0.0071 - val_loss: 8.6197e-04 - val_mse: 5.9544e-04 - val_NMSE: 0.0054\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0010 - mse: 7.8075e-04 - NMSE: 0.0070 - tot_time: 0h 14m 54.7s\n",
      "\n",
      "Epoch 45: val_NMSE improved from 0.00536 to 0.00526, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0010 - mse: 7.8075e-04 - NMSE: 0.0070 - val_loss: 8.4943e-04 - val_mse: 5.8477e-04 - val_NMSE: 0.0053\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0010 - mse: 7.6906e-04 - NMSE: 0.0069 - tot_time: 0h 15m 14.8s\n",
      "\n",
      "Epoch 46: val_NMSE improved from 0.00526 to 0.00524, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0010 - mse: 7.6906e-04 - NMSE: 0.0069 - val_loss: 8.4519e-04 - val_mse: 5.8229e-04 - val_NMSE: 0.0052\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0010 - mse: 7.6793e-04 - NMSE: 0.0069 - tot_time: 0h 15m 34.6s\n",
      "\n",
      "Epoch 47: val_NMSE improved from 0.00524 to 0.00518, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0010 - mse: 7.6793e-04 - NMSE: 0.0069 - val_loss: 8.3714e-04 - val_mse: 5.7594e-04 - val_NMSE: 0.0052\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0010 - mse: 7.6212e-04 - NMSE: 0.0069 - tot_time: 0h 15m 54.4s\n",
      "\n",
      "Epoch 48: val_NMSE did not improve from 0.00518\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0010 - mse: 7.6212e-04 - NMSE: 0.0069 - val_loss: 8.4141e-04 - val_mse: 5.8186e-04 - val_NMSE: 0.0052\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0010 - mse: 7.5713e-04 - NMSE: 0.0068 - tot_time: 0h 16m 13.8s\n",
      "\n",
      "Epoch 49: val_NMSE improved from 0.00518 to 0.00517, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0010 - mse: 7.5713e-04 - NMSE: 0.0068 - val_loss: 8.3235e-04 - val_mse: 5.7434e-04 - val_NMSE: 0.0052\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0010 - mse: 7.4752e-04 - NMSE: 0.0067 - tot_time: 0h 16m 33.5s\n",
      "\n",
      "Epoch 50: val_NMSE improved from 0.00517 to 0.00505, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0010 - mse: 7.4752e-04 - NMSE: 0.0067 - val_loss: 8.1730e-04 - val_mse: 5.6062e-04 - val_NMSE: 0.0050\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.9494e-04 - mse: 7.3896e-04 - NMSE: 0.0067 - tot_time: 0h 16m 53.2s\n",
      "\n",
      "Epoch 51: val_NMSE did not improve from 0.00505\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 9.9494e-04 - mse: 7.3896e-04 - NMSE: 0.0067 - val_loss: 8.2449e-04 - val_mse: 5.6928e-04 - val_NMSE: 0.0051\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.8286e-04 - mse: 7.2832e-04 - NMSE: 0.0066 - tot_time: 0h 17m 12.6s\n",
      "\n",
      "Epoch 52: val_NMSE improved from 0.00505 to 0.00497, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 9.8286e-04 - mse: 7.2832e-04 - NMSE: 0.0066 - val_loss: 8.0627e-04 - val_mse: 5.5248e-04 - val_NMSE: 0.0050\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.6621e-04 - mse: 7.1307e-04 - NMSE: 0.0064 - tot_time: 0h 17m 32.4s\n",
      "\n",
      "Epoch 53: val_NMSE improved from 0.00497 to 0.00497, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 9.6621e-04 - mse: 7.1307e-04 - NMSE: 0.0064 - val_loss: 8.0423e-04 - val_mse: 5.5180e-04 - val_NMSE: 0.0050\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.5938e-04 - mse: 7.0772e-04 - NMSE: 0.0064 - tot_time: 0h 17m 52.1s\n",
      "\n",
      "Epoch 54: val_NMSE improved from 0.00497 to 0.00496, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 9.5938e-04 - mse: 7.0772e-04 - NMSE: 0.0064 - val_loss: 8.0175e-04 - val_mse: 5.5097e-04 - val_NMSE: 0.0050\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.5831e-04 - mse: 7.0827e-04 - NMSE: 0.0064 - tot_time: 0h 18m 11.8s\n",
      "\n",
      "Epoch 55: val_NMSE did not improve from 0.00496\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 9.5831e-04 - mse: 7.0827e-04 - NMSE: 0.0064 - val_loss: 8.1042e-04 - val_mse: 5.6113e-04 - val_NMSE: 0.0051\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.5265e-04 - mse: 7.0398e-04 - NMSE: 0.0063 - tot_time: 0h 18m 31.2s\n",
      "\n",
      "Epoch 56: val_NMSE did not improve from 0.00496\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 9.5265e-04 - mse: 7.0398e-04 - NMSE: 0.0063 - val_loss: 8.0276e-04 - val_mse: 5.5474e-04 - val_NMSE: 0.0050\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.4124e-04 - mse: 6.9379e-04 - NMSE: 0.0062 - tot_time: 0h 18m 50.6s\n",
      "\n",
      "Epoch 57: val_NMSE improved from 0.00496 to 0.00493, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 9.4124e-04 - mse: 6.9379e-04 - NMSE: 0.0062 - val_loss: 7.9482e-04 - val_mse: 5.4806e-04 - val_NMSE: 0.0049\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.3472e-04 - mse: 6.8862e-04 - NMSE: 0.0062 - tot_time: 0h 19m 10.3s\n",
      "\n",
      "Epoch 58: val_NMSE did not improve from 0.00493\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 9.3472e-04 - mse: 6.8862e-04 - NMSE: 0.0062 - val_loss: 8.0008e-04 - val_mse: 5.5474e-04 - val_NMSE: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.2939e-04 - mse: 6.8466e-04 - NMSE: 0.0062 - tot_time: 0h 19m 29.6s\n",
      "\n",
      "Epoch 59: val_NMSE improved from 0.00493 to 0.00491, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 9.2939e-04 - mse: 6.8466e-04 - NMSE: 0.0062 - val_loss: 7.8957e-04 - val_mse: 5.4544e-04 - val_NMSE: 0.0049\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.2172e-04 - mse: 6.7814e-04 - NMSE: 0.0061 - tot_time: 0h 19m 49.5s\n",
      "\n",
      "Epoch 60: val_NMSE did not improve from 0.00491\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 9.2172e-04 - mse: 6.7814e-04 - NMSE: 0.0061 - val_loss: 7.8991e-04 - val_mse: 5.4697e-04 - val_NMSE: 0.0049\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.1291e-04 - mse: 6.7059e-04 - NMSE: 0.0060 - tot_time: 0h 20m 8.8s\n",
      "\n",
      "Epoch 61: val_NMSE improved from 0.00491 to 0.00489, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 9.1291e-04 - mse: 6.7059e-04 - NMSE: 0.0060 - val_loss: 7.8472e-04 - val_mse: 5.4298e-04 - val_NMSE: 0.0049\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.0143e-04 - mse: 6.6027e-04 - NMSE: 0.0059 - tot_time: 0h 20m 28.5s\n",
      "\n",
      "Epoch 62: val_NMSE improved from 0.00489 to 0.00486, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 9.0143e-04 - mse: 6.6027e-04 - NMSE: 0.0059 - val_loss: 7.8077e-04 - val_mse: 5.4033e-04 - val_NMSE: 0.0049\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.0263e-04 - mse: 6.6286e-04 - NMSE: 0.0060 - tot_time: 0h 20m 48.3s\n",
      "\n",
      "Epoch 63: val_NMSE improved from 0.00486 to 0.00482, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 9.0263e-04 - mse: 6.6286e-04 - NMSE: 0.0060 - val_loss: 7.7473e-04 - val_mse: 5.3561e-04 - val_NMSE: 0.0048\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 9.0067e-04 - mse: 6.6208e-04 - NMSE: 0.0060 - tot_time: 0h 21m 8.0s\n",
      "\n",
      "Epoch 64: val_NMSE did not improve from 0.00482\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 9.0067e-04 - mse: 6.6208e-04 - NMSE: 0.0060 - val_loss: 7.8749e-04 - val_mse: 5.4963e-04 - val_NMSE: 0.0049\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.9215e-04 - mse: 6.5472e-04 - NMSE: 0.0059 - tot_time: 0h 21m 27.5s\n",
      "\n",
      "Epoch 65: val_NMSE improved from 0.00482 to 0.00474, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 8.9215e-04 - mse: 6.5472e-04 - NMSE: 0.0059 - val_loss: 7.6375e-04 - val_mse: 5.2681e-04 - val_NMSE: 0.0047\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.6762e-04 - mse: 6.3127e-04 - NMSE: 0.0057 - tot_time: 0h 21m 47.2s\n",
      "\n",
      "Epoch 66: val_NMSE improved from 0.00474 to 0.00461, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 8.6762e-04 - mse: 6.3127e-04 - NMSE: 0.0057 - val_loss: 7.4813e-04 - val_mse: 5.1244e-04 - val_NMSE: 0.0046\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.5198e-04 - mse: 6.1692e-04 - NMSE: 0.0056 - tot_time: 0h 22m 7.0s\n",
      "\n",
      "Epoch 67: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 8.5198e-04 - mse: 6.1692e-04 - NMSE: 0.0056 - val_loss: 7.4893e-04 - val_mse: 5.1464e-04 - val_NMSE: 0.0046\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.4344e-04 - mse: 6.0995e-04 - NMSE: 0.0055 - tot_time: 0h 22m 26.3s\n",
      "\n",
      "Epoch 68: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 8.4344e-04 - mse: 6.0995e-04 - NMSE: 0.0055 - val_loss: 7.5028e-04 - val_mse: 5.1760e-04 - val_NMSE: 0.0047\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.3434e-04 - mse: 6.0233e-04 - NMSE: 0.0054 - tot_time: 0h 22m 45.6s\n",
      "\n",
      "Epoch 69: val_NMSE improved from 0.00461 to 0.00461, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 8.3434e-04 - mse: 6.0233e-04 - NMSE: 0.0054 - val_loss: 7.4321e-04 - val_mse: 5.1204e-04 - val_NMSE: 0.0046\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.3483e-04 - mse: 6.0438e-04 - NMSE: 0.0054 - tot_time: 0h 23m 5.3s\n",
      "\n",
      "Epoch 70: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 8.3483e-04 - mse: 6.0438e-04 - NMSE: 0.0054 - val_loss: 7.4735e-04 - val_mse: 5.1767e-04 - val_NMSE: 0.0047\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.2475e-04 - mse: 5.9562e-04 - NMSE: 0.0054 - tot_time: 0h 23m 24.7s\n",
      "\n",
      "Epoch 71: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 8.2475e-04 - mse: 5.9562e-04 - NMSE: 0.0054 - val_loss: 7.4256e-04 - val_mse: 5.1397e-04 - val_NMSE: 0.0046\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.1614e-04 - mse: 5.8824e-04 - NMSE: 0.0053 - tot_time: 0h 23m 44.0s\n",
      "\n",
      "Epoch 72: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 8.1614e-04 - mse: 5.8824e-04 - NMSE: 0.0053 - val_loss: 7.5466e-04 - val_mse: 5.2742e-04 - val_NMSE: 0.0047\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.1331e-04 - mse: 5.8663e-04 - NMSE: 0.0053 - tot_time: 0h 24m 3.3s\n",
      "\n",
      "Epoch 73: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 8.1331e-04 - mse: 5.8663e-04 - NMSE: 0.0053 - val_loss: 7.6973e-04 - val_mse: 5.4376e-04 - val_NMSE: 0.0049\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.1461e-04 - mse: 5.8918e-04 - NMSE: 0.0053 - tot_time: 0h 24m 22.8s\n",
      "\n",
      "Epoch 74: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 19s 1s/step - loss: 8.1461e-04 - mse: 5.8918e-04 - NMSE: 0.0053 - val_loss: 7.6035e-04 - val_mse: 5.3549e-04 - val_NMSE: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.1475e-04 - mse: 5.9033e-04 - NMSE: 0.0053 - tot_time: 0h 24m 42.6s\n",
      "\n",
      "Epoch 75: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 8.1475e-04 - mse: 5.9033e-04 - NMSE: 0.0053 - val_loss: 7.7460e-04 - val_mse: 5.5067e-04 - val_NMSE: 0.0050\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 8.0881e-04 - mse: 5.8534e-04 - NMSE: 0.0053 - tot_time: 0h 25m 2.5s\n",
      "\n",
      "Epoch 76: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 8.0881e-04 - mse: 5.8534e-04 - NMSE: 0.0053 - val_loss: 7.6169e-04 - val_mse: 5.3870e-04 - val_NMSE: 0.0048\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.9320e-04 - mse: 5.7070e-04 - NMSE: 0.0051 - tot_time: 0h 25m 22.2s\n",
      "\n",
      "Epoch 77: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.9320e-04 - mse: 5.7070e-04 - NMSE: 0.0051 - val_loss: 7.4630e-04 - val_mse: 5.2431e-04 - val_NMSE: 0.0047\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.7499e-04 - mse: 5.5366e-04 - NMSE: 0.0050 - tot_time: 0h 25m 42.0s\n",
      "\n",
      "Epoch 78: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.7499e-04 - mse: 5.5366e-04 - NMSE: 0.0050 - val_loss: 7.3413e-04 - val_mse: 5.1362e-04 - val_NMSE: 0.0046\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6841e-04 - mse: 5.4857e-04 - NMSE: 0.0049Restoring model weights from the end of the best epoch: 69.\n",
      " - tot_time: 0h 26m 2.1s\n",
      "\n",
      "Epoch 79: val_NMSE did not improve from 0.00461\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6841e-04 - mse: 5.4857e-04 - NMSE: 0.0049 - val_loss: 7.5010e-04 - val_mse: 5.3101e-04 - val_NMSE: 0.0048\n",
      "Epoch 79: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.0001 ----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.9006e-04 - mse: 5.5898e-04 - NMSE: 0.0050 - tot_time: 0h 26m 22.4s\n",
      "\n",
      "Epoch 1: val_NMSE improved from 0.00461 to 0.00414, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 21s 1s/step - loss: 7.9006e-04 - mse: 5.5898e-04 - NMSE: 0.0050 - val_loss: 6.9056e-04 - val_mse: 4.5958e-04 - val_NMSE: 0.0041\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.7148e-04 - mse: 5.4057e-04 - NMSE: 0.0049 - tot_time: 0h 26m 43.0s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.00414 to 0.00410, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 21s 1s/step - loss: 7.7148e-04 - mse: 5.4057e-04 - NMSE: 0.0049 - val_loss: 6.8598e-04 - val_mse: 4.5516e-04 - val_NMSE: 0.0041\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6705e-04 - mse: 5.3633e-04 - NMSE: 0.0048 - tot_time: 0h 27m 3.7s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6705e-04 - mse: 5.3633e-04 - NMSE: 0.0048 - val_loss: 6.8689e-04 - val_mse: 4.5629e-04 - val_NMSE: 0.0041\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6425e-04 - mse: 5.3375e-04 - NMSE: 0.0048 - tot_time: 0h 27m 23.9s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6425e-04 - mse: 5.3375e-04 - NMSE: 0.0048 - val_loss: 6.8579e-04 - val_mse: 4.5542e-04 - val_NMSE: 0.0041\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6269e-04 - mse: 5.3243e-04 - NMSE: 0.0048 - tot_time: 0h 27m 44.3s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6269e-04 - mse: 5.3243e-04 - NMSE: 0.0048 - val_loss: 6.8613e-04 - val_mse: 4.5602e-04 - val_NMSE: 0.0041\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6063e-04 - mse: 5.3065e-04 - NMSE: 0.0048 - tot_time: 0h 28m 4.7s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6063e-04 - mse: 5.3065e-04 - NMSE: 0.0048 - val_loss: 6.8745e-04 - val_mse: 4.5761e-04 - val_NMSE: 0.0041\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.5914e-04 - mse: 5.2942e-04 - NMSE: 0.0048 - tot_time: 0h 28m 25.1s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.5914e-04 - mse: 5.2942e-04 - NMSE: 0.0048 - val_loss: 6.8764e-04 - val_mse: 4.5807e-04 - val_NMSE: 0.0041\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.5933e-04 - mse: 5.2989e-04 - NMSE: 0.0048 - tot_time: 0h 28m 45.3s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.5933e-04 - mse: 5.2989e-04 - NMSE: 0.0048 - val_loss: 6.8657e-04 - val_mse: 4.5728e-04 - val_NMSE: 0.0041\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.5694e-04 - mse: 5.2777e-04 - NMSE: 0.0048 - tot_time: 0h 29m 5.6s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.5694e-04 - mse: 5.2777e-04 - NMSE: 0.0048 - val_loss: 6.8841e-04 - val_mse: 4.5939e-04 - val_NMSE: 0.0041\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.5449e-04 - mse: 5.2560e-04 - NMSE: 0.0047 - tot_time: 0h 29m 25.7s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.5449e-04 - mse: 5.2560e-04 - NMSE: 0.0047 - val_loss: 6.8878e-04 - val_mse: 4.6003e-04 - val_NMSE: 0.0041\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.5375e-04 - mse: 5.2513e-04 - NMSE: 0.0047 - tot_time: 0h 29m 45.9s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.5375e-04 - mse: 5.2513e-04 - NMSE: 0.0047 - val_loss: 6.8865e-04 - val_mse: 4.6017e-04 - val_NMSE: 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.5180e-04 - mse: 5.2345e-04 - NMSE: 0.0047Restoring model weights from the end of the best epoch: 2.\n",
      " - tot_time: 0h 30m 6.2s\n",
      "\n",
      "Epoch 12: val_NMSE did not improve from 0.00410\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.5180e-04 - mse: 5.2345e-04 - NMSE: 0.0047 - val_loss: 6.8989e-04 - val_mse: 4.6168e-04 - val_NMSE: 0.0042\n",
      "Epoch 12: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-05 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6573e-04 - mse: 5.3492e-04 - NMSE: 0.0048 - tot_time: 0h 30m 26.4s\n",
      "\n",
      "Epoch 1: val_NMSE improved from 0.00410 to 0.00406, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 21s 1s/step - loss: 7.6573e-04 - mse: 5.3492e-04 - NMSE: 0.0048 - val_loss: 6.8129e-04 - val_mse: 4.5050e-04 - val_NMSE: 0.0041\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6341e-04 - mse: 5.3263e-04 - NMSE: 0.0048 - tot_time: 0h 30m 47.3s\n",
      "\n",
      "Epoch 2: val_NMSE did not improve from 0.00406\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6341e-04 - mse: 5.3263e-04 - NMSE: 0.0048 - val_loss: 6.8223e-04 - val_mse: 4.5146e-04 - val_NMSE: 0.0041\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6290e-04 - mse: 5.3214e-04 - NMSE: 0.0048 - tot_time: 0h 31m 7.3s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00406\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6290e-04 - mse: 5.3214e-04 - NMSE: 0.0048 - val_loss: 6.8195e-04 - val_mse: 4.5120e-04 - val_NMSE: 0.0041\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6147e-04 - mse: 5.3073e-04 - NMSE: 0.0048 - tot_time: 0h 31m 27.4s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.00406 to 0.00405, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6147e-04 - mse: 5.3073e-04 - NMSE: 0.0048 - val_loss: 6.8064e-04 - val_mse: 4.4992e-04 - val_NMSE: 0.0040\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6264e-04 - mse: 5.3192e-04 - NMSE: 0.0048 - tot_time: 0h 31m 48.1s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6264e-04 - mse: 5.3192e-04 - NMSE: 0.0048 - val_loss: 6.8148e-04 - val_mse: 4.5078e-04 - val_NMSE: 0.0041\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6039e-04 - mse: 5.2970e-04 - NMSE: 0.0048 - tot_time: 0h 32m 8.2s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6039e-04 - mse: 5.2970e-04 - NMSE: 0.0048 - val_loss: 6.8375e-04 - val_mse: 4.5308e-04 - val_NMSE: 0.0041\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6189e-04 - mse: 5.3123e-04 - NMSE: 0.0048 - tot_time: 0h 32m 28.6s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6189e-04 - mse: 5.3123e-04 - NMSE: 0.0048 - val_loss: 6.8221e-04 - val_mse: 4.5157e-04 - val_NMSE: 0.0041\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6030e-04 - mse: 5.2967e-04 - NMSE: 0.0048 - tot_time: 0h 32m 48.8s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6030e-04 - mse: 5.2967e-04 - NMSE: 0.0048 - val_loss: 6.8114e-04 - val_mse: 4.5052e-04 - val_NMSE: 0.0041\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6202e-04 - mse: 5.3141e-04 - NMSE: 0.0048 - tot_time: 0h 33m 9.1s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6202e-04 - mse: 5.3141e-04 - NMSE: 0.0048 - val_loss: 6.8255e-04 - val_mse: 4.5196e-04 - val_NMSE: 0.0041\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6144e-04 - mse: 5.3086e-04 - NMSE: 0.0048 - tot_time: 0h 33m 29.5s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6144e-04 - mse: 5.3086e-04 - NMSE: 0.0048 - val_loss: 6.8155e-04 - val_mse: 4.5099e-04 - val_NMSE: 0.0041\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6180e-04 - mse: 5.3125e-04 - NMSE: 0.0048 - tot_time: 0h 33m 50.1s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 21s 1s/step - loss: 7.6180e-04 - mse: 5.3125e-04 - NMSE: 0.0048 - val_loss: 6.8213e-04 - val_mse: 4.5160e-04 - val_NMSE: 0.0041\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6163e-04 - mse: 5.3111e-04 - NMSE: 0.0048 - tot_time: 0h 34m 10.3s\n",
      "\n",
      "Epoch 12: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6163e-04 - mse: 5.3111e-04 - NMSE: 0.0048 - val_loss: 6.8280e-04 - val_mse: 4.5229e-04 - val_NMSE: 0.0041\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6094e-04 - mse: 5.3044e-04 - NMSE: 0.0048 - tot_time: 0h 34m 30.4s\n",
      "\n",
      "Epoch 13: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6094e-04 - mse: 5.3044e-04 - NMSE: 0.0048 - val_loss: 6.8228e-04 - val_mse: 4.5181e-04 - val_NMSE: 0.0041\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.6075e-04 - mse: 5.3029e-04 - NMSE: 0.0048Restoring model weights from the end of the best epoch: 4.\n",
      " - tot_time: 0h 34m 50.7s\n",
      "\n",
      "Epoch 14: val_NMSE did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_ks/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "16/16 [==============================] - 20s 1s/step - loss: 7.6075e-04 - mse: 5.3029e-04 - NMSE: 0.0048 - val_loss: 6.8254e-04 - val_mse: 4.5210e-04 - val_NMSE: 0.0041\n",
      "Epoch 14: early stopping\n"
     ]
    }
   ],
   "source": [
    "# compiling the network\n",
    "rnn_net.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=['mse', NMSE(divisor_arr=time_stddev)],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # this loads the weights/attributes of the optimizer as well\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net.load_weights(wt_file)\n",
    "    else:\n",
    "        rnn_net.load_weights(wt_file)\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # implementing early stopping\n",
    "    baseline = None\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        baseline = np.min(val_loss_hist)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_NMSE',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=True,\n",
    "        min_delta=min_delta,\n",
    "        baseline=baseline\n",
    "    )\n",
    "    #** the two lines below are useless because wait is set to 0 in on_train_begin\n",
    "    # early_stopping_cb.wait = earlystopping_wait\n",
    "    # print('early_stopping_cb.wait : {}\\n'.format(early_stopping_cb.wait))\n",
    "\n",
    "    # time callback for each epoch\n",
    "    timekeeper_cb = mytimecallback()\n",
    "\n",
    "    # model checkpoint callback\n",
    "    dir_name_ckpt = dir_name_rnn+dir_sep+'checkpoints'\n",
    "    if not os.path.isdir(dir_name_ckpt):\n",
    "        os.makedirs(dir_name_ckpt)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dir_name_ckpt+dir_sep+'checkpoint',#+'/checkpoint--loss={loss:.4f}--vall_loss={val_loss:.4f}',\n",
    "        monitor='val_NMSE',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=2,\n",
    "        initial_value_threshold=baseline,\n",
    "        period=1  # saves every `period` epochs\n",
    "    )\n",
    "\n",
    "    # save losses callback\n",
    "    savelosses_cb = SaveLosses(\n",
    "        filepath=dir_name_ckpt+dir_sep+'LossHistoriesCheckpoint',\n",
    "        val_loss_arr=savelosses_cb_vallossarr,\n",
    "        train_loss_arr=savelosses_cb_trainlossarr,\n",
    "        total_epochs=epochs,\n",
    "        period=1)\n",
    "\n",
    "    for i in range(starting_lr_idx, len(learning_rate_list)):\n",
    "        learning_rate = learning_rate_list[i]\n",
    "        K.set_value(rnn_net.optimizer.lr, learning_rate)\n",
    "\n",
    "        savelosses_cb.update_lr_idx(i)\n",
    "\n",
    "        if i == starting_lr_idx:\n",
    "            EPOCHS = num_epochs_left\n",
    "            savelosses_cb.update_offset(epochs-num_epochs_left)\n",
    "        else:\n",
    "            EPOCHS = epochs\n",
    "            savelosses_cb.update_offset(0)\n",
    "\n",
    "        total_s_len = 80\n",
    "        sep_lr_s = ' LEARNING RATE : {} '.format(learning_rate)\n",
    "        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n",
    "        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n",
    "        print('\\n\\n' + '-'*len(sep_lr_s))\n",
    "        print('\\n' + sep_lr_s+'\\n')\n",
    "        print('-'*len(sep_lr_s) + '\\n\\n')\n",
    "        \n",
    "        history = rnn_net.fit(training_data_rnn_input, training_data_rnn_output,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=batch_size,\n",
    "#             validation_split=val_split/train_split,\n",
    "            validation_data=(val_data_rnn_input, val_data_rnn_output),\n",
    "            callbacks=[early_stopping_cb, timekeeper_cb, checkpoint_cb, savelosses_cb],\n",
    "            verbose=1,\n",
    "            shuffle=not stateful,\n",
    "        )\n",
    "\n",
    "        val_loss_hist.extend(history.history['val_loss'])\n",
    "        train_loss_hist.extend(history.history['loss'])\n",
    "        \n",
    "        val_NMSE_hist.extend(history.history['val_NMSE'])\n",
    "        train_NMSE_hist.extend(history.history['NMSE'])\n",
    "\n",
    "        val_MSE_hist.extend(history.history['val_mse'])\n",
    "        train_MSE_hist.extend(history.history['mse'])\n",
    "        \n",
    "        if i == starting_lr_idx:\n",
    "            lr_change[i+1] += len(history.history['val_loss'])\n",
    "        else:\n",
    "            lr_change.append(lr_change[i]+len(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 387, 16) (64, 387, 16)\n",
      "2/2 [==============================] - 1s 255ms/step - loss: 5.8246e-04 - mse: 3.5174e-04 - NMSE: 0.0032\n"
     ]
    }
   ],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    for layer in rnn_net.rnn_list:\n",
    "        if layer.stateful == True:\n",
    "            layer.reset_states()\n",
    "    print(testing_data_rnn_input.shape, testing_data_rnn_output.shape)\n",
    "    eval_dict = rnn_net.evaluate(\n",
    "        testing_data_rnn_input, testing_data_rnn_output,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "\n",
    "    with open(save_path+dir_sep+'losses.txt', 'w') as f:\n",
    "        f.write(str({\n",
    "            'val_loss_hist':val_loss_hist,\n",
    "            'train_loss_hist':train_loss_hist,\n",
    "            'val_MSE_hist':val_MSE_hist,\n",
    "            'train_MSE_hist':train_MSE_hist,\n",
    "            'val_NMSE_hist':val_NMSE_hist,\n",
    "            'train_NMSE_hist':train_NMSE_hist,\n",
    "            'lr_change':lr_change,\n",
    "            'test_loss':eval_dict[0],\n",
    "            'test_MSE':eval_dict[1],\n",
    "            'test_NMSE':eval_dict[2],\n",
    "        }))\n",
    "        \n",
    "    if normalize_dataset == True:\n",
    "        with open(save_path+dir_sep+'rnn_normalization.txt', 'w') as f:\n",
    "            f.write(str({\n",
    "                'normalization_arr':normalization_arr\n",
    "            }))\n",
    "\n",
    "    rnn_net.save_everything(\n",
    "        file_name=save_path+dir_sep+'final_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_kwargs = {'fontsize':15}\n",
    "ylabel_kwargs = {'fontsize':15}\n",
    "legend_kwargs = {'fontsize':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting losses\n",
    "dir_name_plot = dir_name_rnn + '/plots'\n",
    "if not os.path.isdir(dir_name_plot):\n",
    "    os.makedirs(dir_name_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": [
    "# Visualize loss history\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_loss_hist,\n",
    "    val_loss=val_loss_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "\n",
    "plt.savefig(dir_name_plot + '{ds}loss_history.pdf'.format(ds=dir_sep), dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_MSE_hist,\n",
    "    val_loss=val_MSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training MSE', 'Validation MSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='MSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/MSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_NMSE_hist,\n",
    "    val_loss=val_NMSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training NMSE', 'Validation NMSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='NMSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/NMSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": [
    "def rescale_data(data, normalization_arr):\n",
    "    '''\n",
    "    data - [num_batches x num_timesteps x num_states]\n",
    "    normalization_arr = [2 x num_states]\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(data.shape[-1]):\n",
    "        new_data[:, i] -= normalization_arr[0, i]\n",
    "        new_data[:, i] /= normalization_arr[1, i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def norm_sq_time_average(data):\n",
    "    data_norm_sq = np.zeros(shape=data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_norm_sq[:] += data[:, i]**2\n",
    "    # integrating using the trapezoidal rule\n",
    "    norm_sq_time_avg = np.sum(data_norm_sq) - 0.5*(data_norm_sq[0]+data_norm_sq[-1])\n",
    "    norm_sq_time_avg /= data_norm_sq.shape[0]\n",
    "    return norm_sq_time_avg\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = np.empty_like(data)\n",
    "    shape = new_data.shape\n",
    "    # print(shape)\n",
    "    for i in range(shape[-1]):\n",
    "        if len(shape) == 2:\n",
    "            new_data[:, i] = data[:, i]\n",
    "            new_data[:, i] *= normalization_arr[1, i]\n",
    "            new_data[:, i] += normalization_arr[0, i]\n",
    "        elif len(shape) == 3:\n",
    "            new_data[:, :, i] = data[:, :, i]\n",
    "            new_data[:, :, i] *= normalization_arr[1, i]\n",
    "            new_data[:, :, i] += normalization_arr[0, i]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_rnn/rnn_005\n",
      "num_runs : 50\n",
      "    1 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    2 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    3 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    4 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    5 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    6 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    7 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    8 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    9 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    10 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    11 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    12 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    13 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    14 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    15 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    16 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    17 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    18 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    19 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    20 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    21 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    22 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    23 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    24 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    25 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    26 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    27 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    28 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    29 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    30 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    31 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    32 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    33 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    34 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    35 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    36 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    37 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    38 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    39 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    40 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    41 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    42 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    43 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    44 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    45 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    46 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    47 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    48 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    49 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    50 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.5172536095371002, median : 0.49139092906024523\n",
      "ph_min : 0.051725360953710026, ph_max : 1.3448593847964605\n",
      "stddev : 0.21298667173087707, IQR : 0.19397010357641253\n",
      "1st quartile : 0.4138028876296802, 3rd quartile : 0.6077729912060927\n",
      "analysis time : 37.39311408996582 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = AR_testing_data_rnn_input.shape[0]\n",
    "\n",
    "analysis_time = time.time()\n",
    "\n",
    "AR_rnn_net = AR_RNN(\n",
    "    load_file=save_path+'/final_net_class_dict.txt',\n",
    "    T_input=T_sample_input_AR,\n",
    "    T_output=T_sample_output_AR,\n",
    "    stddev=0.0,\n",
    "    batch_size=num_runs,\n",
    "    lambda_reg=lambda_reg,\n",
    ")\n",
    "AR_rnn_net.build(input_shape=tuple(AR_testing_data_rnn_input.shape[0:2]) + tuple(testing_data_rnn_input.shape[2:]))\n",
    "AR_rnn_net.load_weights_from_file(save_path+'/final_net_gru_weights.h5')\n",
    "\n",
    "AR_AERNN_net = AR_AERNN(\n",
    "    ae_net,\n",
    "    AR_rnn_net,\n",
    "    normalization_arr,\n",
    "    normalization_constant_arr_aedata,\n",
    "    covmat_lmda=0.0,\n",
    "    time_stddev_ogdata=time_stddev_ogdata,\n",
    "    time_mean_ogdata=time_mean_ogdata,\n",
    "    loss_weights=None,\n",
    "    clipnorm=None,\n",
    "    global_clipnorm=None\n",
    ")\n",
    "\n",
    "savefig_fname = 'pre_ARtraining-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "npsavedata_fname = '/prediction_horizons-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "plot_dir = '/plots'\n",
    "\n",
    "sidx1 = dir_name_rnn[::-1].index('/')\n",
    "sidx2 = dir_name_rnn[-sidx1-2::-1].index('/')\n",
    "print(dir_name_rnn[-(sidx1+sidx2+1):])\n",
    "print('num_runs :', num_runs)\n",
    "\n",
    "prediction_horizon_arr = np.empty(shape=num_runs)\n",
    "prediction = np.array(AR_AERNN_net(AR_testing_data_rnn_input, training=False))\n",
    "prediction = invert_normalization(prediction, normalization_constant_arr_aedata)\n",
    "\n",
    "data_in_og = AR_testing_data_rnn_input\n",
    "data_out_og = AR_testing_data_rnn_output\n",
    "\n",
    "energySpectrum_dataout = 0.0\n",
    "energySpectrum_pred = 0.0\n",
    "\n",
    "avg_time = 0.\n",
    "for i in range(num_runs):\n",
    "    run_time = time.time()\n",
    "    lyap_time = lyapunov_time_arr[0]\n",
    "\n",
    "    data_out = data_out_og[i]\n",
    "    data_out = invert_normalization(data_out, normalization_constant_arr_aedata)\n",
    "\n",
    "    ### Error and prediction horizon\n",
    "    # error = np.linalg.norm(data_out[:, :] - prediction[i, :, :], axis=1)\n",
    "    error = (data_out[:, :] - prediction[i, :, :])**2\n",
    "    # error /= norm_sq_time_average(data_out)**0.5\n",
    "    error = np.mean(np.divide(error, time_stddev_ogdata**2), axis=1)**0.5\n",
    "\n",
    "    predhor_idx = np.where(error >= error_threshold)[0]\n",
    "    if predhor_idx.shape[0] == 0:\n",
    "        predhor_idx = error.shape[0]\n",
    "    else:\n",
    "        predhor_idx = predhor_idx[0]\n",
    "\n",
    "    prediction_horizon_arr[i] = predhor_idx*dt_rnn/lyap_time\n",
    "\n",
    "    run_time = time.time() - run_time\n",
    "    avg_time = (avg_time*i + run_time)/(i+1)\n",
    "    eta = avg_time * (num_runs-1 - i)\n",
    "    print('    {} / {} -- run_time : {:.2f} s -- eta : {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        i+1,\n",
    "        num_runs,\n",
    "        run_time,\n",
    "        float(eta // 3600),\n",
    "        float((eta%3600)//60),\n",
    "        float((eta%3600)%60),\n",
    "    ))\n",
    "\n",
    "median_idx = int(np.round(0.5*num_runs-1))\n",
    "quartile_1_idx = int(np.round(0.25*num_runs-1))\n",
    "quartile_3_idx = int(np.round(0.75*num_runs-1))\n",
    "\n",
    "prediction_horizon_arr.sort()\n",
    "\n",
    "median = prediction_horizon_arr[median_idx]\n",
    "quartile_1 = prediction_horizon_arr[quartile_1_idx]\n",
    "quartile_3 = prediction_horizon_arr[quartile_3_idx]\n",
    "IQR = quartile_3 - quartile_1\n",
    "\n",
    "prediction_horizon = np.mean(prediction_horizon_arr)\n",
    "stddev_ph = np.std(prediction_horizon_arr)\n",
    "\n",
    "s = 'error_threshold = {}\\n'.format(error_threshold)\n",
    "s += 'prediction_horizon : {}, median : {}\\n'.format(prediction_horizon, median)\n",
    "s += 'ph_min : {}, ph_max : {}\\n'.format(prediction_horizon_arr.min(), prediction_horizon_arr.max())\n",
    "s += 'stddev : {}, IQR : {}\\n'.format(stddev_ph, IQR)\n",
    "s += '1st quartile : {}, 3rd quartile : {}'.format(quartile_1, quartile_3)\n",
    "\n",
    "print('\\n'+s)\n",
    "\n",
    "plot_histogram_and_save(\n",
    "    prediction_horizon_arr, median,\n",
    "    save_dir=dir_name_rnn+plot_dir,\n",
    "    savefig_fname=savefig_fname,\n",
    ")\n",
    "\n",
    "np.savez(\n",
    "    dir_name_rnn+npsavedata_fname,\n",
    "    prediction_horizon_arr=prediction_horizon_arr,\n",
    "    error_threshold=error_threshold,\n",
    ")\n",
    "\n",
    "with open(dir_name_rnn+npsavedata_fname+'--statistics.txt', 'w') as fl:\n",
    "    fl.write(s)\n",
    "\n",
    "print('analysis time : {} s\\n'.format(time.time() - analysis_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'use_trainable_weights_with_reslayers' in rnn_net.__dict__.keys():\n",
    "    if use_trainable_weights_with_reslayers == True:\n",
    "        for i in range(rnn_net.num_skip_connections):\n",
    "            print('reslayer_factor_{} : {}'.format(i, rnn_net.reslayer_factor[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
