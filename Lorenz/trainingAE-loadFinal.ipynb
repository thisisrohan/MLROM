{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4xhxMpe_r-Y5"},"outputs":[],"source":["# enabling 3rd party widgets\n","# from google.colab import output\n","# output.enable_custom_widget_manager()\n","# output.disable_custom_widget_manager()\n","\n","# interactive 3D plot\n","# !pip install ipympl\n","# %matplotlib widget"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5qPupCDsjSz"},"outputs":[],"source":["import os\n","import math\n","from collections import OrderedDict\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import linalg\n","\n","import time as time\n","import platform as platform\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, losses\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.regularizers import L2\n","import h5py"]},{"cell_type":"code","source":[""],"metadata":{"id":"3AVrZNGlZu4Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxAd7iDL0Ami"},"outputs":[],"source":["current_sys = platform.system()\n","\n","if current_sys == 'Windows':\n","    dir_sep = '\\\\'\n","else:\n","    dir_sep = '/'"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjNnPRuk0IIX","executionInfo":{"status":"ok","timestamp":1656593329168,"user_tz":-120,"elapsed":1696,"user":{"displayName":"Rohan Kaushik","userId":"13918477614376051685"}},"outputId":"e46083d6-1bb3-4c23-ae46-c5a1e9dc216a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/')\n","print(os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9REiGIIy0IzV","executionInfo":{"status":"ok","timestamp":1656593329169,"user_tz":-120,"elapsed":27,"user":{"displayName":"Rohan Kaushik","userId":"13918477614376051685"}},"outputId":"bdaa5f59-9a07-4a01-c2cb-6947e644a525"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8S1AHEkl48bn"},"outputs":[],"source":["from tools.misc_tools import create_Lorenz_data, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data, plot_latent_states, readAndReturnLossHistories\n","from tools.ae_v3 import Autoencoder"]},{"cell_type":"code","source":["behaviour = 'initialiseAndTrainFromScratch'\n","# behaviour = 'loadCheckpointAndContinueTraining'\n","# behaviour = 'loadFinalNetAndPlot'"],"metadata":{"id":"-mIQj_v4gzMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting seed for PRNGs\n","if behaviour == 'initialiseAndTrainFromScratch':\n","    prng_seed = 42\n","    np.random.seed(prng_seed)\n","    tf.random.set_seed(prng_seed)"],"metadata":{"id":"QL5n-abCg0nI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"tc3zO9xL_tNl","outputId":"93252cb5-d45b-4fa7-dcd1-418412a5c079","executionInfo":{"status":"ok","timestamp":1656593329463,"user_tz":-120,"elapsed":12,"user":{"displayName":"Rohan Kaushik","userId":"13918477614376051685"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["''"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["tf.test.gpu_device_name()"]},{"cell_type":"markdown","metadata":{"id":"7UbdnOtc4_z9"},"source":["# Lorenz System"]},{"cell_type":"code","source":["# setting up params (and saving, if applicable)\n","from numpy import *\n","\n","if behaviour == 'initialiseAndTrainFromScratch':\n","    # simutlation paramaters\n","    sigma_arr = np.array([10])\n","    rho_arr = np.array([20])\n","    beta_arr = np.array([4/3])\n","\n","    x0 = 1\n","    y0 = 1\n","    z0 = 1\n","\n","    t0 = 0.0\n","    T = 100.0\n","    delta_t = 0.01\n","\n","    return_params_arr = False\n","    normalize_flag = True\n","\n","    # making ae save directory\n","    dir_name_ae = os.getcwd() + dir_sep + 'saved_ae'\n","    if not os.path.isdir(dir_name_ae):\n","        os.makedirs(dir_name_ae)\n","\n","    counter = 0\n","    while True:\n","        dir_check = 'ae_' + str(counter).zfill(3)\n","        if os.path.isdir(dir_name_ae + dir_sep + dir_check):\n","            counter += 1\n","        else:\n","            break\n","\n","    dir_name_ae = dir_name_ae + dir_sep + dir_check\n","    os.makedirs(dir_name_ae)\n","    os.makedirs(dir_name_ae+dir_sep+'plots')\n","\n","    # saving sim data\n","    sim_data = {\n","        'rho_arr':rho_arr,\n","        'sigma_arr':sigma_arr,\n","        'beta_arr':beta_arr,\n","        'x0':x0,\n","        'y0':y0,\n","        'z0':z0,\n","        't0':t0,\n","        'T':T,\n","        'delta_t':delta_t,\n","        'return_params_arr':return_params_arr,\n","        'normalize_flag':normalize_flag\n","    }\n","    with open(dir_name_ae+dir_sep+'sim_data_params.txt', 'w') as f:\n","        f.write(str(sim_data))\n","\n","else:\n","    # simutlation paramaters\n","    dir_name_ae = os.getcwd()+'{ds}saved_ae{ds}ae_003'.format(ds=dir_sep)\n","\n","    with open(dir_name_ae + dir_sep + 'sim_data_params.txt') as f:\n","        lines = f.readlines()\n","\n","    params_dict = eval(''.join(lines))\n","\n","    rho_arr = params_dict['rho_arr']\n","    sigma_arr = params_dict['sigma_arr']\n","    beta_arr = params_dict['beta_arr']\n","\n","    x0 = params_dict['x0']\n","    y0 = params_dict['y0']\n","    z0 = params_dict['z0']\n","\n","    t0 = params_dict['t0']\n","    T = params_dict['T']\n","    delta_t = params_dict['delta_t']\n","\n","    return_params_arr = params_dict['return_params_arr']\n","    normalize_flag = params_dict['normalize_flag']"],"metadata":{"id":"xcNgt4hqg6Xv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7sl7i5H5Dqz"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySVDz_2U5FH5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkQx9q_p5Gro"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDhfYHU45IS8"},"outputs":[],"source":["res_dict = create_Lorenz_data(\n","    T, t0, delta_t,\n","    rho_arr, sigma_arr, beta_arr,\n","    x0, y0, z0, return_params_arr=return_params_arr,\n","    normalize=normalize_flag\n",")\n","\n","all_data = res_dict['all_data']\n","N = res_dict['N']\n","boundary_idx_arr = res_dict['boundary_idx_arr']\n","\n","if return_params_arr == True:\n","    params_arr = res_dict['params_arr']\n","\n","if normalize_flag == True:\n","    normalization_constant_arr = res_dict['normalization_constant_arr']"]},{"cell_type":"code","source":[""],"metadata":{"id":"59kkrSP1GvzO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MJa7P5t5KiC","scrolled":true},"outputs":[],"source":["n = len(boundary_idx_arr)\n","# # '''\n","# num_cols = 1\n","# num_rows = n\n","\n","# fig = plt.figure(figsize=(7.5*num_cols, 7.5*num_rows))\n","\n","# prev_idx = 0\n","# for i in range(n):\n","#     # ax = plt.axes(projection ='3d')\n","#     next_idx = boundary_idx_arr[i]\n","    \n","#     ax_orig = fig.add_subplot(num_rows, num_cols, i+1, projection ='3d')\n","#     ax_orig.plot(all_data[prev_idx:next_idx, 0], all_data[prev_idx:next_idx, 1], all_data[prev_idx:next_idx, 2])\n","#     ax_orig.title.set_text(r'Actual Data - [$\\sigma$, $\\rho$, $\\beta$] = ' + str(all_data[next_idx-1, 3:]))\n","#     ax_orig.set_xlabel('x')\n","#     ax_orig.set_ylabel('y')\n","#     ax_orig.set_zlabel('z')\n","    \n","#     # ax_predict = fig.add_subplot(num_rows, num_cols, 2*i+2, projection ='3d')\n","#     # ax_predict.plot(reconstructed_data[prev_idx:next_idx, 0], reconstructed_data[prev_idx:next_idx, 1], reconstructed_data[prev_idx:next_idx, 2])\n","#     # ax_predict.title.set_text(r'NN Reconstructed Data - [$\\sigma$, $\\rho$, $\\beta$] = ' + str(all_data[next_idx-1, 3:])\n","#     # )\n","#     # ax_predict.set_xlabel('x')\n","#     # ax_predict.set_ylabel('y')\n","#     # ax_predict.set_zlabel('z')\n","\n","#     prev_idx = next_idx\n","# # '''"]},{"cell_type":"markdown","metadata":{"id":"1v6KQEjR5LkK"},"source":["# Autoencoder"]},{"cell_type":"code","source":["# setting up training params\n","if behaviour == 'initialiseAndTrainFromScratch':\n","    learning_rate_list = [0.001, 0.0001, 0.00001]\n","    epochs = 2000\n","    patience = 200  # parameter for early stopping\n","    min_delta = 1e-6  # parameter for early stopping\n","    lambda_reg = 1e-5  # weight for regularizer\n","    train_split = 0.8\n","    val_split = 0.1\n","    test_split = 1 - train_split - val_split\n","    batch_size = 64\n","\n","    # saving training params\n","    training_specific_params = {\n","        'learning_rate_list':learning_rate_list,\n","        'epochs':epochs,\n","        'patience':patience,\n","        'min_delta':min_delta,\n","        'prng_seed':prng_seed,\n","        'train_split':train_split,\n","        'val_split':val_split,\n","        'batch_size':batch_size\n","    }\n","\n","    with open(dir_name_ae+dir_sep+'training_specific_params.txt', 'w') as f:\n","        f.write(str(training_specific_params))\n","else:\n","    with open(dir_name_ae + dir_sep + 'training_specific_params.txt') as f:\n","        lines = f.readlines()\n","\n","    tparams_dict = eval(''.join(lines))\n","\n","    learning_rate_list = tparams_dict['learning_rate_list']\n","    epochs = tparams_dict['epochs']\n","    patience = tparams_dict['patience']\n","    min_delta = tparams_dict['min_delta']\n","    prng_seed = tparams_dict['prng_seed']\n","    train_split = tparams_dict['train_split']\n","    val_split = tparams_dict['val_split']\n","    batch_size = tparams_dict['batch_size']\n","\n","    test_split = 1 - train_split - val_split\n","\n","    # setting seed for PRNGs\n","    np.random.seed(prng_seed)\n","    tf.random.set_seed(prng_seed)"],"metadata":{"id":"c5cjQ1lnjcwt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lovTI3zuhlX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjsRi02g5ORG"},"outputs":[],"source":["# setting up data\n","idx = np.arange(all_data.shape[0])\n","np.random.shuffle(idx)\n","boundary = int(np.round((1-test_split)*all_data.shape[0]))\n","training_data = all_data[idx[0:boundary], :]\n","testing_data = all_data[idx[boundary:], :]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qwietg7eTG-s"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJ-28EnzJ4Ur"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xTsmS7lgpps"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7l5kI1tfMszJ"},"outputs":[],"source":["# Initialize network\n","if behaviour == 'initialiseAndTrainFromScratch':\n","    ae_net = Autoencoder(\n","        data_dim=6,\n","        enc_layers=[16,12,8,8,4,4,2],\n","        dec_layers=[2,4,4,8,8,12,16],\n","        latent_space_dim=2,\n","        lambda_reg=lambda_reg,\n","        reg_name='L2',\n","        enc_layer_act_func='elu',\n","        enc_final_layer_act_func='tanh',\n","        dec_layer_act_func='elu',\n","        dec_final_layer_act_func='linear',\n","        load_file=None)\n","    # saving the AE configuration\n","    save_path = dir_name_ae+dir_sep+'final_net'\n","    if not os.path.isdir(save_path):\n","        os.makedirs(save_path)\n","    ae_net.save_class_dict(save_path+dir_sep+'final_net_class_dict.txt')\n","else:\n","    load_file = dir_name_ae + dir_sep + 'final_net' + 'final_net_class_dict.txt'\n","    ae_net = Autoencoder(data_dim=6, load_file=load_file)\n","    \n","    if behaviour == 'loadCheckpointAndContinueTraining':\n","        wt_file = dir_name_ae+dir_sep+'checkpoints'+dir_sep+'checkpoint'\n","        # ae_net.load_weights(wt_file)\n","    elif behaviour == 'loadFinalNetAndPlot':\n","        wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'\n","    ae_net.load_weights_from_file(wt_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48tkgZxT0Amt"},"outputs":[],"source":[""]},{"cell_type":"code","source":["if behaviour == 'initialiseAndTrainFromScratch':\n","    val_loss_hist = []\n","    train_loss_hist = []\n","    lr_change=[0, 0]\n","    savelosses_cb_vallossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n","    savelosses_cb_trainlossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n","    starting_lr_idx = 0\n","    num_epochs_left = epochs\n","elif behaviour == 'loadCheckpointAndContinueTraining':\n","    val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt = readAndReturnLossHistories(\n","        dir_name_ae=dir_name_ae,\n","        dir_sep=dir_sep,\n","        epochs=epochs,\n","        learning_rate_list=learning_rate_list)\n","    savelosses_cb_vallossarr = val_loss_arr_fromckpt\n","    savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n","elif behaviour == 'loadFinalNetAndPlot':\n","    with open(dir_name_ae+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n","        lines = f.readlines()\n","    \n","    losses_dict = eval(''.join(lines))\n","\n","    val_loss_hist = losses_dict['val_loss_hist']\n","    train_loss_hist = losses_dict['train_loss_hist']\n","    lr_change = losses_dict['lr_change']\n","    test_loss = losses_dict['test_loss']"],"metadata":{"id":"yUChBAKqIFtX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Z0oEGp6WKGu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gELga1WnQeMK","outputId":"82d1c95c-9354-43ed-c640-dfe3ea8f6ade","scrolled":true,"executionInfo":{"status":"error","timestamp":1656593375693,"user_tz":-120,"elapsed":42891,"user":{"displayName":"Rohan Kaushik","userId":"13918477614376051685"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","\n","\n","--------------------------------------------------------------------------------\n","\n","---------------------------- LEARNING RATE : 0.001 -----------------------------\n","\n","--------------------------------------------------------------------------------\n","\n","\n","Epoch 1/2000\n","118/124 [===========================>..] - ETA: 0s - loss: 43.9576 - tot_time: 0h 0m 3.3s\n","124/124 [==============================] - 3s 6ms/step - loss: 42.1691 - val_loss: 0.2456\n","Epoch 2/2000\n","118/124 [===========================>..] - ETA: 0s - loss: 0.0502 - tot_time: 0h 0m 3.7s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.0340\n","Epoch 3/2000\n","115/124 [==========================>...] - ETA: 0s - loss: 0.0341 - tot_time: 0h 0m 4.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0346\n","Epoch 4/2000\n","124/124 [==============================] - ETA: 0s - loss: 0.0341 - tot_time: 0h 0m 4.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0334\n","Epoch 5/2000\n","111/124 [=========================>....] - ETA: 0s - loss: 0.0341 - tot_time: 0h 0m 4.8s\n","\n","Epoch 5: val_loss improved from inf to 0.03364, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0340 - val_loss: 0.0336\n","Epoch 6/2000\n","112/124 [==========================>...] - ETA: 0s - loss: 0.0338 - tot_time: 0h 0m 5.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0336\n","Epoch 7/2000\n","103/124 [=======================>......] - ETA: 0s - loss: 0.0342 - tot_time: 0h 0m 5.7s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0344\n","Epoch 8/2000\n","114/124 [==========================>...] - ETA: 0s - loss: 0.0340 - tot_time: 0h 0m 6.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0333\n","Epoch 9/2000\n","107/124 [========================>.....] - ETA: 0s - loss: 0.0341 - tot_time: 0h 0m 6.4s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0334\n","Epoch 10/2000\n","104/124 [========================>.....] - ETA: 0s - loss: 0.0339 - tot_time: 0h 0m 6.8s\n","\n","Epoch 10: val_loss improved from 0.03364 to 0.03317, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 0.0332\n","Epoch 11/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0340 - tot_time: 0h 0m 7.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0331\n","Epoch 12/2000\n","121/124 [============================>.] - ETA: 0s - loss: 0.0338 - tot_time: 0h 0m 7.7s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0330\n","Epoch 13/2000\n","109/124 [=========================>....] - ETA: 0s - loss: 0.0336 - tot_time: 0h 0m 8.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0333\n","Epoch 14/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0336 - tot_time: 0h 0m 8.4s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0332\n","Epoch 15/2000\n","107/124 [========================>.....] - ETA: 0s - loss: 0.0345 - tot_time: 0h 0m 8.7s\n","\n","Epoch 15: val_loss improved from 0.03317 to 0.03305, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0340 - val_loss: 0.0331\n","Epoch 16/2000\n","111/124 [=========================>....] - ETA: 0s - loss: 0.0338 - tot_time: 0h 0m 9.2s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0333\n","Epoch 17/2000\n","104/124 [========================>.....] - ETA: 0s - loss: 0.0339 - tot_time: 0h 0m 9.6s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0335\n","Epoch 18/2000\n","111/124 [=========================>....] - ETA: 0s - loss: 0.0339 - tot_time: 0h 0m 10.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0330\n","Epoch 19/2000\n","108/124 [=========================>....] - ETA: 0s - loss: 0.0339 - tot_time: 0h 0m 10.4s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0331\n","Epoch 20/2000\n","106/124 [========================>.....] - ETA: 0s - loss: 0.0336 - tot_time: 0h 0m 10.7s\n","\n","Epoch 20: val_loss improved from 0.03305 to 0.03303, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0330\n","Epoch 21/2000\n","124/124 [==============================] - ETA: 0s - loss: 0.0336 - tot_time: 0h 0m 11.2s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0329\n","Epoch 22/2000\n","122/124 [============================>.] - ETA: 0s - loss: 0.0337 - tot_time: 0h 0m 11.6s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0332\n","Epoch 23/2000\n","112/124 [==========================>...] - ETA: 0s - loss: 0.0335 - tot_time: 0h 0m 11.9s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0330\n","Epoch 24/2000\n","114/124 [==========================>...] - ETA: 0s - loss: 0.0336 - tot_time: 0h 0m 12.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0340\n","Epoch 25/2000\n","123/124 [============================>.] - ETA: 0s - loss: 0.0335 - tot_time: 0h 0m 12.7s\n","\n","Epoch 25: val_loss improved from 0.03303 to 0.03288, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0335 - val_loss: 0.0329\n","Epoch 26/2000\n","112/124 [==========================>...] - ETA: 0s - loss: 0.0335 - tot_time: 0h 0m 13.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0335 - val_loss: 0.0329\n","Epoch 27/2000\n","124/124 [==============================] - ETA: 0s - loss: 0.0333 - tot_time: 0h 0m 13.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0337\n","Epoch 28/2000\n","113/124 [==========================>...] - ETA: 0s - loss: 0.0337 - tot_time: 0h 0m 13.8s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0334\n","Epoch 29/2000\n","115/124 [==========================>...] - ETA: 0s - loss: 0.0333 - tot_time: 0h 0m 14.2s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0328\n","Epoch 30/2000\n","116/124 [===========================>..] - ETA: 0s - loss: 0.0334 - tot_time: 0h 0m 14.6s\n","\n","Epoch 30: val_loss improved from 0.03288 to 0.03230, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 1s 4ms/step - loss: 0.0333 - val_loss: 0.0323\n","Epoch 31/2000\n","109/124 [=========================>....] - ETA: 0s - loss: 0.0330 - tot_time: 0h 0m 15.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0383\n","Epoch 32/2000\n","119/124 [===========================>..] - ETA: 0s - loss: 0.0340 - tot_time: 0h 0m 15.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0332\n","Epoch 33/2000\n","113/124 [==========================>...] - ETA: 0s - loss: 0.0340 - tot_time: 0h 0m 15.8s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0331\n","Epoch 34/2000\n","108/124 [=========================>....] - ETA: 0s - loss: 0.0339 - tot_time: 0h 0m 16.2s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0330\n","Epoch 35/2000\n","112/124 [==========================>...] - ETA: 0s - loss: 0.0337 - tot_time: 0h 0m 16.6s\n","\n","Epoch 35: val_loss did not improve from 0.03230\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0333\n","Epoch 36/2000\n","116/124 [===========================>..] - ETA: 0s - loss: 0.0337 - tot_time: 0h 0m 17.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0338\n","Epoch 37/2000\n","106/124 [========================>.....] - ETA: 0s - loss: 0.0337 - tot_time: 0h 0m 17.4s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0326\n","Epoch 38/2000\n","108/124 [=========================>....] - ETA: 0s - loss: 0.0330 - tot_time: 0h 0m 17.7s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0330 - val_loss: 0.0317\n","Epoch 39/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0325 - tot_time: 0h 0m 18.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0324\n","Epoch 40/2000\n","124/124 [==============================] - ETA: 0s - loss: 0.0309 - tot_time: 0h 0m 18.5s\n","\n","Epoch 40: val_loss improved from 0.03230 to 0.03095, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0309 - val_loss: 0.0310\n","Epoch 41/2000\n","115/124 [==========================>...] - ETA: 0s - loss: 0.0281 - tot_time: 0h 0m 19.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0257\n","Epoch 42/2000\n","124/124 [==============================] - ETA: 0s - loss: 0.0225 - tot_time: 0h 0m 19.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0194\n","Epoch 43/2000\n","122/124 [============================>.] - ETA: 0s - loss: 0.0182 - tot_time: 0h 0m 19.8s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0182 - val_loss: 0.0170\n","Epoch 44/2000\n","120/124 [============================>.] - ETA: 0s - loss: 0.0168 - tot_time: 0h 0m 20.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0159\n","Epoch 45/2000\n","121/124 [============================>.] - ETA: 0s - loss: 0.0163 - tot_time: 0h 0m 20.6s\n","\n","Epoch 45: val_loss improved from 0.03095 to 0.01583, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 1s 5ms/step - loss: 0.0163 - val_loss: 0.0158\n","Epoch 46/2000\n","118/124 [===========================>..] - ETA: 0s - loss: 0.0160 - tot_time: 0h 0m 21.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.0152\n","Epoch 47/2000\n","118/124 [===========================>..] - ETA: 0s - loss: 0.0157 - tot_time: 0h 0m 21.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0157 - val_loss: 0.0154\n","Epoch 48/2000\n","116/124 [===========================>..] - ETA: 0s - loss: 0.0155 - tot_time: 0h 0m 21.9s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0154 - val_loss: 0.0152\n","Epoch 49/2000\n","118/124 [===========================>..] - ETA: 0s - loss: 0.0153 - tot_time: 0h 0m 22.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0154 - val_loss: 0.0149\n","Epoch 50/2000\n","116/124 [===========================>..] - ETA: 0s - loss: 0.0153 - tot_time: 0h 0m 22.7s\n","\n","Epoch 50: val_loss improved from 0.01583 to 0.01513, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0151\n","Epoch 51/2000\n","114/124 [==========================>...] - ETA: 0s - loss: 0.0151 - tot_time: 0h 0m 23.2s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 0.0146\n","Epoch 52/2000\n","111/124 [=========================>....] - ETA: 0s - loss: 0.0151 - tot_time: 0h 0m 23.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 0.0148\n","Epoch 53/2000\n","108/124 [=========================>....] - ETA: 0s - loss: 0.0152 - tot_time: 0h 0m 23.9s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0160\n","Epoch 54/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0153 - tot_time: 0h 0m 24.2s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0150\n","Epoch 55/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0148 - tot_time: 0h 0m 24.6s\n","\n","Epoch 55: val_loss improved from 0.01513 to 0.01510, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0149 - val_loss: 0.0151\n","Epoch 56/2000\n","105/124 [========================>.....] - ETA: 0s - loss: 0.0149 - tot_time: 0h 0m 25.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.0164\n","Epoch 57/2000\n","111/124 [=========================>....] - ETA: 0s - loss: 0.0150 - tot_time: 0h 0m 25.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0150 - val_loss: 0.0149\n","Epoch 58/2000\n","123/124 [============================>.] - ETA: 0s - loss: 0.0150 - tot_time: 0h 0m 25.9s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0150 - val_loss: 0.0144\n","Epoch 59/2000\n","105/124 [========================>.....] - ETA: 0s - loss: 0.0148 - tot_time: 0h 0m 26.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0147 - val_loss: 0.0147\n","Epoch 60/2000\n","113/124 [==========================>...] - ETA: 0s - loss: 0.0147 - tot_time: 0h 0m 26.6s\n","\n","Epoch 60: val_loss did not improve from 0.01510\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0147 - val_loss: 0.0156\n","Epoch 61/2000\n","106/124 [========================>.....] - ETA: 0s - loss: 0.0148 - tot_time: 0h 0m 27.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0147 - val_loss: 0.0146\n","Epoch 62/2000\n","121/124 [============================>.] - ETA: 0s - loss: 0.0147 - tot_time: 0h 0m 27.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0146 - val_loss: 0.0143\n","Epoch 63/2000\n","120/124 [============================>.] - ETA: 0s - loss: 0.0145 - tot_time: 0h 0m 27.8s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0145 - val_loss: 0.0150\n","Epoch 64/2000\n","112/124 [==========================>...] - ETA: 0s - loss: 0.0148 - tot_time: 0h 0m 28.2s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0147 - val_loss: 0.0142\n","Epoch 65/2000\n","107/124 [========================>.....] - ETA: 0s - loss: 0.0145 - tot_time: 0h 0m 28.6s\n","\n","Epoch 65: val_loss improved from 0.01510 to 0.01481, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 1s 4ms/step - loss: 0.0146 - val_loss: 0.0148\n","Epoch 66/2000\n","104/124 [========================>.....] - ETA: 0s - loss: 0.0144 - tot_time: 0h 0m 29.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0145 - val_loss: 0.0141\n","Epoch 67/2000\n","109/124 [=========================>....] - ETA: 0s - loss: 0.0145 - tot_time: 0h 0m 29.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0145 - val_loss: 0.0150\n","Epoch 68/2000\n","124/124 [==============================] - ETA: 0s - loss: 0.0144 - tot_time: 0h 0m 29.9s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0144 - val_loss: 0.0147\n","Epoch 69/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0146 - tot_time: 0h 0m 30.2s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0145 - val_loss: 0.0141\n","Epoch 70/2000\n","123/124 [============================>.] - ETA: 0s - loss: 0.0144 - tot_time: 0h 0m 30.6s\n","\n","Epoch 70: val_loss improved from 0.01481 to 0.01384, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.0138\n","Epoch 71/2000\n","114/124 [==========================>...] - ETA: 0s - loss: 0.0141 - tot_time: 0h 0m 31.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0141 - val_loss: 0.0139\n","Epoch 72/2000\n","118/124 [===========================>..] - ETA: 0s - loss: 0.0141 - tot_time: 0h 0m 31.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0141 - val_loss: 0.0147\n","Epoch 73/2000\n","106/124 [========================>.....] - ETA: 0s - loss: 0.0140 - tot_time: 0h 0m 31.9s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0139 - val_loss: 0.0137\n","Epoch 74/2000\n","105/124 [========================>.....] - ETA: 0s - loss: 0.0140 - tot_time: 0h 0m 32.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0140 - val_loss: 0.0140\n","Epoch 75/2000\n","117/124 [===========================>..] - ETA: 0s - loss: 0.0137 - tot_time: 0h 0m 32.6s\n","\n","Epoch 75: val_loss did not improve from 0.01384\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0137 - val_loss: 0.0139\n","Epoch 76/2000\n","109/124 [=========================>....] - ETA: 0s - loss: 0.0135 - tot_time: 0h 0m 33.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 0.0138\n","Epoch 77/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0135 - tot_time: 0h 0m 33.4s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0135 - val_loss: 0.0135\n","Epoch 78/2000\n","109/124 [=========================>....] - ETA: 0s - loss: 0.0133 - tot_time: 0h 0m 33.8s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.0132\n","Epoch 79/2000\n","114/124 [==========================>...] - ETA: 0s - loss: 0.0132 - tot_time: 0h 0m 34.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.0146\n","Epoch 80/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0128 - tot_time: 0h 0m 34.5s\n","\n","Epoch 80: val_loss improved from 0.01384 to 0.01295, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0129\n","Epoch 81/2000\n","107/124 [========================>.....] - ETA: 0s - loss: 0.0128 - tot_time: 0h 0m 35.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0125\n","Epoch 82/2000\n","120/124 [============================>.] - ETA: 0s - loss: 0.0127 - tot_time: 0h 0m 35.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0136\n","Epoch 83/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0127 - tot_time: 0h 0m 35.7s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0133\n","Epoch 84/2000\n","109/124 [=========================>....] - ETA: 0s - loss: 0.0123 - tot_time: 0h 0m 36.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0125\n","Epoch 85/2000\n","108/124 [=========================>....] - ETA: 0s - loss: 0.0122 - tot_time: 0h 0m 36.4s\n","\n","Epoch 85: val_loss improved from 0.01295 to 0.01256, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0126\n","Epoch 86/2000\n","114/124 [==========================>...] - ETA: 0s - loss: 0.0119 - tot_time: 0h 0m 36.9s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0121\n","Epoch 87/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0119 - tot_time: 0h 0m 37.3s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0120\n","Epoch 88/2000\n","105/124 [========================>.....] - ETA: 0s - loss: 0.0119 - tot_time: 0h 0m 37.6s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0119\n","Epoch 89/2000\n","108/124 [=========================>....] - ETA: 0s - loss: 0.0118 - tot_time: 0h 0m 38.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0120\n","Epoch 90/2000\n","116/124 [===========================>..] - ETA: 0s - loss: 0.0117 - tot_time: 0h 0m 38.3s\n","\n","Epoch 90: val_loss improved from 0.01256 to 0.01201, saving model to /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/checkpoint\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0120\n","Epoch 91/2000\n","107/124 [========================>.....] - ETA: 0s - loss: 0.0120 - tot_time: 0h 0m 38.8s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0124\n","Epoch 92/2000\n","104/124 [========================>.....] - ETA: 0s - loss: 0.0117 - tot_time: 0h 0m 39.1s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0121\n","Epoch 93/2000\n","109/124 [=========================>....] - ETA: 0s - loss: 0.0118 - tot_time: 0h 0m 39.5s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0117\n","Epoch 94/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0115 - tot_time: 0h 0m 39.9s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0116\n","Epoch 95/2000\n","109/124 [=========================>....] - ETA: 0s - loss: 0.0116 - tot_time: 0h 0m 40.3s\n","\n","Epoch 95: val_loss did not improve from 0.01201\n"," - saving loss histories at /content/drive/MyDrive/Colab Notebooks/Thesis/Lorenz/saved_ae/ae_002/checkpoints/LossHistoriesCheckpoint\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0159\n","Epoch 96/2000\n","106/124 [========================>.....] - ETA: 0s - loss: 0.0123 - tot_time: 0h 0m 40.6s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.0132\n","Epoch 97/2000\n","110/124 [=========================>....] - ETA: 0s - loss: 0.0116 - tot_time: 0h 0m 41.0s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0116\n","Epoch 98/2000\n","104/124 [========================>.....] - ETA: 0s - loss: 0.0118 - tot_time: 0h 0m 41.4s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0121\n","Epoch 99/2000\n","104/124 [========================>.....] - ETA: 0s - loss: 0.0115 - tot_time: 0h 0m 41.8s\n","124/124 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0123\n","Epoch 100/2000\n"," 96/124 [======================>.......] - ETA: 0s - loss: 0.0114"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-7a059d54768e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_split\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimekeeper_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavelosses_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         )\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;34m\"\"\"Updates the progbar.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_init_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# One-indexed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_maybe_init_progbar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1067\u001b[0m       \u001b[0;31m# step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m       self.stateful_metrics = self.stateful_metrics.union(\n\u001b[0;32m-> 1069\u001b[0;31m           set(m.name for m in self.model.metrics))\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m       \u001b[0;31m# so that attr names are not load-bearing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# compiling the network\n","ae_net.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n","    loss=losses.MeanSquaredError(),\n","    run_eagerly=False\n",")\n","\n","if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n","    # implementing early stopping\n","    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        patience=patience,\n","        restore_best_weights=True,\n","        verbose=True,\n","        min_delta=min_delta\n","    )\n","\n","    # time callback for each epoch\n","    timekeeper_cb = mytimecallback()\n","\n","    # model checkpoint callback\n","    dir_name_ckpt = dir_name_ae+dir_sep+'checkpoints'\n","    if not os.path.isdir(dir_name_ckpt):\n","        os.makedirs(dir_name_ckpt)\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=dir_name_ckpt+dir_sep+'checkpoint',#+'/checkpoint--loss={loss:.4f}--vall_loss={val_loss:.4f}',\n","        monitor='val_loss',\n","        save_best_only=True,\n","        save_weights_only=True,\n","        verbose=2,\n","        period=5  # saves every 5 epochs\n","    )\n","\n","    # save losses callback\n","    savelosses_cb = SaveLosses(\n","        filepath=dir_name_ckpt+dir_sep+'LossHistoriesCheckpoint',\n","        val_loss_arr=savelosses_cb_vallossarr,\n","        train_loss_arr=savelosses_cb_trainlossarr,\n","        total_epochs=epochs,\n","        period=5)\n","\n","    # training the network\n","    for i in range(starting_lr_idx, len(learning_rate_list)):\n","        learning_rate = learning_rate_list[i]\n","        K.set_value(ae_net.optimizer.lr, learning_rate)\n","\n","        savelosses_cb.update_lr_idx(i)\n","\n","        if i == starting_lr_idx:\n","            EPOCHS = num_epochs_left\n","            savelosses_cb.update_offset(epochs-num_epochs_left)\n","        else:\n","            EPOCHS = epochs\n","            savelosses_cb.update_offset(0)\n","\n","        total_s_len = 80\n","        sep_lr_s = ' LEARNING RATE : {} '.format(learning_rate)\n","        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n","        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n","        print('\\n\\n' + '-'*len(sep_lr_s))\n","        print('\\n' + sep_lr_s+'\\n')\n","        print('-'*len(sep_lr_s) + '\\n\\n')\n","        \n","        history = ae_net.fit(training_data, training_data,\n","            epochs=EPOCHS,\n","            batch_size=batch_size,\n","            validation_split=val_split/train_split,\n","            callbacks=[early_stopping_cb, timekeeper_cb, checkpoint_cb, savelosses_cb],\n","            verbose=1\n","        )\n","\n","        val_loss_hist.extend(history.history['val_loss'])\n","        train_loss_hist.extend(history.history['loss'])\n","        \n","        if i == starting_lr_idx:\n","            lr_change[i+1] += len(history.history['val_loss'])\n","        else:\n","            lr_change.append(lr_change[i]+len(history.history['val_loss']))"]},{"cell_type":"code","source":["if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n","    test_loss = ae_net.evaluate(\n","        testing_data, testing_data,\n","    )\n","\n","    save_path = dir_name_ae+dir_sep+'final_net'\n","\n","    if not os.path.isdir(save_path):\n","        os.makedirs(save_path)\n","\n","\n","    with open(save_path+dir_sep+'losses.txt', 'w') as f:\n","        f.write(str({\n","            'val_loss_hist':val_loss_hist,\n","            'train_loss_hist':train_loss_hist,\n","            'lr_change':lr_change,\n","            'test_loss':test_loss\n","        }))\n","\n","    ae_net.save_everything(\n","        save_dir=save_path+dir_sep+'final_net')"],"metadata":{"id":"d_Od0ul4P9bK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WZWXz2WjRzbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"C-TW6oTlSUbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JAFlvxJBUom3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6NVzLHfwU2SW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oXJhKlboU9gp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_change"],"metadata":{"id":"Dy8GNcgMVD4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Upzed-grUiCb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewTz1COFSocM"},"outputs":[],"source":["# plotting losses\n","\n","# Visualize loss history\n","fig, ax = plot_losses(\n","    training_loss=train_loss_hist,\n","    val_loss=val_loss_hist,\n","    lr_change=lr_change,\n","    learning_rate_list=learning_rate_list\n",")\n","\n","plt.savefig(dir_name_ae+'{ds}plots{ds}loss_history.png'.format(ds=dir_sep), dpi=300, bbox_inches='tight')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwt4brHcOaXi"},"outputs":[],"source":["reconstructed_data = ae_net.predict(all_data)"]},{"cell_type":"code","source":["fig = plot_reconstructed_data(\n","    boundary_idx_arr=boundary_idx_arr,\n","    dir_name_ae=dir_name_ae,\n","    all_data=all_data,\n","    reconstructed_data=reconstructed_data,\n","    save_figs=False)"],"metadata":{"id":"-ALLN0mFaeSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zl6ZvgtNtA_u","scrolled":false},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPMTMcqk0Amv"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXs1pAq80Amv"},"outputs":[],"source":["plot_reconstructed_data(\n","    boundary_idx_arr=boundary_idx_arr,\n","    dir_name_ae=dir_name_ae,\n","    all_data=all_data,\n","    reconstructed_data=reconstructed_data,\n","    save_figs=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVqsAwsY0Amw"},"outputs":[],"source":["# create data\n","latent_states_all = ae_net.encoder_net.predict(all_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjgPNitSrt5p"},"outputs":[],"source":[""]},{"cell_type":"code","source":["fig, ax = plot_latent_states(\n","    boundary_idx_arr=boundary_idx_arr,\n","    latent_states_all=latent_states_all,\n","    all_data=all_data,\n","    xlim=[-1,1],\n","    ylim=[-1,1],\n","    cmap_name='gist_rainbow',\n","    legend_markerscale=10\n","    )\n","\n","plt.savefig(dir_name_ae + '{ds}plots{ds}latent_space.png'.format(ds=dir_sep), dpi=300, bbox_inches='tight')"],"metadata":{"id":"Jv8PgBgzV1_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnLnqg0Jrt5t"},"outputs":[],"source":["# ae_net.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOJE8vREtque"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"vEm2A0sB0Amx"},"outputs":[],"source":["n = len(boundary_idx_arr)\n","num_cols = 1\n","num_rows = 3*n\n","\n","# plt.ion()\n","\n","fig = plt.figure(figsize=(7.5*num_cols, 7.5*num_rows))\n","\n","Tt = N * delta_t\n","time_arr = np.arange(0, N+1) * Tt / N\n","\n","y_labels = [r'$x_1$', r'$x_2$', r'$x_3$']\n","\n","prev_idx = 0\n","for i in range(n):\n","    # ax = plt.axes(projection ='3d')\n","    next_idx = boundary_idx_arr[i]\n","\n","    for j in range(3):\n","        ax = fig.add_subplot(num_rows, num_cols, 3*i+j+1)\n","        ax.plot(time_arr, all_data[prev_idx:next_idx, j], label='original')\n","        ax.plot(time_arr, reconstructed_data[prev_idx:next_idx, j], label='reconstructed')\n","        ax.set_ylabel(y_labels[j])\n","        ax.set_xlabel('time')\n","        ax.grid(True)\n","\n","\n","    prev_idx = next_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvwVNuUl0Amy"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"8IAcFjRRn_IQ"},"source":["# LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPVqWNwjoAGP"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S21-VEUYrkk-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGnj8uQQ83-y"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0t2_8mzI1fhX"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIsWCXkbr7ws"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hx9ZaSpEMmv"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EENXaWqcKW7j"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8isZN1tYBifp"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixetsZHjCMKO"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hh1pbKjCcO4"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbLa0AwlDBWh"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGQN5p7rNVV3"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"trainingAE-loadFinal.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}