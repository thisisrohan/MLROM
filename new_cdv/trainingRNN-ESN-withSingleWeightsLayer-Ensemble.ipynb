{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "array = np.array\n",
    "float32 = np.float32\n",
    "int32 = np.int32\n",
    "float64 = np.float64\n",
    "int64 = np.int64\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/new_cdv\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses\n",
    "from tools.ae_v2 import Autoencoder\n",
    "from tools.ESN_v1 import ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": [
    "# setting seed for PRNGs\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    prng_seed = 42\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 20:31:33.222139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:33.222471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:33.343864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:33.344442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:33.344913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:33.345358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:33.352497: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 20:31:33.353333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:33.353864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:33.354302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:34.493896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:34.494356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from S"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 0\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "2.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:34.494773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 20:31:34.495059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3355 MB memory:  -> device: 0, name: Quadro K2200, pci bus id: 0000:02:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print(tf.config.list_logical_devices())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_ESN_ensemble/ESN_ensemble_006\n",
      "use_ae_data : True, dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_ae/ae_008\n",
      "data_dir_idx: 004\n",
      "normalize_flag_ogdata: False\n"
     ]
    }
   ],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "# from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # making RNN save directory\n",
    "    dir_name_rnn = os.getcwd() + dir_sep + 'saved_ESN_ensemble'\n",
    "    if not os.path.isdir(dir_name_rnn):\n",
    "        os.makedirs(dir_name_rnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'ESN_ensemble_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_rnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_rnn = dir_name_rnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_rnn)\n",
    "    os.makedirs(dir_name_rnn+dir_sep+'plots')\n",
    "    \n",
    "    # whether to use AE data or just work on raw data\n",
    "    use_ae_data = True # if false, specifying ae_idx will only show which dataset to use\n",
    "    \n",
    "    # autoencoder directory\n",
    "    ae_idx = '008'\n",
    "    dir_name_ae = os.getcwd()+'{ds}saved_ae{ds}ae_'.format(ds=dir_sep)+ae_idx\n",
    "else:\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_ESN/ESN_008'\n",
    "\n",
    "    # reading AE directory\n",
    "    with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_dict = eval(''.join(lines))\n",
    "\n",
    "    use_ae_data = params_dict['use_ae_data']\n",
    "    \n",
    "    dir_name_ae = params_dict['dir_name_ae']\n",
    "    ae_idx = dir_name_ae[-3:]\n",
    "    dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "\n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_rnn_dict['T_sample_output']\n",
    "    T_offset = params_rnn_dict['T_offset']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in RNN_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "    \n",
    "\n",
    "    normalization_arr = None\n",
    "    try:\n",
    "        with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        rnn_norm_arr_dict = eval(lines)\n",
    "        normalization_arr = rnn_norm_arr_dict['normalization_arr']\n",
    "    except:\n",
    "        pass\n",
    "    if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "        with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "            normalization_arr = fl['normalization_arr'][0]\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in ae_data, set to 'True'.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('use_ae_data : ' + str(use_ae_data) + ', dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state = params_dict['init_state']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data'].astype(FTYPE)\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case : 1, lyapunov exponent : 0.027086916024239262, lyapunov time : 36.91819381713867s\n"
     ]
    }
   ],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delaing with normalizing the data before feeding into autoencoder\n",
    "num_params = params_mat.shape[1]\n",
    "og_vars = all_data.shape[1]\n",
    "if alldata_withparams_flag == True:\n",
    "    og_vars -= num_params\n",
    "\n",
    "if use_ae_data == True:\n",
    "    if ae_data_with_params == True and alldata_withparams_flag == False:\n",
    "        new_all_data = np.empty(shape=(all_data.shape[0], og_vars+num_params), dtype=FTYPE)\n",
    "        new_all_data[:, 0:og_vars] = all_data[:, 0:og_vars]\n",
    "        del(all_data)\n",
    "        all_data = new_all_data\n",
    "        prev_idx = 0\n",
    "        for i in range(boundary_idx_arr.shape[0]):\n",
    "            all_data[prev_idx:boundary_idx_arr[i], num_params:] = params_mat[i]\n",
    "            prev_idx = boundary_idx_arr[i]\n",
    "\n",
    "    if normalizeforae_flag == True:\n",
    "        for i in range(all_data.shape[1]):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "    if ae_data_with_params == False:\n",
    "        all_data = all_data[:, 0:og_vars]\n",
    "else:\n",
    "    # using raw data, neglecting the params attached (if any)\n",
    "    all_data = all_data[:, 0:og_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "    wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "    ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Zl6ZvgtNtA_u",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776554,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lXpoaKRIneQc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1667868777509,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Q3a8HHyvneQc",
    "outputId": "51084913-6faf-4bb5-db69-2cbea705dd28"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "if use_ae_data == True:\n",
    "    latent_states_all = ae_net.encoder_net.predict(all_data)\n",
    "    del(all_data)\n",
    "else:\n",
    "    latent_states_all = all_data\n",
    "num_latent_states = latent_states_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1667868778304,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wjgPNitSrt5p",
    "outputId": "0c916524-33ec-47bf-a16a-51e53d2e25f6"
   },
   "outputs": [],
   "source": [
    "# plot_latent_states_KS(\n",
    "#     boundary_idx_arr,\n",
    "#     latent_states_all,\n",
    "#     delta_t,\n",
    "#     dir_name_ae,\n",
    "#     xticks_snapto=int(40*np.round((T//10)/40)),\n",
    "#     num_yticks=11,\n",
    "#     save_figs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868778305,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wnLnqg0Jrt5t"
   },
   "outputs": [],
   "source": [
    "# for i in range(ae_net.layers):\n",
    "#     tf.keras.utils.plot_model(\n",
    "#         ae_net.layers[i],\n",
    "#         to_file=dir_name_ae+'/plots/netlayer_{}.png'.format(i),\n",
    "#         show_shapes=True,\n",
    "#         dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BOJE8vREtque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fwjcsAxKneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "aFd7XgwVneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    dt_rnn = 0.2\n",
    "    num_input_tsteps = 25000\n",
    "    T_sample_input = num_input_tsteps*dt_rnn\n",
    "    T_sample_output = T_sample_input\n",
    "    T_offset = dt_rnn\n",
    "    normalize_dataset = True # whether the data for the RNN should be normalized by the dataset's mean and std\n",
    "    normalization_arr = None\n",
    "    stddev_multiplier = 3\n",
    "    skip_intermediate = 'full sample'\n",
    "    noise_type = 'normal' # can be 'uniform' or 'normal'\n",
    "    normalization_type = 'stddev' # can be 'minmax' or 'stddev', if it is\n",
    "                                  # 'minmax' then stddev_multiplier has no effect\n",
    "    ESN_layers_units = [4000] # [num_latent_states*1500]\n",
    "    stateful = True\n",
    "    omega_in = [0.5]\n",
    "    degree_of_connectivity = [3]\n",
    "    rho_res = [0.95]\n",
    "    usebias_Win = [False]\n",
    "    alpha = [0.5]\n",
    "    ESN_cell_activations = ['tanh']\n",
    "    usebias_Wout = True\n",
    "    \n",
    "    activation_post_Wout = 'linear'\n",
    "    use_weights_post_dense = False\n",
    "    \n",
    "    # computing sparsity\n",
    "    sparsity = [1-degree_of_connectivity[i]/(ESN_layers_units[i]-1) for i in range(len(ESN_layers_units))]\n",
    "        \n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "        \n",
    "\n",
    "    # saving simulation data\n",
    "    sim_data = {\n",
    "        'params_mat':params_mat,\n",
    "        'init_state_mat':init_state_mat,\n",
    "        't0':t0,\n",
    "        'T':T,\n",
    "        'delta_t':delta_t,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'dir_name_ae':dir_name_ae,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'use_ae_data':use_ae_data,\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'sim_data_AE_params.txt', 'w') as f:\n",
    "        f.write(str(sim_data))\n",
    "        \n",
    "    # saving RNN specific data\n",
    "    RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_input_tsteps':num_input_tsteps,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':ESN.__module__,\n",
    "        'noise_type':noise_type,\n",
    "        'normalization_type':normalization_type,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "UGnj8uQQ83-y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "0t2_8mzI1fhX"
   },
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    latent_states_all,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=normalize_dataset,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type,\n",
    "    FTYPE=FTYPE,\n",
    "    ITYPE=ITYPE)\n",
    "    \n",
    "data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "num_samples = rnn_res_dict['num_samples']\n",
    "normalization_arr = rnn_res_dict['normalization_arr']\n",
    "rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "pIsWCXkbr7ws"
   },
   "outputs": [],
   "source": [
    "temp = np.divide(latent_states_all-normalization_arr[0], normalization_arr[1])\n",
    "time_stddev = np.std(temp, axis=0)\n",
    "timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "del(org_data_idx_arr_input)\n",
    "del(org_data_idx_arr_output)\n",
    "del(latent_states_all)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Hem_9PUqneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778791,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uskBAAXpneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    num_ensemble_mems = 10\n",
    "    epochs = 1\n",
    "    lambda_reg = 3.692433157836884e-07 # weight for regularizer\n",
    "    min_delta = 1e-6\n",
    "    patience = 5\n",
    "    train_split = 0.8\n",
    "    val_split = 0.1\n",
    "    test_split = 1 - train_split - val_split\n",
    "    batch_size = 1\n",
    "    fRMS = 1e-3\n",
    "    use_best = False\n",
    "\n",
    "    stddev = fRMS * np.mean(time_stddev[0:og_vars]) # fRMS*timeMeanofSpaceRMS\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'epochs':epochs,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'timeMeanofSpaceRMS':timeMeanofSpaceRMS,\n",
    "        'stddev':stddev,\n",
    "        'lambda_reg':lambda_reg,\n",
    "        'min_delta':min_delta,\n",
    "        'patience':patience,\n",
    "        'use_best':use_best,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_rnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr],\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    # dir_name_rnn_og = dir_name_rnn\n",
    "    # dir_name_rnn_temp = '/home/rkaushik/Documents/Thesis/MLROM/CDV/saved_rnn/rnn_'+dir_name_rnn_og[-3:]\n",
    "    # dir_name_rnn = dir_name_rnn_temp\n",
    "\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    epochs = tparams_dict['epochs']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "    lambda_reg = tparams_dict['lambda_reg']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    patience = tparams_dict['patience']\n",
    "    try:\n",
    "        use_best = tparams_dict['use_best']\n",
    "    except:\n",
    "        print(\"'use_best' not present in 'training_specific_params', set to True.\")\n",
    "        use_best = True\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # setting seed for PRNGs\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "# idx = np.arange(data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# boundary = int(np.round(train_split*data_rnn_input.shape[0]))\n",
    "\n",
    "# training_data_rnn_input = data_rnn_input[idx[0:boundary]]\n",
    "# training_data_rnn_output = data_rnn_output[idx[0:boundary]]\n",
    "\n",
    "# testing_data_rnn_input = data_rnn_input[idx[boundary:]]\n",
    "# testing_data_rnn_output = data_rnn_output[idx[boundary:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1667868779601,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "EENXaWqcKW7j"
   },
   "outputs": [],
   "source": [
    "cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "# idx = np.arange(cum_samples)\n",
    "# np.random.shuffle(idx)\n",
    "num_train = 0\n",
    "num_val = 0\n",
    "begin_idx = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    num_samples = rnn_data_boundary_idx_arr[i] - begin_idx\n",
    "    num_train += int( np.round(train_split*num_samples) )\n",
    "    num_val += int( np.round(val_split*num_samples) )\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# defining shapes\n",
    "training_input_shape = [num_train]\n",
    "training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "training_output_shape = [num_train]\n",
    "training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "val_input_shape = [num_val]\n",
    "val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "val_output_shape = [num_train]\n",
    "val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "testing_input_shape = [cum_samples-num_train-num_val]\n",
    "testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "testing_output_shape = [cum_samples-num_train-num_val]\n",
    "testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "# defining required arrays\n",
    "training_data_rnn_input = np.empty(shape=training_input_shape, dtype=FTYPE)\n",
    "training_data_rnn_output = np.empty(shape=training_output_shape, dtype=FTYPE)\n",
    "\n",
    "val_data_rnn_input = np.empty(shape=val_input_shape, dtype=FTYPE)\n",
    "val_data_rnn_output = np.empty(shape=val_output_shape, dtype=FTYPE)\n",
    "\n",
    "testing_data_rnn_input = np.empty(shape=testing_input_shape, dtype=FTYPE)\n",
    "testing_data_rnn_output = np.empty(shape=testing_output_shape, dtype=FTYPE)\n",
    "\n",
    "begin_idx = 0\n",
    "training_data_rolling_count = 0\n",
    "val_data_rolling_count = 0\n",
    "testing_data_rolling_count = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    idx = np.arange(begin_idx, rnn_data_boundary_idx_arr[i])\n",
    "    # np.random.shuffle(idx)\n",
    "    num_samples = idx.shape[0]\n",
    "    num_train = int( np.round(train_split*num_samples) )\n",
    "    num_val = int( np.round(val_split*num_samples) )\n",
    "\n",
    "    training_data_rnn_input[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_input[idx[0:num_train]]\n",
    "    training_data_rnn_output[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_output[idx[0:num_train]]\n",
    "    training_data_rolling_count += num_train\n",
    "\n",
    "    val_data_rnn_input[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_input[idx[num_train:num_train+num_val]]\n",
    "    val_data_rnn_output[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_output[idx[num_train:num_train+num_val]]\n",
    "    val_data_rolling_count += num_val\n",
    "\n",
    "    num_test = num_samples-num_train-num_val+1\n",
    "    testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_input[idx[num_train+num_val:]]\n",
    "    testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_output[idx[num_train+num_val:]]\n",
    "    testing_data_rolling_count += num_test\n",
    "\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# cleaning up\n",
    "del(data_rnn_input)\n",
    "del(data_rnn_output)\n",
    "\n",
    "# further shuffling\n",
    "# idx = np.arange(0, training_data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# training_data_rnn_input = training_data_rnn_input[idx]\n",
    "# training_data_rnn_output = training_data_rnn_output[idx]\n",
    "\n",
    "# idx = np.arange(0, val_data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# val_data_rnn_input = val_data_rnn_input[idx]\n",
    "# val_data_rnn_output = val_data_rnn_output[idx]\n",
    "\n",
    "# idx = np.arange(0, testing_data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# testing_data_rnn_input = testing_data_rnn_input[idx]\n",
    "# testing_data_rnn_output = testing_data_rnn_output[idx]\n",
    "\n",
    "# del(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868779603,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8isZN1tYBifp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data_rnn_input.shape :  (7, 25000, 5)\n",
      " testing_data_rnn_input.shape :  (1, 25000, 5)\n",
      "     val_data_rnn_input.shape :  (1, 25000, 5)\n"
     ]
    }
   ],
   "source": [
    "print('training_data_rnn_input.shape : ', training_data_rnn_input.shape)\n",
    "print(' testing_data_rnn_input.shape : ', testing_data_rnn_input.shape)\n",
    "print('     val_data_rnn_input.shape : ', val_data_rnn_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779605,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "x3KglJsgneQj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ixetsZHjCMKO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_NSTtZuyneQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeMeanofSpaceRMS : 0.30974782\n",
      "stddev : 0.00033330994844436645\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "    print('timeMeanofSpaceRMS :', timeMeanofSpaceRMS)\n",
    "    print('stddev :', stddev)\n",
    "    if return_params_arr != False:\n",
    "        data_dim = num_latent_states + 3\n",
    "    else:\n",
    "        data_dim = num_latent_states\n",
    "\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    ensemble_lst = []\n",
    "    for i in range(num_ensemble_mems):\n",
    "        rnn_net = ESN(\n",
    "            data_dim=data_dim,\n",
    "            dt_rnn=dt_rnn,\n",
    "            lambda_reg=lambda_reg,\n",
    "            ESN_layers_units=ESN_layers_units,\n",
    "            stddev=stddev,\n",
    "            noise_type=noise_type,\n",
    "            stateful=stateful,\n",
    "            omega_in=omega_in,\n",
    "            sparsity=sparsity,\n",
    "            rho_res=rho_res,\n",
    "            usebias_Win=usebias_Win,\n",
    "            alpha=alpha,\n",
    "            ESN_cell_activations=ESN_cell_activations,\n",
    "            prng_seed=np.random.randint(low=0, high=prng_seed)*prng_seed,\n",
    "            usebias_Wout=usebias_Wout,\n",
    "            use_weights_post_dense=use_weights_post_dense,\n",
    "            activation_post_Wout=activation_post_Wout,\n",
    "            scalar_weights=[],\n",
    "        )\n",
    "        rnn_net.build(input_shape=(1,) + training_data_rnn_input.shape[1:])\n",
    "        rnn_net.save_class_dict(save_path+dir_sep+'{}_final_net_class_dict.txt'.format(i))\n",
    "        ensemble_lst.append(rnn_net)\n",
    "else:\n",
    "    load_file = dir_name_rnn + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "    \n",
    "    rnn_net = ESN(\n",
    "        load_file=load_file,\n",
    "    )\n",
    "    \n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        wt_file = tf.train.latest_checkpoint(dir_name_rnn+dir_sep+'checkpoints')\n",
    "    elif behaviour == 'loadFinalNetAndPlot':\n",
    "        wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'final_net_ESN_weights.h5'\n",
    "        # wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'f2'#+dir_sep+'saved_model.pb'\n",
    "        rnn_net.load_weights_from_file(wt_file)\n",
    "    \n",
    "    # this forces the model to initialize its kernel weights/biases\n",
    "    # temp = rnn_net.predict(tf.ones(shape=[batch_size, int(T_sample_input//dt_rnn), rnn_net.data_dim]))\n",
    "    # this loads just the kernel wieghts and biases of the model\n",
    "#     rnn_net.load_weights_from_file(wt_file)\n",
    "\n",
    "    # rnn_net = tf.keras.models.load_model(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    val_loss_hist = []\n",
    "    train_loss_hist = []\n",
    "# elif behaviour == 'loadCheckpointAndContinueTraining':\n",
    "#     val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt, earlystopping_wait = readAndReturnLossHistories(\n",
    "#         dir_name_ae=dir_name_rnn,\n",
    "#         dir_sep=dir_sep,\n",
    "#         epochs=epochs,\n",
    "#         learning_rate_list=learning_rate_list,\n",
    "#         return_earlystopping_wait=True)\n",
    "#     savelosses_cb_vallossarr = val_loss_arr_fromckpt\n",
    "#     savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n",
    "elif behaviour == 'loadFinalNetAndPlot':\n",
    "    with open(dir_name_rnn+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    losses_dict = eval(''.join(lines))\n",
    "\n",
    "    val_loss_hist = losses_dict['val_loss_hist']\n",
    "    train_loss_hist = losses_dict['train_loss_hist']\n",
    "    lr_change = losses_dict['lr_change']\n",
    "    test_loss = losses_dict['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_fn(y):\n",
    "    # post_Wout_activation = 'linear'\n",
    "    return y\n",
    "\n",
    "# def invert_fn(y):\n",
    "#     # post_Wout_activation = 'tanh'\n",
    "#     y = np.where(y < 1.0, y, 1-1e-6)\n",
    "#     y = np.where(y > -1.0, y, -1+1e-6)\n",
    "#     y = 0.5*(np.log(1+y) - np.log(1-y))\n",
    "#     return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_rnn_input.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ENSEMBLE MEMBER 1/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 14.04 s -- eta : 0h 1m 24s\n",
      "2 / 7 -- Wout batch_time : 13.97 s -- eta : 0h 1m 10s\n",
      "3 / 7 -- Wout batch_time : 15.33 s -- eta : 0h 0m 58s\n",
      "4 / 7 -- Wout batch_time : 13.97 s -- eta : 0h 0m 43s\n",
      "5 / 7 -- Wout batch_time : 15.08 s -- eta : 0h 0m 29s\n",
      "6 / 7 -- Wout batch_time : 16.24 s -- eta : 0h 0m 15s\n",
      "7 / 7 -- Wout batch_time : 9.74 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 104.72743201255798 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 8.135983943939209 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 8.145909309387207 sec\n",
      "2 / 7 -- batch_time : 8.464182615280151 sec\n",
      "3 / 7 -- batch_time : 8.95358657836914 sec\n",
      "4 / 7 -- batch_time : 3.3452587127685547 sec\n",
      "5 / 7 -- batch_time : 3.0219078063964844 sec\n",
      "6 / 7 -- batch_time : 3.128196954727173 sec\n",
      "7 / 7 -- batch_time : 3.3382997512817383 sec\n",
      "\n",
      "train_mse : 5.146285048900674e-09\n",
      "val_mse : 3.491984898573719e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 151.30119347572327 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.124385107686976e-06\n",
      "--- ENSEMBLE MEMBER 2/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 8.19 s -- eta : 0h 0m 49s\n",
      "2 / 7 -- Wout batch_time : 8.38 s -- eta : 0h 0m 41s\n",
      "3 / 7 -- Wout batch_time : 8.07 s -- eta : 0h 0m 33s\n",
      "4 / 7 -- Wout batch_time : 9.39 s -- eta : 0h 0m 26s\n",
      "5 / 7 -- Wout batch_time : 14.05 s -- eta : 0h 0m 19s\n",
      "6 / 7 -- Wout batch_time : 10.62 s -- eta : 0h 0m 10s\n",
      "7 / 7 -- Wout batch_time : 13.56 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 77.21131324768066 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 8.290699243545532 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 5.631591558456421 sec\n",
      "2 / 7 -- batch_time : 5.38115668296814 sec\n",
      "3 / 7 -- batch_time : 7.183993816375732 sec\n",
      "4 / 7 -- batch_time : 5.65360951423645 sec\n",
      "5 / 7 -- batch_time : 7.541264057159424 sec\n",
      "6 / 7 -- batch_time : 7.911566495895386 sec\n",
      "7 / 7 -- batch_time : 4.22701358795166 sec\n",
      "\n",
      "train_mse : 4.820173022628589e-09\n",
      "val_mse : 3.2598320103716105e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 129.0670886039734 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.269002723347512e-06\n",
      "--- ENSEMBLE MEMBER 3/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 13.44 s -- eta : 0h 1m 21s\n",
      "2 / 7 -- Wout batch_time : 14.90 s -- eta : 0h 1m 11s\n",
      "3 / 7 -- Wout batch_time : 11.79 s -- eta : 0h 0m 54s\n",
      "4 / 7 -- Wout batch_time : 14.39 s -- eta : 0h 0m 41s\n",
      "5 / 7 -- Wout batch_time : 14.51 s -- eta : 0h 0m 28s\n",
      "6 / 7 -- Wout batch_time : 10.75 s -- eta : 0h 0m 13s\n",
      "7 / 7 -- Wout batch_time : 13.93 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 99.76548886299133 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 9.510465383529663 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 4.303524494171143 sec\n",
      "2 / 7 -- batch_time : 8.550751447677612 sec\n",
      "3 / 7 -- batch_time : 7.427254676818848 sec\n",
      "4 / 7 -- batch_time : 5.3883185386657715 sec\n",
      "5 / 7 -- batch_time : 7.254589319229126 sec\n",
      "6 / 7 -- batch_time : 5.837985277175903 sec\n",
      "7 / 7 -- batch_time : 6.120746612548828 sec\n",
      "\n",
      "train_mse : 3.4118241565295193e-09\n",
      "val_mse : 3.550000474206172e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 154.20695400238037 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.5973207609931706e-06\n",
      "--- ENSEMBLE MEMBER 4/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 12.33 s -- eta : 0h 1m 14s\n",
      "2 / 7 -- Wout batch_time : 13.17 s -- eta : 0h 1m 4s\n",
      "3 / 7 -- Wout batch_time : 12.90 s -- eta : 0h 0m 51s\n",
      "4 / 7 -- Wout batch_time : 12.61 s -- eta : 0h 0m 38s\n",
      "5 / 7 -- Wout batch_time : 14.97 s -- eta : 0h 0m 26s\n",
      "6 / 7 -- Wout batch_time : 11.11 s -- eta : 0h 0m 13s\n",
      "7 / 7 -- Wout batch_time : 14.73 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 97.24366235733032 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 9.3352530002594 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 4.1906797885894775 sec\n",
      "2 / 7 -- batch_time : 2.6354267597198486 sec\n",
      "3 / 7 -- batch_time : 5.3124847412109375 sec\n",
      "4 / 7 -- batch_time : 8.190089464187622 sec\n",
      "5 / 7 -- batch_time : 8.815557479858398 sec\n",
      "6 / 7 -- batch_time : 7.181950569152832 sec\n",
      "7 / 7 -- batch_time : 9.779764175415039 sec\n",
      "\n",
      "train_mse : 7.0802204099607025e-09\n",
      "val_mse : 3.29375070577953e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 152.7368733882904 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.2173463801445905e-06\n",
      "--- ENSEMBLE MEMBER 5/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 8.16 s -- eta : 0h 0m 49s\n",
      "2 / 7 -- Wout batch_time : 8.15 s -- eta : 0h 0m 41s\n",
      "3 / 7 -- Wout batch_time : 8.12 s -- eta : 0h 0m 33s\n",
      "4 / 7 -- Wout batch_time : 8.19 s -- eta : 0h 0m 24s\n",
      "5 / 7 -- Wout batch_time : 8.24 s -- eta : 0h 0m 16s\n",
      "6 / 7 -- Wout batch_time : 8.26 s -- eta : 0h 0m 8s\n",
      "7 / 7 -- Wout batch_time : 13.37 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 68.22529196739197 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 3.787371873855591 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 8.647048950195312 sec\n",
      "2 / 7 -- batch_time : 6.943952798843384 sec\n",
      "3 / 7 -- batch_time : 4.751731634140015 sec\n",
      "4 / 7 -- batch_time : 9.09156847000122 sec\n",
      "5 / 7 -- batch_time : 5.103975057601929 sec\n",
      "6 / 7 -- batch_time : 9.295833349227905 sec\n",
      "7 / 7 -- batch_time : 7.232837438583374 sec\n",
      "\n",
      "train_mse : 6.34528264851854e-09\n",
      "val_mse : 3.438123167143203e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 123.12166905403137 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.1894428553205216e-06\n",
      "--- ENSEMBLE MEMBER 6/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 15.23 s -- eta : 0h 1m 31s\n",
      "2 / 7 -- Wout batch_time : 14.24 s -- eta : 0h 1m 14s\n",
      "3 / 7 -- Wout batch_time : 13.69 s -- eta : 0h 0m 58s\n",
      "4 / 7 -- Wout batch_time : 15.20 s -- eta : 0h 0m 44s\n",
      "5 / 7 -- Wout batch_time : 15.72 s -- eta : 0h 0m 30s\n",
      "6 / 7 -- Wout batch_time : 11.77 s -- eta : 0h 0m 14s\n",
      "7 / 7 -- Wout batch_time : 14.08 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 105.77206492424011 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 10.375778913497925 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 4.4355247020721436 sec\n",
      "2 / 7 -- batch_time : 8.207523107528687 sec\n",
      "3 / 7 -- batch_time : 7.559987783432007 sec\n",
      "4 / 7 -- batch_time : 5.547651290893555 sec\n",
      "5 / 7 -- batch_time : 10.592593908309937 sec\n",
      "6 / 7 -- batch_time : 4.262284278869629 sec\n",
      "7 / 7 -- batch_time : 9.29610276222229 sec\n",
      "\n",
      "train_mse : 3.99067303012503e-09\n",
      "val_mse : 3.5381137422518805e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 166.0970995426178 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.1285621894785436e-06\n",
      "--- ENSEMBLE MEMBER 7/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 15.02 s -- eta : 0h 1m 30s\n",
      "2 / 7 -- Wout batch_time : 12.43 s -- eta : 0h 1m 9s\n",
      "3 / 7 -- Wout batch_time : 15.86 s -- eta : 0h 0m 58s\n",
      "4 / 7 -- Wout batch_time : 16.12 s -- eta : 0h 0m 45s\n",
      "5 / 7 -- Wout batch_time : 11.34 s -- eta : 0h 0m 28s\n",
      "6 / 7 -- Wout batch_time : 13.72 s -- eta : 0h 0m 14s\n",
      "7 / 7 -- Wout batch_time : 15.17 s -- eta : 0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch_time : 104.59240651130676 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 5.856088399887085 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 8.556782245635986 sec\n",
      "2 / 7 -- batch_time : 4.801592588424683 sec\n",
      "3 / 7 -- batch_time : 2.79732346534729 sec\n",
      "4 / 7 -- batch_time : 6.119753837585449 sec\n",
      "5 / 7 -- batch_time : 7.161787271499634 sec\n",
      "6 / 7 -- batch_time : 9.399003505706787 sec\n",
      "7 / 7 -- batch_time : 10.860198974609375 sec\n",
      "\n",
      "train_mse : 6.322474861113798e-09\n",
      "val_mse : 3.377768734935671e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 160.17418456077576 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.0405985904071713e-06\n",
      "--- ENSEMBLE MEMBER 8/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 8.81 s -- eta : 0h 0m 53s\n",
      "2 / 7 -- Wout batch_time : 8.10 s -- eta : 0h 0m 42s\n",
      "3 / 7 -- Wout batch_time : 8.18 s -- eta : 0h 0m 33s\n",
      "4 / 7 -- Wout batch_time : 8.07 s -- eta : 0h 0m 25s\n",
      "5 / 7 -- Wout batch_time : 8.24 s -- eta : 0h 0m 17s\n",
      "6 / 7 -- Wout batch_time : 8.06 s -- eta : 0h 0m 8s\n",
      "7 / 7 -- Wout batch_time : 8.25 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 60.81371450424194 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 3.0328876972198486 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 3.0129990577697754 sec\n",
      "2 / 7 -- batch_time : 3.037273406982422 sec\n",
      "3 / 7 -- batch_time : 3.0129551887512207 sec\n",
      "4 / 7 -- batch_time : 3.0996034145355225 sec\n",
      "5 / 7 -- batch_time : 3.0013039112091064 sec\n",
      "6 / 7 -- batch_time : 3.0891075134277344 sec\n",
      "7 / 7 -- batch_time : 3.0595743656158447 sec\n",
      "\n",
      "train_mse : 5.573520629147717e-09\n",
      "val_mse : 4.007396273664199e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 85.19763565063477 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.6377250631194329e-06\n",
      "--- ENSEMBLE MEMBER 9/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 8.03 s -- eta : 0h 0m 48s\n",
      "2 / 7 -- Wout batch_time : 8.03 s -- eta : 0h 0m 40s\n",
      "3 / 7 -- Wout batch_time : 8.34 s -- eta : 0h 0m 33s\n",
      "4 / 7 -- Wout batch_time : 8.34 s -- eta : 0h 0m 25s\n",
      "5 / 7 -- Wout batch_time : 8.25 s -- eta : 0h 0m 16s\n",
      "6 / 7 -- Wout batch_time : 8.32 s -- eta : 0h 0m 8s\n",
      "7 / 7 -- Wout batch_time : 8.63 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 60.65567088127136 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 3.3477349281311035 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 3.2670841217041016 sec\n",
      "2 / 7 -- batch_time : 3.3054256439208984 sec\n",
      "3 / 7 -- batch_time : 3.2678589820861816 sec\n",
      "4 / 7 -- batch_time : 3.4226598739624023 sec\n",
      "5 / 7 -- batch_time : 3.276890993118286 sec\n",
      "6 / 7 -- batch_time : 3.3621420860290527 sec\n",
      "7 / 7 -- batch_time : 3.3231308460235596 sec\n",
      "\n",
      "train_mse : 4.54657340347759e-09\n",
      "val_mse : 3.292222390882671e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 87.2558867931366 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.07374739855004e-06\n",
      "--- ENSEMBLE MEMBER 10/10 ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------- EPOCH : 1 -----------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "1 / 7 -- Wout batch_time : 8.35 s -- eta : 0h 0m 50s\n",
      "2 / 7 -- Wout batch_time : 8.09 s -- eta : 0h 0m 41s\n",
      "3 / 7 -- Wout batch_time : 8.23 s -- eta : 0h 0m 33s\n",
      "4 / 7 -- Wout batch_time : 8.01 s -- eta : 0h 0m 25s\n",
      "5 / 7 -- Wout batch_time : 8.32 s -- eta : 0h 0m 16s\n",
      "6 / 7 -- Wout batch_time : 8.46 s -- eta : 0h 0m 8s\n",
      "7 / 7 -- Wout batch_time : 8.25 s -- eta : 0h 0m 0s\n",
      "\n",
      "epoch_time : 59.849435806274414 sec\n",
      "\n",
      "val mse\n",
      "1 / 1 -- batch_time : 3.312991142272949 sec\n",
      "\n",
      "training mse\n",
      "1 / 7 -- batch_time : 3.339367151260376 sec\n",
      "2 / 7 -- batch_time : 3.203206777572632 sec\n",
      "3 / 7 -- batch_time : 3.3878543376922607 sec\n",
      "4 / 7 -- batch_time : 3.20361065864563 sec\n",
      "5 / 7 -- batch_time : 3.434370994567871 sec\n",
      "6 / 7 -- batch_time : 3.3855905532836914 sec\n",
      "7 / 7 -- batch_time : 3.3475615978240967 sec\n",
      "\n",
      "train_mse : 5.8337643287537895e-09\n",
      "val_mse : 3.2905616535572335e-05\n",
      "val_mse improved from inf\n",
      "\n",
      "Total epoch computation time : 86.49709129333496 sec\n",
      "\n",
      "test mse\n",
      "1 / 1\n",
      "test_mse : 1.0742785434558755e-06\n"
     ]
    }
   ],
   "source": [
    "for i_en in range(num_ensemble_mems):\n",
    "    print('--- ENSEMBLE MEMBER {}/{} ---'.format(i_en+1, num_ensemble_mems))\n",
    "    rnn_net = ensemble_lst[i_en]\n",
    "    # compiling the network\n",
    "    rnn_net.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=losses.MeanSquaredError(),\n",
    "        metrics=['mse'],\n",
    "        run_eagerly=False\n",
    "    )\n",
    "\n",
    "    lambda_reg = float(lambda_reg)\n",
    "\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        # this loads the weights/attributes of the optimizer as well\n",
    "        rnn_net.load_weights(wt_file)\n",
    "\n",
    "    if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "\n",
    "        Wout_best = 0\n",
    "        val_mse_best = np.inf\n",
    "        Wout_candidate = 0\n",
    "        wait = 0\n",
    "        if use_weights_post_dense == True:\n",
    "            postWout_candidate = 0\n",
    "            h_activation = tf.keras.activations.get(activation_post_Wout)\n",
    "\n",
    "\n",
    "        hidden_units = ESN_layers_units[-1]\n",
    "        output_units = rnn_net.data_dim\n",
    "\n",
    "        Hb_shape = [hidden_units, hidden_units]\n",
    "        Yb_shape = [output_units, hidden_units]\n",
    "        if usebias_Wout == True:\n",
    "            Hb_shape[0] += 1\n",
    "            Hb_shape[1] += 1\n",
    "            Yb_shape[1] += 1\n",
    "\n",
    "        Hb = np.zeros(shape=Hb_shape, dtype=FTYPE)\n",
    "        Yb = np.zeros(shape=Yb_shape, dtype=FTYPE)\n",
    "        eye_Hb = np.eye(Hb.shape[0], dtype=FTYPE)\n",
    "\n",
    "        num_batches = training_data_rnn_input.shape[0]\n",
    "\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # for layer in rnn_net.ESN_layers:\n",
    "            #     layer.reset_states()\n",
    "\n",
    "            epoch_totaltime = time.time()\n",
    "\n",
    "            total_s_len = 80\n",
    "            sep_lr_s = ' EPOCH : {} '.format(i+1)\n",
    "            sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n",
    "            sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n",
    "            print('\\n\\n' + '-'*len(sep_lr_s))\n",
    "            print('\\n' + sep_lr_s+'\\n')\n",
    "            print('-'*len(sep_lr_s) + '\\n\\n')\n",
    "\n",
    "            # '''\n",
    "            ### computing Wout\n",
    "            Hb[:, :] = 0\n",
    "            Yb[:, :] = 0\n",
    "            epoch_time = time.time()\n",
    "            avg_time = 0.\n",
    "            num_runs = training_data_rnn_input.shape[0]\n",
    "            for j in range(num_runs):\n",
    "                batch_time = time.time()\n",
    "                h = np.array(rnn_net(training_data_rnn_input[j:j+1], manual_training=True))\n",
    "                # h = rnn_net(training_data_rnn_input[j:j+1], training=True)\n",
    "                # print(h.shape)\n",
    "                h = h[0]\n",
    "                # y = tf.constant(training_data_rnn_output[j])\n",
    "                y = invert_fn(training_data_rnn_output[j])\n",
    "                if usebias_Wout == True:\n",
    "                    h = np.concatenate((h, np.ones(shape=(h.shape[0], 1))), axis=1)\n",
    "                Hb = Hb + np.matmul(np.transpose(h), h)\n",
    "                Yb = Yb + np.matmul(np.transpose(y), h)\n",
    "                # Hb = Hb + tf.linalg.matmul(tf.transpose(h), h)\n",
    "                # Yb = Yb + tf.linalg.matmul(tf.transpose(y), h)\n",
    "                batch_time = time.time() - batch_time\n",
    "                avg_time = (avg_time*j + batch_time)/(j+1)\n",
    "                eta = avg_time * (num_runs-1 - j)\n",
    "                print('{} / {} -- Wout batch_time : {:.2f} s -- eta : {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "                    j+1,\n",
    "                    num_runs,\n",
    "                    batch_time,\n",
    "                    float(eta // 3600),\n",
    "                    float((eta%3600)//60),\n",
    "                    float((eta%3600)%60),\n",
    "                ))\n",
    "\n",
    "            Wout = np.matmul(\n",
    "                Yb,\n",
    "                np.linalg.inv(Hb + lambda_reg*np.eye(Hb.shape[0]))\n",
    "            )\n",
    "            Wout = np.transpose(Wout)\n",
    "            # Wout = tf.linalg.matmul(\n",
    "            #     Yb,\n",
    "            #     tf.linalg.inv(Hb + lambda_reg*tf.eye(Hb.shape[0]))\n",
    "            # )\n",
    "            # Wout = tf.transpose(Wout)\n",
    "\n",
    "            if use_weights_post_dense == True:\n",
    "                ### computing postWout\n",
    "                HYb = 0\n",
    "                HHb = 0\n",
    "                for j in range(training_data_rnn_input.shape[0]):\n",
    "                    batch_time = time.time()\n",
    "                    h = np.array(rnn_net(training_data_rnn_input[j:j+1], training=True))\n",
    "                    # h = rnn_net(training_data_rnn_input[j:j+1], training=True)\n",
    "                    # print(h.shape)\n",
    "                    h = h[0]\n",
    "                    h = np.matmul(h, Wout[0:ESN_layers_units[-1], :])\n",
    "                    if usebias_Wout == True:\n",
    "                        h = h + Wout[ESN_layers_units[-1]:, :]\n",
    "                    # y = tf.constant(training_data_rnn_output[j])\n",
    "                    h = np.array(h_activation(h))\n",
    "                    y = training_data_rnn_output[j]\n",
    "                    HYb = HYb + np.sum(h*y, axis=0)\n",
    "                    HHb = HHb + np.sum(h**2, axis=0)\n",
    "                    # Hb = Hb + tf.linalg.matmul(tf.transpose(h), h)\n",
    "                    # Yb = Yb + tf.linalg.matmul(tf.transpose(y), h)\n",
    "\n",
    "                    print('{} / {} -- postWout batch_time : {} sec'.format(\n",
    "                        j+1,\n",
    "                        training_data_rnn_input.shape[0],\n",
    "                        time.time() - batch_time\n",
    "                    ))\n",
    "\n",
    "                postWout = HYb / (HHb + lambda_reg)\n",
    "\n",
    "            print('\\nepoch_time : {} sec'.format(time.time() - epoch_time))\n",
    "\n",
    "            '''\n",
    "            epoch_time = time.time()\n",
    "            Wout = batched_computation(\n",
    "                num_batches,\n",
    "                rnn_net,\n",
    "                training_data_rnn_input,\n",
    "                training_data_rnn_output,\n",
    "                usebias_Wout,\n",
    "                Hb, Yb,\n",
    "                hidden_units, output_units,\n",
    "                lambda_reg, eye_Hb)\n",
    "            print('\\nepoch_time : {} sec'.format(time.time() - epoch_time))\n",
    "\n",
    "            Hb[:, :] = 0.0\n",
    "            Yb[:, :] = 0.0\n",
    "            '''\n",
    "\n",
    "            Wout_candidate = Wout_candidate*i/(i+1) + Wout*1/(i+1)\n",
    "            tf.keras.backend.set_value(rnn_net.Wout.kernel, Wout_candidate[0:ESN_layers_units[-1], :])\n",
    "            if usebias_Wout == True:\n",
    "                tf.keras.backend.set_value(rnn_net.Wout.bias, Wout_candidate[-1, :])\n",
    "\n",
    "            if use_weights_post_dense == True:\n",
    "                postWout_candidate = postWout_candidate*i/(i+1) + postWout*1/(i+1)\n",
    "                tf.keras.backend.set_value(rnn_net.postWout.individual_weights, postWout_candidate)\n",
    "\n",
    "            for layer in rnn_net.ESN_layers:\n",
    "                layer.reset_states()\n",
    "\n",
    "            print('\\nval mse')\n",
    "            # '''\n",
    "            val_mse = 0\n",
    "            for j in range(val_data_rnn_input.shape[0]):\n",
    "                batch_time = time.time()\n",
    "                val_pred = np.array(rnn_net(val_data_rnn_input[j:j+1], training=False))\n",
    "                temp = (val_pred - val_data_rnn_output[j:j+1])**2\n",
    "                temp = np.mean(temp, axis=-1) # do a sqrt here to get rmse\n",
    "                temp = np.mean(temp, axis=-1)\n",
    "                temp = np.mean(temp, axis=-1)\n",
    "                val_mse = val_mse*j/(j+1) + temp*1/(j+1)\n",
    "                print('{} / {} -- batch_time : {} sec'.format(\n",
    "                    j+1,\n",
    "                    val_data_rnn_input.shape[0],\n",
    "                    time.time() - batch_time\n",
    "                ))\n",
    "            '''\n",
    "            mse_time = time.time()\n",
    "            val_mse = compute_mse(\n",
    "                val_data_rnn_input,\n",
    "                val_data_rnn_output,\n",
    "                rnn_net)\n",
    "            mse_time = time.time() - mse_time\n",
    "            print('val_mse compute time : {} sec'.format(mse_time))\n",
    "            '''\n",
    "\n",
    "            for layer in rnn_net.ESN_layers:\n",
    "                layer.reset_states()\n",
    "\n",
    "            print('\\ntraining mse')\n",
    "            # '''\n",
    "            train_mse = 0\n",
    "            for j in range(training_data_rnn_input.shape[0]):\n",
    "                batch_time = time.time()\n",
    "                train_pred = np.array(rnn_net(training_data_rnn_input[j:j+1], training=False))\n",
    "                temp = (train_pred - training_data_rnn_output[j:j+1])**2\n",
    "                temp = np.mean(temp, axis=-1) # do a sqrt here to get rmse\n",
    "                temp = np.mean(temp, axis=-1)\n",
    "                temp = np.mean(temp, axis=-1)\n",
    "                train_mse = train_mse*j/(j+1) + temp*1/(j+1)\n",
    "                print('{} / {} -- batch_time : {} sec'.format(\n",
    "                    j+1,\n",
    "                    training_data_rnn_input.shape[0],\n",
    "                    time.time() - batch_time\n",
    "                ))\n",
    "            '''\n",
    "            mse_time = time.time()\n",
    "            train_mse = compute_mse(\n",
    "                training_data_rnn_input,\n",
    "                training_data_rnn_output,\n",
    "                rnn_net)\n",
    "            mse_time = time.time() - mse_time\n",
    "            print('train_mse compute time : {} sec'.format(mse_time))\n",
    "            '''\n",
    "\n",
    "            for layer in rnn_net.ESN_layers:\n",
    "                layer.reset_states()\n",
    "\n",
    "    #         print('\\ntesting mse')\n",
    "    #         test_mse = 0\n",
    "    #         for j in range(testing_data_rnn_input.shape[0]):\n",
    "    #             print('{} / {}'.format(j+1, testing_data_rnn_input.shape[0]))\n",
    "    #             test_pred = np.array(rnn_net(testing_data_rnn_input[j:j+1], training=False))\n",
    "    #             temp = (test_pred - testing_data_rnn_output[j:j+1])**2\n",
    "    #             temp = np.mean(temp, axis=-1) # do a sqrt here to get rmse\n",
    "    #             temp = np.mean(temp, axis=-1)\n",
    "    #             temp = np.mean(temp, axis=-1)\n",
    "    #             test_mse = test_mse*j/(j+1) + temp*1/(j+1)\n",
    "\n",
    "    #         for layer in rnn_net.ESN_layers:\n",
    "    #             layer.reset_states()\n",
    "\n",
    "            val_loss_hist.append(val_mse)\n",
    "            train_loss_hist.append(train_mse)\n",
    "\n",
    "            # print('\\ntest_mse : {}'.format(test_mse))\n",
    "            print('\\ntrain_mse : {}'.format(train_mse))\n",
    "            print('val_mse : {}'.format(val_mse))\n",
    "            if val_mse + min_delta <= val_mse_best:\n",
    "                print('val_mse improved from {}'.format(val_mse_best))\n",
    "                Wout_best = Wout_candidate\n",
    "                val_mse_best = val_mse\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                print('val_mse did not improve from {}, wait : {}'.format(val_mse_best, wait))\n",
    "\n",
    "            print('\\nTotal epoch computation time : {} sec'.format(time.time()-epoch_totaltime))\n",
    "\n",
    "            if wait >= patience:\n",
    "                print('\\nearly stopping')\n",
    "                break\n",
    "\n",
    "    #         val_loss_hist.extend(history.history['val_loss'])\n",
    "    #         train_loss_hist.extend(history.history['loss'])\n",
    "\n",
    "    #         if i == starting_lr_idx:\n",
    "    #             lr_change[i+1] += len(history.history['val_loss'])\n",
    "    #         else:\n",
    "    #             lr_change.append(lr_change[i]+len(history.history['val_loss']))\n",
    "\n",
    "    # tf.keras.backend.set_value(rnn_net.Wout, Wout_best)\n",
    "\n",
    "    if use_best == True:\n",
    "        tf.keras.backend.set_value(rnn_net.Wout.kernel, Wout_best[0:ESN_layers_units[-1], :])\n",
    "        if usebias_Wout == True:\n",
    "            tf.keras.backend.set_value(rnn_net.Wout.bias, Wout_best[-1, :])\n",
    "    print('\\ntest mse')\n",
    "    test_mse = 0\n",
    "    for j in range(testing_data_rnn_input.shape[0]):\n",
    "        print('{} / {}'.format(j+1, testing_data_rnn_input.shape[0]))\n",
    "        test_pred = np.array(rnn_net(testing_data_rnn_input[j:j+1], training=False))\n",
    "        temp = (test_pred - testing_data_rnn_output[j:j+1])**2\n",
    "        temp = np.mean(temp, axis=-1) # do a sqrt here to get rmse\n",
    "        temp = np.mean(temp, axis=-1)\n",
    "        temp = np.mean(temp, axis=-1)\n",
    "        test_mse = test_mse*j/(j+1) + temp*1/(j+1)\n",
    "    print('test_mse : {}'.format(test_mse))\n",
    "\n",
    "    for layer in rnn_net.ESN_layers:\n",
    "        layer.reset_states()\n",
    "        \n",
    "    with open(save_path+dir_sep+'{}_losses.txt'.format(i_en), 'w') as f:\n",
    "        f.write(str({\n",
    "            'val_loss_hist':val_loss_hist,\n",
    "            'train_loss_hist':train_loss_hist,\n",
    "#             'lr_change':lr_change,\n",
    "            'test_loss':test_mse\n",
    "        }))\n",
    "\n",
    "\n",
    "    rnn_net.save_everything(\n",
    "        file_name=save_path+dir_sep+'{}_final_net'.format(i_en))\n",
    "    \n",
    "    # # plotting losses\n",
    "    # dir_name_plot = dir_name_rnn+dir_sep+'plots'\n",
    "    # if not os.path.isdir(dir_name_plot):\n",
    "    #     os.makedirs(dir_name_plot)\n",
    "\n",
    "    # # Visualize loss history\n",
    "    # fig, ax = plot_losses(\n",
    "    #     training_loss=train_loss_hist,\n",
    "    #     val_loss=val_loss_hist,\n",
    "    #     lr_change=None,\n",
    "    #     learning_rate_list=None\n",
    "    # )\n",
    "\n",
    "    # plt.savefig(dir_name_rnn+'{ds}plots{ds}{oo}_loss_history.png'.format(ds=dir_sep, oo=i_en), dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    if normalize_dataset == True:\n",
    "        with open(save_path+dir_sep+'rnn_normalization.txt', 'w') as f:\n",
    "            f.write(str({\n",
    "                'normalization_arr':normalization_arr\n",
    "            }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": [
    "def get_ensemble_prediction(ensemble_lst, inputs, kwargs={}):\n",
    "    pred = 0\n",
    "    for i in range(len(ensemble_lst)):\n",
    "        pred += np.array(ensemble_lst[i](inputs, **kwargs))\n",
    "    pred /= len(ensemble_lst)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprediction\u001b[49m\u001b[38;5;241m.\u001b[39mshape, data_out\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "prediction.shape, data_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": [
    "def rescale_data(data, normalization_arr):\n",
    "    '''\n",
    "    data - [num_batches x num_timesteps x num_states]\n",
    "    normalization_arr = [2 x num_states]\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(data.shape[-1]):\n",
    "        new_data[:, i] -= normalization_arr[0, i]\n",
    "        new_data[:, i] /= normalization_arr[1, i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def norm_sq_time_average(data):\n",
    "    data_norm_sq = np.zeros(shape=data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_norm_sq[:] += data[:, i]**2\n",
    "    # integrating using the trapezoidal rule\n",
    "    norm_sq_time_avg = np.sum(data_norm_sq) - 0.5*(data_norm_sq[0]+data_norm_sq[-1])\n",
    "    norm_sq_time_avg /= data_norm_sq.shape[0]\n",
    "    return norm_sq_time_avg\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(shape[-1]):\n",
    "        if len(shape) == 2:\n",
    "            new_data[:, i] *= normalization_arr[1, i]\n",
    "            new_data[:, i] += normalization_arr[0, i]\n",
    "        elif len(shape) == 3:\n",
    "            new_data[:, :, i] *= normalization_arr[1, i]\n",
    "            new_data[:, :, i] += normalization_arr[0, i]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_in = testing_data_rnn_input\n",
    "data_out = testing_data_rnn_output\n",
    "\n",
    "data_idx = np.arange(data_in.shape[0])\n",
    "np.random.shuffle(data_idx)\n",
    "data_idx = data_idx[0]\n",
    "# data_idx = 11269\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    if data_idx < rnn_data_boundary_idx_arr[i]:\n",
    "        case_idx = i\n",
    "        break\n",
    "lyap_time = lyapunov_time_arr[case_idx]\n",
    "\n",
    "print('case {}, data_idx : {}'.format(case_idx+1, data_idx))\n",
    "\n",
    "# data_in = data_in[data_idx]\n",
    "data_out = data_out[data_idx][0:300]\n",
    "# data_out = rescale_data(data_out, normalization_arr)\n",
    "# data_out = invert_normalization(data_out, normalization_arr)\n",
    "\n",
    "for rnn_net in ensemble_lst:\n",
    "    for layer in rnn_net.ESN_layers:\n",
    "        layer.reset_states()\n",
    "\n",
    "# prediction = rnn_net.predict(data_in[data_idx:data_idx+1, :, :])\n",
    "prediction = np.array(\n",
    "    get_ensemble_prediction(\n",
    "        ensemble_lst,\n",
    "        data_in[data_idx:data_idx+1, :, :],\n",
    "        {'training':False}\n",
    "    )\n",
    ")[:, 0:300, :]\n",
    "# prediction = rnn_net.call(data_in[data_idx:data_idx+1, :, :], training=False)\n",
    "# prediction = invert_normalization(prediction, normalization_arr)\n",
    "\n",
    "for rnn_net in ensemble_lst:\n",
    "    for layer in rnn_net.ESN_layers:\n",
    "        layer.reset_states()\n",
    "\n",
    "n = 1\n",
    "num_latent_states = data_out.shape[-1]\n",
    "N = data_out.shape[0]\n",
    "\n",
    "num_cols = 1\n",
    "num_rows = n*num_latent_states\n",
    "\n",
    "ax_ylabels = ['$x^*_{' +str(i)+'}$' for i in range(1, num_latent_states+1)]\n",
    "\n",
    "fig, ax = plt.subplots(num_latent_states, 1, sharex=True, figsize=(7.5*num_cols, 2.5*num_rows))\n",
    "if num_latent_states == 1:\n",
    "    ax = [ax]\n",
    "input_time = np.arange(0, N)*dt_rnn/lyap_time\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 2*n)]\n",
    "\n",
    "prev_idx = 0\n",
    "\n",
    "mpl_ax_artist_list = []\n",
    "for j in range(num_latent_states):\n",
    "    for i in range(n):\n",
    "        obj_in = ax[j].plot(input_time, data_out[:, j], linewidth=1, color=colors[2*i], label='Case {} - actual data'.format(i+1))\n",
    "        obj_out = ax[j].plot(input_time, prediction[0, :, j], linewidth=1, color=colors[2*i+1], label='Case {} - predicted data'.format(i+1))\n",
    "        mpl_ax_artist_list.append(obj_in[0])\n",
    "        mpl_ax_artist_list.append(obj_out[0])\n",
    "    ax[j].set_ylabel(ax_ylabels[j])\n",
    "    # if xlim is not None:\n",
    "    #     ax[j].set_xlim(xlim)\n",
    "    # if ylim is not None:\n",
    "    #     ax[j].set_ylim(ylim)\n",
    "    ax[j].grid(True)\n",
    "    ax[j].set_axisbelow(True)\n",
    "\n",
    "\n",
    "ax[-1].set_xlabel('Time$^+$')\n",
    "\n",
    "max_rows = 10\n",
    "max_rows = float(max_rows)\n",
    "ncols = int(np.ceil(len(boundary_idx_arr) / max_rows))\n",
    "ax[0].legend(\n",
    "    loc='best',\n",
    "    ncol=ncols,\n",
    ")\n",
    "ax[0].set_title(r'Latent States', size=12)\n",
    "plt.show()\n",
    "# plt.savefig('AR-GRU.png', dpi=300, bbox_inches='tight')\n",
    "print('')\n",
    "\n",
    "### Error and prediction horizon\n",
    "# error = np.linalg.norm(data_out[:, :] - prediction[0, :, :], axis=1)\n",
    "error = (data_out[:, :] - prediction[0, :, :])**2\n",
    "# error /= norm_sq_time_average(data_out)**0.5\n",
    "error = np.mean(np.divide(error, time_stddev**2), axis=1)**0.5\n",
    "\n",
    "# print(norm_sq_time_average(data_out)**0.5)\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(7.5, 2.5))\n",
    "ax2.plot(input_time, error)\n",
    "ax2.grid(True)\n",
    "ax2.set_axisbelow(True)\n",
    "ax2.set_xlabel('Time$^+$')\n",
    "ax2.set_ylabel('Normalized Error')\n",
    "\n",
    "error_threshold = 0.5\n",
    "\n",
    "predhor_idx = np.where(error >= error_threshold)[0]\n",
    "if predhor_idx.shape[0] == 0:\n",
    "    predhor_idx = error.shape[0]-1\n",
    "else:\n",
    "    predhor_idx = predhor_idx[0]\n",
    "\n",
    "ax2.plot(input_time[predhor_idx], error[predhor_idx], 'o', color='k')\n",
    "ax2.axhline(error[predhor_idx], linewidth=0.9, linestyle='--', color='k')\n",
    "ax2.axvline(input_time[predhor_idx], linewidth=0.9, linestyle='--', color='k')\n",
    "\n",
    "prediction_horizon = predhor_idx*dt_rnn/lyap_time\n",
    "print(prediction_horizon)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Regressive ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_in = testing_data_rnn_input.shape\n",
    "testing_data_rnn_input = testing_data_rnn_input.reshape((1, s_in[0]*s_in[1]) + s_in[2:])\n",
    "\n",
    "s_out = testing_data_rnn_output.shape\n",
    "testing_data_rnn_output = testing_data_rnn_output.reshape((1, s_out[0]*s_out[1]) + s_out[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_sample_input_AR = 1*np.mean(lyapunov_time_arr)#50.1*dt_rnn\n",
    "num_sample_input_AR = int((T_sample_input_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "T_sample_output_AR = 10*np.mean(lyapunov_time_arr)\n",
    "num_sample_output_AR = int((T_sample_output_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "num_offset_AR = num_sample_input_AR\n",
    "T_offset_AR = num_offset_AR*dt_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_to_consider = 'training'\n",
    "data_to_consider = 'testing'\n",
    "\n",
    "data_in = eval(data_to_consider+'_data_rnn_input')\n",
    "data_out = eval(data_to_consider+'_data_rnn_output')\n",
    "\n",
    "batch_idx = np.random.randint(low=0, high=data_in.shape[0])\n",
    "maxpossible_num_runs = data_in.shape[1]-(num_sample_input_AR+num_sample_output_AR)\n",
    "\n",
    "data_idx = np.random.randint(low=0, high=maxpossible_num_runs)\n",
    "\n",
    "print('batch_idx : {}'.format(batch_idx))\n",
    "print('data_idx : {}'.format(data_idx))\n",
    "data_ = data_in[batch_idx:batch_idx+1, data_idx:data_idx+(num_sample_input_AR+num_sample_output_AR), :]\n",
    "print('data_.shape :', data_.shape)\n",
    "\n",
    "plt.plot(dt_rnn*np.arange(num_sample_input_AR), data_[0, 0:num_sample_input_AR, 0])\n",
    "plt.plot(dt_rnn*num_sample_input_AR+dt_rnn*np.arange(num_sample_output_AR), data_[0, num_sample_input_AR:, 0])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_pred_time = time.time()\n",
    "prediction_lst = []\n",
    "\n",
    "for rnn_net in ensemble_lst:\n",
    "    for layer in rnn_net.ESN_layers:\n",
    "        layer.reset_states()\n",
    "    \n",
    "input_preds = np.array(get_ensemble_prediction(\n",
    "    ensemble_lst,\n",
    "    data_[:, 0:num_sample_input_AR, :],\n",
    "    {'training':False}\n",
    "))[0]\n",
    "\n",
    "prediction_lst.append(input_preds[-1])\n",
    "\n",
    "for i in range(1, num_sample_output_AR):\n",
    "    data_in_i = np.array([[prediction_lst[-1]]])\n",
    "    output = np.array(get_ensemble_prediction(\n",
    "        ensemble_lst,\n",
    "        data_in_i, \n",
    "        {'training':False}\n",
    "    ))[0, 0]\n",
    "    prediction_lst.append(output)\n",
    "    \n",
    "AR_pred_time = time.time() - AR_pred_time\n",
    "print('pred time : {:.0f}m {:.0f}s'.format(AR_pred_time//60, AR_pred_time%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lst = np.stack(prediction_lst)\n",
    "\n",
    "print('input_preds.shape : {}'.format(input_preds.shape))\n",
    "print('prediction_lst.shape : {}'.format(prediction_lst.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lyap_time = np.mean(lyapunov_time_arr)\n",
    "\n",
    "n = 1\n",
    "num_latent_states = input_preds.shape[-1]\n",
    "N = num_sample_output_AR#num_output_timesteps\n",
    "\n",
    "num_cols = 1\n",
    "num_rows = n*num_latent_states\n",
    "\n",
    "ax_ylabels = [r'$x^*_{'+str(i+1)+'}$' for i in range(num_latent_states)]\n",
    "\n",
    "fig, ax = plt.subplots(num_latent_states, 1, sharex=True, figsize=(7.5*num_cols, 2.5*num_rows))\n",
    "if num_latent_states == 1:\n",
    "    ax = [ax]\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 2*n)]\n",
    "\n",
    "prev_idx = 0\n",
    "\n",
    "time_arr_warmup1 = (np.arange(num_sample_input_AR) - num_sample_input_AR)*dt_rnn\n",
    "time_arr_warmup2 = time_arr_warmup1 + dt_rnn\n",
    "\n",
    "time_arr_warmup1 /= lyap_time\n",
    "time_arr_warmup2 /= lyap_time\n",
    "\n",
    "input_time = time_arr_warmup2[-1]+np.arange(N, dtype=float)*dt_rnn/lyap_time\n",
    "\n",
    "for j in range(num_latent_states):\n",
    "    for i in range(n):\n",
    "        ax[j].plot(time_arr_warmup1, data_[0, 0:num_sample_input_AR, j], linestyle='--', linewidth=1, color=colors[2*i], label='Case {} - actual warmup data'.format(i+1))\n",
    "        ax[j].plot(time_arr_warmup2, input_preds[:, j], linestyle='--', linewidth=1, color=colors[2*i+1], label='Case {} - predicted warmup data'.format(i+1))\n",
    "        ax[j].plot(input_time, data_[0, num_sample_input_AR:num_sample_input_AR+N, j], linewidth=1, color=colors[2*i], label='Case {} - actual data'.format(i+1))\n",
    "        ax[j].plot(input_time, prediction_lst[:, j], linewidth=1, color=colors[2*i+1], label='Case {} - predicted data'.format(i+1))\n",
    "    ax[j].set_ylabel(ax_ylabels[j])\n",
    "    ax[j].grid(True)\n",
    "    ax[j].set_axisbelow(True)\n",
    "    ax[j].set_ylim([-1, 1])\n",
    "    ax[j].set_xlim([input_time[0] - 0.5, input_time[-1]])\n",
    "\n",
    "\n",
    "ax[-1].set_xlabel('Time$^+$')\n",
    "\n",
    "max_rows = 10\n",
    "max_rows = float(max_rows)\n",
    "ncols = int(np.ceil(len(boundary_idx_arr) / max_rows))\n",
    "ax[0].legend(\n",
    "    loc='best',\n",
    "    ncol=ncols,\n",
    ")\n",
    "ax[0].set_title(r'Latent States', size=12)\n",
    "\n",
    "\n",
    "### error computation\n",
    "error = (data_[0, num_sample_input_AR:num_sample_input_AR+N, :] - prediction_lst[:, :])**2\n",
    "# error /= norm_sq_time_average(data_out)**0.5\n",
    "error = np.mean(np.divide(error, time_stddev**2), axis=1)**0.5\n",
    "\n",
    "# print(norm_sq_time_average(data_out)**0.5)\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(7.5, 2.5))\n",
    "ax2.plot(input_time, error)\n",
    "ax2.grid(True)\n",
    "ax2.set_axisbelow(True)\n",
    "ax2.set_xlabel('Time$^+$')\n",
    "ax2.set_ylabel('Normalized Error')\n",
    "\n",
    "error_threshold = 0.5\n",
    "\n",
    "predhor_idx = np.where(error >= error_threshold)[0]\n",
    "if len(predhor_idx.shape) == 0 or predhor_idx.shape[0] == 0:\n",
    "    predhor_idx = error.shape[0]-1\n",
    "else:\n",
    "    predhor_idx = predhor_idx[0]\n",
    "ax2.plot(input_time[predhor_idx], error[predhor_idx], 'o', color='k')\n",
    "ax2.axhline(error[predhor_idx], linewidth=0.9, linestyle='--', color='k')\n",
    "ax2.axvline(input_time[predhor_idx], linewidth=0.9, linestyle='--', color='k')\n",
    "ax2.set_ylim([0, 2])\n",
    "\n",
    "prediction_horizon = predhor_idx*dt_rnn/lyap_time\n",
    "print(prediction_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predhor_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_weights_post_dense == True:\n",
    "    print(rnn_net.postWout.individual_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction horizon computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_and_save(\n",
    "        prediction_horizon_arr, median,\n",
    "        save_dir,\n",
    "        savefig_fname='pre_ARtraining',\n",
    "        bin_width=0.1,\n",
    "        bin_begin=0.0,\n",
    "        xlabel_kwargs={\"fontsize\":15},\n",
    "        ylabel_kwargs={\"fontsize\":15},\n",
    "        title_kwargs={\"fontsize\":18},\n",
    "        legend_kwargs={\"fontsize\":12},\n",
    "        title_text = None,\n",
    "    ):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    prediction_horizon_arr.sort()\n",
    "\n",
    "    ph_mean = np.mean(prediction_horizon_arr)\n",
    "    ph_stddev = np.std(prediction_horizon_arr)\n",
    "    ph_max = np.max(prediction_horizon_arr)\n",
    "    ph_min = np.min(prediction_horizon_arr)\n",
    "    \n",
    "    bin_end = bin_width*np.round((np.max(prediction_horizon_arr)+0.5*bin_width)//bin_width)\n",
    "    nbins = int(np.round(bin_end/bin_width))\n",
    "\n",
    "    ax.hist(prediction_horizon_arr, bins=nbins, range = [bin_begin, bin_end], density=True)\n",
    "    ax.axvline(ph_mean, linewidth=0.9, linestyle='--', color='k')\n",
    "\n",
    "    ax.set_xlabel('Prediction Horizon (Lyapunov times)', **xlabel_kwargs)\n",
    "    ax.set_ylabel('PDF', **ylabel_kwargs)\n",
    "\n",
    "    ax.grid(True)\n",
    "    # ax.set_axisbelow(True)\n",
    "\n",
    "    ax.text(\n",
    "        0.01 + ax.transAxes.inverted().transform(ax.transData.transform([ph_mean, 0]))[0],\n",
    "        0.8,\n",
    "        'mean',\n",
    "        rotation=90,\n",
    "        verticalalignment='bottom',\n",
    "        horizontalalignment='left',\n",
    "        bbox=dict(facecolor=np.array([255,255,153])/255, alpha=0.6, boxstyle='square,pad=0.2'),\n",
    "        transform=ax.transAxes\n",
    "    )\n",
    "\n",
    "    text_xy = [0.95, 0.95]\n",
    "    ax.text(\n",
    "        text_xy[0],\n",
    "        text_xy[1],\n",
    "        'mean : {:.4f}\\nmedian : {:.4f}\\nmax : {:.4f}\\nmin : {:.4f}\\nstddev : {:.4f}'.format(\n",
    "            ph_mean,\n",
    "            median,\n",
    "            ph_max,\n",
    "            ph_min,\n",
    "            ph_stddev,\n",
    "        ),\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round\",\n",
    "            ec=(0.6, 0.6, 1),\n",
    "            fc=(0.9, 0.9, 1),\n",
    "            alpha=0.6,\n",
    "        ),\n",
    "        # bbox=dict(facecolor='C0', alpha=0.5, boxstyle='round,pad=0.2'),\n",
    "        horizontalalignment='right',\n",
    "        verticalalignment='top',\n",
    "        **legend_kwargs\n",
    "    )\n",
    "\n",
    "    if title_text == None:\n",
    "        title_text = 'nbins = {}'.format(nbins)\n",
    "    ax.set_title(title_text, **title_kwargs)\n",
    "    \n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    fig.savefig(save_dir+'/'+savefig_fname+'.pdf', dpi=300, bbox_inches='tight')\n",
    "    fig.clear()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_runs = 100\n",
    "num_runs = np.min([num_runs, maxpossible_num_runs])\n",
    "\n",
    "data_idx_arr = np.linspace(0, maxpossible_num_runs-1, num_runs, dtype=np.int32)\n",
    "\n",
    "savefig_fname = 'pre_ARtraining-'+data_to_consider+'data'\n",
    "npsavedata_fname = '/prediction_horizons-'+data_to_consider+'data'\n",
    "plot_dir = '/plots'\n",
    "\n",
    "analysis_time = time.time()\n",
    "\n",
    "sidx1 = dir_name_rnn[::-1].index('/')\n",
    "sidx2 = dir_name_rnn[-sidx1-2::-1].index('/')\n",
    "print(dir_name_rnn[-(sidx1+sidx2+1):])\n",
    "print('num_runs :', num_runs)\n",
    "\n",
    "prediction_horizon_arr = np.empty(shape=num_runs)\n",
    "\n",
    "avg_time = 0.\n",
    "for i in range(num_runs):\n",
    "    run_time = time.time()\n",
    "    data_idx = data_idx_arr[i]\n",
    "\n",
    "    # for j in range(len(rnn_data_boundary_idx_arr)):\n",
    "    #     if data_idx < rnn_data_boundary_idx_arr[j]:\n",
    "    #         case_idx = j\n",
    "    #         break\n",
    "    lyap_time = lyapunov_time_arr[0]\n",
    "\n",
    "    ### picking the data\n",
    "    data_ = data_in[0:1, data_idx:data_idx+(num_sample_input_AR+num_sample_output_AR), :]\n",
    "\n",
    "    ### doing the predictions\n",
    "    prediction_lst = []\n",
    "    \n",
    "    for rnn_net in ensemble_lst:\n",
    "        for layer in rnn_net.ESN_layers:\n",
    "            layer.reset_states()\n",
    "\n",
    "    input_preds = np.array(get_ensemble_prediction(\n",
    "        ensemble_lst,\n",
    "        data_[:, 0:num_sample_input_AR, :],\n",
    "        {'training':False}\n",
    "    ))[0]\n",
    "\n",
    "    prediction_lst.append(input_preds[-1])\n",
    "\n",
    "    for j in range(1, num_sample_output_AR):\n",
    "        data_in_j = np.array([[prediction_lst[-1]]])\n",
    "        output = np.array(get_ensemble_prediction(\n",
    "            ensemble_lst,\n",
    "            data_in_j,\n",
    "            {'training':False}\n",
    "        ))[0, 0]\n",
    "        prediction_lst.append(output)\n",
    "    prediction_lst = np.stack(prediction_lst)\n",
    "    # prediction_lst = invert_normalization(prediction_lst, normalization_arr)\n",
    "    \n",
    "    data_out = data_[0, num_sample_input_AR:num_sample_input_AR+num_sample_output_AR, :]\n",
    "    # data_out = invert_normalization(data_out, normalization_arr)\n",
    "\n",
    "    ### Error and prediction horizon\n",
    "    # error = np.linalg.norm(data_out[:, :] - prediction[i, :, :], axis=1)\n",
    "    error = (data_out[:, :] - prediction_lst[:, :])**2\n",
    "    # error /= norm_sq_time_average(data_out)**0.5\n",
    "    error = np.mean(np.divide(error, time_stddev**2), axis=1)**0.5\n",
    "\n",
    "    predhor_idx = np.where(error >= error_threshold)[0]\n",
    "    if predhor_idx.shape[0] == 0:\n",
    "        predhor_idx = error.shape[0]\n",
    "    else:\n",
    "        predhor_idx = predhor_idx[0]\n",
    "\n",
    "    prediction_horizon_arr[i] = predhor_idx*dt_rnn/lyap_time\n",
    "\n",
    "    run_time = time.time() - run_time\n",
    "    avg_time = (avg_time*i + run_time)/(i+1)\n",
    "    eta = avg_time * (num_runs-1 - i)\n",
    "    print('    {} / {} -- run_time : {:.2f} s -- eta : {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        i+1,\n",
    "        num_runs,\n",
    "        run_time,\n",
    "        float(eta // 3600),\n",
    "        float((eta%3600)//60),\n",
    "        float((eta%3600)%60),\n",
    "    ))\n",
    "\n",
    "median_idx = int(np.round(0.5*num_runs-1))\n",
    "quartile_1_idx = int(np.round(0.25*num_runs-1))\n",
    "quartile_3_idx = int(np.round(0.75*num_runs-1))\n",
    "\n",
    "prediction_horizon_arr.sort()\n",
    "\n",
    "median = prediction_horizon_arr[median_idx]\n",
    "quartile_1 = prediction_horizon_arr[quartile_1_idx]\n",
    "quartile_3 = prediction_horizon_arr[quartile_3_idx]\n",
    "IQR = quartile_3 - quartile_1\n",
    "\n",
    "prediction_horizon = np.mean(prediction_horizon_arr)\n",
    "stddev_ph = np.std(prediction_horizon_arr)\n",
    "\n",
    "s = 'error_threshold = {}\\n'.format(error_threshold)\n",
    "s += 'prediction_horizon : {}, median : {}\\n'.format(prediction_horizon, median)\n",
    "s += 'ph_min : {}, ph_max : {}\\n'.format(prediction_horizon_arr.min(), prediction_horizon_arr.max())\n",
    "s += 'stddev : {}, IQR : {}\\n'.format(stddev_ph, IQR)\n",
    "s += '1st quartile : {}, 3rd quartile : {}'.format(quartile_1, quartile_3)\n",
    "\n",
    "print('\\n'+s)\n",
    "\n",
    "plot_histogram_and_save(\n",
    "    prediction_horizon_arr, median,\n",
    "    save_dir=dir_name_rnn+plot_dir,\n",
    "    savefig_fname=savefig_fname\n",
    ")\n",
    "\n",
    "npsavedata_fname = '/prediction_horizons-'+data_to_consider+'data'\n",
    "np.savez(\n",
    "    dir_name_rnn+npsavedata_fname,\n",
    "    prediction_horizon_arr=prediction_horizon_arr,\n",
    "    error_threshold=error_threshold,\n",
    ")\n",
    "\n",
    "with open(dir_name_rnn+npsavedata_fname+'--statistics.txt', 'w') as fl:\n",
    "    fl.write(s)\n",
    "\n",
    "print('analysis time : {} s\\n'.format(time.time() - analysis_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
