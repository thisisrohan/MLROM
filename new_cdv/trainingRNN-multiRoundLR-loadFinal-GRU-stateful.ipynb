{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/new_cdv\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data_KS, plot_latent_states_KS , readAndReturnLossHistories, plot_histogram_and_save\n",
    "from tools.ae_v2 import Autoencoder\n",
    "from tools.GRU_SingleStep_v1 import RNN_GRU\n",
    "# from tools.LSTM_SingleStep_v2 import RNN_GRU\n",
    "# from tools.SimpleRNN_SingleStep_v2 import RNN_GRU\n",
    "from tools.GRU_AR_v1 import AR_RNN_GRU as AR_RNN\n",
    "from tools.AEGRU_AR_v1 import AR_AERNN_GRU as AR_AERNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": [
    "# setting seed for PRNGs\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    prng_seed = 42\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 00:47:21.823117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:21.823383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:21.871199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:21.871514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:21.871754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:21.871980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:21.873962: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-29 00:47:21.874593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:21.874843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:21.875071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:22.444067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:22.444305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:22.444509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 00:47:22.444673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3365 MB memory:  -> device: 1, name: Quadro K2200, pci bus id: 0000:03:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 1\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "2.8.1\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print('')\n",
    "print(tf.config.list_logical_devices())\n",
    "print('')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000\n",
      "use_ae_data : True, dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_ae/ae_008\n",
      "data_dir_idx: 004\n",
      "normalize_flag_ogdata: False\n"
     ]
    }
   ],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # making RNN save directory\n",
    "    dir_name_rnn = os.getcwd() + dir_sep + 'saved_rnn'\n",
    "    if not os.path.isdir(dir_name_rnn):\n",
    "        os.makedirs(dir_name_rnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'rnn_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_rnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_rnn = dir_name_rnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_rnn)\n",
    "    os.makedirs(dir_name_rnn+dir_sep+'plots')\n",
    "\n",
    "    # whether to use AE data or just work on raw data\n",
    "    use_ae_data = True # if false, specifying ae_idx will only show which dataset to use\n",
    "\n",
    "    # autoencoder directory\n",
    "    ae_idx = '008'\n",
    "    dir_name_ae = os.getcwd()+'{ds}saved_ae{ds}ae_'.format(ds=dir_sep)+ae_idx\n",
    "else:\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_rnn/rnn_015'\n",
    "\n",
    "    # reading AE directory\n",
    "    with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_dict = eval(''.join(lines))\n",
    "\n",
    "    try:\n",
    "        use_ae_data = params_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in sim_data_AE_params, set to True.\")\n",
    "        normalize_dataset = True\n",
    "    \n",
    "    dir_name_ae = params_dict['dir_name_ae']\n",
    "    ae_idx = dir_name_ae[-3:]\n",
    "    dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "\n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_rnn_dict['T_sample_output']\n",
    "    T_offset = params_rnn_dict['T_offset']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in RNN_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "    try:\n",
    "        dense_layer_act_func = params_rnn_dict['dense_layer_act_func']\n",
    "    except:\n",
    "        print(\"'dense_layer_act_func' not present in RNN_specific_data, set to 'linear'.\")\n",
    "        dense_layer_act_func = 'linear'\n",
    "    try:\n",
    "        stateful = params_rnn_dict['stateful']\n",
    "    except:\n",
    "        print(\"'stateful' not present in RNN_specific_data, set to True.\")\n",
    "        stateful = True\n",
    "    try:\n",
    "        use_learnable_state = params_rnn_dict['use_learnable_state']\n",
    "    except:\n",
    "        print(\"'use_learnable_state' not present in RNN_specific_data, set to False.\")\n",
    "        use_learnable_state = False\n",
    "    try:\n",
    "        use_weights_post_dense = params_rnn_dict['use_weights_post_dense']\n",
    "    except:\n",
    "        print(\"'use_weights_post_dense' not present in RNN_specific_data, set to False.\")\n",
    "        use_weights_post_dense = False\n",
    "    try:\n",
    "        use_ae_data = params_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "\n",
    "    \n",
    "\n",
    "    normalization_arr = None\n",
    "    try:\n",
    "        with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        rnn_norm_arr_dict = eval(lines)\n",
    "        normalization_arr = rnn_norm_arr_dict['normalization_arr']\n",
    "    except:\n",
    "        pass\n",
    "    if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "        with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "            normalization_arr = fl['normalization_arr'][0]\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in ae_data, set to 'True'.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('use_ae_data : ' + str(use_ae_data) + ', dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state = params_dict['init_state']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data'].astype(FTYPE)\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case : 1, lyapunov exponent : 0.027086916024239262, lyapunov time : 36.91819381713867s\n"
     ]
    }
   ],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delaing with normalizing the data before feeding into autoencoder\n",
    "num_params = params_mat.shape[1]\n",
    "og_vars = all_data.shape[1]\n",
    "if alldata_withparams_flag == True:\n",
    "    og_vars -= num_params\n",
    "\n",
    "time_stddev_ogdata = np.std(all_data[:, 0:og_vars], axis=0)\n",
    "time_mean_ogdata = np.mean(all_data[:, 0:og_vars], axis=0)\n",
    "    \n",
    "if use_ae_data == True:\n",
    "    if ae_data_with_params == True and alldata_withparams_flag == False:\n",
    "        new_all_data = np.empty(shape=(all_data.shape[0], og_vars+num_params), dtype=FTYPE)\n",
    "        new_all_data[:, 0:og_vars] = all_data[:, 0:og_vars]\n",
    "        del(all_data)\n",
    "        all_data = new_all_data\n",
    "        prev_idx = 0\n",
    "        for i in range(boundary_idx_arr.shape[0]):\n",
    "            all_data[prev_idx:boundary_idx_arr[i], num_params:] = params_mat[i]\n",
    "            prev_idx = boundary_idx_arr[i]\n",
    "\n",
    "    if normalizeforae_flag == True:\n",
    "        for i in range(all_data.shape[1]):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "    if ae_data_with_params == False:\n",
    "        all_data = all_data[:, 0:og_vars]\n",
    "else:\n",
    "    # using raw data, neglecting the params attached (if any)\n",
    "    all_data = all_data[:, 0:og_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "    wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "    ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Zl6ZvgtNtA_u",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776554,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lXpoaKRIneQc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1667868777509,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Q3a8HHyvneQc",
    "outputId": "51084913-6faf-4bb5-db69-2cbea705dd28"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "if use_ae_data == True:\n",
    "    latent_states_all = ae_net.encoder_net.predict(all_data)\n",
    "    # del(all_data)\n",
    "else:\n",
    "    latent_states_all = all_data\n",
    "num_latent_states = latent_states_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1667868778304,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wjgPNitSrt5p",
    "outputId": "0c916524-33ec-47bf-a16a-51e53d2e25f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868778305,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wnLnqg0Jrt5t"
   },
   "outputs": [],
   "source": [
    "# for i in range(ae_net.layers):\n",
    "#     tf.keras.utils.plot_model(\n",
    "#         ae_net.layers[i],\n",
    "#         to_file=dir_name_ae+'/plots/netlayer_{}.png'.format(i),\n",
    "#         show_shapes=True,\n",
    "#         dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BOJE8vREtque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fwjcsAxKneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "aFd7XgwVneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    num_lyaptimesteps_totrain = 3 # int(5000/np.mean(lyapunov_time_arr))#\n",
    "    dt_rnn = 0.2\n",
    "    T_sample_input = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_sample_output = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_offset = dt_rnn\n",
    "    normalize_dataset = True # whether the data for the RNN should be normalized by the dataset's mean and std\n",
    "    normalization_arr = None\n",
    "    skip_intermediate = 'full sample'\n",
    "    noise_type = 'normal' # can be 'uniform' or 'normal'\n",
    "\n",
    "    # can be 'minmax', 'minmax2', 'stddev', or a list with\n",
    "    # sequential order of any of these; if it is 'minmax'\n",
    "    # then stddev_multiplier has no effect\n",
    "    normalization_type = 'stddev'\n",
    "    stddev_multiplier = 3\n",
    "\n",
    "    dense_layer_act_func = ['tanh']\n",
    "    use_weights_post_dense = True\n",
    "    stateful = True\n",
    "    use_learnable_state = False\n",
    "    use_trainable_weights_with_reslayers = False\n",
    "        \n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "        \n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "\n",
    "    # saving simulation data\n",
    "    sim_data = {\n",
    "        'params_mat':params_mat,\n",
    "        'init_state_mat':init_state_mat,\n",
    "        't0':t0,\n",
    "        'T':T,\n",
    "        'delta_t':delta_t,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'dir_name_ae':dir_name_ae,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'use_ae_data':use_ae_data,\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'sim_data_AE_params.txt', 'w') as f:\n",
    "        f.write(str(sim_data))\n",
    "        \n",
    "    # saving RNN specific data\n",
    "    RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_lyaptimesteps_totrain':num_lyaptimesteps_totrain,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':RNN_GRU.__module__,\n",
    "        'noise_type':noise_type,\n",
    "        'normalization_type':normalization_type,\n",
    "        'dense_layer_act_func':dense_layer_act_func,\n",
    "        'stateful':stateful,\n",
    "        'use_learnable_state':use_learnable_state,\n",
    "        'use_weights_post_dense':use_weights_post_dense,\n",
    "        'use_trainable_weights_with_reslayers':use_trainable_weights_with_reslayers,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "UGnj8uQQ83-y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "0t2_8mzI1fhX"
   },
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    latent_states_all,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=normalize_dataset,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type)\n",
    "    \n",
    "data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "num_samples = rnn_res_dict['num_samples']\n",
    "normalization_arr = rnn_res_dict['normalization_arr']\n",
    "rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "pIsWCXkbr7ws"
   },
   "outputs": [],
   "source": [
    "temp = np.divide(latent_states_all-normalization_arr[0], normalization_arr[1])\n",
    "time_stddev = np.std(temp, axis=0)\n",
    "timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "del(org_data_idx_arr_input)\n",
    "del(org_data_idx_arr_output)\n",
    "del(latent_states_all)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    all_data,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=False,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type,\n",
    "    FTYPE=FTYPE,\n",
    "    ITYPE=ITYPE)\n",
    "    \n",
    "AR_data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "AR_data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "AR_org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "AR_org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "AR_num_samples = rnn_res_dict['num_samples']\n",
    "AR_normalization_arr = rnn_res_dict['normalization_arr']\n",
    "AR_rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']\n",
    "\n",
    "del(all_data)\n",
    "del(AR_org_data_idx_arr_input)\n",
    "del(AR_org_data_idx_arr_output)\n",
    "del(AR_rnn_data_boundary_idx_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Hem_9PUqneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778791,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uskBAAXpneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "\n",
    "# ph computation parameters\n",
    "num_runs = 50\n",
    "T_sample_input_AR_ratio = 1\n",
    "T_sample_output_AR_ratio = 3\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [1e-3, 1e-4, 1e-5]\n",
    "    epochs = 200\n",
    "    patience = 10 # parameter for early stopping\n",
    "    min_delta = 1e-6  # parameter for early stopping\n",
    "    lambda_reg = 5.74802807e-07  # weight for regularizer\n",
    "    train_split = 0.8\n",
    "    val_split = 0.1\n",
    "    test_split = 1 - train_split - val_split\n",
    "    batch_size = 32\n",
    "    fRMS = 9.84468949e-02\n",
    "    zoneout_rate = 1.09893940e-02*2/8\n",
    "    rnncell_dropout_rate = 0.0\n",
    "    denselayer_dropout_rate = 0.0\n",
    "    \n",
    "\n",
    "    stddev = fRMS*timeMeanofSpaceRMS\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'timeMeanofSpaceRMS':timeMeanofSpaceRMS,\n",
    "        'stddev':stddev,\n",
    "        'zoneout_rate':zoneout_rate,\n",
    "        'rnncell_dropout_rate':rnncell_dropout_rate,\n",
    "        'denselayer_dropout_rate':denselayer_dropout_rate,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_rnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr],\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    # dir_name_rnn_og = dir_name_rnn\n",
    "    # dir_name_rnn_temp = '/home/rkaushik/Documents/Thesis/MLROM/CDV/saved_rnn/rnn_'+dir_name_rnn_og[-3:]\n",
    "    # dir_name_rnn = dir_name_rnn_temp\n",
    "\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # setting seed for PRNGs\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "# idx = np.arange(data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# boundary = int(np.round(train_split*data_rnn_input.shape[0]))\n",
    "\n",
    "# training_data_rnn_input = data_rnn_input[idx[0:boundary]]\n",
    "# training_data_rnn_output = data_rnn_output[idx[0:boundary]]\n",
    "\n",
    "# testing_data_rnn_input = data_rnn_input[idx[boundary:]]\n",
    "# testing_data_rnn_output = data_rnn_output[idx[boundary:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1667868779601,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "EENXaWqcKW7j"
   },
   "outputs": [],
   "source": [
    "cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "# idx = np.arange(cum_samples)\n",
    "# np.random.shuffle(idx)\n",
    "num_train_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_val_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_test_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_samples_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "begin_idx = 0\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    num_samples = rnn_data_boundary_idx_arr[i] - begin_idx\n",
    "    num_train_arr[i] = batch_size * int( np.round(train_split*num_samples/batch_size) )\n",
    "    num_val_arr[i] = batch_size * int( np.round(val_split*num_samples/batch_size) )\n",
    "    num_test_arr[i] = batch_size * int( np.round((num_samples - num_train_arr[i] - num_val_arr[i])/batch_size) )\n",
    "    num_samples_arr[i] = num_train_arr[i] + num_val_arr[i] + num_test_arr[i]\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# defining shapes\n",
    "training_input_shape = [np.sum(num_train_arr)]\n",
    "training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "training_output_shape = [np.sum(num_train_arr)]\n",
    "training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "val_input_shape = [np.sum(num_val_arr)]\n",
    "val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "val_output_shape = [np.sum(num_val_arr)]\n",
    "val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "testing_input_shape = [np.sum(num_test_arr)]\n",
    "testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "testing_output_shape = [np.sum(num_test_arr)]\n",
    "testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "# defining required arrays\n",
    "training_data_rnn_input = np.empty(shape=training_input_shape, dtype=FTYPE)\n",
    "training_data_rnn_output = np.empty(shape=training_output_shape, dtype=FTYPE)\n",
    "\n",
    "val_data_rnn_input = np.empty(shape=val_input_shape, dtype=FTYPE)\n",
    "val_data_rnn_output = np.empty(shape=val_output_shape, dtype=FTYPE)\n",
    "\n",
    "testing_data_rnn_input = np.empty(shape=testing_input_shape, dtype=FTYPE)\n",
    "testing_data_rnn_output = np.empty(shape=testing_output_shape, dtype=FTYPE)\n",
    "\n",
    "AR_testing_data_rnn_input = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "AR_testing_data_rnn_output = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "\n",
    "begin_idx = 0\n",
    "training_data_rolling_count = 0\n",
    "val_data_rolling_count = 0\n",
    "testing_data_rolling_count = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    idx = np.arange(begin_idx, rnn_data_boundary_idx_arr[i])\n",
    "    # np.random.shuffle(idx)\n",
    "    # num_samples = idx.shape[0]\n",
    "    # num_train = int( np.round(train_split*num_samples/batch_size) )*batch_size\n",
    "    # num_val = int( np.round(val_split*num_samples/batch_size) )*batch_size\n",
    "    \n",
    "    num_samples = num_samples_arr[i]\n",
    "    num_train = num_train_arr[i]\n",
    "    num_val = num_val_arr[i]\n",
    "    num_test = num_test_arr[i]\n",
    "    \n",
    "    nbatches_train = num_train // batch_size\n",
    "    nbatches_val = num_val // batch_size\n",
    "    nbatches_test = num_test // batch_size\n",
    "\n",
    "    for j in range(batch_size):\n",
    "        training_data_rnn_input[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_input[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        training_data_rnn_output[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_output[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        \n",
    "        val_data_rnn_input[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_input[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "        val_data_rnn_output[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_output[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "\n",
    "        testing_data_rnn_input[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_input[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        testing_data_rnn_output[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_output[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        \n",
    "        AR_testing_data_rnn_input[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = AR_data_rnn_input[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        AR_testing_data_rnn_output[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = AR_data_rnn_output[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "\n",
    "\n",
    "    # training_data_rnn_input[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_input[idx[0:num_train]]\n",
    "    # training_data_rnn_output[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_output[idx[0:num_train]]\n",
    "    training_data_rolling_count += num_train\n",
    "\n",
    "    # val_data_rnn_input[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_input[idx[num_train:num_train+num_val]]\n",
    "    # val_data_rnn_output[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_output[idx[num_train:num_train+num_val]]\n",
    "    val_data_rolling_count += num_val\n",
    "\n",
    "    # num_test = num_samples-num_train-num_val+1\n",
    "    # testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_input[idx[num_train+num_val:]]\n",
    "    # testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_output[idx[num_train+num_val:]]\n",
    "    testing_data_rolling_count += num_test\n",
    "\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# cleaning up\n",
    "del(data_rnn_input)\n",
    "del(data_rnn_output)\n",
    "del(AR_data_rnn_input)\n",
    "del(AR_data_rnn_output)\n",
    "\n",
    "# further shuffling\n",
    "if stateful == False:\n",
    "    idx = np.arange(0, training_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    training_data_rnn_input = training_data_rnn_input[idx]\n",
    "    training_data_rnn_output = training_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, val_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    val_data_rnn_input = val_data_rnn_input[idx]\n",
    "    val_data_rnn_output = val_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, testing_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    testing_data_rnn_input = testing_data_rnn_input[idx]\n",
    "    testing_data_rnn_output = testing_data_rnn_output[idx]\n",
    "\n",
    "    del(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868779603,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8isZN1tYBifp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_runs :  50\n"
     ]
    }
   ],
   "source": [
    "s_in = AR_testing_data_rnn_input.shape\n",
    "AR_testing_data_rnn_input = AR_testing_data_rnn_input.reshape((1, s_in[0]*s_in[1]) + s_in[2:])\n",
    "\n",
    "s_out = AR_testing_data_rnn_output.shape\n",
    "AR_testing_data_rnn_output = AR_testing_data_rnn_output.reshape((1, s_out[0]*s_out[1]) + s_out[2:])\n",
    "\n",
    "T_sample_input_AR = T_sample_input_AR_ratio*np.mean(lyapunov_time_arr)#50.1*dt_rnn\n",
    "num_sample_input_AR = int((T_sample_input_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "T_sample_output_AR = T_sample_output_AR_ratio*np.mean(lyapunov_time_arr)\n",
    "num_sample_output_AR = int((T_sample_output_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "num_offset_AR = num_sample_input_AR\n",
    "T_offset_AR = num_offset_AR*dt_rnn\n",
    "\n",
    "batch_idx = np.random.randint(low=0, high=AR_testing_data_rnn_input.shape[0])\n",
    "maxpossible_num_runs = AR_testing_data_rnn_input.shape[1]-(num_sample_input_AR+num_sample_output_AR)\n",
    "\n",
    "num_runs = np.min([num_runs, maxpossible_num_runs])\n",
    "\n",
    "print('num_runs : ', num_runs)\n",
    "\n",
    "data_idx_arr = np.linspace(0, maxpossible_num_runs-1, num_runs, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779605,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "x3KglJsgneQj"
   },
   "outputs": [],
   "source": [
    "AR_data_in = np.empty(shape=(num_runs, num_sample_input_AR)+tuple(s_in[2:]))\n",
    "AR_data_out = np.empty(shape=(num_runs, num_sample_output_AR)+tuple(s_out[2:]))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    d_idx = data_idx_arr[i]\n",
    "    AR_data_in[i] = AR_testing_data_rnn_input[0, d_idx:d_idx+num_sample_input_AR]\n",
    "    AR_data_out[i] = AR_testing_data_rnn_input[0, d_idx+num_sample_input_AR:d_idx+num_sample_input_AR+num_sample_output_AR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ixetsZHjCMKO"
   },
   "outputs": [],
   "source": [
    "del(AR_testing_data_rnn_input)\n",
    "del(AR_testing_data_rnn_output)\n",
    "AR_testing_data_rnn_input = AR_data_in\n",
    "AR_testing_data_rnn_output = AR_data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   training_data_rnn_input.shape :  (352, 554, 5)\n",
      "  training_data_rnn_output.shape :  (352, 554, 5)\n",
      "    testing_data_rnn_input.shape :  (64, 554, 5)\n",
      "   testing_data_rnn_output.shape :  (64, 554, 5)\n",
      "        val_data_rnn_input.shape :  (32, 554, 5)\n",
      "       val_data_rnn_output.shape :  (32, 554, 5)\n",
      "\n",
      " AR_testing_data_rnn_input.shape :  (50, 185, 6)\n",
      "AR_testing_data_rnn_output.shape :  (50, 554, 6)\n"
     ]
    }
   ],
   "source": [
    "print('   training_data_rnn_input.shape : ', training_data_rnn_input.shape)\n",
    "print('  training_data_rnn_output.shape : ', training_data_rnn_output.shape)\n",
    "print('    testing_data_rnn_input.shape : ', testing_data_rnn_input.shape)\n",
    "print('   testing_data_rnn_output.shape : ', testing_data_rnn_output.shape)\n",
    "print('        val_data_rnn_input.shape : ', val_data_rnn_input.shape)\n",
    "print('       val_data_rnn_output.shape : ', val_data_rnn_output.shape)\n",
    "print('')\n",
    "print(' AR_testing_data_rnn_input.shape : ', AR_testing_data_rnn_input.shape)\n",
    "print('AR_testing_data_rnn_output.shape : ', AR_testing_data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_NSTtZuyneQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeMeanofSpaceRMS : 0.30974782\n",
      "stddev : 0.030493710601818526\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "#     rnn_layers_units = [500]*3\n",
    "    scalar_weights = None\n",
    "#     scalar_weights = [\n",
    "#         0.5, \n",
    "#         0.0, 0.5,\n",
    "#         0.0, 0.0, 1.0,\n",
    "#         1/6, 1/3, 1/3, 1/6\n",
    "#     ] # RK4\n",
    "    # scalar_weights = [\n",
    "    #     1.0,\n",
    "    #     0.25, 0.25,\n",
    "    #     1/6, 1/6, 2/3\n",
    "    # ] # TVD RK3\n",
    "#     scalar_weights = [\n",
    "#         1.0,\n",
    "#         0.5, 0.5\n",
    "#     ] # TVD RK2\n",
    "    num_rnn_layers = 1\n",
    "    if not isinstance(scalar_weights, type(None)):\n",
    "        num_rnn_layers += int( ((8*len(scalar_weights)+1)**0.5 - 1)/2 )\n",
    "    rnn_layers_units = [50*num_latent_states]*num_rnn_layers\n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "    print('timeMeanofSpaceRMS :', timeMeanofSpaceRMS)\n",
    "    print('stddev :', stddev)\n",
    "    if return_params_arr != False:\n",
    "        data_dim = num_latent_states + 3\n",
    "    else:\n",
    "        data_dim = num_latent_states\n",
    "\n",
    "    dense_dim = [rnn_layers_units[-1]]*(len(dense_layer_act_func)-1)\n",
    "    dense_dim.append(data_dim)\n",
    "        \n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_GRU(\n",
    "                data_dim=data_dim,\n",
    "            #     in_steps=int(T_sample_input // dt_rnn),\n",
    "            #     out_steps=int(T_sample_output // dt_rnn),\n",
    "                dt_rnn=dt_rnn,\n",
    "                lambda_reg=lambda_reg,\n",
    "                reg_name='L2',\n",
    "                rnn_layers_units=rnn_layers_units,\n",
    "                dense_layer_act_func=dense_layer_act_func,\n",
    "                load_file=None,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                stddev=stddev,\n",
    "                noise_type=noise_type,\n",
    "                dense_dim=dense_dim,\n",
    "                use_learnable_state=use_learnable_state,\n",
    "                stateful=stateful,\n",
    "                zoneout_rate=zoneout_rate,\n",
    "                batch_size=batch_size,\n",
    "                use_weights_post_dense=use_weights_post_dense,\n",
    "                rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "                denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "                scalar_weights=scalar_weights, # corresponding to RK4\n",
    "                use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_GRU(\n",
    "            data_dim=data_dim,\n",
    "        #     in_steps=int(T_sample_input // dt_rnn),\n",
    "        #     out_steps=int(T_sample_output // dt_rnn),\n",
    "            dt_rnn=dt_rnn,\n",
    "            lambda_reg=lambda_reg,\n",
    "            reg_name='L2',\n",
    "            rnn_layers_units=rnn_layers_units,\n",
    "            dense_layer_act_func=dense_layer_act_func,\n",
    "            load_file=None,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            stddev=stddev,\n",
    "            noise_type=noise_type,\n",
    "            dense_dim=dense_dim,\n",
    "            use_learnable_state=use_learnable_state,\n",
    "            stateful=stateful,\n",
    "            zoneout_rate=zoneout_rate,\n",
    "            batch_size=batch_size,\n",
    "            use_weights_post_dense=use_weights_post_dense,\n",
    "            rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "            denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "            scalar_weights=scalar_weights, # corresponding to RK4\n",
    "            use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "        )\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    rnn_net.save_class_dict(save_path+dir_sep+'final_net_class_dict.txt')\n",
    "else:\n",
    "    load_file = dir_name_rnn + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_GRU(\n",
    "                load_file=load_file,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                batch_size=batch_size,\n",
    "                \n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_GRU(\n",
    "            load_file=load_file,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    rnn_net.build(input_shape=(batch_size, None, num_latent_states))\n",
    "    \n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        wt_file = tf.train.latest_checkpoint(dir_name_rnn+dir_sep+'checkpoints')\n",
    "    elif behaviour == 'loadFinalNetAndPlot':\n",
    "        wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'final_net_gru_weights.h5'\n",
    "        # wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'f2'#+dir_sep+'saved_model.pb'\n",
    "        rnn_net.load_weights_from_file(wt_file)\n",
    "    \n",
    "    # this forces the model to initialize its kernel weights/biases\n",
    "    # temp = rnn_net.predict(tf.ones(shape=[batch_size, int(T_sample_input//dt_rnn), rnn_net.data_dim]))\n",
    "    # this loads just the kernel wieghts and biases of the model\n",
    "#     rnn_net.load_weights_from_file(wt_file)\n",
    "\n",
    "    # rnn_net = tf.keras.models.load_model(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    val_loss_hist = []\n",
    "    train_loss_hist = []\n",
    "    lr_change=[0, 0]\n",
    "    savelosses_cb_vallossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    savelosses_cb_trainlossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    starting_lr_idx = 0\n",
    "    num_epochs_left = epochs\n",
    "    earlystopping_wait = 0\n",
    "elif behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt, earlystopping_wait = readAndReturnLossHistories(\n",
    "        dir_name_ae=dir_name_rnn,\n",
    "        dir_sep=dir_sep,\n",
    "        epochs=epochs,\n",
    "        learning_rate_list=learning_rate_list,\n",
    "        return_earlystopping_wait=True)\n",
    "    savelosses_cb_vallossarr = val_loss_arr_fromckpt\n",
    "    savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n",
    "elif behaviour == 'loadFinalNetAndPlot':\n",
    "    with open(dir_name_rnn+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    losses_dict = eval(''.join(lines))\n",
    "\n",
    "    val_loss_hist = losses_dict['val_loss_hist']\n",
    "    train_loss_hist = losses_dict['train_loss_hist']\n",
    "    lr_change = losses_dict['lr_change']\n",
    "    test_loss = losses_dict['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NMSE_hist = []\n",
    "val_NMSE_hist = []\n",
    "\n",
    "train_MSE_hist = []\n",
    "val_MSE_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMSE(tf.keras.metrics.MeanSquaredError):\n",
    "    def __init__(self, divisor_arr, name='NMSE', **kwargs):\n",
    "        super(NMSE, self).__init__(name, **kwargs)\n",
    "        self.divisor_arr = divisor_arr\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = y_true / self.divisor_arr\n",
    "        y_pred = y_pred / self.divisor_arr\n",
    "        return super(NMSE, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.001 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0602 - mse: 0.0601 - NMSE: 0.5412 - tot_time: 0h 0m 7.5s\n",
      "\n",
      "Epoch 1: val_NMSE improved from inf to 0.18866, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 8s 558ms/step - loss: 0.0602 - mse: 0.0601 - NMSE: 0.5412 - val_loss: 0.0210 - val_mse: 0.0210 - val_NMSE: 0.1887\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0128 - mse: 0.0127 - NMSE: 0.1146 - tot_time: 0h 0m 13.3s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.18866 to 0.06731, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 522ms/step - loss: 0.0128 - mse: 0.0127 - NMSE: 0.1146 - val_loss: 0.0076 - val_mse: 0.0075 - val_NMSE: 0.0673\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0053 - mse: 0.0052 - NMSE: 0.0466 - tot_time: 0h 0m 19.0s\n",
      "\n",
      "Epoch 3: val_NMSE improved from 0.06731 to 0.03085, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 521ms/step - loss: 0.0053 - mse: 0.0052 - NMSE: 0.0466 - val_loss: 0.0035 - val_mse: 0.0034 - val_NMSE: 0.0308\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0022 - mse: 0.0021 - NMSE: 0.0190 - tot_time: 0h 0m 24.7s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.03085 to 0.01299, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 522ms/step - loss: 0.0022 - mse: 0.0021 - NMSE: 0.0190 - val_loss: 0.0015 - val_mse: 0.0014 - val_NMSE: 0.0130\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0012 - mse: 0.0012 - NMSE: 0.0105 - tot_time: 0h 0m 30.1s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.01299 to 0.01015, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 5s 484ms/step - loss: 0.0012 - mse: 0.0012 - NMSE: 0.0105 - val_loss: 0.0012 - val_mse: 0.0011 - val_NMSE: 0.0102\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 9.6679e-04 - mse: 8.8690e-04 - NMSE: 0.0080 - tot_time: 0h 0m 36.2s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.01015 to 0.00840, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 554ms/step - loss: 9.6679e-04 - mse: 8.8690e-04 - NMSE: 0.0080 - val_loss: 0.0010 - val_mse: 9.3289e-04 - val_NMSE: 0.0084\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 8.5175e-04 - mse: 7.7153e-04 - NMSE: 0.0069 - tot_time: 0h 0m 41.8s\n",
      "\n",
      "Epoch 7: val_NMSE improved from 0.00840 to 0.00771, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 511ms/step - loss: 8.5175e-04 - mse: 7.7153e-04 - NMSE: 0.0069 - val_loss: 9.3705e-04 - val_mse: 8.5671e-04 - val_NMSE: 0.0077\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 7.7850e-04 - mse: 6.9804e-04 - NMSE: 0.0063 - tot_time: 0h 0m 48.1s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.00771 to 0.00709, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 584ms/step - loss: 7.7850e-04 - mse: 6.9804e-04 - NMSE: 0.0063 - val_loss: 8.6842e-04 - val_mse: 7.8779e-04 - val_NMSE: 0.0071\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 7.2160e-04 - mse: 6.4079e-04 - NMSE: 0.0058 - tot_time: 0h 0m 54.1s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.00709 to 0.00669, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 543ms/step - loss: 7.2160e-04 - mse: 6.4079e-04 - NMSE: 0.0058 - val_loss: 8.2465e-04 - val_mse: 7.4361e-04 - val_NMSE: 0.0067\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.8210e-04 - mse: 6.0085e-04 - NMSE: 0.0054 - tot_time: 0h 1m 0.4s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.00669 to 0.00640, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 564ms/step - loss: 6.8210e-04 - mse: 6.0085e-04 - NMSE: 0.0054 - val_loss: 7.9235e-04 - val_mse: 7.1084e-04 - val_NMSE: 0.0064\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.4636e-04 - mse: 5.6464e-04 - NMSE: 0.0051 - tot_time: 0h 1m 6.6s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00640 to 0.00610, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 563ms/step - loss: 6.4636e-04 - mse: 5.6464e-04 - NMSE: 0.0051 - val_loss: 7.5997e-04 - val_mse: 6.7800e-04 - val_NMSE: 0.0061\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 6.1322e-04 - mse: 5.3104e-04 - NMSE: 0.0048 - tot_time: 0h 1m 12.9s\n",
      "\n",
      "Epoch 12: val_NMSE improved from 0.00610 to 0.00583, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 6.1322e-04 - mse: 5.3104e-04 - NMSE: 0.0048 - val_loss: 7.2962e-04 - val_mse: 6.4720e-04 - val_NMSE: 0.0058\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.8748e-04 - mse: 5.0487e-04 - NMSE: 0.0045 - tot_time: 0h 1m 19.2s\n",
      "\n",
      "Epoch 13: val_NMSE improved from 0.00583 to 0.00564, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 574ms/step - loss: 5.8748e-04 - mse: 5.0487e-04 - NMSE: 0.0045 - val_loss: 7.0913e-04 - val_mse: 6.2629e-04 - val_NMSE: 0.0056\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.6426e-04 - mse: 4.8124e-04 - NMSE: 0.0043 - tot_time: 0h 1m 25.4s\n",
      "\n",
      "Epoch 14: val_NMSE improved from 0.00564 to 0.00543, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 555ms/step - loss: 5.6426e-04 - mse: 4.8124e-04 - NMSE: 0.0043 - val_loss: 6.8655e-04 - val_mse: 6.0332e-04 - val_NMSE: 0.0054\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.4623e-04 - mse: 4.6285e-04 - NMSE: 0.0042 - tot_time: 0h 1m 31.6s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00543 to 0.00527, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 558ms/step - loss: 5.4623e-04 - mse: 4.6285e-04 - NMSE: 0.0042 - val_loss: 6.6886e-04 - val_mse: 5.8530e-04 - val_NMSE: 0.0053\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.2890e-04 - mse: 4.4522e-04 - NMSE: 0.0040 - tot_time: 0h 1m 37.8s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00527 to 0.00514, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 572ms/step - loss: 5.2890e-04 - mse: 4.4522e-04 - NMSE: 0.0040 - val_loss: 6.5447e-04 - val_mse: 5.7065e-04 - val_NMSE: 0.0051\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.1564e-04 - mse: 4.3173e-04 - NMSE: 0.0039 - tot_time: 0h 1m 43.6s\n",
      "\n",
      "Epoch 17: val_NMSE improved from 0.00514 to 0.00504, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 530ms/step - loss: 5.1564e-04 - mse: 4.3173e-04 - NMSE: 0.0039 - val_loss: 6.4426e-04 - val_mse: 5.6024e-04 - val_NMSE: 0.0050\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 5.0287e-04 - mse: 4.1877e-04 - NMSE: 0.0038 - tot_time: 0h 1m 50.0s\n",
      "\n",
      "Epoch 18: val_NMSE improved from 0.00504 to 0.00497, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 577ms/step - loss: 5.0287e-04 - mse: 4.1877e-04 - NMSE: 0.0038 - val_loss: 6.3608e-04 - val_mse: 5.5191e-04 - val_NMSE: 0.0050\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.9149e-04 - mse: 4.0728e-04 - NMSE: 0.0037 - tot_time: 0h 1m 55.7s\n",
      "\n",
      "Epoch 19: val_NMSE improved from 0.00497 to 0.00490, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 529ms/step - loss: 4.9149e-04 - mse: 4.0728e-04 - NMSE: 0.0037 - val_loss: 6.2864e-04 - val_mse: 5.4439e-04 - val_NMSE: 0.0049\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.8172e-04 - mse: 3.9745e-04 - NMSE: 0.0036 - tot_time: 0h 2m 1.6s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00490 to 0.00483, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 543ms/step - loss: 4.8172e-04 - mse: 3.9745e-04 - NMSE: 0.0036 - val_loss: 6.2050e-04 - val_mse: 5.3622e-04 - val_NMSE: 0.0048\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.7527e-04 - mse: 3.9101e-04 - NMSE: 0.0035 - tot_time: 0h 2m 7.9s\n",
      "\n",
      "Epoch 21: val_NMSE improved from 0.00483 to 0.00476, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 571ms/step - loss: 4.7527e-04 - mse: 3.9101e-04 - NMSE: 0.0035 - val_loss: 6.1300e-04 - val_mse: 5.2876e-04 - val_NMSE: 0.0048\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.6342e-04 - mse: 3.7922e-04 - NMSE: 0.0034 - tot_time: 0h 2m 13.9s\n",
      "\n",
      "Epoch 22: val_NMSE improved from 0.00476 to 0.00473, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 543ms/step - loss: 4.6342e-04 - mse: 3.7922e-04 - NMSE: 0.0034 - val_loss: 6.0958e-04 - val_mse: 5.2543e-04 - val_NMSE: 0.0047\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.5895e-04 - mse: 3.7486e-04 - NMSE: 0.0034 - tot_time: 0h 2m 20.3s\n",
      "\n",
      "Epoch 23: val_NMSE improved from 0.00473 to 0.00469, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 582ms/step - loss: 4.5895e-04 - mse: 3.7486e-04 - NMSE: 0.0034 - val_loss: 6.0540e-04 - val_mse: 5.2138e-04 - val_NMSE: 0.0047\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.5054e-04 - mse: 3.6659e-04 - NMSE: 0.0033 - tot_time: 0h 2m 26.6s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00469 to 0.00465, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 586ms/step - loss: 4.5054e-04 - mse: 3.6659e-04 - NMSE: 0.0033 - val_loss: 6.0088e-04 - val_mse: 5.1702e-04 - val_NMSE: 0.0047\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.4468e-04 - mse: 3.6092e-04 - NMSE: 0.0032 - tot_time: 0h 2m 32.7s\n",
      "\n",
      "Epoch 25: val_NMSE improved from 0.00465 to 0.00463, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 546ms/step - loss: 4.4468e-04 - mse: 3.6092e-04 - NMSE: 0.0032 - val_loss: 5.9769e-04 - val_mse: 5.1403e-04 - val_NMSE: 0.0046\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.3984e-04 - mse: 3.5629e-04 - NMSE: 0.0032 - tot_time: 0h 2m 39.0s\n",
      "\n",
      "Epoch 26: val_NMSE improved from 0.00463 to 0.00456, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 563ms/step - loss: 4.3984e-04 - mse: 3.5629e-04 - NMSE: 0.0032 - val_loss: 5.8974e-04 - val_mse: 5.0631e-04 - val_NMSE: 0.0046\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.3324e-04 - mse: 3.4993e-04 - NMSE: 0.0031 - tot_time: 0h 2m 45.6s\n",
      "\n",
      "Epoch 27: val_NMSE improved from 0.00456 to 0.00453, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 7s 607ms/step - loss: 4.3324e-04 - mse: 3.4993e-04 - NMSE: 0.0031 - val_loss: 5.8602e-04 - val_mse: 5.0286e-04 - val_NMSE: 0.0045\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.3093e-04 - mse: 3.4790e-04 - NMSE: 0.0031 - tot_time: 0h 2m 51.6s\n",
      "\n",
      "Epoch 28: val_NMSE did not improve from 0.00453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 534ms/step - loss: 4.3093e-04 - mse: 3.4790e-04 - NMSE: 0.0031 - val_loss: 5.8653e-04 - val_mse: 5.0364e-04 - val_NMSE: 0.0045\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.2427e-04 - mse: 3.4152e-04 - NMSE: 0.0031 - tot_time: 0h 2m 57.7s\n",
      "\n",
      "Epoch 29: val_NMSE improved from 0.00453 to 0.00448, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 555ms/step - loss: 4.2427e-04 - mse: 3.4152e-04 - NMSE: 0.0031 - val_loss: 5.8045e-04 - val_mse: 4.9786e-04 - val_NMSE: 0.0045\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.2003e-04 - mse: 3.3758e-04 - NMSE: 0.0030 - tot_time: 0h 3m 3.7s\n",
      "\n",
      "Epoch 30: val_NMSE did not improve from 0.00448\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 552ms/step - loss: 4.2003e-04 - mse: 3.3758e-04 - NMSE: 0.0030 - val_loss: 5.8214e-04 - val_mse: 4.9986e-04 - val_NMSE: 0.0045\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.1446e-04 - mse: 3.3232e-04 - NMSE: 0.0030 - tot_time: 0h 3m 9.9s\n",
      "\n",
      "Epoch 31: val_NMSE improved from 0.00448 to 0.00446, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 570ms/step - loss: 4.1446e-04 - mse: 3.3232e-04 - NMSE: 0.0030 - val_loss: 5.7707e-04 - val_mse: 4.9511e-04 - val_NMSE: 0.0045\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.1161e-04 - mse: 3.2980e-04 - NMSE: 0.0030 - tot_time: 0h 3m 16.1s\n",
      "\n",
      "Epoch 32: val_NMSE did not improve from 0.00446\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 565ms/step - loss: 4.1161e-04 - mse: 3.2980e-04 - NMSE: 0.0030 - val_loss: 5.7871e-04 - val_mse: 4.9708e-04 - val_NMSE: 0.0045\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.0641e-04 - mse: 3.2493e-04 - NMSE: 0.0029 - tot_time: 0h 3m 22.5s\n",
      "\n",
      "Epoch 33: val_NMSE improved from 0.00446 to 0.00441, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 593ms/step - loss: 4.0641e-04 - mse: 3.2493e-04 - NMSE: 0.0029 - val_loss: 5.7071e-04 - val_mse: 4.8940e-04 - val_NMSE: 0.0044\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.0147e-04 - mse: 3.2032e-04 - NMSE: 0.0029 - tot_time: 0h 3m 28.8s\n",
      "\n",
      "Epoch 34: val_NMSE improved from 0.00441 to 0.00438, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 581ms/step - loss: 4.0147e-04 - mse: 3.2032e-04 - NMSE: 0.0029 - val_loss: 5.6732e-04 - val_mse: 4.8637e-04 - val_NMSE: 0.0044\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.9767e-04 - mse: 3.1688e-04 - NMSE: 0.0029 - tot_time: 0h 3m 35.1s\n",
      "\n",
      "Epoch 35: val_NMSE did not improve from 0.00438\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 586ms/step - loss: 3.9767e-04 - mse: 3.1688e-04 - NMSE: 0.0029 - val_loss: 5.7059e-04 - val_mse: 4.8999e-04 - val_NMSE: 0.0044\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.9537e-04 - mse: 3.1494e-04 - NMSE: 0.0028 - tot_time: 0h 3m 41.7s\n",
      "\n",
      "Epoch 36: val_NMSE did not improve from 0.00438\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 7s 600ms/step - loss: 3.9537e-04 - mse: 3.1494e-04 - NMSE: 0.0028 - val_loss: 5.6724e-04 - val_mse: 4.8699e-04 - val_NMSE: 0.0044\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.9137e-04 - mse: 3.1128e-04 - NMSE: 0.0028 - tot_time: 0h 3m 47.7s\n",
      "\n",
      "Epoch 37: val_NMSE did not improve from 0.00438\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 549ms/step - loss: 3.9137e-04 - mse: 3.1128e-04 - NMSE: 0.0028 - val_loss: 5.6763e-04 - val_mse: 4.8774e-04 - val_NMSE: 0.0044\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.8834e-04 - mse: 3.0861e-04 - NMSE: 0.0028 - tot_time: 0h 3m 54.1s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00438 to 0.00434, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 577ms/step - loss: 3.8834e-04 - mse: 3.0861e-04 - NMSE: 0.0028 - val_loss: 5.6215e-04 - val_mse: 4.8261e-04 - val_NMSE: 0.0043\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.8427e-04 - mse: 3.0490e-04 - NMSE: 0.0027 - tot_time: 0h 4m 0.3s\n",
      "\n",
      "Epoch 39: val_NMSE improved from 0.00434 to 0.00432, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 568ms/step - loss: 3.8427e-04 - mse: 3.0490e-04 - NMSE: 0.0027 - val_loss: 5.5877e-04 - val_mse: 4.7960e-04 - val_NMSE: 0.0043\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.7983e-04 - mse: 3.0081e-04 - NMSE: 0.0027 - tot_time: 0h 4m 6.5s\n",
      "\n",
      "Epoch 40: val_NMSE improved from 0.00432 to 0.00431, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 575ms/step - loss: 3.7983e-04 - mse: 3.0081e-04 - NMSE: 0.0027 - val_loss: 5.5799e-04 - val_mse: 4.7916e-04 - val_NMSE: 0.0043\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.7674e-04 - mse: 2.9807e-04 - NMSE: 0.0027 - tot_time: 0h 4m 12.8s\n",
      "\n",
      "Epoch 41: val_NMSE improved from 0.00431 to 0.00428, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 3.7674e-04 - mse: 2.9807e-04 - NMSE: 0.0027 - val_loss: 5.5436e-04 - val_mse: 4.7590e-04 - val_NMSE: 0.0043\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.7428e-04 - mse: 2.9599e-04 - NMSE: 0.0027 - tot_time: 0h 4m 18.8s\n",
      "\n",
      "Epoch 42: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 554ms/step - loss: 3.7428e-04 - mse: 2.9599e-04 - NMSE: 0.0027 - val_loss: 5.6019e-04 - val_mse: 4.8209e-04 - val_NMSE: 0.0043\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.7173e-04 - mse: 2.9380e-04 - NMSE: 0.0026 - tot_time: 0h 4m 24.5s\n",
      "\n",
      "Epoch 43: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 521ms/step - loss: 3.7173e-04 - mse: 2.9380e-04 - NMSE: 0.0026 - val_loss: 5.5706e-04 - val_mse: 4.7932e-04 - val_NMSE: 0.0043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.6727e-04 - mse: 2.8969e-04 - NMSE: 0.0026 - tot_time: 0h 4m 30.5s\n",
      "\n",
      "Epoch 44: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 3.6727e-04 - mse: 2.8969e-04 - NMSE: 0.0026 - val_loss: 5.5712e-04 - val_mse: 4.7974e-04 - val_NMSE: 0.0043\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.6538e-04 - mse: 2.8816e-04 - NMSE: 0.0026 - tot_time: 0h 4m 36.3s\n",
      "\n",
      "Epoch 45: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 526ms/step - loss: 3.6538e-04 - mse: 2.8816e-04 - NMSE: 0.0026 - val_loss: 5.5547e-04 - val_mse: 4.7844e-04 - val_NMSE: 0.0043\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.6355e-04 - mse: 2.8669e-04 - NMSE: 0.0026 - tot_time: 0h 4m 42.2s\n",
      "\n",
      "Epoch 46: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 546ms/step - loss: 3.6355e-04 - mse: 2.8669e-04 - NMSE: 0.0026 - val_loss: 5.5479e-04 - val_mse: 4.7813e-04 - val_NMSE: 0.0043\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.5873e-04 - mse: 2.8223e-04 - NMSE: 0.0025 - tot_time: 0h 4m 48.1s\n",
      "\n",
      "Epoch 47: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 532ms/step - loss: 3.5873e-04 - mse: 2.8223e-04 - NMSE: 0.0025 - val_loss: 5.5418e-04 - val_mse: 4.7789e-04 - val_NMSE: 0.0043\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.5775e-04 - mse: 2.8161e-04 - NMSE: 0.0025 - tot_time: 0h 4m 53.9s\n",
      "\n",
      "Epoch 48: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 525ms/step - loss: 3.5775e-04 - mse: 2.8161e-04 - NMSE: 0.0025 - val_loss: 5.5365e-04 - val_mse: 4.7770e-04 - val_NMSE: 0.0043\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.5360e-04 - mse: 2.7781e-04 - NMSE: 0.0025 - tot_time: 0h 4m 59.9s\n",
      "\n",
      "Epoch 49: val_NMSE improved from 0.00428 to 0.00428, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 558ms/step - loss: 3.5360e-04 - mse: 2.7781e-04 - NMSE: 0.0025 - val_loss: 5.5135e-04 - val_mse: 4.7575e-04 - val_NMSE: 0.0043\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.5183e-04 - mse: 2.7639e-04 - NMSE: 0.0025 - tot_time: 0h 5m 5.8s\n",
      "\n",
      "Epoch 50: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 536ms/step - loss: 3.5183e-04 - mse: 2.7639e-04 - NMSE: 0.0025 - val_loss: 5.5187e-04 - val_mse: 4.7663e-04 - val_NMSE: 0.0043\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4779e-04 - mse: 2.7272e-04 - NMSE: 0.0025 - tot_time: 0h 5m 11.9s\n",
      "\n",
      "Epoch 51: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 560ms/step - loss: 3.4779e-04 - mse: 2.7272e-04 - NMSE: 0.0025 - val_loss: 5.5138e-04 - val_mse: 4.7650e-04 - val_NMSE: 0.0043\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4547e-04 - mse: 2.7076e-04 - NMSE: 0.0024 - tot_time: 0h 5m 17.7s\n",
      "\n",
      "Epoch 52: val_NMSE did not improve from 0.00428\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 529ms/step - loss: 3.4547e-04 - mse: 2.7076e-04 - NMSE: 0.0024 - val_loss: 5.5094e-04 - val_mse: 4.7642e-04 - val_NMSE: 0.0043\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4398e-04 - mse: 2.6962e-04 - NMSE: 0.0024 - tot_time: 0h 5m 23.7s\n",
      "\n",
      "Epoch 53: val_NMSE improved from 0.00428 to 0.00427, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 540ms/step - loss: 3.4398e-04 - mse: 2.6962e-04 - NMSE: 0.0024 - val_loss: 5.4865e-04 - val_mse: 4.7449e-04 - val_NMSE: 0.0043\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4117e-04 - mse: 2.6717e-04 - NMSE: 0.0024 - tot_time: 0h 5m 29.7s\n",
      "\n",
      "Epoch 54: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 556ms/step - loss: 3.4117e-04 - mse: 2.6717e-04 - NMSE: 0.0024 - val_loss: 5.4955e-04 - val_mse: 4.7574e-04 - val_NMSE: 0.0043\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3832e-04 - mse: 2.6469e-04 - NMSE: 0.0024 - tot_time: 0h 5m 35.7s\n",
      "\n",
      "Epoch 55: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 544ms/step - loss: 3.3832e-04 - mse: 2.6469e-04 - NMSE: 0.0024 - val_loss: 5.4852e-04 - val_mse: 4.7509e-04 - val_NMSE: 0.0043\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3741e-04 - mse: 2.6414e-04 - NMSE: 0.0024 - tot_time: 0h 5m 41.5s\n",
      "\n",
      "Epoch 56: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 527ms/step - loss: 3.3741e-04 - mse: 2.6414e-04 - NMSE: 0.0024 - val_loss: 5.4785e-04 - val_mse: 4.7477e-04 - val_NMSE: 0.0043\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3575e-04 - mse: 2.6283e-04 - NMSE: 0.0024 - tot_time: 0h 5m 47.4s\n",
      "\n",
      "Epoch 57: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 538ms/step - loss: 3.3575e-04 - mse: 2.6283e-04 - NMSE: 0.0024 - val_loss: 5.5004e-04 - val_mse: 4.7731e-04 - val_NMSE: 0.0043\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3305e-04 - mse: 2.6046e-04 - NMSE: 0.0023 - tot_time: 0h 5m 53.7s\n",
      "\n",
      "Epoch 58: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 576ms/step - loss: 3.3305e-04 - mse: 2.6046e-04 - NMSE: 0.0023 - val_loss: 5.4691e-04 - val_mse: 4.7449e-04 - val_NMSE: 0.0043\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3004e-04 - mse: 2.5779e-04 - NMSE: 0.0023 - tot_time: 0h 5m 59.8s\n",
      "\n",
      "Epoch 59: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 552ms/step - loss: 3.3004e-04 - mse: 2.5779e-04 - NMSE: 0.0023 - val_loss: 5.4749e-04 - val_mse: 4.7545e-04 - val_NMSE: 0.0043\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.2867e-04 - mse: 2.5678e-04 - NMSE: 0.0023 - tot_time: 0h 6m 5.6s\n",
      "\n",
      "Epoch 60: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 520ms/step - loss: 3.2867e-04 - mse: 2.5678e-04 - NMSE: 0.0023 - val_loss: 5.4721e-04 - val_mse: 4.7551e-04 - val_NMSE: 0.0043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.2678e-04 - mse: 2.5524e-04 - NMSE: 0.0023 - tot_time: 0h 6m 11.7s\n",
      "\n",
      "Epoch 61: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 565ms/step - loss: 3.2678e-04 - mse: 2.5524e-04 - NMSE: 0.0023 - val_loss: 5.4812e-04 - val_mse: 4.7677e-04 - val_NMSE: 0.0043\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.2486e-04 - mse: 2.5366e-04 - NMSE: 0.0023 - tot_time: 0h 6m 17.7s\n",
      "\n",
      "Epoch 62: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 540ms/step - loss: 3.2486e-04 - mse: 2.5366e-04 - NMSE: 0.0023 - val_loss: 5.4903e-04 - val_mse: 4.7802e-04 - val_NMSE: 0.0043\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.2160e-04 - mse: 2.5075e-04 - NMSE: 0.0023Restoring model weights from the end of the best epoch: 53.\n",
      " - tot_time: 0h 6m 24.0s\n",
      "\n",
      "Epoch 63: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 579ms/step - loss: 3.2160e-04 - mse: 2.5075e-04 - NMSE: 0.0023 - val_loss: 5.4860e-04 - val_mse: 4.7792e-04 - val_NMSE: 0.0043\n",
      "Epoch 63: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.0001 ----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4315e-04 - mse: 2.6901e-04 - NMSE: 0.0024 - tot_time: 0h 6m 30.0s\n",
      "\n",
      "Epoch 1: val_NMSE did not improve from 0.00427\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 551ms/step - loss: 3.4315e-04 - mse: 2.6901e-04 - NMSE: 0.0024 - val_loss: 5.4863e-04 - val_mse: 4.7450e-04 - val_NMSE: 0.0043\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4122e-04 - mse: 2.6711e-04 - NMSE: 0.0024 - tot_time: 0h 6m 36.1s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.00427 to 0.00426, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 550ms/step - loss: 3.4122e-04 - mse: 2.6711e-04 - NMSE: 0.0024 - val_loss: 5.4770e-04 - val_mse: 4.7362e-04 - val_NMSE: 0.0043\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4172e-04 - mse: 2.6766e-04 - NMSE: 0.0024 - tot_time: 0h 6m 42.1s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00426\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 551ms/step - loss: 3.4172e-04 - mse: 2.6766e-04 - NMSE: 0.0024 - val_loss: 5.4971e-04 - val_mse: 4.7567e-04 - val_NMSE: 0.0043\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4096e-04 - mse: 2.6694e-04 - NMSE: 0.0024 - tot_time: 0h 6m 48.2s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00426\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 3.4096e-04 - mse: 2.6694e-04 - NMSE: 0.0024 - val_loss: 5.4956e-04 - val_mse: 4.7556e-04 - val_NMSE: 0.0043\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4096e-04 - mse: 2.6699e-04 - NMSE: 0.0024 - tot_time: 0h 6m 54.2s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00426\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 549ms/step - loss: 3.4096e-04 - mse: 2.6699e-04 - NMSE: 0.0024 - val_loss: 5.4871e-04 - val_mse: 4.7476e-04 - val_NMSE: 0.0043\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4133e-04 - mse: 2.6740e-04 - NMSE: 0.0024 - tot_time: 0h 7m 0.2s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.00426 to 0.00426, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 547ms/step - loss: 3.4133e-04 - mse: 2.6740e-04 - NMSE: 0.0024 - val_loss: 5.4696e-04 - val_mse: 4.7305e-04 - val_NMSE: 0.0043\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4033e-04 - mse: 2.6644e-04 - NMSE: 0.0024 - tot_time: 0h 7m 6.4s\n",
      "\n",
      "Epoch 7: val_NMSE improved from 0.00426 to 0.00425, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 3.4033e-04 - mse: 2.6644e-04 - NMSE: 0.0024 - val_loss: 5.4554e-04 - val_mse: 4.7167e-04 - val_NMSE: 0.0042\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4059e-04 - mse: 2.6674e-04 - NMSE: 0.0024 - tot_time: 0h 7m 12.3s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 525ms/step - loss: 3.4059e-04 - mse: 2.6674e-04 - NMSE: 0.0024 - val_loss: 5.5211e-04 - val_mse: 4.7828e-04 - val_NMSE: 0.0043\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3982e-04 - mse: 2.6602e-04 - NMSE: 0.0024 - tot_time: 0h 7m 18.6s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 579ms/step - loss: 3.3982e-04 - mse: 2.6602e-04 - NMSE: 0.0024 - val_loss: 5.5004e-04 - val_mse: 4.7625e-04 - val_NMSE: 0.0043\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3970e-04 - mse: 2.6594e-04 - NMSE: 0.0024 - tot_time: 0h 7m 25.1s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 7s 594ms/step - loss: 3.3970e-04 - mse: 2.6594e-04 - NMSE: 0.0024 - val_loss: 5.4771e-04 - val_mse: 4.7397e-04 - val_NMSE: 0.0043\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3925e-04 - mse: 2.6553e-04 - NMSE: 0.0024 - tot_time: 0h 7m 30.6s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 5s 499ms/step - loss: 3.3925e-04 - mse: 2.6553e-04 - NMSE: 0.0024 - val_loss: 5.4982e-04 - val_mse: 4.7612e-04 - val_NMSE: 0.0043\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3903e-04 - mse: 2.6535e-04 - NMSE: 0.0024 - tot_time: 0h 7m 37.1s\n",
      "\n",
      "Epoch 12: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 585ms/step - loss: 3.3903e-04 - mse: 2.6535e-04 - NMSE: 0.0024 - val_loss: 5.4936e-04 - val_mse: 4.7571e-04 - val_NMSE: 0.0043\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3822e-04 - mse: 2.6459e-04 - NMSE: 0.0024 - tot_time: 0h 7m 42.9s\n",
      "\n",
      "Epoch 13: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 541ms/step - loss: 3.3822e-04 - mse: 2.6459e-04 - NMSE: 0.0024 - val_loss: 5.4901e-04 - val_mse: 4.7540e-04 - val_NMSE: 0.0043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3813e-04 - mse: 2.6455e-04 - NMSE: 0.0024 - tot_time: 0h 7m 49.0s\n",
      "\n",
      "Epoch 14: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 554ms/step - loss: 3.3813e-04 - mse: 2.6455e-04 - NMSE: 0.0024 - val_loss: 5.4535e-04 - val_mse: 4.7179e-04 - val_NMSE: 0.0042\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3828e-04 - mse: 2.6474e-04 - NMSE: 0.0024 - tot_time: 0h 7m 55.2s\n",
      "\n",
      "Epoch 15: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 577ms/step - loss: 3.3828e-04 - mse: 2.6474e-04 - NMSE: 0.0024 - val_loss: 5.4890e-04 - val_mse: 4.7539e-04 - val_NMSE: 0.0043\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3838e-04 - mse: 2.6489e-04 - NMSE: 0.0024 - tot_time: 0h 8m 1.2s\n",
      "\n",
      "Epoch 16: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 536ms/step - loss: 3.3838e-04 - mse: 2.6489e-04 - NMSE: 0.0024 - val_loss: 5.5111e-04 - val_mse: 4.7763e-04 - val_NMSE: 0.0043\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3781e-04 - mse: 2.6436e-04 - NMSE: 0.0024Restoring model weights from the end of the best epoch: 7.\n",
      " - tot_time: 0h 8m 7.4s\n",
      "\n",
      "Epoch 17: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 564ms/step - loss: 3.3781e-04 - mse: 2.6436e-04 - NMSE: 0.0024 - val_loss: 5.4553e-04 - val_mse: 4.7210e-04 - val_NMSE: 0.0042\n",
      "Epoch 17: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-05 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4069e-04 - mse: 2.6683e-04 - NMSE: 0.0024 - tot_time: 0h 8m 13.4s\n",
      "\n",
      "Epoch 1: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 551ms/step - loss: 3.4069e-04 - mse: 2.6683e-04 - NMSE: 0.0024 - val_loss: 5.4902e-04 - val_mse: 4.7515e-04 - val_NMSE: 0.0043\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4044e-04 - mse: 2.6658e-04 - NMSE: 0.0024 - tot_time: 0h 8m 19.7s\n",
      "\n",
      "Epoch 2: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 567ms/step - loss: 3.4044e-04 - mse: 2.6658e-04 - NMSE: 0.0024 - val_loss: 5.4870e-04 - val_mse: 4.7484e-04 - val_NMSE: 0.0043\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3979e-04 - mse: 2.6593e-04 - NMSE: 0.0024 - tot_time: 0h 8m 25.8s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 544ms/step - loss: 3.3979e-04 - mse: 2.6593e-04 - NMSE: 0.0024 - val_loss: 5.4856e-04 - val_mse: 4.7470e-04 - val_NMSE: 0.0043\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4030e-04 - mse: 2.6644e-04 - NMSE: 0.0024 - tot_time: 0h 8m 31.6s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 531ms/step - loss: 3.4030e-04 - mse: 2.6644e-04 - NMSE: 0.0024 - val_loss: 5.5048e-04 - val_mse: 4.7663e-04 - val_NMSE: 0.0043\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4034e-04 - mse: 2.6650e-04 - NMSE: 0.0024 - tot_time: 0h 8m 38.0s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 588ms/step - loss: 3.4034e-04 - mse: 2.6650e-04 - NMSE: 0.0024 - val_loss: 5.4738e-04 - val_mse: 4.7353e-04 - val_NMSE: 0.0043\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4058e-04 - mse: 2.6673e-04 - NMSE: 0.0024 - tot_time: 0h 8m 43.8s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 531ms/step - loss: 3.4058e-04 - mse: 2.6673e-04 - NMSE: 0.0024 - val_loss: 5.4902e-04 - val_mse: 4.7518e-04 - val_NMSE: 0.0043\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3955e-04 - mse: 2.6571e-04 - NMSE: 0.0024 - tot_time: 0h 8m 50.0s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 566ms/step - loss: 3.3955e-04 - mse: 2.6571e-04 - NMSE: 0.0024 - val_loss: 5.4724e-04 - val_mse: 4.7340e-04 - val_NMSE: 0.0043\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4061e-04 - mse: 2.6678e-04 - NMSE: 0.0024 - tot_time: 0h 8m 56.1s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 565ms/step - loss: 3.4061e-04 - mse: 2.6678e-04 - NMSE: 0.0024 - val_loss: 5.5021e-04 - val_mse: 4.7638e-04 - val_NMSE: 0.0043\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3952e-04 - mse: 2.6569e-04 - NMSE: 0.0024 - tot_time: 0h 9m 2.3s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 560ms/step - loss: 3.3952e-04 - mse: 2.6569e-04 - NMSE: 0.0024 - val_loss: 5.5162e-04 - val_mse: 4.7779e-04 - val_NMSE: 0.0043\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4011e-04 - mse: 2.6629e-04 - NMSE: 0.0024 - tot_time: 0h 9m 8.6s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 565ms/step - loss: 3.4011e-04 - mse: 2.6629e-04 - NMSE: 0.0024 - val_loss: 5.4833e-04 - val_mse: 4.7451e-04 - val_NMSE: 0.0043\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3998e-04 - mse: 2.6616e-04 - NMSE: 0.0024 - tot_time: 0h 9m 14.5s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 544ms/step - loss: 3.3998e-04 - mse: 2.6616e-04 - NMSE: 0.0024 - val_loss: 5.4653e-04 - val_mse: 4.7272e-04 - val_NMSE: 0.0043\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4029e-04 - mse: 2.6647e-04 - NMSE: 0.0024 - tot_time: 0h 9m 20.5s\n",
      "\n",
      "Epoch 12: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 550ms/step - loss: 3.4029e-04 - mse: 2.6647e-04 - NMSE: 0.0024 - val_loss: 5.4557e-04 - val_mse: 4.7176e-04 - val_NMSE: 0.0042\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 3.3936e-04 - mse: 2.6555e-04 - NMSE: 0.0024 - tot_time: 0h 9m 26.3s\n",
      "\n",
      "Epoch 13: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 526ms/step - loss: 3.3936e-04 - mse: 2.6555e-04 - NMSE: 0.0024 - val_loss: 5.5082e-04 - val_mse: 4.7702e-04 - val_NMSE: 0.0043\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3955e-04 - mse: 2.6574e-04 - NMSE: 0.0024 - tot_time: 0h 9m 32.5s\n",
      "\n",
      "Epoch 14: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 562ms/step - loss: 3.3955e-04 - mse: 2.6574e-04 - NMSE: 0.0024 - val_loss: 5.4864e-04 - val_mse: 4.7484e-04 - val_NMSE: 0.0043\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3993e-04 - mse: 2.6613e-04 - NMSE: 0.0024 - tot_time: 0h 9m 38.3s\n",
      "\n",
      "Epoch 15: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 530ms/step - loss: 3.3993e-04 - mse: 2.6613e-04 - NMSE: 0.0024 - val_loss: 5.4904e-04 - val_mse: 4.7524e-04 - val_NMSE: 0.0043\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4055e-04 - mse: 2.6676e-04 - NMSE: 0.0024 - tot_time: 0h 9m 44.2s\n",
      "\n",
      "Epoch 16: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 532ms/step - loss: 3.4055e-04 - mse: 2.6676e-04 - NMSE: 0.0024 - val_loss: 5.4870e-04 - val_mse: 4.7491e-04 - val_NMSE: 0.0043\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3990e-04 - mse: 2.6611e-04 - NMSE: 0.0024 - tot_time: 0h 9m 50.0s\n",
      "\n",
      "Epoch 17: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 526ms/step - loss: 3.3990e-04 - mse: 2.6611e-04 - NMSE: 0.0024 - val_loss: 5.4875e-04 - val_mse: 4.7496e-04 - val_NMSE: 0.0043\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4003e-04 - mse: 2.6624e-04 - NMSE: 0.0024 - tot_time: 0h 9m 56.3s\n",
      "\n",
      "Epoch 18: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 569ms/step - loss: 3.4003e-04 - mse: 2.6624e-04 - NMSE: 0.0024 - val_loss: 5.4749e-04 - val_mse: 4.7371e-04 - val_NMSE: 0.0043\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4032e-04 - mse: 2.6654e-04 - NMSE: 0.0024 - tot_time: 0h 10m 2.4s\n",
      "\n",
      "Epoch 19: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 550ms/step - loss: 3.4032e-04 - mse: 2.6654e-04 - NMSE: 0.0024 - val_loss: 5.4977e-04 - val_mse: 4.7600e-04 - val_NMSE: 0.0043\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3998e-04 - mse: 2.6621e-04 - NMSE: 0.0024 - tot_time: 0h 10m 8.5s\n",
      "\n",
      "Epoch 20: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 570ms/step - loss: 3.3998e-04 - mse: 2.6621e-04 - NMSE: 0.0024 - val_loss: 5.4640e-04 - val_mse: 4.7263e-04 - val_NMSE: 0.0043\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.3875e-04 - mse: 2.6498e-04 - NMSE: 0.0024 - tot_time: 0h 10m 14.6s\n",
      "\n",
      "Epoch 21: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 559ms/step - loss: 3.3875e-04 - mse: 2.6498e-04 - NMSE: 0.0024 - val_loss: 5.5350e-04 - val_mse: 4.7973e-04 - val_NMSE: 0.0043\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.4026e-04 - mse: 2.6650e-04 - NMSE: 0.0024Restoring model weights from the end of the best epoch: 12.\n",
      " - tot_time: 0h 10m 20.9s\n",
      "\n",
      "Epoch 22: val_NMSE did not improve from 0.00425\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_cdv/saved_rnn/rnn_000/checkpoints/LossHistoriesCheckpoint\n",
      "11/11 [==============================] - 6s 570ms/step - loss: 3.4026e-04 - mse: 2.6650e-04 - NMSE: 0.0024 - val_loss: 5.4966e-04 - val_mse: 4.7590e-04 - val_NMSE: 0.0043\n",
      "Epoch 22: early stopping\n"
     ]
    }
   ],
   "source": [
    "# compiling the network\n",
    "rnn_net.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=['mse', NMSE(divisor_arr=time_stddev)],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # this loads the weights/attributes of the optimizer as well\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net.load_weights(wt_file)\n",
    "    else:\n",
    "        rnn_net.load_weights(wt_file)\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # implementing early stopping\n",
    "    baseline = None\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        baseline = np.min(val_loss_hist)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_NMSE',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=True,\n",
    "        min_delta=min_delta,\n",
    "        baseline=baseline\n",
    "    )\n",
    "    #** the two lines below are useless because wait is set to 0 in on_train_begin\n",
    "    # early_stopping_cb.wait = earlystopping_wait\n",
    "    # print('early_stopping_cb.wait : {}\\n'.format(early_stopping_cb.wait))\n",
    "\n",
    "    # time callback for each epoch\n",
    "    timekeeper_cb = mytimecallback()\n",
    "\n",
    "    # model checkpoint callback\n",
    "    dir_name_ckpt = dir_name_rnn+dir_sep+'checkpoints'\n",
    "    if not os.path.isdir(dir_name_ckpt):\n",
    "        os.makedirs(dir_name_ckpt)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dir_name_ckpt+dir_sep+'checkpoint',#+'/checkpoint--loss={loss:.4f}--vall_loss={val_loss:.4f}',\n",
    "        monitor='val_NMSE',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=2,\n",
    "        initial_value_threshold=baseline,\n",
    "        period=1  # saves every `period` epochs\n",
    "    )\n",
    "\n",
    "    # save losses callback\n",
    "    savelosses_cb = SaveLosses(\n",
    "        filepath=dir_name_ckpt+dir_sep+'LossHistoriesCheckpoint',\n",
    "        val_loss_arr=savelosses_cb_vallossarr,\n",
    "        train_loss_arr=savelosses_cb_trainlossarr,\n",
    "        total_epochs=epochs,\n",
    "        period=1)\n",
    "\n",
    "    for i in range(starting_lr_idx, len(learning_rate_list)):\n",
    "        learning_rate = learning_rate_list[i]\n",
    "        K.set_value(rnn_net.optimizer.lr, learning_rate)\n",
    "\n",
    "        savelosses_cb.update_lr_idx(i)\n",
    "\n",
    "        if i == starting_lr_idx:\n",
    "            EPOCHS = num_epochs_left\n",
    "            savelosses_cb.update_offset(epochs-num_epochs_left)\n",
    "        else:\n",
    "            EPOCHS = epochs\n",
    "            savelosses_cb.update_offset(0)\n",
    "\n",
    "        total_s_len = 80\n",
    "        sep_lr_s = ' LEARNING RATE : {} '.format(learning_rate)\n",
    "        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n",
    "        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n",
    "        print('\\n\\n' + '-'*len(sep_lr_s))\n",
    "        print('\\n' + sep_lr_s+'\\n')\n",
    "        print('-'*len(sep_lr_s) + '\\n\\n')\n",
    "        \n",
    "        history = rnn_net.fit(training_data_rnn_input, training_data_rnn_output,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=batch_size,\n",
    "#             validation_split=val_split/train_split,\n",
    "            validation_data=(val_data_rnn_input, val_data_rnn_output),\n",
    "            callbacks=[early_stopping_cb, timekeeper_cb, checkpoint_cb, savelosses_cb],\n",
    "            verbose=1,\n",
    "            shuffle=not stateful,\n",
    "        )\n",
    "\n",
    "        val_loss_hist.extend(history.history['val_loss'])\n",
    "        train_loss_hist.extend(history.history['loss'])\n",
    "        \n",
    "        val_NMSE_hist.extend(history.history['val_NMSE'])\n",
    "        train_NMSE_hist.extend(history.history['NMSE'])\n",
    "\n",
    "        val_MSE_hist.extend(history.history['val_mse'])\n",
    "        train_MSE_hist.extend(history.history['mse'])\n",
    "        \n",
    "        if i == starting_lr_idx:\n",
    "            lr_change[i+1] += len(history.history['val_loss'])\n",
    "        else:\n",
    "            lr_change.append(lr_change[i]+len(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 554, 5) (64, 554, 5)\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.3323e-04 - mse: 2.5942e-04 - NMSE: 0.0023\n"
     ]
    }
   ],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    for layer in rnn_net.rnn_list:\n",
    "        if layer.stateful == True:\n",
    "            layer.reset_states()\n",
    "    print(testing_data_rnn_input.shape, testing_data_rnn_output.shape)\n",
    "    eval_dict = rnn_net.evaluate(\n",
    "        testing_data_rnn_input, testing_data_rnn_output,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "\n",
    "    with open(save_path+dir_sep+'losses.txt', 'w') as f:\n",
    "        f.write(str({\n",
    "            'val_loss_hist':val_loss_hist,\n",
    "            'train_loss_hist':train_loss_hist,\n",
    "            'val_MSE_hist':val_MSE_hist,\n",
    "            'train_MSE_hist':train_MSE_hist,\n",
    "            'val_NMSE_hist':val_NMSE_hist,\n",
    "            'train_NMSE_hist':train_NMSE_hist,\n",
    "            'lr_change':lr_change,\n",
    "            'test_loss':eval_dict[0],\n",
    "            'test_MSE':eval_dict[1],\n",
    "            'test_NMSE':eval_dict[2],\n",
    "        }))\n",
    "        \n",
    "    if normalize_dataset == True:\n",
    "        with open(save_path+dir_sep+'rnn_normalization.txt', 'w') as f:\n",
    "            f.write(str({\n",
    "                'normalization_arr':normalization_arr\n",
    "            }))\n",
    "\n",
    "    rnn_net.save_everything(\n",
    "        file_name=save_path+dir_sep+'final_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_kwargs = {'fontsize':15}\n",
    "ylabel_kwargs = {'fontsize':15}\n",
    "legend_kwargs = {'fontsize':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting losses\n",
    "dir_name_plot = dir_name_rnn + '/plots'\n",
    "if not os.path.isdir(dir_name_plot):\n",
    "    os.makedirs(dir_name_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": [
    "# Visualize loss history\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_loss_hist,\n",
    "    val_loss=val_loss_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "\n",
    "plt.savefig(dir_name_plot + '{ds}loss_history.pdf'.format(ds=dir_sep), dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_MSE_hist,\n",
    "    val_loss=val_MSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training MSE', 'Validation MSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='MSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/MSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_NMSE_hist,\n",
    "    val_loss=val_NMSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training NMSE', 'Validation NMSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='NMSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/NMSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": [
    "def rescale_data(data, normalization_arr):\n",
    "    '''\n",
    "    data - [num_batches x num_timesteps x num_states]\n",
    "    normalization_arr = [2 x num_states]\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(data.shape[-1]):\n",
    "        new_data[:, i] -= normalization_arr[0, i]\n",
    "        new_data[:, i] /= normalization_arr[1, i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def norm_sq_time_average(data):\n",
    "    data_norm_sq = np.zeros(shape=data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_norm_sq[:] += data[:, i]**2\n",
    "    # integrating using the trapezoidal rule\n",
    "    norm_sq_time_avg = np.sum(data_norm_sq) - 0.5*(data_norm_sq[0]+data_norm_sq[-1])\n",
    "    norm_sq_time_avg /= data_norm_sq.shape[0]\n",
    "    return norm_sq_time_avg\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = np.empty_like(data)\n",
    "    shape = new_data.shape\n",
    "    # print(shape)\n",
    "    for i in range(shape[-1]):\n",
    "        if len(shape) == 2:\n",
    "            new_data[:, i] = data[:, i]\n",
    "            new_data[:, i] *= normalization_arr[1, i]\n",
    "            new_data[:, i] += normalization_arr[0, i]\n",
    "        elif len(shape) == 3:\n",
    "            new_data[:, :, i] = data[:, :, i]\n",
    "            new_data[:, :, i] *= normalization_arr[1, i]\n",
    "            new_data[:, :, i] += normalization_arr[0, i]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_rnn/rnn_000\n",
      "num_runs : 50\n",
      "    1 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    2 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    3 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    4 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    5 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    6 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    7 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    8 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    9 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    10 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    11 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    12 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    13 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    14 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    15 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    16 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    17 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    18 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    19 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    20 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    21 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    22 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    23 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    24 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    25 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    26 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    27 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    28 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    29 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    30 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    31 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    32 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    33 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    34 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    35 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    36 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    37 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    38 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    39 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    40 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    41 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    42 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    43 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    44 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    45 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    46 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    47 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    48 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    49 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "    50 / 50 -- run_time : 0.00 s -- eta : 0h 0m 0s\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.782486817829891, median : 0.7150945718190642\n",
      "ph_min : 0.0, ph_max : 2.1452837154571927\n",
      "stddev : 0.47988372243382965, IQR : 0.47131233142620144\n",
      "1st quartile : 0.47131233142620144, 3rd quartile : 0.9426246628524029\n",
      "analysis time : 108.44430088996887 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = AR_testing_data_rnn_input.shape[0]\n",
    "\n",
    "analysis_time = time.time()\n",
    "\n",
    "AR_rnn_net = AR_RNN(\n",
    "    load_file=save_path+'/final_net_class_dict.txt',\n",
    "    T_input=T_sample_input_AR,\n",
    "    T_output=T_sample_output_AR,\n",
    "    stddev=0.0,\n",
    "    batch_size=num_runs,\n",
    "    lambda_reg=lambda_reg,\n",
    ")\n",
    "AR_rnn_net.build(input_shape=tuple(AR_testing_data_rnn_input.shape[0:2]) + tuple(testing_data_rnn_input.shape[2:]))\n",
    "AR_rnn_net.load_weights_from_file(save_path+'/final_net_gru_weights.h5')\n",
    "\n",
    "AR_AERNN_net = AR_AERNN(\n",
    "    ae_net,\n",
    "    AR_rnn_net,\n",
    "    normalization_arr,\n",
    "    normalization_constant_arr_aedata,\n",
    "    covmat_lmda=0.0,\n",
    "    time_stddev_ogdata=time_stddev_ogdata,\n",
    "    time_mean_ogdata=time_mean_ogdata,\n",
    "    loss_weights=None,\n",
    "    clipnorm=None,\n",
    "    global_clipnorm=None\n",
    ")\n",
    "\n",
    "savefig_fname = 'pre_ARtraining-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "npsavedata_fname = '/prediction_horizons-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "plot_dir = '/plots'\n",
    "\n",
    "sidx1 = dir_name_rnn[::-1].index('/')\n",
    "sidx2 = dir_name_rnn[-sidx1-2::-1].index('/')\n",
    "print(dir_name_rnn[-(sidx1+sidx2+1):])\n",
    "print('num_runs :', num_runs)\n",
    "\n",
    "prediction_horizon_arr = np.empty(shape=num_runs)\n",
    "prediction = np.array(AR_AERNN_net(AR_testing_data_rnn_input, training=False))\n",
    "prediction = invert_normalization(prediction, normalization_constant_arr_aedata)\n",
    "\n",
    "data_in_og = AR_testing_data_rnn_input\n",
    "data_out_og = AR_testing_data_rnn_output\n",
    "\n",
    "energySpectrum_dataout = 0.0\n",
    "energySpectrum_pred = 0.0\n",
    "\n",
    "avg_time = 0.\n",
    "for i in range(num_runs):\n",
    "    run_time = time.time()\n",
    "    lyap_time = lyapunov_time_arr[0]\n",
    "\n",
    "    data_out = data_out_og[i]\n",
    "    data_out = invert_normalization(data_out, normalization_constant_arr_aedata)\n",
    "\n",
    "    ### Error and prediction horizon\n",
    "    # error = np.linalg.norm(data_out[:, :] - prediction[i, :, :], axis=1)\n",
    "    error = (data_out[:, :] - prediction[i, :, :])**2\n",
    "    # error /= norm_sq_time_average(data_out)**0.5\n",
    "    error = np.mean(np.divide(error, time_stddev_ogdata**2), axis=1)**0.5\n",
    "\n",
    "    predhor_idx = np.where(error >= error_threshold)[0]\n",
    "    if predhor_idx.shape[0] == 0:\n",
    "        predhor_idx = error.shape[0]\n",
    "    else:\n",
    "        predhor_idx = predhor_idx[0]\n",
    "\n",
    "    prediction_horizon_arr[i] = predhor_idx*dt_rnn/lyap_time\n",
    "\n",
    "    run_time = time.time() - run_time\n",
    "    avg_time = (avg_time*i + run_time)/(i+1)\n",
    "    eta = avg_time * (num_runs-1 - i)\n",
    "    print('    {} / {} -- run_time : {:.2f} s -- eta : {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        i+1,\n",
    "        num_runs,\n",
    "        run_time,\n",
    "        float(eta // 3600),\n",
    "        float((eta%3600)//60),\n",
    "        float((eta%3600)%60),\n",
    "    ))\n",
    "\n",
    "median_idx = int(np.round(0.5*num_runs-1))\n",
    "quartile_1_idx = int(np.round(0.25*num_runs-1))\n",
    "quartile_3_idx = int(np.round(0.75*num_runs-1))\n",
    "\n",
    "prediction_horizon_arr.sort()\n",
    "\n",
    "median = prediction_horizon_arr[median_idx]\n",
    "quartile_1 = prediction_horizon_arr[quartile_1_idx]\n",
    "quartile_3 = prediction_horizon_arr[quartile_3_idx]\n",
    "IQR = quartile_3 - quartile_1\n",
    "\n",
    "prediction_horizon = np.mean(prediction_horizon_arr)\n",
    "stddev_ph = np.std(prediction_horizon_arr)\n",
    "\n",
    "s = 'error_threshold = {}\\n'.format(error_threshold)\n",
    "s += 'prediction_horizon : {}, median : {}\\n'.format(prediction_horizon, median)\n",
    "s += 'ph_min : {}, ph_max : {}\\n'.format(prediction_horizon_arr.min(), prediction_horizon_arr.max())\n",
    "s += 'stddev : {}, IQR : {}\\n'.format(stddev_ph, IQR)\n",
    "s += '1st quartile : {}, 3rd quartile : {}'.format(quartile_1, quartile_3)\n",
    "\n",
    "print('\\n'+s)\n",
    "\n",
    "plot_histogram_and_save(\n",
    "    prediction_horizon_arr, median,\n",
    "    save_dir=dir_name_rnn+plot_dir,\n",
    "    savefig_fname=savefig_fname,\n",
    ")\n",
    "\n",
    "np.savez(\n",
    "    dir_name_rnn+npsavedata_fname,\n",
    "    prediction_horizon_arr=prediction_horizon_arr,\n",
    "    error_threshold=error_threshold,\n",
    ")\n",
    "\n",
    "with open(dir_name_rnn+npsavedata_fname+'--statistics.txt', 'w') as fl:\n",
    "    fl.write(s)\n",
    "\n",
    "print('analysis time : {} s\\n'.format(time.time() - analysis_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'use_trainable_weights_with_reslayers' in rnn_net.__dict__.keys():\n",
    "    if use_trainable_weights_with_reslayers == True:\n",
    "        for i in range(rnn_net.num_skip_connections):\n",
    "            print('reslayer_factor_{} : {}'.format(i, rnn_net.reslayer_factor[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
