{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from keras.engine import data_adapter\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data_KS, plot_latent_states_KS , readAndReturnLossHistories, sigmoidWarmupAndDecayLRSchedule\n",
    "from tools.ae_v1 import Autoencoder\n",
    "from tools.ESN_v1_ensembleAR import ESN_ensemble as AR_RNN\n",
    "from tools.AEESN_AR_v1 import AR_AERNN_ESN as AR_AERNN\n",
    "from tools.trainAEESN_ensemble import trainAERNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 0\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print('')\n",
    "print(tf.config.list_logical_devices())\n",
    "print('')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_ESN_ensemble/ESN_ensemble_008'\n",
    "\n",
    "    # making AR-RNN save directory\n",
    "    dir_name_ARrnn = os.getcwd() + dir_sep + 'saved_AR_AEESN_rnn'\n",
    "    if not os.path.isdir(dir_name_ARrnn):\n",
    "        os.makedirs(dir_name_ARrnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'AR_ESN_ensemble_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_ARrnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_ARrnn = dir_name_ARrnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_ARrnn)\n",
    "    os.makedirs(dir_name_ARrnn+dir_sep+'plots')\n",
    "    \n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in rnn_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "        \n",
    "    \n",
    "    # training params\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "    try:\n",
    "        fRMS = tparams_dict['fRMS']\n",
    "    except:\n",
    "        fRMS = 0.0\n",
    "\n",
    "    loss_weights = 0.98\n",
    "else:\n",
    "    # AR-RNN directory\n",
    "    dir_name_ARrnn = os.getcwd()+'/saved_AR_AERNN_rnn/AR_rnn_014'\n",
    "\n",
    "    # reading AR-RNN parameters\n",
    "    with open(dir_name_ARrnn + '/AR_RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    params_AR_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dir_name_rnn = params_AR_rnn_dict['dir_name_rnn']\n",
    "    rnn_idx = dir_name_rnn[-3:]\n",
    "    dir_name_rnn = os.getcwd()+'/saved_ESN/ESN_'+rnn_idx\n",
    "\n",
    "    dt_rnn = params_AR_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_AR_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_AR_rnn_dict['T_sample_output']\n",
    "    T_offset = params_AR_rnn_dict['T_offset']\n",
    "    return_params_arr = params_AR_rnn_dict['return_params_arr']\n",
    "    params = params_AR_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_AR_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in AR_rnn_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_AR_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in AR_RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_AR_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in AR_RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        use_ae_data = params_AR_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in AR_RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "    try:\n",
    "        normalization_type = params_AR_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in AR_RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "\n",
    "    # training params\n",
    "    with open(dir_name_ARrnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "    covmat_lmda = tparams_dict['covmat_lmda']\n",
    "    try:\n",
    "        lambda_reg = tparams_dict['lambda_reg']\n",
    "    except:\n",
    "        lambda_reg = 1e-6\n",
    "    try:\n",
    "        fRMS = tparams_dict['fRMS']\n",
    "    except:\n",
    "        fRMS = 0.0\n",
    "    try:\n",
    "        loss_weights = tparams_dict['loss_weights']\n",
    "    except:\n",
    "        loss_weights = None\n",
    "    if 'freeze_layers' in tparams_dict.keys():\n",
    "        freeze_layers = tparams_dict['freeze_layers']\n",
    "    else:\n",
    "        freeze_layers = None\n",
    "    if 'clipnorm' in tparams_dict.keys():\n",
    "        clipnorm = tparams_dict['clipnorm']\n",
    "    else:\n",
    "        clipnorm = None\n",
    "    \n",
    "\n",
    "\n",
    "# reading stddev\n",
    "with open(dir_name_rnn + '/final_net/0_final_net_class_dict.txt') as f:\n",
    "    lines = f.readlines()\n",
    "finalnet_dict = eval(''.join(lines))\n",
    "stddev = finalnet_dict['stddev']\n",
    "# stddev = 0.0\n",
    "\n",
    "# reading RNN normalization constants\n",
    "normalization_arr_rnn = None\n",
    "if normalize_dataset == True:\n",
    "    with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    normarr_rnn_dict = eval(''.join(lines))\n",
    "    normalization_arr_rnn = normarr_rnn_dict['normalization_arr']\n",
    "\n",
    "if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_arr_rnn = fl['normalization_arr'][0]\n",
    "\n",
    "# reading AE directory\n",
    "with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "params_dict = eval(''.join(lines))\n",
    "\n",
    "dir_name_ae = params_dict['dir_name_ae']\n",
    "ae_idx = dir_name_ae[-3:]\n",
    "dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "try:\n",
    "    use_ae_data = params_dict['use_ae_data']\n",
    "except:\n",
    "    print(\"'use_ae_data' not present in sim_data_AE_params, set to True.\")\n",
    "    use_ae_data = True\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in ae_data, set to True.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "print('dir_name_AR_AErnn:', dir_name_ARrnn)\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state_mat = params_dict['init_state_mat']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "# numpoints_xgrid = params_dict['numpoints_xgrid']\n",
    "# length = params_dict['length']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "xgrid = length*np.linspace(0, 1, numpoints_xgrid)\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data']\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']\n",
    "\n",
    "\n",
    "test_split = 1 - train_split - val_split\n",
    "\n",
    "# setting seed for PRNGs\n",
    "np.random.seed(prng_seed)\n",
    "tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delaing with normalizing the data before feeding into autoencoder\n",
    "num_params = params_mat.shape[1]\n",
    "og_vars = all_data.shape[1]\n",
    "if alldata_withparams_flag == True:\n",
    "    og_vars -= num_params\n",
    "\n",
    "# if use_ae_data == True:\n",
    "#     if ae_data_with_params == True and alldata_withparams_flag == False:\n",
    "#         new_all_data = np.empty(shape=(all_data.shape[0], og_vars+num_params), dtype=FTYPE)\n",
    "#         new_all_data[:, 0:og_vars] = all_data[:, 0:og_vars]\n",
    "#         del(all_data)\n",
    "#         all_data = new_all_data\n",
    "#         prev_idx = 0\n",
    "#         for i in range(boundary_idx_arr.shape[0]):\n",
    "#             all_data[prev_idx:boundary_idx_arr[i], num_params:] = params_mat[i]\n",
    "#             prev_idx = boundary_idx_arr[i]\n",
    "\n",
    "#     if normalizeforae_flag == True:\n",
    "#         for i in range(all_data.shape[1]):\n",
    "#             all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "#             all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "#     if ae_data_with_params == False:\n",
    "#         all_data = all_data[:, 0:og_vars]\n",
    "# else:\n",
    "#     # using raw data, neglecting the params attached (if any)\n",
    "#     all_data = all_data[:, 0:og_vars]\n",
    "\n",
    "if use_ae_data == True and ae_data_with_params == False:\n",
    "    all_data = all_data[:, 0:og_vars]\n",
    "else:\n",
    "    all_data = all_data[:, 0:og_vars]\n",
    "    \n",
    "normalization_constant_arr_aedata = normalization_constant_arr_aedata[:, 0:all_data.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [],
   "source": [
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_data.dtype : ', all_data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "# if use_ae_data == True:\n",
    "#     load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "#     wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "# if use_ae_data == True:\n",
    "#     ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "#     ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    num_lyaptimesteps_totrain = np.array([\n",
    "        # 5,\n",
    "        10,\n",
    "        20,\n",
    "        40,\n",
    "        60,\n",
    "    ])*dt_rnn/np.mean(lyapunov_time_arr)\n",
    "    num_timesteps_warmup = 1*np.mean(lyapunov_time_arr)/dt_rnn\n",
    "    T_sample_input = num_timesteps_warmup*dt_rnn\n",
    "    T_sample_output = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_offset = T_sample_input\n",
    "    skip_intermediate = 'full sample'\n",
    "    stateful = True\n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "\n",
    "    # saving AR RNN specific data\n",
    "    AR_RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_lyaptimesteps_totrain':num_lyaptimesteps_totrain,\n",
    "        'num_timesteps_warmup':num_timesteps_warmup,\n",
    "        'dir_name_rnn':dir_name_rnn,\n",
    "        'dir_name_ae':dir_name_ae,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':AR_RNN.__module__,\n",
    "        'normalization_type':normalization_type,\n",
    "        'use_ae_data':use_ae_data,\n",
    "        'stateful':stateful,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_ARrnn+dir_sep+'AR_RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(AR_RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [\n",
    "        [1e-4, 5e-5, 1e-5],\n",
    "        [1e-5, 5e-6, 1e-6],\n",
    "        [1e-6, 5e-7, 1e-7],\n",
    "        [1e-6, 5e-7, 1e-7],\n",
    "    ]\n",
    "    epochs = [\n",
    "        [200]*3,\n",
    "        [200]*3,\n",
    "        [200]*3,\n",
    "        [200]*3,\n",
    "    ]\n",
    "    patience = [\n",
    "        [10]*3,#[25],\n",
    "        [10]*3,#[30],\n",
    "        [10]*3,#[35],\n",
    "        [10]*3,#[40],\n",
    "    ] # parameter for early stopping\n",
    "    min_delta = 1e-6  # parameter for early stopping\n",
    "    lambda_reg = 7e-9  # weight for regularizer\n",
    "    covmat_lmda = 8e-5  # weight for the covmat loss\n",
    "\n",
    "    if loss_weights is None:\n",
    "        loss_weights = 1.0\n",
    "        \n",
    "    freeze_layers = [\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    ]\n",
    "    \n",
    "    clipnorm = None #1.0\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_alpha = [False]*3\n",
    "    train_omega_in = [False]*3\n",
    "    train_rho_res = [False]*3\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'loss_weights':loss_weights,\n",
    "        'stddev':stddev,\n",
    "        'covmat_lmda':covmat_lmda,\n",
    "        'freeze_layers':freeze_layers,\n",
    "        'clipnorm':clipnorm,\n",
    "        'lambda_reg':lambda_reg,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_ARrnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_ARrnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr_rnn],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "rnn_kwargs = {}\n",
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    load_file_rnn = dir_name_rnn + '/final_net/final_net_class_dict.txt'\n",
    "    wt_file_rnn = dir_name_rnn+'/final_net/final_net_ESN_weights.hdf5'\n",
    "    \n",
    "    load_file_ae = dir_name_ae+'/final_net/final_net_class_dict.txt'\n",
    "    wt_file_ae = dir_name_ae+'/final_net/final_net_ae_weights.h5'\n",
    "    \n",
    "    rnn_kwargs = {\n",
    "        'train_alpha':train_alpha,\n",
    "        'train_omega_in':train_omega_in,\n",
    "        'train_rho_res':train_rho_res,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_return_load_wt_file_lists(\n",
    "        load_dir,\n",
    "        wt_matcher='weights.hdf5',\n",
    "        classdict_matcher='class_dict.txt',\n",
    "    ):\n",
    "    contents_load_dir = [f for f in os.listdir(load_dir) if os.path.isfile(os.path.join(load_dir, f))]\n",
    "    load_files_lst = [f for f in contents_load_dir if f.endswith(classdict_matcher)]\n",
    "    wt_files_lst = [f for f in contents_load_dir if f.endswith(wt_matcher)]\n",
    "\n",
    "    load_files_lst_startingidx = []\n",
    "    for i in range(len(load_files_lst)):\n",
    "        fn = load_files_lst[i]\n",
    "        idx = fn.find('_')\n",
    "        load_files_lst_startingidx.append(int(fn[0:idx]))\n",
    "\n",
    "    wt_files_lst_startingidx = []\n",
    "    for i in range(len(wt_files_lst)):\n",
    "        fn = wt_files_lst[i]\n",
    "        idx = fn.find('_')\n",
    "        wt_files_lst_startingidx.append(int(fn[0:idx]))\n",
    "\n",
    "    load_files_sortidx = np.argsort(load_files_lst_startingidx)\n",
    "    wt_files_sortidx = np.argsort(wt_files_lst_startingidx)\n",
    "\n",
    "    load_files_lst = np.array(load_files_lst)[load_files_sortidx]\n",
    "    wt_files_lst = np.array(wt_files_lst)[wt_files_sortidx]\n",
    "\n",
    "    load_file_rnn = [load_dir + '/' + fn for fn in load_files_lst]\n",
    "    wt_file_rnn = [load_dir + '/' +  fn for fn in wt_files_lst]\n",
    "    \n",
    "    return load_file_rnn, wt_file_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = dir_name_rnn + '/final_net'\n",
    "load_file_rnn, wt_file_rnn = find_and_return_load_wt_file_lists(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_clipnorm = None\n",
    "for kk in range(len(T_sample_output)):\n",
    "\n",
    "    num_outsteps = int((T_sample_output[kk] + 0.5*dt_rnn)//dt_rnn)\n",
    "    if type(freeze_layers) == type(None):\n",
    "        freeze_layers_thisoutstep = []\n",
    "    else:\n",
    "        if kk > len(freeze_layers) - 1:\n",
    "            freeze_layers_thisoutstep = freeze_layers[-1]\n",
    "        else:\n",
    "            freeze_layers_thisoutstep = freeze_layers[kk]\n",
    "        \n",
    "        if type(freeze_layers_thisoutstep) == type(None):\n",
    "            freeze_layers_thisoutstep = []\n",
    "\n",
    "    total_s_len = 80\n",
    "    sep_lr_s = ' num_outsteps : {} '.format(num_outsteps)\n",
    "    sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'>' + sep_lr_s\n",
    "    sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'<'\n",
    "    print('\\n\\n' + '*'*len(sep_lr_s))\n",
    "    print('' + sep_lr_s+'')\n",
    "    print('*'*len(sep_lr_s) + '\\n\\n')\n",
    "\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        if kk < len(T_sample_output) - 1:\n",
    "            temp = int((T_sample_output[kk+1] + 0.5*dt_rnn)//dt_rnn)\n",
    "        else:\n",
    "            temp = num_outsteps\n",
    "        checkfile1 = dir_name_ARrnn+'/final_net/final_net-{}_outsteps_rnn_weights.hdf5'.format(temp)\n",
    "        checkfile2 = dir_name_ARrnn+'/final_net/final_net-{}_outsteps_ae_weights.h5'.format(temp)\n",
    "        check1 = os.path.exists(checkfile1)\n",
    "        check2 = os.path.exists(checkfile2)\n",
    "        if check1 and check2:\n",
    "            # move on to checking the next time-step\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    print('clipnorm : {}, global_clipnorm : {}'.format(clipnorm, global_clipnorm))\n",
    "    \n",
    "    trainAERNN(\n",
    "        create_data_for_RNN,\n",
    "        Autoencoder,\n",
    "        AR_RNN,\n",
    "        all_data,\n",
    "        AR_AERNN,\n",
    "        dt_rnn=dt_rnn,\n",
    "        T_sample_input=T_sample_input,\n",
    "        T_sample_output=T_sample_output[kk],\n",
    "        T_offset=T_offset,\n",
    "        boundary_idx_arr=boundary_idx_arr,\n",
    "        delta_t=delta_t,\n",
    "        params=params,\n",
    "        normalize_dataset=normalize_dataset,\n",
    "        stddev_multiplier=stddev_multiplier,\n",
    "        skip_intermediate=skip_intermediate,\n",
    "        normalization_type=normalization_type,\n",
    "        normalization_constant_arr_aedata=normalization_constant_arr_aedata,\n",
    "        normalization_constant_arr_rnndata=normalization_arr_rnn,\n",
    "        learning_rate_list=learning_rate_list[kk],\n",
    "        epochs=epochs[kk],\n",
    "        patience=patience[kk],\n",
    "        loss_weights=loss_weights,\n",
    "        min_delta=min_delta,\n",
    "        lambda_reg=lambda_reg,\n",
    "        stddev_rnn=stddev,\n",
    "        stateful=False,\n",
    "        behaviour=behaviour,\n",
    "        strategy=strategy,\n",
    "        dir_name_rnn=dir_name_rnn,\n",
    "        dir_name_AR_AErnn=dir_name_ARrnn,\n",
    "        batch_size=batch_size,\n",
    "        load_file_rnn=load_file_rnn,\n",
    "        wt_file_rnn=wt_file_rnn,\n",
    "        load_file_ae=load_file_ae,\n",
    "        wt_file_ae=wt_file_ae,\n",
    "        covmat_lmda=covmat_lmda,\n",
    "        readAndReturnLossHistories=readAndReturnLossHistories,\n",
    "        mytimecallback=mytimecallback,\n",
    "        plot_losses=plot_losses,\n",
    "        SaveLosses=SaveLosses,\n",
    "        train_split=train_split,\n",
    "        test_split=test_split,\n",
    "        val_split=val_split,\n",
    "        freeze_layers=freeze_layers_thisoutstep,\n",
    "        clipnorm=clipnorm,\n",
    "        global_clipnorm=global_clipnorm,\n",
    "        ESN_flag=True,\n",
    "        rnn_kwargs=rnn_kwargs,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    load_dir = dir_name_ARrnn+'/final_net/{}_outsteps'.format(num_outsteps)\n",
    "    load_file_rnn, wt_file_rnn = find_and_return_load_wt_file_lists(\n",
    "        load_dir,\n",
    "        wt_matcher='ESN_weights.hdf5',\n",
    "        classdict_matcher='ESN_class_dict.txt'\n",
    "    )\n",
    "    \n",
    "    load_file_ae = load_dir + '/final_net-{}_outsteps_ae_class_dict.txt'.format(num_outsteps)\n",
    "    wt_file_ae = load_dir + '/final_net-{}_outsteps_ae_weights.h5'.format(num_outsteps)\n",
    "    \n",
    "    with open(load_dir+'/losses-{}_outsteps.txt'.format(num_outsteps), 'r') as fl:\n",
    "        lines = fl.readlines()\n",
    "\n",
    "    loss_dict = eval(''.join(lines))\n",
    "    train_global_gradnorm_hist = loss_dict['train_global_gradnorm_hist']\n",
    "    # lr_change = loss_dict['lr_change']\n",
    "    # trained_epochs = len(train_global_gradnorm_hist)\n",
    "    # if lr_change[-1] - lr_change[-2] == epochs[kk][-1]:\n",
    "    #     global_clipnorm = train_global_gradnorm_hist[-1]\n",
    "    # else:\n",
    "    #     global_clipnorm = train_global_gradnorm_hist[-patience[kk][-1]]\n",
    "\n",
    "    # alpha1 = 0.9\n",
    "    # alpha2 = 0.1\n",
    "    # global_clipnorm = train_global_gradnorm_hist[0]\n",
    "    # for i in range(1, len(train_global_gradnorm_hist)):\n",
    "    #     global_clipnorm = alpha1*global_clipnorm + alpha2*train_global_gradnorm_hist[i]\n",
    "\n",
    "    idxs_to_ignore = 0\n",
    "    global_clipnorm = np.max(train_global_gradnorm_hist[idxs_to_ignore:])\n",
    "    global_clipnorm = 0.1 * np.round(10*global_clipnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the combined AE-RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
