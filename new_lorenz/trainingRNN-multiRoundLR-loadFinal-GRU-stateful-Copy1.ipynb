{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "array = np.array\n",
    "float32 = np.float32\n",
    "int32 = np.int32\n",
    "float64 = np.float64\n",
    "int64 = np.int64\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/new_lorenz\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data_KS, plot_latent_states_KS , readAndReturnLossHistories, plot_histogram_and_save\n",
    "from tools.ae_v2 import Autoencoder\n",
    "from tools.GRU_SingleStep_v1 import RNN_GRU\n",
    "# from tools.LSTM_SingleStep_v2 import RNN_GRU\n",
    "# from tools.SimpleRNN_SingleStep_v2 import RNN_GRU\n",
    "from tools.GRU_AR_v1 import AR_RNN_GRU as AR_RNN\n",
    "from tools.AEGRU_AR_v1 import AR_AERNN_GRU as AR_AERNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": [
    "# setting seed for PRNGs\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    prng_seed = 42\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 06:13:35.977850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 06:13:35.978161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.019671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.019901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.020078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.020247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.021602: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-09 06:13:36.022076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.022264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.022434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.539933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.540151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.540334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 06:13:36.540478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3058 MB memory:  -> device: 0, name: Quadro K2200, pci bus id: 0000:02:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 0\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "2.8.1\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print('')\n",
    "print(tf.config.list_logical_devices())\n",
    "print('')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004\n",
      "dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_ae/ae_024\n",
      "data_dir_idx: 010\n",
      "normalize_flag_ogdata: False\n"
     ]
    }
   ],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # making RNN save directory\n",
    "    dir_name_rnn = os.getcwd() + dir_sep + 'saved_rnn'\n",
    "    if not os.path.isdir(dir_name_rnn):\n",
    "        os.makedirs(dir_name_rnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'rnn_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_rnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_rnn = dir_name_rnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_rnn)\n",
    "    os.makedirs(dir_name_rnn+dir_sep+'plots')\n",
    "\n",
    "    # whether to use AE data or just work on raw data\n",
    "    use_ae_data = True # if false, specifying ae_idx will only show which dataset to use\n",
    "\n",
    "    # autoencoder directory\n",
    "    ae_idx = '024'\n",
    "    dir_name_ae = os.getcwd()+'{ds}saved_ae{ds}ae_'.format(ds=dir_sep)+ae_idx\n",
    "else:\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_rnn/rnn_015'\n",
    "\n",
    "    # reading AE directory\n",
    "    with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_dict = eval(''.join(lines))\n",
    "\n",
    "    try:\n",
    "        use_ae_data = params_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in sim_data_AE_params, set to True.\")\n",
    "        normalize_dataset = True\n",
    "    \n",
    "    dir_name_ae = params_dict['dir_name_ae']\n",
    "    ae_idx = dir_name_ae[-3:]\n",
    "    dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "\n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_rnn_dict['T_sample_output']\n",
    "    T_offset = params_rnn_dict['T_offset']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in RNN_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "    try:\n",
    "        dense_layer_act_func = params_rnn_dict['dense_layer_act_func']\n",
    "    except:\n",
    "        print(\"'dense_layer_act_func' not present in RNN_specific_data, set to 'linear'.\")\n",
    "        dense_layer_act_func = 'linear'\n",
    "    try:\n",
    "        stateful = params_rnn_dict['stateful']\n",
    "    except:\n",
    "        print(\"'stateful' not present in RNN_specific_data, set to True.\")\n",
    "        stateful = True\n",
    "    try:\n",
    "        use_learnable_state = params_rnn_dict['use_learnable_state']\n",
    "    except:\n",
    "        print(\"'use_learnable_state' not present in RNN_specific_data, set to False.\")\n",
    "        use_learnable_state = False\n",
    "    try:\n",
    "        use_weights_post_dense = params_rnn_dict['use_weights_post_dense']\n",
    "    except:\n",
    "        print(\"'use_weights_post_dense' not present in RNN_specific_data, set to False.\")\n",
    "        use_weights_post_dense = False\n",
    "    try:\n",
    "        use_ae_data = params_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "\n",
    "    \n",
    "\n",
    "    normalization_arr = None\n",
    "    try:\n",
    "        with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        rnn_norm_arr_dict = eval(lines)\n",
    "        normalization_arr = rnn_norm_arr_dict['normalization_arr']\n",
    "    except:\n",
    "        pass\n",
    "    if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "        with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "            normalization_arr = fl['normalization_arr'][0]\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in ae_data, set to 'True'.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state = params_dict['init_state']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data'].astype(FTYPE)\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case : 1, lyapunov exponent : 0.9058021372262592, lyapunov time : 1.1039938926696777s\n"
     ]
    }
   ],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delaing with normalizing the data before feeding into autoencoder\n",
    "num_params = params_mat.shape[1]\n",
    "og_vars = all_data.shape[1]\n",
    "if alldata_withparams_flag == True:\n",
    "    og_vars -= num_params\n",
    "\n",
    "time_stddev_ogdata = np.std(all_data[:, 0:og_vars], axis=0)\n",
    "time_mean_ogdata = np.mean(all_data[:, 0:og_vars], axis=0)\n",
    "    \n",
    "if use_ae_data == True:\n",
    "    if ae_data_with_params == True and alldata_withparams_flag == False:\n",
    "        new_all_data = np.empty(shape=(all_data.shape[0], og_vars+num_params), dtype=FTYPE)\n",
    "        new_all_data[:, 0:og_vars] = all_data[:, 0:og_vars]\n",
    "        del(all_data)\n",
    "        all_data = new_all_data\n",
    "        prev_idx = 0\n",
    "        for i in range(boundary_idx_arr.shape[0]):\n",
    "            all_data[prev_idx:boundary_idx_arr[i], num_params:] = params_mat[i]\n",
    "            prev_idx = boundary_idx_arr[i]\n",
    "\n",
    "    if normalizeforae_flag == True:\n",
    "        for i in range(all_data.shape[1]):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "    if ae_data_with_params == False:\n",
    "        all_data = all_data[:, 0:og_vars]\n",
    "else:\n",
    "    # using raw data, neglecting the params attached (if any)\n",
    "    all_data = all_data[:, 0:og_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "    wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "    ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Zl6ZvgtNtA_u",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776554,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lXpoaKRIneQc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1667868777509,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Q3a8HHyvneQc",
    "outputId": "51084913-6faf-4bb5-db69-2cbea705dd28"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "if use_ae_data == True:\n",
    "    latent_states_all = ae_net.encoder_net.predict(all_data)\n",
    "    # del(all_data)\n",
    "else:\n",
    "    latent_states_all = all_data\n",
    "num_latent_states = latent_states_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1667868778304,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wjgPNitSrt5p",
    "outputId": "0c916524-33ec-47bf-a16a-51e53d2e25f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868778305,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wnLnqg0Jrt5t"
   },
   "outputs": [],
   "source": [
    "# for i in range(ae_net.layers):\n",
    "#     tf.keras.utils.plot_model(\n",
    "#         ae_net.layers[i],\n",
    "#         to_file=dir_name_ae+'/plots/netlayer_{}.png'.format(i),\n",
    "#         show_shapes=True,\n",
    "#         dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BOJE8vREtque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fwjcsAxKneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "aFd7XgwVneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    num_lyaptimesteps_totrain = 50 # int(5000/np.mean(lyapunov_time_arr))#\n",
    "    dt_rnn = 0.1\n",
    "    T_sample_input = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_sample_output = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_offset = dt_rnn\n",
    "    normalize_dataset = True # whether the data for the RNN should be normalized by the dataset's mean and std\n",
    "    normalization_arr = None\n",
    "    skip_intermediate = 'full sample'\n",
    "    noise_type = 'normal' # can be 'uniform' or 'normal'\n",
    "\n",
    "    # can be 'minmax', 'minmax2', 'stddev', or a list with\n",
    "    # sequential order of any of these; if it is 'minmax'\n",
    "    # then stddev_multiplier has no effect\n",
    "    normalization_type = 'stddev'\n",
    "    stddev_multiplier = 3\n",
    "\n",
    "    dense_layer_act_func = ['tanh']\n",
    "    use_weights_post_dense = True\n",
    "    stateful = True\n",
    "    use_learnable_state = False\n",
    "    use_trainable_weights_with_reslayers = False\n",
    "        \n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "        \n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "\n",
    "    # saving simulation data\n",
    "    sim_data = {\n",
    "        'params_mat':params_mat,\n",
    "        'init_state_mat':init_state_mat,\n",
    "        't0':t0,\n",
    "        'T':T,\n",
    "        'delta_t':delta_t,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'dir_name_ae':dir_name_ae,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'use_ae_data':use_ae_data,\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'sim_data_AE_params.txt', 'w') as f:\n",
    "        f.write(str(sim_data))\n",
    "        \n",
    "    # saving RNN specific data\n",
    "    RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_lyaptimesteps_totrain':num_lyaptimesteps_totrain,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':RNN_GRU.__module__,\n",
    "        'noise_type':noise_type,\n",
    "        'normalization_type':normalization_type,\n",
    "        'dense_layer_act_func':dense_layer_act_func,\n",
    "        'stateful':stateful,\n",
    "        'use_learnable_state':use_learnable_state,\n",
    "        'use_weights_post_dense':use_weights_post_dense,\n",
    "        'use_trainable_weights_with_reslayers':use_trainable_weights_with_reslayers,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "UGnj8uQQ83-y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "0t2_8mzI1fhX"
   },
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    latent_states_all,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=normalize_dataset,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type)\n",
    "    \n",
    "data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "num_samples = rnn_res_dict['num_samples']\n",
    "normalization_arr = rnn_res_dict['normalization_arr']\n",
    "rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "pIsWCXkbr7ws"
   },
   "outputs": [],
   "source": [
    "temp = np.divide(latent_states_all-normalization_arr[0], normalization_arr[1])\n",
    "time_stddev = np.std(temp, axis=0)\n",
    "timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "del(org_data_idx_arr_input)\n",
    "del(org_data_idx_arr_output)\n",
    "del(latent_states_all)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    all_data,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=False,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type,\n",
    "    FTYPE=FTYPE,\n",
    "    ITYPE=ITYPE)\n",
    "    \n",
    "AR_data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "AR_data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "AR_org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "AR_org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "AR_num_samples = rnn_res_dict['num_samples']\n",
    "AR_normalization_arr = rnn_res_dict['normalization_arr']\n",
    "AR_rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']\n",
    "\n",
    "del(all_data)\n",
    "del(AR_org_data_idx_arr_input)\n",
    "del(AR_org_data_idx_arr_output)\n",
    "del(AR_rnn_data_boundary_idx_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Hem_9PUqneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778791,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uskBAAXpneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "\n",
    "# ph computation parameters\n",
    "num_runs = 100\n",
    "T_sample_input_AR_ratio = 1\n",
    "T_sample_output_AR_ratio = 3\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    epochs = 200\n",
    "    patience = 10 # parameter for early stopping\n",
    "    min_delta = 1e-6  # parameter for early stopping\n",
    "    lambda_reg = 3.72759372e-07  # weight for regularizer\n",
    "    train_split = 0.8\n",
    "    val_split = 0.1\n",
    "    test_split = 1 - train_split - val_split\n",
    "    batch_size = 32\n",
    "    fRMS = 5.17947468e-03\n",
    "    zoneout_rate = 0.0\n",
    "    rnncell_dropout_rate = 0.0\n",
    "    denselayer_dropout_rate = 0.0\n",
    "    \n",
    "\n",
    "    stddev = fRMS*np.mean(time_stddev[0:og_vars])\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'timeMeanofSpaceRMS':timeMeanofSpaceRMS,\n",
    "        'stddev':stddev,\n",
    "        'zoneout_rate':zoneout_rate,\n",
    "        'rnncell_dropout_rate':rnncell_dropout_rate,\n",
    "        'denselayer_dropout_rate':denselayer_dropout_rate,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_rnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr],\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    # dir_name_rnn_og = dir_name_rnn\n",
    "    # dir_name_rnn_temp = '/home/rkaushik/Documents/Thesis/MLROM/CDV/saved_rnn/rnn_'+dir_name_rnn_og[-3:]\n",
    "    # dir_name_rnn = dir_name_rnn_temp\n",
    "\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # setting seed for PRNGs\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "# idx = np.arange(data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# boundary = int(np.round(train_split*data_rnn_input.shape[0]))\n",
    "\n",
    "# training_data_rnn_input = data_rnn_input[idx[0:boundary]]\n",
    "# training_data_rnn_output = data_rnn_output[idx[0:boundary]]\n",
    "\n",
    "# testing_data_rnn_input = data_rnn_input[idx[boundary:]]\n",
    "# testing_data_rnn_output = data_rnn_output[idx[boundary:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1667868779601,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "EENXaWqcKW7j"
   },
   "outputs": [],
   "source": [
    "cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "# idx = np.arange(cum_samples)\n",
    "# np.random.shuffle(idx)\n",
    "num_train_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_val_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_test_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_samples_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "begin_idx = 0\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    num_samples = batch_size * int((rnn_data_boundary_idx_arr[i] - begin_idx) // batch_size)\n",
    "    num_train_arr[i] = batch_size * int( np.round(train_split*num_samples/batch_size) )\n",
    "    num_val_arr[i] = batch_size * int( np.round(val_split*num_samples/batch_size) )\n",
    "    num_test_arr[i] = batch_size * int( np.round((num_samples - num_train_arr[i] - num_val_arr[i])/batch_size) )\n",
    "    num_samples_arr[i] = num_train_arr[i] + num_val_arr[i] + num_test_arr[i]\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# defining shapes\n",
    "training_input_shape = [np.sum(num_train_arr)]\n",
    "training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "training_output_shape = [np.sum(num_train_arr)]\n",
    "training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "val_input_shape = [np.sum(num_val_arr)]\n",
    "val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "val_output_shape = [np.sum(num_val_arr)]\n",
    "val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "testing_input_shape = [np.sum(num_test_arr)]\n",
    "testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "testing_output_shape = [np.sum(num_test_arr)]\n",
    "testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "# defining required arrays\n",
    "training_data_rnn_input = np.empty(shape=training_input_shape, dtype=FTYPE)\n",
    "training_data_rnn_output = np.empty(shape=training_output_shape, dtype=FTYPE)\n",
    "\n",
    "val_data_rnn_input = np.empty(shape=val_input_shape, dtype=FTYPE)\n",
    "val_data_rnn_output = np.empty(shape=val_output_shape, dtype=FTYPE)\n",
    "\n",
    "testing_data_rnn_input = np.empty(shape=testing_input_shape, dtype=FTYPE)\n",
    "testing_data_rnn_output = np.empty(shape=testing_output_shape, dtype=FTYPE)\n",
    "\n",
    "AR_testing_data_rnn_input = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "AR_testing_data_rnn_output = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "\n",
    "begin_idx = 0\n",
    "training_data_rolling_count = 0\n",
    "val_data_rolling_count = 0\n",
    "testing_data_rolling_count = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    idx = np.arange(begin_idx, rnn_data_boundary_idx_arr[i])\n",
    "    # np.random.shuffle(idx)\n",
    "    # num_samples = idx.shape[0]\n",
    "    # num_train = int( np.round(train_split*num_samples/batch_size) )*batch_size\n",
    "    # num_val = int( np.round(val_split*num_samples/batch_size) )*batch_size\n",
    "    \n",
    "    num_samples = num_samples_arr[i]\n",
    "    num_train = num_train_arr[i]\n",
    "    num_val = num_val_arr[i]\n",
    "    num_test = num_test_arr[i]\n",
    "    \n",
    "    nbatches_train = num_train // batch_size\n",
    "    nbatches_val = num_val // batch_size\n",
    "    nbatches_test = num_test // batch_size\n",
    "\n",
    "    for j in range(batch_size):\n",
    "        training_data_rnn_input[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_input[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        training_data_rnn_output[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_output[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        \n",
    "        val_data_rnn_input[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_input[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "        val_data_rnn_output[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_output[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "\n",
    "        testing_data_rnn_input[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_input[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        testing_data_rnn_output[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_output[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "\n",
    "    AR_testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = AR_data_rnn_input[idx[num_train+num_val:num_samples]]\n",
    "    AR_testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = AR_data_rnn_output[idx[num_train+num_val:num_samples]]\n",
    "\n",
    "    # training_data_rnn_input[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_input[idx[0:num_train]]\n",
    "    # training_data_rnn_output[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_output[idx[0:num_train]]\n",
    "    training_data_rolling_count += num_train\n",
    "\n",
    "    # val_data_rnn_input[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_input[idx[num_train:num_train+num_val]]\n",
    "    # val_data_rnn_output[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_output[idx[num_train:num_train+num_val]]\n",
    "    val_data_rolling_count += num_val\n",
    "\n",
    "    # num_test = num_samples-num_train-num_val+1\n",
    "    # testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_input[idx[num_train+num_val:]]\n",
    "    # testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_output[idx[num_train+num_val:]]\n",
    "    testing_data_rolling_count += num_test\n",
    "\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# cleaning up\n",
    "del(data_rnn_input)\n",
    "del(data_rnn_output)\n",
    "del(AR_data_rnn_input)\n",
    "del(AR_data_rnn_output)\n",
    "\n",
    "# further shuffling\n",
    "if stateful == False:\n",
    "    idx = np.arange(0, training_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    training_data_rnn_input = training_data_rnn_input[idx]\n",
    "    training_data_rnn_output = training_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, val_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    val_data_rnn_input = val_data_rnn_input[idx]\n",
    "    val_data_rnn_output = val_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, testing_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    testing_data_rnn_input = testing_data_rnn_input[idx]\n",
    "    testing_data_rnn_output = testing_data_rnn_output[idx]\n",
    "\n",
    "    del(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868779603,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8isZN1tYBifp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_runs :  100\n"
     ]
    }
   ],
   "source": [
    "s_in = AR_testing_data_rnn_input.shape\n",
    "AR_testing_data_rnn_input = AR_testing_data_rnn_input.reshape((1, s_in[0]*s_in[1]) + s_in[2:])\n",
    "\n",
    "s_out = AR_testing_data_rnn_output.shape\n",
    "AR_testing_data_rnn_output = AR_testing_data_rnn_output.reshape((1, s_out[0]*s_out[1]) + s_out[2:])\n",
    "\n",
    "T_sample_input_AR = T_sample_input_AR_ratio*np.mean(lyapunov_time_arr)#50.1*dt_rnn\n",
    "num_sample_input_AR = int((T_sample_input_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "T_sample_output_AR = T_sample_output_AR_ratio*np.mean(lyapunov_time_arr)\n",
    "num_sample_output_AR = int((T_sample_output_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "num_offset_AR = num_sample_input_AR\n",
    "T_offset_AR = num_offset_AR*dt_rnn\n",
    "\n",
    "batch_idx = np.random.randint(low=0, high=AR_testing_data_rnn_input.shape[0])\n",
    "maxpossible_num_runs = AR_testing_data_rnn_input.shape[1]-(num_sample_input_AR+num_sample_output_AR)\n",
    "\n",
    "num_runs = np.min([num_runs, maxpossible_num_runs])\n",
    "\n",
    "print('num_runs : ', num_runs)\n",
    "\n",
    "data_idx_arr = np.linspace(0, maxpossible_num_runs-1, num_runs, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779605,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "x3KglJsgneQj"
   },
   "outputs": [],
   "source": [
    "AR_data_in = np.empty(shape=(num_runs, num_sample_input_AR)+tuple(s_in[2:]))\n",
    "AR_data_out = np.empty(shape=(num_runs, num_sample_output_AR)+tuple(s_out[2:]))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    d_idx = data_idx_arr[i]\n",
    "    AR_data_in[i] = AR_testing_data_rnn_input[0, d_idx:d_idx+num_sample_input_AR]\n",
    "    AR_data_out[i] = AR_testing_data_rnn_input[0, d_idx+num_sample_input_AR:d_idx+num_sample_input_AR+num_sample_output_AR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ixetsZHjCMKO"
   },
   "outputs": [],
   "source": [
    "del(AR_testing_data_rnn_input)\n",
    "del(AR_testing_data_rnn_output)\n",
    "AR_testing_data_rnn_input = AR_data_in\n",
    "AR_testing_data_rnn_output = AR_data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   training_data_rnn_input.shape :  (576, 552, 2)\n",
      "  training_data_rnn_output.shape :  (576, 552, 2)\n",
      "    testing_data_rnn_input.shape :  (96, 552, 2)\n",
      "   testing_data_rnn_output.shape :  (96, 552, 2)\n",
      "        val_data_rnn_input.shape :  (64, 552, 2)\n",
      "       val_data_rnn_output.shape :  (64, 552, 2)\n",
      "\n",
      " AR_testing_data_rnn_input.shape :  (100, 11, 3)\n",
      "AR_testing_data_rnn_output.shape :  (100, 33, 3)\n"
     ]
    }
   ],
   "source": [
    "print('   training_data_rnn_input.shape : ', training_data_rnn_input.shape)\n",
    "print('  training_data_rnn_output.shape : ', training_data_rnn_output.shape)\n",
    "print('    testing_data_rnn_input.shape : ', testing_data_rnn_input.shape)\n",
    "print('   testing_data_rnn_output.shape : ', testing_data_rnn_output.shape)\n",
    "print('        val_data_rnn_input.shape : ', val_data_rnn_input.shape)\n",
    "print('       val_data_rnn_output.shape : ', val_data_rnn_output.shape)\n",
    "print('')\n",
    "print(' AR_testing_data_rnn_input.shape : ', AR_testing_data_rnn_input.shape)\n",
    "print('AR_testing_data_rnn_output.shape : ', AR_testing_data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_NSTtZuyneQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeMeanofSpaceRMS : 0.2933353\n",
      "stddev : 0.0017228189150698256\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "#     rnn_layers_units = [500]*3\n",
    "#     scalar_weights = None\n",
    "#     scalar_weights = [\n",
    "#         1.0,\n",
    "#     ] # Euler\n",
    "#     scalar_weights = [\n",
    "#         0.5, \n",
    "#         0.0, 0.5,\n",
    "#         0.0, 0.0, 1.0,\n",
    "#         1/6, 1/3, 1/3, 1/6\n",
    "#     ] # RK4\n",
    "    # scalar_weights = [\n",
    "    #     1.0,\n",
    "    #     0.25, 0.25,\n",
    "    #     1/6, 1/6, 2/3\n",
    "    # ] # TVD RK3\n",
    "    scalar_weights = [\n",
    "        1.0,\n",
    "        0.5, 0.5\n",
    "    ] # TVD RK2\n",
    "    num_rnn_layers = 1\n",
    "    if not isinstance(scalar_weights, type(None)):\n",
    "        num_rnn_layers += int( ((8*len(scalar_weights)+1)**0.5 - 1)/2 )\n",
    "    rnn_layers_units = [80*num_latent_states]*num_rnn_layers\n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "    print('timeMeanofSpaceRMS :', timeMeanofSpaceRMS)\n",
    "    print('stddev :', stddev)\n",
    "    if return_params_arr != False:\n",
    "        data_dim = num_latent_states + 3\n",
    "    else:\n",
    "        data_dim = num_latent_states\n",
    "\n",
    "    dense_dim = [rnn_layers_units[-1]]*(len(dense_layer_act_func)-1)\n",
    "    dense_dim.append(data_dim)\n",
    "        \n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_GRU(\n",
    "                data_dim=data_dim,\n",
    "            #     in_steps=int(T_sample_input // dt_rnn),\n",
    "            #     out_steps=int(T_sample_output // dt_rnn),\n",
    "                dt_rnn=dt_rnn,\n",
    "                lambda_reg=lambda_reg,\n",
    "                reg_name='L2',\n",
    "                rnn_layers_units=rnn_layers_units,\n",
    "                dense_layer_act_func=dense_layer_act_func,\n",
    "                load_file=None,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                stddev=stddev,\n",
    "                noise_type=noise_type,\n",
    "                dense_dim=dense_dim,\n",
    "                use_learnable_state=use_learnable_state,\n",
    "                stateful=stateful,\n",
    "                zoneout_rate=zoneout_rate,\n",
    "                batch_size=batch_size,\n",
    "                use_weights_post_dense=use_weights_post_dense,\n",
    "                rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "                denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "                scalar_weights=scalar_weights, # corresponding to RK4\n",
    "                use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_GRU(\n",
    "            data_dim=data_dim,\n",
    "        #     in_steps=int(T_sample_input // dt_rnn),\n",
    "        #     out_steps=int(T_sample_output // dt_rnn),\n",
    "            dt_rnn=dt_rnn,\n",
    "            lambda_reg=lambda_reg,\n",
    "            reg_name='L2',\n",
    "            rnn_layers_units=rnn_layers_units,\n",
    "            dense_layer_act_func=dense_layer_act_func,\n",
    "            load_file=None,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            stddev=stddev,\n",
    "            noise_type=noise_type,\n",
    "            dense_dim=dense_dim,\n",
    "            use_learnable_state=use_learnable_state,\n",
    "            stateful=stateful,\n",
    "            zoneout_rate=zoneout_rate,\n",
    "            batch_size=batch_size,\n",
    "            use_weights_post_dense=use_weights_post_dense,\n",
    "            rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "            denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "            scalar_weights=scalar_weights, # corresponding to RK4\n",
    "            use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "        )\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    rnn_net.save_class_dict(save_path+dir_sep+'final_net_class_dict.txt')\n",
    "else:\n",
    "    load_file = dir_name_rnn + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_GRU(\n",
    "                load_file=load_file,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                batch_size=batch_size,\n",
    "                \n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_GRU(\n",
    "            load_file=load_file,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    rnn_net.build(input_shape=(batch_size, None, num_latent_states))\n",
    "    \n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        wt_file = tf.train.latest_checkpoint(dir_name_rnn+dir_sep+'checkpoints')\n",
    "    elif behaviour == 'loadFinalNetAndPlot':\n",
    "        wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'final_net_gru_weights.h5'\n",
    "        # wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'f2'#+dir_sep+'saved_model.pb'\n",
    "        rnn_net.load_weights_from_file(wt_file)\n",
    "    \n",
    "    # this forces the model to initialize its kernel weights/biases\n",
    "    # temp = rnn_net.predict(tf.ones(shape=[batch_size, int(T_sample_input//dt_rnn), rnn_net.data_dim]))\n",
    "    # this loads just the kernel wieghts and biases of the model\n",
    "#     rnn_net.load_weights_from_file(wt_file)\n",
    "\n",
    "    # rnn_net = tf.keras.models.load_model(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    val_loss_hist = []\n",
    "    train_loss_hist = []\n",
    "    lr_change=[0, 0]\n",
    "    savelosses_cb_vallossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    savelosses_cb_trainlossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    starting_lr_idx = 0\n",
    "    num_epochs_left = epochs\n",
    "    earlystopping_wait = 0\n",
    "elif behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt, earlystopping_wait = readAndReturnLossHistories(\n",
    "        dir_name_ae=dir_name_rnn,\n",
    "        dir_sep=dir_sep,\n",
    "        epochs=epochs,\n",
    "        learning_rate_list=learning_rate_list,\n",
    "        return_earlystopping_wait=True)\n",
    "    savelosses_cb_vallossarr = val_loss_arr_fromckpt\n",
    "    savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n",
    "elif behaviour == 'loadFinalNetAndPlot':\n",
    "    with open(dir_name_rnn+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    losses_dict = eval(''.join(lines))\n",
    "\n",
    "    val_loss_hist = losses_dict['val_loss_hist']\n",
    "    train_loss_hist = losses_dict['train_loss_hist']\n",
    "    lr_change = losses_dict['lr_change']\n",
    "    test_loss = losses_dict['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NMSE_hist = []\n",
    "val_NMSE_hist = []\n",
    "\n",
    "train_MSE_hist = []\n",
    "val_MSE_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMSE(tf.keras.metrics.MeanSquaredError):\n",
    "    def __init__(self, divisor_arr, name='NMSE', **kwargs):\n",
    "        super(NMSE, self).__init__(name, **kwargs)\n",
    "        self.divisor_arr = divisor_arr\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = y_true / self.divisor_arr\n",
    "        y_pred = y_pred / self.divisor_arr\n",
    "        return super(NMSE, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------- LEARNING RATE : 0.01 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0685 - mse: 0.0682 - NMSE: 0.6163 - tot_time: 0h 0m 22.8s\n",
      "\n",
      "Epoch 1: val_NMSE improved from inf to 0.16548, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 23s 1s/step - loss: 0.0685 - mse: 0.0682 - NMSE: 0.6163 - val_loss: 0.0187 - val_mse: 0.0183 - val_NMSE: 0.1655\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0112 - mse: 0.0107 - NMSE: 0.0968 - tot_time: 0h 0m 42.7s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.16548 to 0.04546, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 0.0112 - mse: 0.0107 - NMSE: 0.0968 - val_loss: 0.0056 - val_mse: 0.0050 - val_NMSE: 0.0455\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0035 - mse: 0.0028 - NMSE: 0.0257 - tot_time: 0h 1m 2.4s\n",
      "\n",
      "Epoch 3: val_NMSE improved from 0.04546 to 0.01400, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 0.0035 - mse: 0.0028 - NMSE: 0.0257 - val_loss: 0.0023 - val_mse: 0.0015 - val_NMSE: 0.0140\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0016 - mse: 9.1549e-04 - NMSE: 0.0083 - tot_time: 0h 1m 22.1s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.01400 to 0.00681, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 0.0016 - mse: 9.1549e-04 - NMSE: 0.0083 - val_loss: 0.0015 - val_mse: 7.5309e-04 - val_NMSE: 0.0068\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0012 - mse: 5.1725e-04 - NMSE: 0.0047 - tot_time: 0h 1m 41.8s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.00681 to 0.00478, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 0.0012 - mse: 5.1725e-04 - NMSE: 0.0047 - val_loss: 0.0012 - val_mse: 5.2845e-04 - val_NMSE: 0.0048\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0011 - mse: 3.7316e-04 - NMSE: 0.0034 - tot_time: 0h 2m 1.2s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.00478 to 0.00388, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.0011 - mse: 3.7316e-04 - NMSE: 0.0034 - val_loss: 0.0011 - val_mse: 4.2901e-04 - val_NMSE: 0.0039\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 9.5471e-04 - mse: 3.0853e-04 - NMSE: 0.0028 - tot_time: 0h 2m 20.4s\n",
      "\n",
      "Epoch 7: val_NMSE improved from 0.00388 to 0.00342, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 9.5471e-04 - mse: 3.0853e-04 - NMSE: 0.0028 - val_loss: 0.0010 - val_mse: 3.7826e-04 - val_NMSE: 0.0034\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 8.7618e-04 - mse: 2.7201e-04 - NMSE: 0.0025 - tot_time: 0h 2m 40.2s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.00342 to 0.00315, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 8.7618e-04 - mse: 2.7201e-04 - NMSE: 0.0025 - val_loss: 9.3026e-04 - val_mse: 3.4800e-04 - val_NMSE: 0.0031\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 9.5446e-04 - mse: 3.9098e-04 - NMSE: 0.0035 - tot_time: 0h 2m 59.5s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00315\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 9.5446e-04 - mse: 3.9098e-04 - NMSE: 0.0035 - val_loss: 9.0362e-04 - val_mse: 3.6018e-04 - val_NMSE: 0.0033\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 8.2704e-04 - mse: 2.9781e-04 - NMSE: 0.0027 - tot_time: 0h 3m 19.1s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.00315 to 0.00309, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 8.2704e-04 - mse: 2.9781e-04 - NMSE: 0.0027 - val_loss: 8.5568e-04 - val_mse: 3.4145e-04 - val_NMSE: 0.0031\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.2106e-04 - mse: 2.2073e-04 - NMSE: 0.0020 - tot_time: 0h 3m 38.5s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00309 to 0.00263, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 7.2106e-04 - mse: 2.2073e-04 - NMSE: 0.0020 - val_loss: 7.7502e-04 - val_mse: 2.9044e-04 - val_NMSE: 0.0026\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.6935e-04 - mse: 1.9889e-04 - NMSE: 0.0018 - tot_time: 0h 3m 58.1s\n",
      "\n",
      "Epoch 12: val_NMSE improved from 0.00263 to 0.00261, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 6.6935e-04 - mse: 1.9889e-04 - NMSE: 0.0018 - val_loss: 7.4393e-04 - val_mse: 2.8886e-04 - val_NMSE: 0.0026\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 9.4054e-04 - mse: 4.9752e-04 - NMSE: 0.0045 - tot_time: 0h 4m 18.2s\n",
      "\n",
      "Epoch 13: val_NMSE did not improve from 0.00261\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 9.4054e-04 - mse: 4.9752e-04 - NMSE: 0.0045 - val_loss: 9.0280e-04 - val_mse: 4.6921e-04 - val_NMSE: 0.0042\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.8034e-04 - mse: 2.5266e-04 - NMSE: 0.0023 - tot_time: 0h 4m 37.7s\n",
      "\n",
      "Epoch 14: val_NMSE did not improve from 0.00261\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 6.8034e-04 - mse: 2.5266e-04 - NMSE: 0.0023 - val_loss: 7.1354e-04 - val_mse: 2.9415e-04 - val_NMSE: 0.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.9367e-04 - mse: 1.8381e-04 - NMSE: 0.0017 - tot_time: 0h 4m 57.3s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00261 to 0.00227, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 5.9367e-04 - mse: 1.8381e-04 - NMSE: 0.0017 - val_loss: 6.5045e-04 - val_mse: 2.5113e-04 - val_NMSE: 0.0023\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.5560e-04 - mse: 1.6579e-04 - NMSE: 0.0015 - tot_time: 0h 5m 17.5s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00227 to 0.00215, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 5.5560e-04 - mse: 1.6579e-04 - NMSE: 0.0015 - val_loss: 6.1677e-04 - val_mse: 2.3746e-04 - val_NMSE: 0.0021\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.2505e-04 - mse: 3.5455e-04 - NMSE: 0.0032 - tot_time: 0h 5m 37.7s\n",
      "\n",
      "Epoch 17: val_NMSE did not improve from 0.00215\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 7.2505e-04 - mse: 3.5455e-04 - NMSE: 0.0032 - val_loss: 7.1079e-04 - val_mse: 3.4931e-04 - val_NMSE: 0.0032\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.3350e-04 - mse: 2.7403e-04 - NMSE: 0.0025 - tot_time: 0h 5m 58.1s\n",
      "\n",
      "Epoch 18: val_NMSE did not improve from 0.00215\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 6.3350e-04 - mse: 2.7403e-04 - NMSE: 0.0025 - val_loss: 6.1290e-04 - val_mse: 2.5605e-04 - val_NMSE: 0.0023\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.2188e-04 - mse: 1.7032e-04 - NMSE: 0.0015 - tot_time: 0h 6m 17.2s\n",
      "\n",
      "Epoch 19: val_NMSE improved from 0.00215 to 0.00206, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 5.2188e-04 - mse: 1.7032e-04 - NMSE: 0.0015 - val_loss: 5.7327e-04 - val_mse: 2.2820e-04 - val_NMSE: 0.0021\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.8756e-04 - mse: 1.4887e-04 - NMSE: 0.0013 - tot_time: 0h 6m 36.5s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00206 to 0.00196, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.8756e-04 - mse: 1.4887e-04 - NMSE: 0.0013 - val_loss: 5.4838e-04 - val_mse: 2.1701e-04 - val_NMSE: 0.0020\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.6502e-04 - mse: 1.3981e-04 - NMSE: 0.0013 - tot_time: 0h 6m 55.8s\n",
      "\n",
      "Epoch 21: val_NMSE improved from 0.00196 to 0.00188, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.6502e-04 - mse: 1.3981e-04 - NMSE: 0.0013 - val_loss: 5.2602e-04 - val_mse: 2.0760e-04 - val_NMSE: 0.0019\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.7996e-04 - mse: 3.6715e-04 - NMSE: 0.0033 - tot_time: 0h 7m 15.4s\n",
      "\n",
      "Epoch 22: val_NMSE did not improve from 0.00188\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 6.7996e-04 - mse: 3.6715e-04 - NMSE: 0.0033 - val_loss: 6.0009e-04 - val_mse: 2.9305e-04 - val_NMSE: 0.0026\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.7188e-04 - mse: 2.6325e-04 - NMSE: 0.0024 - tot_time: 0h 7m 34.8s\n",
      "\n",
      "Epoch 23: val_NMSE did not improve from 0.00188\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 5.7188e-04 - mse: 2.6325e-04 - NMSE: 0.0024 - val_loss: 5.5573e-04 - val_mse: 2.4637e-04 - val_NMSE: 0.0022\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.5843e-04 - mse: 1.5166e-04 - NMSE: 0.0014 - tot_time: 0h 7m 54.0s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00188 to 0.00185, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.5843e-04 - mse: 1.5166e-04 - NMSE: 0.0014 - val_loss: 5.0803e-04 - val_mse: 2.0501e-04 - val_NMSE: 0.0019\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.2967e-04 - mse: 1.3107e-04 - NMSE: 0.0012 - tot_time: 0h 8m 13.5s\n",
      "\n",
      "Epoch 25: val_NMSE improved from 0.00185 to 0.00172, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.2967e-04 - mse: 1.3107e-04 - NMSE: 0.0012 - val_loss: 4.8364e-04 - val_mse: 1.9008e-04 - val_NMSE: 0.0017\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.1484e-04 - mse: 1.2560e-04 - NMSE: 0.0011 - tot_time: 0h 8m 32.9s\n",
      "\n",
      "Epoch 26: val_NMSE improved from 0.00172 to 0.00169, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.1484e-04 - mse: 1.2560e-04 - NMSE: 0.0011 - val_loss: 4.7177e-04 - val_mse: 1.8727e-04 - val_NMSE: 0.0017\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.8204e-04 - mse: 3.0076e-04 - NMSE: 0.0027 - tot_time: 0h 8m 52.6s\n",
      "\n",
      "Epoch 27: val_NMSE did not improve from 0.00169\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 5.8204e-04 - mse: 3.0076e-04 - NMSE: 0.0027 - val_loss: 5.2843e-04 - val_mse: 2.4864e-04 - val_NMSE: 0.0022\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.3312e-04 - mse: 1.5355e-04 - NMSE: 0.0014 - tot_time: 0h 9m 12.0s\n",
      "\n",
      "Epoch 28: val_NMSE did not improve from 0.00169\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.3312e-04 - mse: 1.5355e-04 - NMSE: 0.0014 - val_loss: 4.6982e-04 - val_mse: 1.9226e-04 - val_NMSE: 0.0017\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.9642e-04 - mse: 1.2151e-04 - NMSE: 0.0011 - tot_time: 0h 9m 31.6s\n",
      "\n",
      "Epoch 29: val_NMSE improved from 0.00169 to 0.00154, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.9642e-04 - mse: 1.2151e-04 - NMSE: 0.0011 - val_loss: 4.4149e-04 - val_mse: 1.7014e-04 - val_NMSE: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.6360e-04 - mse: 3.9548e-04 - NMSE: 0.0036 - tot_time: 0h 9m 50.7s\n",
      "\n",
      "Epoch 30: val_NMSE did not improve from 0.00154\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 6.6360e-04 - mse: 3.9548e-04 - NMSE: 0.0036 - val_loss: 5.7549e-04 - val_mse: 3.1080e-04 - val_NMSE: 0.0028\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.9619e-04 - mse: 3.2884e-04 - NMSE: 0.0030 - tot_time: 0h 10m 10.1s\n",
      "\n",
      "Epoch 31: val_NMSE did not improve from 0.00154\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 5.9619e-04 - mse: 3.2884e-04 - NMSE: 0.0030 - val_loss: 4.8565e-04 - val_mse: 2.1510e-04 - val_NMSE: 0.0019\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.1911e-04 - mse: 1.4932e-04 - NMSE: 0.0013 - tot_time: 0h 10m 29.3s\n",
      "\n",
      "Epoch 32: val_NMSE did not improve from 0.00154\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.1911e-04 - mse: 1.4932e-04 - NMSE: 0.0013 - val_loss: 4.5539e-04 - val_mse: 1.8798e-04 - val_NMSE: 0.0017\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.8391e-04 - mse: 1.1907e-04 - NMSE: 0.0011 - tot_time: 0h 10m 48.9s\n",
      "\n",
      "Epoch 33: val_NMSE improved from 0.00154 to 0.00151, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.8391e-04 - mse: 1.1907e-04 - NMSE: 0.0011 - val_loss: 4.2898e-04 - val_mse: 1.6732e-04 - val_NMSE: 0.0015\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6820e-04 - mse: 1.0948e-04 - NMSE: 9.8968e-04 - tot_time: 0h 11m 8.2s\n",
      "\n",
      "Epoch 34: val_NMSE improved from 0.00151 to 0.00143, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.6820e-04 - mse: 1.0948e-04 - NMSE: 9.8968e-04 - val_loss: 4.1413e-04 - val_mse: 1.5866e-04 - val_NMSE: 0.0014\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6077e-04 - mse: 1.0798e-04 - NMSE: 9.7615e-04 - tot_time: 0h 11m 27.7s\n",
      "\n",
      "Epoch 35: val_NMSE did not improve from 0.00143\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.6077e-04 - mse: 1.0798e-04 - NMSE: 9.7615e-04 - val_loss: 4.5075e-04 - val_mse: 2.0098e-04 - val_NMSE: 0.0018\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.9408e-04 - mse: 3.4536e-04 - NMSE: 0.0031 - tot_time: 0h 11m 47.0s\n",
      "\n",
      "Epoch 36: val_NMSE did not improve from 0.00143\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 5.9408e-04 - mse: 3.4536e-04 - NMSE: 0.0031 - val_loss: 5.0338e-04 - val_mse: 2.5403e-04 - val_NMSE: 0.0023\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.9175e-04 - mse: 1.4243e-04 - NMSE: 0.0013 - tot_time: 0h 12m 6.3s\n",
      "\n",
      "Epoch 37: val_NMSE did not improve from 0.00143\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.9175e-04 - mse: 1.4243e-04 - NMSE: 0.0013 - val_loss: 4.0771e-04 - val_mse: 1.5916e-04 - val_NMSE: 0.0014\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5559e-04 - mse: 1.0907e-04 - NMSE: 9.8602e-04 - tot_time: 0h 12m 25.7s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00143 to 0.00139, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.5559e-04 - mse: 1.0907e-04 - NMSE: 9.8602e-04 - val_loss: 3.9788e-04 - val_mse: 1.5382e-04 - val_NMSE: 0.0014\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 8.6873e-04 - mse: 6.2514e-04 - NMSE: 0.0057 - tot_time: 0h 12m 45.1s\n",
      "\n",
      "Epoch 39: val_NMSE did not improve from 0.00139\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 8.6873e-04 - mse: 6.2514e-04 - NMSE: 0.0057 - val_loss: 6.5359e-04 - val_mse: 4.0550e-04 - val_NMSE: 0.0037\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.6844e-04 - mse: 2.1583e-04 - NMSE: 0.0020 - tot_time: 0h 13m 4.5s\n",
      "\n",
      "Epoch 40: val_NMSE did not improve from 0.00139\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.6844e-04 - mse: 2.1583e-04 - NMSE: 0.0020 - val_loss: 4.6175e-04 - val_mse: 2.0727e-04 - val_NMSE: 0.0019\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.8041e-04 - mse: 1.2740e-04 - NMSE: 0.0012 - tot_time: 0h 13m 24.0s\n",
      "\n",
      "Epoch 41: val_NMSE did not improve from 0.00139\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.8041e-04 - mse: 1.2740e-04 - NMSE: 0.0012 - val_loss: 4.1243e-04 - val_mse: 1.6208e-04 - val_NMSE: 0.0015\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5493e-04 - mse: 1.0723e-04 - NMSE: 9.6936e-04 - tot_time: 0h 13m 43.1s\n",
      "\n",
      "Epoch 42: val_NMSE improved from 0.00139 to 0.00136, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.5493e-04 - mse: 1.0723e-04 - NMSE: 9.6936e-04 - val_loss: 3.9499e-04 - val_mse: 1.5035e-04 - val_NMSE: 0.0014\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4210e-04 - mse: 1.0011e-04 - NMSE: 9.0504e-04 - tot_time: 0h 14m 2.7s\n",
      "\n",
      "Epoch 43: val_NMSE improved from 0.00136 to 0.00129, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.4210e-04 - mse: 1.0011e-04 - NMSE: 9.0504e-04 - val_loss: 3.8222e-04 - val_mse: 1.4314e-04 - val_NMSE: 0.0013\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3303e-04 - mse: 9.6299e-05 - NMSE: 8.7056e-04 - tot_time: 0h 14m 22.4s\n",
      "\n",
      "Epoch 44: val_NMSE improved from 0.00129 to 0.00126, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.3303e-04 - mse: 9.6299e-05 - NMSE: 8.7056e-04 - val_loss: 3.7334e-04 - val_mse: 1.3919e-04 - val_NMSE: 0.0013\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6535e-04 - mse: 1.3333e-04 - NMSE: 0.0012   - tot_time: 0h 14m 41.8s\n",
      "\n",
      "Epoch 45: val_NMSE did not improve from 0.00126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.6535e-04 - mse: 1.3333e-04 - NMSE: 0.0012 - val_loss: 7.2803e-04 - val_mse: 4.9835e-04 - val_NMSE: 0.0045\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.2074e-04 - mse: 1.9244e-04 - NMSE: 0.0017 - tot_time: 0h 15m 1.3s\n",
      "\n",
      "Epoch 46: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.2074e-04 - mse: 1.9244e-04 - NMSE: 0.0017 - val_loss: 3.7414e-04 - val_mse: 1.4617e-04 - val_NMSE: 0.0013\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3088e-04 - mse: 1.0385e-04 - NMSE: 9.3881e-04 - tot_time: 0h 15m 20.6s\n",
      "\n",
      "Epoch 47: val_NMSE improved from 0.00126 to 0.00126, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.3088e-04 - mse: 1.0385e-04 - NMSE: 9.3881e-04 - val_loss: 3.6449e-04 - val_mse: 1.3902e-04 - val_NMSE: 0.0013\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1617e-04 - mse: 9.2302e-05 - NMSE: 8.3443e-04 - tot_time: 0h 15m 40.0s\n",
      "\n",
      "Epoch 48: val_NMSE improved from 0.00126 to 0.00119, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.1617e-04 - mse: 9.2302e-05 - NMSE: 8.3443e-04 - val_loss: 3.5335e-04 - val_mse: 1.3143e-04 - val_NMSE: 0.0012\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0013 - mse: 0.0011 - NMSE: 0.0101 - tot_time: 0h 15m 59.0s\n",
      "\n",
      "Epoch 49: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.0013 - mse: 0.0011 - NMSE: 0.0101 - val_loss: 0.0010 - val_mse: 7.7820e-04 - val_NMSE: 0.0070\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.6397e-04 - mse: 3.2021e-04 - NMSE: 0.0029 - tot_time: 0h 16m 18.3s\n",
      "\n",
      "Epoch 50: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 5.6397e-04 - mse: 3.2021e-04 - NMSE: 0.0029 - val_loss: 4.7655e-04 - val_mse: 2.2804e-04 - val_NMSE: 0.0021\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.9319e-04 - mse: 1.4548e-04 - NMSE: 0.0013 - tot_time: 0h 16m 37.7s\n",
      "\n",
      "Epoch 51: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.9319e-04 - mse: 1.4548e-04 - NMSE: 0.0013 - val_loss: 4.2071e-04 - val_mse: 1.7574e-04 - val_NMSE: 0.0016\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5320e-04 - mse: 1.1154e-04 - NMSE: 0.0010 - tot_time: 0h 16m 56.7s\n",
      "\n",
      "Epoch 52: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.5320e-04 - mse: 1.1154e-04 - NMSE: 0.0010 - val_loss: 3.9349e-04 - val_mse: 1.5552e-04 - val_NMSE: 0.0014\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3503e-04 - mse: 1.0027e-04 - NMSE: 9.0645e-04 - tot_time: 0h 17m 15.8s\n",
      "\n",
      "Epoch 53: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.3503e-04 - mse: 1.0027e-04 - NMSE: 9.0645e-04 - val_loss: 3.7690e-04 - val_mse: 1.4557e-04 - val_NMSE: 0.0013\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2327e-04 - mse: 9.4624e-05 - NMSE: 8.5542e-04 - tot_time: 0h 17m 35.1s\n",
      "\n",
      "Epoch 54: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.2327e-04 - mse: 9.4624e-05 - NMSE: 8.5542e-04 - val_loss: 3.6544e-04 - val_mse: 1.3971e-04 - val_NMSE: 0.0013\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1523e-04 - mse: 9.1831e-05 - NMSE: 8.3017e-04 - tot_time: 0h 17m 54.3s\n",
      "\n",
      "Epoch 55: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.1523e-04 - mse: 9.1831e-05 - NMSE: 8.3017e-04 - val_loss: 3.5884e-04 - val_mse: 1.3796e-04 - val_NMSE: 0.0012\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.4897e-04 - mse: 2.2987e-04 - NMSE: 0.0021 - tot_time: 0h 18m 13.3s\n",
      "\n",
      "Epoch 56: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.4897e-04 - mse: 2.2987e-04 - NMSE: 0.0021 - val_loss: 3.9274e-04 - val_mse: 1.7458e-04 - val_NMSE: 0.0016\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3255e-04 - mse: 1.1428e-04 - NMSE: 0.0010 - tot_time: 0h 18m 32.5s\n",
      "\n",
      "Epoch 57: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.3255e-04 - mse: 1.1428e-04 - NMSE: 0.0010 - val_loss: 3.5458e-04 - val_mse: 1.3709e-04 - val_NMSE: 0.0012\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0830e-04 - mse: 9.2431e-05 - NMSE: 8.3559e-04Restoring model weights from the end of the best epoch: 48.\n",
      " - tot_time: 0h 18m 51.5s\n",
      "\n",
      "Epoch 58: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0830e-04 - mse: 9.2431e-05 - NMSE: 8.3559e-04 - val_loss: 3.4650e-04 - val_mse: 1.3251e-04 - val_NMSE: 0.0012\n",
      "Epoch 58: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.001 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0984e-04 - mse: 8.8071e-05 - NMSE: 7.9618e-04 - tot_time: 0h 19m 11.0s\n",
      "\n",
      "Epoch 1: val_NMSE improved from 0.00119 to 0.00116, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0984e-04 - mse: 8.8071e-05 - NMSE: 7.9618e-04 - val_loss: 3.5023e-04 - val_mse: 1.2865e-04 - val_NMSE: 0.0012\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0878e-04 - mse: 8.7365e-05 - NMSE: 7.8980e-04 - tot_time: 0h 19m 30.5s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.00116 to 0.00115, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.0878e-04 - mse: 8.7365e-05 - NMSE: 7.8980e-04 - val_loss: 3.4895e-04 - val_mse: 1.2774e-04 - val_NMSE: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0793e-04 - mse: 8.6906e-05 - NMSE: 7.8565e-04 - tot_time: 0h 19m 50.9s\n",
      "\n",
      "Epoch 3: val_NMSE improved from 0.00115 to 0.00115, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.0793e-04 - mse: 8.6906e-05 - NMSE: 7.8565e-04 - val_loss: 3.4818e-04 - val_mse: 1.2736e-04 - val_NMSE: 0.0012\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0743e-04 - mse: 8.6794e-05 - NMSE: 7.8464e-04 - tot_time: 0h 20m 10.6s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.00115 to 0.00115, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.0743e-04 - mse: 8.6794e-05 - NMSE: 7.8464e-04 - val_loss: 3.4711e-04 - val_mse: 1.2668e-04 - val_NMSE: 0.0011\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0657e-04 - mse: 8.6331e-05 - NMSE: 7.8046e-04 - tot_time: 0h 20m 30.6s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.00115 to 0.00114, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.0657e-04 - mse: 8.6331e-05 - NMSE: 7.8046e-04 - val_loss: 3.4639e-04 - val_mse: 1.2636e-04 - val_NMSE: 0.0011\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0603e-04 - mse: 8.6176e-05 - NMSE: 7.7905e-04 - tot_time: 0h 20m 50.3s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.00114 to 0.00114, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.0603e-04 - mse: 8.6176e-05 - NMSE: 7.7905e-04 - val_loss: 3.4562e-04 - val_mse: 1.2598e-04 - val_NMSE: 0.0011\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0522e-04 - mse: 8.5767e-05 - NMSE: 7.7536e-04 - tot_time: 0h 21m 9.7s\n",
      "\n",
      "Epoch 7: val_NMSE improved from 0.00114 to 0.00114, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0522e-04 - mse: 8.5767e-05 - NMSE: 7.7536e-04 - val_loss: 3.4501e-04 - val_mse: 1.2577e-04 - val_NMSE: 0.0011\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0456e-04 - mse: 8.5499e-05 - NMSE: 7.7293e-04 - tot_time: 0h 21m 29.1s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.00114 to 0.00113, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0456e-04 - mse: 8.5499e-05 - NMSE: 7.7293e-04 - val_loss: 3.4397e-04 - val_mse: 1.2512e-04 - val_NMSE: 0.0011\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0388e-04 - mse: 8.5215e-05 - NMSE: 7.7037e-04 - tot_time: 0h 21m 48.6s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.00113 to 0.00113, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0388e-04 - mse: 8.5215e-05 - NMSE: 7.7037e-04 - val_loss: 3.4332e-04 - val_mse: 1.2487e-04 - val_NMSE: 0.0011\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0321e-04 - mse: 8.4949e-05 - NMSE: 7.6796e-04 - tot_time: 0h 22m 8.5s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.00113 to 0.00113, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.0321e-04 - mse: 8.4949e-05 - NMSE: 7.6796e-04 - val_loss: 3.4255e-04 - val_mse: 1.2450e-04 - val_NMSE: 0.0011\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0250e-04 - mse: 8.4634e-05 - NMSE: 7.6511e-04 - tot_time: 0h 22m 27.9s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00113 to 0.00112, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0250e-04 - mse: 8.4634e-05 - NMSE: 7.6511e-04 - val_loss: 3.4189e-04 - val_mse: 1.2424e-04 - val_NMSE: 0.0011\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0174e-04 - mse: 8.4277e-05 - NMSE: 7.6189e-04 - tot_time: 0h 22m 47.5s\n",
      "\n",
      "Epoch 12: val_NMSE improved from 0.00112 to 0.00112, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.0174e-04 - mse: 8.4277e-05 - NMSE: 7.6189e-04 - val_loss: 3.4093e-04 - val_mse: 1.2368e-04 - val_NMSE: 0.0011\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0120e-04 - mse: 8.4144e-05 - NMSE: 7.6068e-04 - tot_time: 0h 23m 6.9s\n",
      "\n",
      "Epoch 13: val_NMSE improved from 0.00112 to 0.00111, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0120e-04 - mse: 8.4144e-05 - NMSE: 7.6068e-04 - val_loss: 3.4014e-04 - val_mse: 1.2329e-04 - val_NMSE: 0.0011\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0043e-04 - mse: 8.3773e-05 - NMSE: 7.5733e-04 - tot_time: 0h 23m 26.3s\n",
      "\n",
      "Epoch 14: val_NMSE improved from 0.00111 to 0.00111, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0043e-04 - mse: 8.3773e-05 - NMSE: 7.5733e-04 - val_loss: 3.3943e-04 - val_mse: 1.2299e-04 - val_NMSE: 0.0011\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9983e-04 - mse: 8.3580e-05 - NMSE: 7.5558e-04 - tot_time: 0h 23m 45.7s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00111 to 0.00111, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.9983e-04 - mse: 8.3580e-05 - NMSE: 7.5558e-04 - val_loss: 3.3877e-04 - val_mse: 1.2273e-04 - val_NMSE: 0.0011\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9907e-04 - mse: 8.3228e-05 - NMSE: 7.5240e-04 - tot_time: 0h 24m 5.5s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00111 to 0.00110, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.9907e-04 - mse: 8.3228e-05 - NMSE: 7.5240e-04 - val_loss: 3.3783e-04 - val_mse: 1.2220e-04 - val_NMSE: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9833e-04 - mse: 8.2887e-05 - NMSE: 7.4932e-04 - tot_time: 0h 24m 24.9s\n",
      "\n",
      "Epoch 17: val_NMSE improved from 0.00110 to 0.00110, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.9833e-04 - mse: 8.2887e-05 - NMSE: 7.4932e-04 - val_loss: 3.3714e-04 - val_mse: 1.2192e-04 - val_NMSE: 0.0011\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9775e-04 - mse: 8.2716e-05 - NMSE: 7.4777e-04 - tot_time: 0h 24m 44.1s\n",
      "\n",
      "Epoch 18: val_NMSE improved from 0.00110 to 0.00110, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.9775e-04 - mse: 8.2716e-05 - NMSE: 7.4777e-04 - val_loss: 3.3610e-04 - val_mse: 1.2128e-04 - val_NMSE: 0.0011\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9711e-04 - mse: 8.2472e-05 - NMSE: 7.4557e-04 - tot_time: 0h 25m 3.8s\n",
      "\n",
      "Epoch 19: val_NMSE improved from 0.00110 to 0.00109, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.9711e-04 - mse: 8.2472e-05 - NMSE: 7.4557e-04 - val_loss: 3.3542e-04 - val_mse: 1.2100e-04 - val_NMSE: 0.0011\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9627e-04 - mse: 8.2044e-05 - NMSE: 7.4170e-04 - tot_time: 0h 25m 23.4s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00109 to 0.00109, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.9627e-04 - mse: 8.2044e-05 - NMSE: 7.4170e-04 - val_loss: 3.3462e-04 - val_mse: 1.2062e-04 - val_NMSE: 0.0011\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9559e-04 - mse: 8.1782e-05 - NMSE: 7.3933e-04 - tot_time: 0h 25m 42.6s\n",
      "\n",
      "Epoch 21: val_NMSE improved from 0.00109 to 0.00109, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.9559e-04 - mse: 8.1782e-05 - NMSE: 7.3933e-04 - val_loss: 3.3385e-04 - val_mse: 1.2026e-04 - val_NMSE: 0.0011\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9494e-04 - mse: 8.1537e-05 - NMSE: 7.3712e-04 - tot_time: 0h 26m 2.2s\n",
      "\n",
      "Epoch 22: val_NMSE improved from 0.00109 to 0.00108, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.9494e-04 - mse: 8.1537e-05 - NMSE: 7.3712e-04 - val_loss: 3.3308e-04 - val_mse: 1.1989e-04 - val_NMSE: 0.0011\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9434e-04 - mse: 8.1347e-05 - NMSE: 7.3539e-04 - tot_time: 0h 26m 21.7s\n",
      "\n",
      "Epoch 23: val_NMSE improved from 0.00108 to 0.00108, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.9434e-04 - mse: 8.1347e-05 - NMSE: 7.3539e-04 - val_loss: 3.3231e-04 - val_mse: 1.1953e-04 - val_NMSE: 0.0011\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9333e-04 - mse: 8.0741e-05 - NMSE: 7.2992e-04 - tot_time: 0h 26m 41.1s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00108 to 0.00108, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.9333e-04 - mse: 8.0741e-05 - NMSE: 7.2992e-04 - val_loss: 3.3133e-04 - val_mse: 1.1896e-04 - val_NMSE: 0.0011\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9281e-04 - mse: 8.0626e-05 - NMSE: 7.2888e-04 - tot_time: 0h 27m 0.9s\n",
      "\n",
      "Epoch 25: val_NMSE improved from 0.00108 to 0.00107, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.9281e-04 - mse: 8.0626e-05 - NMSE: 7.2888e-04 - val_loss: 3.3070e-04 - val_mse: 1.1874e-04 - val_NMSE: 0.0011\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9199e-04 - mse: 8.0224e-05 - NMSE: 7.2525e-04 - tot_time: 0h 27m 20.3s\n",
      "\n",
      "Epoch 26: val_NMSE improved from 0.00107 to 0.00107, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.9199e-04 - mse: 8.0224e-05 - NMSE: 7.2525e-04 - val_loss: 3.2976e-04 - val_mse: 1.1821e-04 - val_NMSE: 0.0011\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9144e-04 - mse: 8.0079e-05 - NMSE: 7.2394e-04 - tot_time: 0h 27m 40.3s\n",
      "\n",
      "Epoch 27: val_NMSE improved from 0.00107 to 0.00107, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.9144e-04 - mse: 8.0079e-05 - NMSE: 7.2394e-04 - val_loss: 3.2910e-04 - val_mse: 1.1796e-04 - val_NMSE: 0.0011\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9076e-04 - mse: 7.9809e-05 - NMSE: 7.2149e-04 - tot_time: 0h 27m 59.9s\n",
      "\n",
      "Epoch 28: val_NMSE improved from 0.00107 to 0.00106, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.9076e-04 - mse: 7.9809e-05 - NMSE: 7.2149e-04 - val_loss: 3.2833e-04 - val_mse: 1.1760e-04 - val_NMSE: 0.0011\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9003e-04 - mse: 7.9489e-05 - NMSE: 7.1860e-04 - tot_time: 0h 28m 19.5s\n",
      "\n",
      "Epoch 29: val_NMSE improved from 0.00106 to 0.00106, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.9003e-04 - mse: 7.9489e-05 - NMSE: 7.1860e-04 - val_loss: 3.2758e-04 - val_mse: 1.1727e-04 - val_NMSE: 0.0011\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8932e-04 - mse: 7.9194e-05 - NMSE: 7.1594e-04 - tot_time: 0h 28m 39.0s\n",
      "\n",
      "Epoch 30: val_NMSE improved from 0.00106 to 0.00106, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.8932e-04 - mse: 7.9194e-05 - NMSE: 7.1594e-04 - val_loss: 3.2670e-04 - val_mse: 1.1679e-04 - val_NMSE: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8875e-04 - mse: 7.9026e-05 - NMSE: 7.1441e-04 - tot_time: 0h 28m 58.8s\n",
      "\n",
      "Epoch 31: val_NMSE improved from 0.00106 to 0.00105, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8875e-04 - mse: 7.9026e-05 - NMSE: 7.1441e-04 - val_loss: 3.2583e-04 - val_mse: 1.1633e-04 - val_NMSE: 0.0011\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8790e-04 - mse: 7.8581e-05 - NMSE: 7.1039e-04 - tot_time: 0h 29m 18.9s\n",
      "\n",
      "Epoch 32: val_NMSE improved from 0.00105 to 0.00105, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8790e-04 - mse: 7.8581e-05 - NMSE: 7.1039e-04 - val_loss: 3.2497e-04 - val_mse: 1.1587e-04 - val_NMSE: 0.0010\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8725e-04 - mse: 7.8346e-05 - NMSE: 7.0827e-04 - tot_time: 0h 29m 38.5s\n",
      "\n",
      "Epoch 33: val_NMSE improved from 0.00105 to 0.00104, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8725e-04 - mse: 7.8346e-05 - NMSE: 7.0827e-04 - val_loss: 3.2425e-04 - val_mse: 1.1557e-04 - val_NMSE: 0.0010\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8643e-04 - mse: 7.7930e-05 - NMSE: 7.0451e-04 - tot_time: 0h 29m 58.1s\n",
      "\n",
      "Epoch 34: val_NMSE improved from 0.00104 to 0.00104, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8643e-04 - mse: 7.7930e-05 - NMSE: 7.0451e-04 - val_loss: 3.2338e-04 - val_mse: 1.1510e-04 - val_NMSE: 0.0010\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8579e-04 - mse: 7.7698e-05 - NMSE: 7.0241e-04 - tot_time: 0h 30m 17.9s\n",
      "\n",
      "Epoch 35: val_NMSE improved from 0.00104 to 0.00104, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8579e-04 - mse: 7.7698e-05 - NMSE: 7.0241e-04 - val_loss: 3.2257e-04 - val_mse: 1.1469e-04 - val_NMSE: 0.0010\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8509e-04 - mse: 7.7408e-05 - NMSE: 6.9979e-04 - tot_time: 0h 30m 37.5s\n",
      "\n",
      "Epoch 36: val_NMSE improved from 0.00104 to 0.00103, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8509e-04 - mse: 7.7408e-05 - NMSE: 6.9979e-04 - val_loss: 3.2162e-04 - val_mse: 1.1415e-04 - val_NMSE: 0.0010\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8434e-04 - mse: 7.7055e-05 - NMSE: 6.9660e-04 - tot_time: 0h 30m 56.9s\n",
      "\n",
      "Epoch 37: val_NMSE improved from 0.00103 to 0.00103, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.8434e-04 - mse: 7.7055e-05 - NMSE: 6.9660e-04 - val_loss: 3.2073e-04 - val_mse: 1.1366e-04 - val_NMSE: 0.0010\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8373e-04 - mse: 7.6847e-05 - NMSE: 6.9472e-04 - tot_time: 0h 31m 16.5s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00103 to 0.00103, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8373e-04 - mse: 7.6847e-05 - NMSE: 6.9472e-04 - val_loss: 3.2027e-04 - val_mse: 1.1361e-04 - val_NMSE: 0.0010\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8293e-04 - mse: 7.6451e-05 - NMSE: 6.9113e-04 - tot_time: 0h 31m 36.1s\n",
      "\n",
      "Epoch 39: val_NMSE improved from 0.00103 to 0.00102, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8293e-04 - mse: 7.6451e-05 - NMSE: 6.9113e-04 - val_loss: 3.1929e-04 - val_mse: 1.1303e-04 - val_NMSE: 0.0010\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8216e-04 - mse: 7.6083e-05 - NMSE: 6.8781e-04 - tot_time: 0h 31m 55.4s\n",
      "\n",
      "Epoch 40: val_NMSE improved from 0.00102 to 0.00102, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.8216e-04 - mse: 7.6083e-05 - NMSE: 6.8781e-04 - val_loss: 3.1830e-04 - val_mse: 1.1244e-04 - val_NMSE: 0.0010\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8142e-04 - mse: 7.5744e-05 - NMSE: 6.8474e-04 - tot_time: 0h 32m 14.8s\n",
      "\n",
      "Epoch 41: val_NMSE improved from 0.00102 to 0.00101, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.8142e-04 - mse: 7.5744e-05 - NMSE: 6.8474e-04 - val_loss: 3.1741e-04 - val_mse: 1.1195e-04 - val_NMSE: 0.0010\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8054e-04 - mse: 7.5269e-05 - NMSE: 6.8045e-04 - tot_time: 0h 32m 35.0s\n",
      "\n",
      "Epoch 42: val_NMSE improved from 0.00101 to 0.00101, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8054e-04 - mse: 7.5269e-05 - NMSE: 6.8045e-04 - val_loss: 3.1665e-04 - val_mse: 1.1158e-04 - val_NMSE: 0.0010\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8010e-04 - mse: 7.5221e-05 - NMSE: 6.8002e-04 - tot_time: 0h 32m 54.5s\n",
      "\n",
      "Epoch 43: val_NMSE improved from 0.00101 to 0.00100, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8010e-04 - mse: 7.5221e-05 - NMSE: 6.8002e-04 - val_loss: 3.1580e-04 - val_mse: 1.1113e-04 - val_NMSE: 0.0010\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7948e-04 - mse: 7.4995e-05 - NMSE: 6.7797e-04 - tot_time: 0h 33m 13.9s\n",
      "\n",
      "Epoch 44: val_NMSE improved from 0.00100 to 0.00100, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7948e-04 - mse: 7.4995e-05 - NMSE: 6.7797e-04 - val_loss: 3.1517e-04 - val_mse: 1.1088e-04 - val_NMSE: 0.0010\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7868e-04 - mse: 7.4578e-05 - NMSE: 6.7420e-04 - tot_time: 0h 33m 33.5s\n",
      "\n",
      "Epoch 45: val_NMSE improved from 0.00100 to 0.00100, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.7868e-04 - mse: 7.4578e-05 - NMSE: 6.7420e-04 - val_loss: 3.1452e-04 - val_mse: 1.1063e-04 - val_NMSE: 0.0010\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7786e-04 - mse: 7.4151e-05 - NMSE: 6.7034e-04 - tot_time: 0h 33m 52.8s\n",
      "\n",
      "Epoch 46: val_NMSE improved from 0.00100 to 0.00099, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7786e-04 - mse: 7.4151e-05 - NMSE: 6.7034e-04 - val_loss: 3.1345e-04 - val_mse: 1.0994e-04 - val_NMSE: 9.9390e-04\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7701e-04 - mse: 7.3679e-05 - NMSE: 6.6607e-04 - tot_time: 0h 34m 12.4s\n",
      "\n",
      "Epoch 47: val_NMSE improved from 0.00099 to 0.00099, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.7701e-04 - mse: 7.3679e-05 - NMSE: 6.6607e-04 - val_loss: 3.1237e-04 - val_mse: 1.0924e-04 - val_NMSE: 9.8755e-04\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7629e-04 - mse: 7.3337e-05 - NMSE: 6.6299e-04 - tot_time: 0h 34m 31.9s\n",
      "\n",
      "Epoch 48: val_NMSE improved from 0.00099 to 0.00098, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7629e-04 - mse: 7.3337e-05 - NMSE: 6.6299e-04 - val_loss: 3.1157e-04 - val_mse: 1.0882e-04 - val_NMSE: 9.8377e-04\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7568e-04 - mse: 7.3107e-05 - NMSE: 6.6090e-04 - tot_time: 0h 34m 51.3s\n",
      "\n",
      "Epoch 49: val_NMSE improved from 0.00098 to 0.00098, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7568e-04 - mse: 7.3107e-05 - NMSE: 6.6090e-04 - val_loss: 3.1080e-04 - val_mse: 1.0842e-04 - val_NMSE: 9.8018e-04\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7529e-04 - mse: 7.3086e-05 - NMSE: 6.6072e-04 - tot_time: 0h 35m 10.9s\n",
      "\n",
      "Epoch 50: val_NMSE improved from 0.00098 to 0.00098, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.7529e-04 - mse: 7.3086e-05 - NMSE: 6.6072e-04 - val_loss: 3.1036e-04 - val_mse: 1.0835e-04 - val_NMSE: 9.7952e-04\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7516e-04 - mse: 7.3324e-05 - NMSE: 6.6286e-04 - tot_time: 0h 35m 30.4s\n",
      "\n",
      "Epoch 51: val_NMSE improved from 0.00098 to 0.00097, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7516e-04 - mse: 7.3324e-05 - NMSE: 6.6286e-04 - val_loss: 3.0936e-04 - val_mse: 1.0773e-04 - val_NMSE: 9.7387e-04\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4126e-04 - mse: 1.3984e-04 - NMSE: 0.0013 - tot_time: 0h 35m 49.6s\n",
      "\n",
      "Epoch 52: val_NMSE did not improve from 0.00097\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.4126e-04 - mse: 1.3984e-04 - NMSE: 0.0013 - val_loss: 3.9228e-04 - val_mse: 1.9124e-04 - val_NMSE: 0.0017\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9593e-04 - mse: 9.5139e-05 - NMSE: 8.6008e-04 - tot_time: 0h 36m 9.0s\n",
      "\n",
      "Epoch 53: val_NMSE did not improve from 0.00097\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.9593e-04 - mse: 9.5139e-05 - NMSE: 8.6008e-04 - val_loss: 3.2355e-04 - val_mse: 1.2296e-04 - val_NMSE: 0.0011\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7599e-04 - mse: 7.5560e-05 - NMSE: 6.8308e-04 - tot_time: 0h 36m 28.2s\n",
      "\n",
      "Epoch 54: val_NMSE improved from 0.00097 to 0.00097, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7599e-04 - mse: 7.5560e-05 - NMSE: 6.8308e-04 - val_loss: 3.0759e-04 - val_mse: 1.0733e-04 - val_NMSE: 9.7033e-04\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7194e-04 - mse: 7.1816e-05 - NMSE: 6.4923e-04 - tot_time: 0h 36m 47.6s\n",
      "\n",
      "Epoch 55: val_NMSE improved from 0.00097 to 0.00097, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7194e-04 - mse: 7.1816e-05 - NMSE: 6.4923e-04 - val_loss: 3.0672e-04 - val_mse: 1.0676e-04 - val_NMSE: 9.6512e-04\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7099e-04 - mse: 7.1171e-05 - NMSE: 6.4341e-04 - tot_time: 0h 37m 7.2s\n",
      "\n",
      "Epoch 56: val_NMSE improved from 0.00097 to 0.00096, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.7099e-04 - mse: 7.1171e-05 - NMSE: 6.4341e-04 - val_loss: 3.0564e-04 - val_mse: 1.0598e-04 - val_NMSE: 9.5812e-04\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7013e-04 - mse: 7.0623e-05 - NMSE: 6.3845e-04 - tot_time: 0h 37m 26.6s\n",
      "\n",
      "Epoch 57: val_NMSE improved from 0.00096 to 0.00095, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7013e-04 - mse: 7.0623e-05 - NMSE: 6.3845e-04 - val_loss: 3.0493e-04 - val_mse: 1.0560e-04 - val_NMSE: 9.5467e-04\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6963e-04 - mse: 7.0446e-05 - NMSE: 6.3685e-04 - tot_time: 0h 37m 45.8s\n",
      "\n",
      "Epoch 58: val_NMSE did not improve from 0.00095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.6963e-04 - mse: 7.0446e-05 - NMSE: 6.3685e-04 - val_loss: 3.0463e-04 - val_mse: 1.0563e-04 - val_NMSE: 9.5488e-04\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7441e-04 - mse: 7.5560e-05 - NMSE: 6.8305e-04 - tot_time: 0h 38m 5.0s\n",
      "\n",
      "Epoch 59: val_NMSE did not improve from 0.00095\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7441e-04 - mse: 7.5560e-05 - NMSE: 6.8305e-04 - val_loss: 3.7452e-04 - val_mse: 1.7584e-04 - val_NMSE: 0.0016\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1416e-04 - mse: 1.1579e-04 - NMSE: 0.0010 - tot_time: 0h 38m 24.7s\n",
      "\n",
      "Epoch 60: val_NMSE did not improve from 0.00095\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.1416e-04 - mse: 1.1579e-04 - NMSE: 0.0010 - val_loss: 3.2306e-04 - val_mse: 1.2497e-04 - val_NMSE: 0.0011\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7225e-04 - mse: 7.4304e-05 - NMSE: 6.7170e-04 - tot_time: 0h 38m 44.3s\n",
      "\n",
      "Epoch 61: val_NMSE improved from 0.00095 to 0.00094, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.7225e-04 - mse: 7.4304e-05 - NMSE: 6.7170e-04 - val_loss: 3.0154e-04 - val_mse: 1.0376e-04 - val_NMSE: 9.3800e-04\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6768e-04 - mse: 7.0038e-05 - NMSE: 6.3316e-04 - tot_time: 0h 39m 3.9s\n",
      "\n",
      "Epoch 62: val_NMSE did not improve from 0.00094\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.6768e-04 - mse: 7.0038e-05 - NMSE: 6.3316e-04 - val_loss: 3.0198e-04 - val_mse: 1.0448e-04 - val_NMSE: 9.4455e-04\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6630e-04 - mse: 6.8936e-05 - NMSE: 6.2319e-04 - tot_time: 0h 39m 23.1s\n",
      "\n",
      "Epoch 63: val_NMSE improved from 0.00094 to 0.00094, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.6630e-04 - mse: 6.8936e-05 - NMSE: 6.2319e-04 - val_loss: 3.0071e-04 - val_mse: 1.0350e-04 - val_NMSE: 9.3565e-04\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3110e-04 - mse: 1.3404e-04 - NMSE: 0.0012 - tot_time: 0h 39m 42.4s\n",
      "\n",
      "Epoch 64: val_NMSE did not improve from 0.00094\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.3110e-04 - mse: 1.3404e-04 - NMSE: 0.0012 - val_loss: 3.1022e-04 - val_mse: 1.1344e-04 - val_NMSE: 0.0010\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0040e-04 - mse: 1.0387e-04 - NMSE: 9.3898e-04 - tot_time: 0h 40m 1.6s\n",
      "\n",
      "Epoch 65: val_NMSE did not improve from 0.00094\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0040e-04 - mse: 1.0387e-04 - NMSE: 9.3898e-04 - val_loss: 3.1885e-04 - val_mse: 1.2248e-04 - val_NMSE: 0.0011\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6973e-04 - mse: 7.3486e-05 - NMSE: 6.6433e-04 - tot_time: 0h 40m 21.3s\n",
      "\n",
      "Epoch 66: val_NMSE did not improve from 0.00094\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.6973e-04 - mse: 7.3486e-05 - NMSE: 6.6433e-04 - val_loss: 3.0053e-04 - val_mse: 1.0442e-04 - val_NMSE: 9.4401e-04\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6483e-04 - mse: 6.8850e-05 - NMSE: 6.2242e-04 - tot_time: 0h 40m 40.5s\n",
      "\n",
      "Epoch 67: val_NMSE improved from 0.00094 to 0.00093, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.6483e-04 - mse: 6.8850e-05 - NMSE: 6.2242e-04 - val_loss: 2.9826e-04 - val_mse: 1.0242e-04 - val_NMSE: 9.2591e-04\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6333e-04 - mse: 6.7609e-05 - NMSE: 6.1120e-04 - tot_time: 0h 40m 59.9s\n",
      "\n",
      "Epoch 68: val_NMSE improved from 0.00093 to 0.00092, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.6333e-04 - mse: 6.7609e-05 - NMSE: 6.1120e-04 - val_loss: 2.9708e-04 - val_mse: 1.0151e-04 - val_NMSE: 9.1762e-04\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6263e-04 - mse: 6.7187e-05 - NMSE: 6.0738e-04 - tot_time: 0h 41m 19.9s\n",
      "\n",
      "Epoch 69: val_NMSE improved from 0.00092 to 0.00091, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.6263e-04 - mse: 6.7187e-05 - NMSE: 6.0738e-04 - val_loss: 2.9603e-04 - val_mse: 1.0075e-04 - val_NMSE: 9.1075e-04\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6213e-04 - mse: 6.6982e-05 - NMSE: 6.0553e-04 - tot_time: 0h 41m 39.5s\n",
      "\n",
      "Epoch 70: val_NMSE improved from 0.00091 to 0.00091, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.6213e-04 - mse: 6.6982e-05 - NMSE: 6.0553e-04 - val_loss: 2.9567e-04 - val_mse: 1.0068e-04 - val_NMSE: 9.1019e-04\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6156e-04 - mse: 6.6700e-05 - NMSE: 6.0298e-04 - tot_time: 0h 41m 59.6s\n",
      "\n",
      "Epoch 71: val_NMSE did not improve from 0.00091\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.6156e-04 - mse: 6.6700e-05 - NMSE: 6.0298e-04 - val_loss: 2.9912e-04 - val_mse: 1.0443e-04 - val_NMSE: 9.4402e-04\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6007e-04 - mse: 1.6567e-04 - NMSE: 0.0015 - tot_time: 0h 42m 19.6s\n",
      "\n",
      "Epoch 72: val_NMSE did not improve from 0.00091\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.6007e-04 - mse: 1.6567e-04 - NMSE: 0.0015 - val_loss: 2.9868e-04 - val_mse: 1.0471e-04 - val_NMSE: 9.4655e-04\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7371e-04 - mse: 7.9840e-05 - NMSE: 7.2172e-04 - tot_time: 0h 42m 39.0s\n",
      "\n",
      "Epoch 73: val_NMSE did not improve from 0.00091\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7371e-04 - mse: 7.9840e-05 - NMSE: 7.2172e-04 - val_loss: 2.9556e-04 - val_mse: 1.0180e-04 - val_NMSE: 9.2029e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6179e-04 - mse: 6.8148e-05 - NMSE: 6.1606e-04 - tot_time: 0h 42m 58.5s\n",
      "\n",
      "Epoch 74: val_NMSE did not improve from 0.00091\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.6179e-04 - mse: 6.8148e-05 - NMSE: 6.1606e-04 - val_loss: 2.9472e-04 - val_mse: 1.0120e-04 - val_NMSE: 9.1488e-04\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5953e-04 - mse: 6.6112e-05 - NMSE: 5.9766e-04 - tot_time: 0h 43m 18.2s\n",
      "\n",
      "Epoch 75: val_NMSE improved from 0.00091 to 0.00089, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5953e-04 - mse: 6.6112e-05 - NMSE: 5.9766e-04 - val_loss: 2.9217e-04 - val_mse: 9.8882e-05 - val_NMSE: 8.9391e-04\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5852e-04 - mse: 6.5353e-05 - NMSE: 5.9080e-04 - tot_time: 0h 43m 37.8s\n",
      "\n",
      "Epoch 76: val_NMSE improved from 0.00089 to 0.00089, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5852e-04 - mse: 6.5353e-05 - NMSE: 5.9080e-04 - val_loss: 2.9153e-04 - val_mse: 9.8509e-05 - val_NMSE: 8.9054e-04\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5795e-04 - mse: 6.5049e-05 - NMSE: 5.8806e-04 - tot_time: 0h 43m 57.3s\n",
      "\n",
      "Epoch 77: val_NMSE improved from 0.00089 to 0.00089, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.5795e-04 - mse: 6.5049e-05 - NMSE: 5.8806e-04 - val_loss: 2.9114e-04 - val_mse: 9.8400e-05 - val_NMSE: 8.8955e-04\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0058e-04 - mse: 1.0797e-04 - NMSE: 9.7610e-04 - tot_time: 0h 44m 16.7s\n",
      "\n",
      "Epoch 78: val_NMSE did not improve from 0.00089\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0058e-04 - mse: 1.0797e-04 - NMSE: 9.7610e-04 - val_loss: 3.0937e-04 - val_mse: 1.1700e-04 - val_NMSE: 0.0011\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7208e-04 - mse: 7.9876e-05 - NMSE: 7.2210e-04 - tot_time: 0h 44m 36.6s\n",
      "\n",
      "Epoch 79: val_NMSE did not improve from 0.00089\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.7208e-04 - mse: 7.9876e-05 - NMSE: 7.2210e-04 - val_loss: 2.9057e-04 - val_mse: 9.8524e-05 - val_NMSE: 8.9067e-04\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5793e-04 - mse: 6.6003e-05 - NMSE: 5.9668e-04 - tot_time: 0h 44m 56.3s\n",
      "\n",
      "Epoch 80: val_NMSE improved from 0.00089 to 0.00089, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5793e-04 - mse: 6.6003e-05 - NMSE: 5.9668e-04 - val_loss: 2.8974e-04 - val_mse: 9.7951e-05 - val_NMSE: 8.8549e-04\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5556e-04 - mse: 6.3881e-05 - NMSE: 5.7750e-04 - tot_time: 0h 45m 15.7s\n",
      "\n",
      "Epoch 81: val_NMSE improved from 0.00089 to 0.00087, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.5556e-04 - mse: 6.3881e-05 - NMSE: 5.7750e-04 - val_loss: 2.8798e-04 - val_mse: 9.6433e-05 - val_NMSE: 8.7177e-04\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5621e-04 - mse: 6.4789e-05 - NMSE: 5.8570e-04 - tot_time: 0h 45m 35.3s\n",
      "\n",
      "Epoch 82: val_NMSE did not improve from 0.00087\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5621e-04 - mse: 6.4789e-05 - NMSE: 5.8570e-04 - val_loss: 3.0286e-04 - val_mse: 1.1159e-04 - val_NMSE: 0.0010\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2950e-04 - mse: 1.3844e-04 - NMSE: 0.0013 - tot_time: 0h 45m 54.6s\n",
      "\n",
      "Epoch 83: val_NMSE did not improve from 0.00087\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.2950e-04 - mse: 1.3844e-04 - NMSE: 0.0013 - val_loss: 2.9992e-04 - val_mse: 1.0915e-04 - val_NMSE: 9.8671e-04\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6322e-04 - mse: 7.2540e-05 - NMSE: 6.5578e-04 - tot_time: 0h 46m 14.3s\n",
      "\n",
      "Epoch 84: val_NMSE did not improve from 0.00087\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.6322e-04 - mse: 7.2540e-05 - NMSE: 6.5578e-04 - val_loss: 2.8773e-04 - val_mse: 9.7186e-05 - val_NMSE: 8.7858e-04\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5438e-04 - mse: 6.3941e-05 - NMSE: 5.7804e-04 - tot_time: 0h 46m 34.1s\n",
      "\n",
      "Epoch 85: val_NMSE improved from 0.00087 to 0.00086, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5438e-04 - mse: 6.3941e-05 - NMSE: 5.7804e-04 - val_loss: 2.8561e-04 - val_mse: 9.5297e-05 - val_NMSE: 8.6150e-04\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5256e-04 - mse: 6.2359e-05 - NMSE: 5.6374e-04 - tot_time: 0h 46m 53.7s\n",
      "\n",
      "Epoch 86: val_NMSE improved from 0.00086 to 0.00085, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5256e-04 - mse: 6.2359e-05 - NMSE: 5.6374e-04 - val_loss: 2.8439e-04 - val_mse: 9.4316e-05 - val_NMSE: 8.5263e-04\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5194e-04 - mse: 6.1981e-05 - NMSE: 5.6032e-04 - tot_time: 0h 47m 13.3s\n",
      "\n",
      "Epoch 87: val_NMSE improved from 0.00085 to 0.00085, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5194e-04 - mse: 6.1981e-05 - NMSE: 5.6032e-04 - val_loss: 2.8339e-04 - val_mse: 9.3572e-05 - val_NMSE: 8.4591e-04\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5954e-04 - mse: 6.9844e-05 - NMSE: 6.3137e-04 - tot_time: 0h 47m 33.3s\n",
      "\n",
      "Epoch 88: val_NMSE did not improve from 0.00085\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5954e-04 - mse: 6.9844e-05 - NMSE: 6.3137e-04 - val_loss: 4.3228e-04 - val_mse: 2.4274e-04 - val_NMSE: 0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6708e-04 - mse: 1.7792e-04 - NMSE: 0.0016 - tot_time: 0h 47m 53.1s\n",
      "\n",
      "Epoch 89: val_NMSE did not improve from 0.00085\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.6708e-04 - mse: 1.7792e-04 - NMSE: 0.0016 - val_loss: 3.2397e-04 - val_mse: 1.3511e-04 - val_NMSE: 0.0012\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6252e-04 - mse: 7.3719e-05 - NMSE: 6.6639e-04 - tot_time: 0h 48m 12.7s\n",
      "\n",
      "Epoch 90: val_NMSE did not improve from 0.00085\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.6252e-04 - mse: 7.3719e-05 - NMSE: 6.6639e-04 - val_loss: 2.8310e-04 - val_mse: 9.4402e-05 - val_NMSE: 8.5341e-04\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5211e-04 - mse: 6.3512e-05 - NMSE: 5.7415e-04 - tot_time: 0h 48m 32.2s\n",
      "\n",
      "Epoch 91: val_NMSE did not improve from 0.00085\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.5211e-04 - mse: 6.3512e-05 - NMSE: 5.7415e-04 - val_loss: 2.8341e-04 - val_mse: 9.4936e-05 - val_NMSE: 8.5822e-04\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4964e-04 - mse: 6.1255e-05 - NMSE: 5.5375e-04 - tot_time: 0h 48m 51.4s\n",
      "\n",
      "Epoch 92: val_NMSE improved from 0.00085 to 0.00084, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.4964e-04 - mse: 6.1255e-05 - NMSE: 5.5375e-04 - val_loss: 2.8080e-04 - val_mse: 9.2536e-05 - val_NMSE: 8.3654e-04\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4877e-04 - mse: 6.0611e-05 - NMSE: 5.4794e-04 - tot_time: 0h 49m 11.0s\n",
      "\n",
      "Epoch 93: val_NMSE improved from 0.00084 to 0.00084, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4877e-04 - mse: 6.0611e-05 - NMSE: 5.4794e-04 - val_loss: 2.8052e-04 - val_mse: 9.2497e-05 - val_NMSE: 8.3618e-04\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4791e-04 - mse: 6.0006e-05 - NMSE: 5.4246e-04 - tot_time: 0h 49m 30.6s\n",
      "\n",
      "Epoch 94: val_NMSE improved from 0.00084 to 0.00082, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4791e-04 - mse: 6.0006e-05 - NMSE: 5.4246e-04 - val_loss: 2.7842e-04 - val_mse: 9.0649e-05 - val_NMSE: 8.1948e-04\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2958e-04 - mse: 1.4194e-04 - NMSE: 0.0013 - tot_time: 0h 49m 50.1s\n",
      "\n",
      "Epoch 95: val_NMSE did not improve from 0.00082\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.2958e-04 - mse: 1.4194e-04 - NMSE: 0.0013 - val_loss: 2.9864e-04 - val_mse: 1.1128e-04 - val_NMSE: 0.0010\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8725e-04 - mse: 1.0014e-04 - NMSE: 9.0534e-04 - tot_time: 0h 50m 9.8s\n",
      "\n",
      "Epoch 96: val_NMSE did not improve from 0.00082\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8725e-04 - mse: 1.0014e-04 - NMSE: 9.0534e-04 - val_loss: 2.7964e-04 - val_mse: 9.2680e-05 - val_NMSE: 8.3783e-04\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5251e-04 - mse: 6.5633e-05 - NMSE: 5.9334e-04 - tot_time: 0h 50m 29.3s\n",
      "\n",
      "Epoch 97: val_NMSE did not improve from 0.00082\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.5251e-04 - mse: 6.5633e-05 - NMSE: 5.9334e-04 - val_loss: 2.8190e-04 - val_mse: 9.5127e-05 - val_NMSE: 8.5996e-04\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4700e-04 - mse: 6.0322e-05 - NMSE: 5.4532e-04 - tot_time: 0h 50m 48.9s\n",
      "\n",
      "Epoch 98: val_NMSE improved from 0.00082 to 0.00082, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4700e-04 - mse: 6.0322e-05 - NMSE: 5.4532e-04 - val_loss: 2.7715e-04 - val_mse: 9.0586e-05 - val_NMSE: 8.1890e-04\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4511e-04 - mse: 5.8643e-05 - NMSE: 5.3014e-04 - tot_time: 0h 51m 8.5s\n",
      "\n",
      "Epoch 99: val_NMSE improved from 0.00082 to 0.00081, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4511e-04 - mse: 5.8643e-05 - NMSE: 5.3014e-04 - val_loss: 2.7599e-04 - val_mse: 8.9639e-05 - val_NMSE: 8.1034e-04\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4441e-04 - mse: 5.8172e-05 - NMSE: 5.2588e-04 - tot_time: 0h 51m 28.0s\n",
      "\n",
      "Epoch 100: val_NMSE improved from 0.00081 to 0.00081, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.4441e-04 - mse: 5.8172e-05 - NMSE: 5.2588e-04 - val_loss: 2.7519e-04 - val_mse: 8.9082e-05 - val_NMSE: 8.0531e-04\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4375e-04 - mse: 5.7761e-05 - NMSE: 5.2216e-04 - tot_time: 0h 51m 47.7s\n",
      "\n",
      "Epoch 101: val_NMSE improved from 0.00081 to 0.00080, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4375e-04 - mse: 5.7761e-05 - NMSE: 5.2216e-04 - val_loss: 2.7411e-04 - val_mse: 8.8256e-05 - val_NMSE: 7.9784e-04\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4382e-04 - mse: 5.8082e-05 - NMSE: 5.2507e-04 - tot_time: 0h 52m 6.9s\n",
      "\n",
      "Epoch 102: val_NMSE did not improve from 0.00080\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.4382e-04 - mse: 5.8082e-05 - NMSE: 5.2507e-04 - val_loss: 2.7503e-04 - val_mse: 8.9439e-05 - val_NMSE: 8.0853e-04\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7106e-04 - mse: 8.5588e-05 - NMSE: 7.7374e-04 - tot_time: 0h 52m 26.4s\n",
      "\n",
      "Epoch 103: val_NMSE did not improve from 0.00080\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7106e-04 - mse: 8.5588e-05 - NMSE: 7.7374e-04 - val_loss: 8.7031e-04 - val_mse: 6.8501e-04 - val_NMSE: 0.0062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.4797e-04 - mse: 2.6317e-04 - NMSE: 0.0024 - tot_time: 0h 52m 45.7s\n",
      "\n",
      "Epoch 104: val_NMSE did not improve from 0.00080\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 4.4797e-04 - mse: 2.6317e-04 - NMSE: 0.0024 - val_loss: 2.8489e-04 - val_mse: 1.0035e-04 - val_NMSE: 9.0721e-04\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6182e-04 - mse: 7.7319e-05 - NMSE: 6.9898e-04 - tot_time: 0h 53m 5.0s\n",
      "\n",
      "Epoch 105: val_NMSE did not improve from 0.00080\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.6182e-04 - mse: 7.7319e-05 - NMSE: 6.9898e-04 - val_loss: 2.7788e-04 - val_mse: 9.3444e-05 - val_NMSE: 8.4473e-04\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4571e-04 - mse: 6.1335e-05 - NMSE: 5.5448e-04 - tot_time: 0h 53m 24.5s\n",
      "\n",
      "Epoch 106: val_NMSE did not improve from 0.00080\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.4571e-04 - mse: 6.1335e-05 - NMSE: 5.5448e-04 - val_loss: 2.7539e-04 - val_mse: 9.1092e-05 - val_NMSE: 8.2348e-04\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4290e-04 - mse: 5.8659e-05 - NMSE: 5.3029e-04 - tot_time: 0h 53m 44.1s\n",
      "\n",
      "Epoch 107: val_NMSE did not improve from 0.00080\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4290e-04 - mse: 5.8659e-05 - NMSE: 5.3029e-04 - val_loss: 2.7345e-04 - val_mse: 8.9293e-05 - val_NMSE: 8.0721e-04\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4164e-04 - mse: 5.7580e-05 - NMSE: 5.2053e-04 - tot_time: 0h 54m 3.5s\n",
      "\n",
      "Epoch 108: val_NMSE improved from 0.00080 to 0.00080, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.4164e-04 - mse: 5.7580e-05 - NMSE: 5.2053e-04 - val_loss: 2.7201e-04 - val_mse: 8.8061e-05 - val_NMSE: 7.9608e-04\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4055e-04 - mse: 5.6701e-05 - NMSE: 5.1258e-04 - tot_time: 0h 54m 22.9s\n",
      "\n",
      "Epoch 109: val_NMSE improved from 0.00080 to 0.00079, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.4055e-04 - mse: 5.6701e-05 - NMSE: 5.1258e-04 - val_loss: 2.7088e-04 - val_mse: 8.7162e-05 - val_NMSE: 7.8795e-04\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3961e-04 - mse: 5.6003e-05 - NMSE: 5.0628e-04 - tot_time: 0h 54m 42.6s\n",
      "\n",
      "Epoch 110: val_NMSE improved from 0.00079 to 0.00078, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.3961e-04 - mse: 5.6003e-05 - NMSE: 5.0628e-04 - val_loss: 2.6964e-04 - val_mse: 8.6170e-05 - val_NMSE: 7.7898e-04\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3862e-04 - mse: 5.5266e-05 - NMSE: 4.9961e-04 - tot_time: 0h 55m 2.2s\n",
      "\n",
      "Epoch 111: val_NMSE improved from 0.00078 to 0.00077, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.3862e-04 - mse: 5.5266e-05 - NMSE: 4.9961e-04 - val_loss: 2.6847e-04 - val_mse: 8.5246e-05 - val_NMSE: 7.7063e-04\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3791e-04 - mse: 5.4808e-05 - NMSE: 4.9547e-04 - tot_time: 0h 55m 21.9s\n",
      "\n",
      "Epoch 112: val_NMSE improved from 0.00077 to 0.00076, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.3791e-04 - mse: 5.4808e-05 - NMSE: 4.9547e-04 - val_loss: 2.6716e-04 - val_mse: 8.4203e-05 - val_NMSE: 7.6120e-04\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3722e-04 - mse: 5.4379e-05 - NMSE: 4.9159e-04 - tot_time: 0h 55m 41.2s\n",
      "\n",
      "Epoch 113: val_NMSE improved from 0.00076 to 0.00075, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.3722e-04 - mse: 5.4379e-05 - NMSE: 4.9159e-04 - val_loss: 2.6608e-04 - val_mse: 8.3377e-05 - val_NMSE: 7.5373e-04\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4370e-04 - mse: 6.1121e-05 - NMSE: 5.5251e-04 - tot_time: 0h 56m 0.9s\n",
      "\n",
      "Epoch 114: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4370e-04 - mse: 6.1121e-05 - NMSE: 5.5251e-04 - val_loss: 3.2576e-04 - val_mse: 1.4334e-04 - val_NMSE: 0.0013\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8550e-04 - mse: 1.0330e-04 - NMSE: 9.3361e-04 - tot_time: 0h 56m 20.4s\n",
      "\n",
      "Epoch 115: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.8550e-04 - mse: 1.0330e-04 - NMSE: 9.3361e-04 - val_loss: 2.8212e-04 - val_mse: 1.0013e-04 - val_NMSE: 9.0513e-04\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4191e-04 - mse: 6.0043e-05 - NMSE: 5.4276e-04 - tot_time: 0h 56m 40.0s\n",
      "\n",
      "Epoch 116: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4191e-04 - mse: 6.0043e-05 - NMSE: 5.4276e-04 - val_loss: 2.6754e-04 - val_mse: 8.5815e-05 - val_NMSE: 7.7575e-04\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3524e-04 - mse: 5.3616e-05 - NMSE: 4.8469e-04 - tot_time: 0h 56m 59.7s\n",
      "\n",
      "Epoch 117: val_NMSE improved from 0.00075 to 0.00073, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.3524e-04 - mse: 5.3616e-05 - NMSE: 4.8469e-04 - val_loss: 2.6258e-04 - val_mse: 8.1086e-05 - val_NMSE: 7.3302e-04\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3368e-04 - mse: 5.2282e-05 - NMSE: 4.7262e-04 - tot_time: 0h 57m 19.4s\n",
      "\n",
      "Epoch 118: val_NMSE improved from 0.00073 to 0.00072, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.3368e-04 - mse: 5.2282e-05 - NMSE: 4.7262e-04 - val_loss: 2.6098e-04 - val_mse: 7.9713e-05 - val_NMSE: 7.2061e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5699e-04 - mse: 7.5837e-05 - NMSE: 6.8549e-04 - tot_time: 0h 57m 39.0s\n",
      "\n",
      "Epoch 119: val_NMSE did not improve from 0.00072\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5699e-04 - mse: 7.5837e-05 - NMSE: 6.8549e-04 - val_loss: 4.2084e-04 - val_mse: 2.3985e-04 - val_NMSE: 0.0022\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7118e-04 - mse: 9.0389e-05 - NMSE: 8.1699e-04 - tot_time: 0h 57m 58.5s\n",
      "\n",
      "Epoch 120: val_NMSE did not improve from 0.00072\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.7118e-04 - mse: 9.0389e-05 - NMSE: 8.1699e-04 - val_loss: 2.6114e-04 - val_mse: 8.0543e-05 - val_NMSE: 7.2811e-04\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3387e-04 - mse: 5.3367e-05 - NMSE: 4.8244e-04 - tot_time: 0h 58m 18.1s\n",
      "\n",
      "Epoch 121: val_NMSE improved from 0.00072 to 0.00072, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.3387e-04 - mse: 5.3367e-05 - NMSE: 4.8244e-04 - val_loss: 2.5952e-04 - val_mse: 7.9143e-05 - val_NMSE: 7.1544e-04\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5006e-04 - mse: 6.9784e-05 - NMSE: 6.3086e-04 - tot_time: 0h 58m 37.8s\n",
      "\n",
      "Epoch 122: val_NMSE did not improve from 0.00072\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.5006e-04 - mse: 6.9784e-05 - NMSE: 6.3086e-04 - val_loss: 5.0716e-04 - val_mse: 3.2702e-04 - val_NMSE: 0.0030\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4469e-04 - mse: 1.6490e-04 - NMSE: 0.0015 - tot_time: 0h 58m 57.5s\n",
      "\n",
      "Epoch 123: val_NMSE did not improve from 0.00072\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.4469e-04 - mse: 1.6490e-04 - NMSE: 0.0015 - val_loss: 2.7094e-04 - val_mse: 9.1423e-05 - val_NMSE: 8.2647e-04\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4306e-04 - mse: 6.3633e-05 - NMSE: 5.7525e-04 - tot_time: 0h 59m 17.3s\n",
      "\n",
      "Epoch 124: val_NMSE did not improve from 0.00072\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.4306e-04 - mse: 6.3633e-05 - NMSE: 5.7525e-04 - val_loss: 2.6505e-04 - val_mse: 8.5735e-05 - val_NMSE: 7.7504e-04\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3151e-04 - mse: 5.2267e-05 - NMSE: 4.7249e-04 - tot_time: 0h 59m 36.8s\n",
      "\n",
      "Epoch 125: val_NMSE improved from 0.00072 to 0.00071, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.3151e-04 - mse: 5.2267e-05 - NMSE: 4.7249e-04 - val_loss: 2.5742e-04 - val_mse: 7.8257e-05 - val_NMSE: 7.0744e-04\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2910e-04 - mse: 5.0014e-05 - NMSE: 4.5213e-04 - tot_time: 0h 59m 56.7s\n",
      "\n",
      "Epoch 126: val_NMSE improved from 0.00071 to 0.00070, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.2910e-04 - mse: 5.0014e-05 - NMSE: 4.5213e-04 - val_loss: 2.5616e-04 - val_mse: 7.7165e-05 - val_NMSE: 6.9757e-04\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2793e-04 - mse: 4.9028e-05 - NMSE: 4.4321e-04 - tot_time: 1h 0m 16.2s\n",
      "\n",
      "Epoch 127: val_NMSE improved from 0.00070 to 0.00068, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.2793e-04 - mse: 4.9028e-05 - NMSE: 4.4321e-04 - val_loss: 2.5455e-04 - val_mse: 7.5765e-05 - val_NMSE: 6.8491e-04\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2706e-04 - mse: 4.8368e-05 - NMSE: 4.3725e-04 - tot_time: 1h 0m 36.1s\n",
      "\n",
      "Epoch 128: val_NMSE did not improve from 0.00068\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.2706e-04 - mse: 4.8368e-05 - NMSE: 4.3725e-04 - val_loss: 2.5441e-04 - val_mse: 7.5846e-05 - val_NMSE: 6.8564e-04\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2796e-04 - mse: 4.9500e-05 - NMSE: 4.4747e-04 - tot_time: 1h 0m 55.5s\n",
      "\n",
      "Epoch 129: val_NMSE did not improve from 0.00068\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.2796e-04 - mse: 4.9500e-05 - NMSE: 4.4747e-04 - val_loss: 2.6319e-04 - val_mse: 8.4857e-05 - val_NMSE: 7.6707e-04\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7647e-04 - mse: 9.8294e-05 - NMSE: 8.8840e-04 - tot_time: 1h 1m 15.1s\n",
      "\n",
      "Epoch 130: val_NMSE did not improve from 0.00068\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.7647e-04 - mse: 9.8294e-05 - NMSE: 8.8840e-04 - val_loss: 2.5944e-04 - val_mse: 8.1493e-05 - val_NMSE: 7.3667e-04\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3448e-04 - mse: 5.6661e-05 - NMSE: 5.1218e-04 - tot_time: 1h 1m 34.5s\n",
      "\n",
      "Epoch 131: val_NMSE did not improve from 0.00068\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.3448e-04 - mse: 5.6661e-05 - NMSE: 5.1218e-04 - val_loss: 2.5470e-04 - val_mse: 7.7023e-05 - val_NMSE: 6.9627e-04\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2600e-04 - mse: 4.8425e-05 - NMSE: 4.3775e-04 - tot_time: 1h 1m 54.2s\n",
      "\n",
      "Epoch 132: val_NMSE improved from 0.00068 to 0.00067, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.2600e-04 - mse: 4.8425e-05 - NMSE: 4.3775e-04 - val_loss: 2.5148e-04 - val_mse: 7.4028e-05 - val_NMSE: 6.6920e-04\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4302e-04 - mse: 1.6567e-04 - NMSE: 0.0015   - tot_time: 1h 2m 13.6s\n",
      "\n",
      "Epoch 133: val_NMSE did not improve from 0.00067\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.4302e-04 - mse: 1.6567e-04 - NMSE: 0.0015 - val_loss: 8.2315e-04 - val_mse: 6.4602e-04 - val_NMSE: 0.0058\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4147e-04 - mse: 1.6479e-04 - NMSE: 0.0015 - tot_time: 1h 2m 33.0s\n",
      "\n",
      "Epoch 134: val_NMSE did not improve from 0.00067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.4147e-04 - mse: 1.6479e-04 - NMSE: 0.0015 - val_loss: 2.7297e-04 - val_mse: 9.6510e-05 - val_NMSE: 8.7245e-04\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3605e-04 - mse: 5.9660e-05 - NMSE: 5.3933e-04 - tot_time: 1h 2m 52.4s\n",
      "\n",
      "Epoch 135: val_NMSE did not improve from 0.00067\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.3605e-04 - mse: 5.9660e-05 - NMSE: 5.3933e-04 - val_loss: 2.5528e-04 - val_mse: 7.8965e-05 - val_NMSE: 7.1384e-04\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2596e-04 - mse: 4.9696e-05 - NMSE: 4.4925e-04 - tot_time: 1h 3m 11.7s\n",
      "\n",
      "Epoch 136: val_NMSE did not improve from 0.00067\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.2596e-04 - mse: 4.9696e-05 - NMSE: 4.4925e-04 - val_loss: 2.5203e-04 - val_mse: 7.5815e-05 - val_NMSE: 6.8536e-04\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2390e-04 - mse: 4.7745e-05 - NMSE: 4.3161e-04 - tot_time: 1h 3m 31.2s\n",
      "\n",
      "Epoch 137: val_NMSE improved from 0.00067 to 0.00067, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.2390e-04 - mse: 4.7745e-05 - NMSE: 4.3161e-04 - val_loss: 2.5002e-04 - val_mse: 7.3951e-05 - val_NMSE: 6.6851e-04\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2249e-04 - mse: 4.6511e-05 - NMSE: 4.2046e-04 - tot_time: 1h 3m 50.4s\n",
      "\n",
      "Epoch 138: val_NMSE improved from 0.00067 to 0.00065, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.2249e-04 - mse: 4.6511e-05 - NMSE: 4.2046e-04 - val_loss: 2.4819e-04 - val_mse: 7.2313e-05 - val_NMSE: 6.5371e-04\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2124e-04 - mse: 4.5458e-05 - NMSE: 4.1094e-04 - tot_time: 1h 4m 9.8s\n",
      "\n",
      "Epoch 139: val_NMSE improved from 0.00065 to 0.00064, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.2124e-04 - mse: 4.5458e-05 - NMSE: 4.1094e-04 - val_loss: 2.4671e-04 - val_mse: 7.1049e-05 - val_NMSE: 6.4227e-04\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2009e-04 - mse: 4.4528e-05 - NMSE: 4.0253e-04 - tot_time: 1h 4m 29.3s\n",
      "\n",
      "Epoch 140: val_NMSE improved from 0.00064 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.2009e-04 - mse: 4.4528e-05 - NMSE: 4.0253e-04 - val_loss: 2.4519e-04 - val_mse: 6.9750e-05 - val_NMSE: 6.3053e-04\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1910e-04 - mse: 4.3766e-05 - NMSE: 3.9564e-04 - tot_time: 1h 4m 48.6s\n",
      "\n",
      "Epoch 141: val_NMSE improved from 0.00063 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.1910e-04 - mse: 4.3766e-05 - NMSE: 3.9564e-04 - val_loss: 2.4425e-04 - val_mse: 6.9050e-05 - val_NMSE: 6.2420e-04\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1808e-04 - mse: 4.2987e-05 - NMSE: 3.8859e-04 - tot_time: 1h 5m 7.9s\n",
      "\n",
      "Epoch 142: val_NMSE improved from 0.00062 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.1808e-04 - mse: 4.2987e-05 - NMSE: 3.8859e-04 - val_loss: 2.4237e-04 - val_mse: 6.7420e-05 - val_NMSE: 6.0947e-04\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2380e-04 - mse: 4.8969e-05 - NMSE: 4.4264e-04 - tot_time: 1h 5m 27.3s\n",
      "\n",
      "Epoch 143: val_NMSE did not improve from 0.00061\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.2380e-04 - mse: 4.8969e-05 - NMSE: 4.4264e-04 - val_loss: 2.8172e-04 - val_mse: 1.0704e-04 - val_NMSE: 9.6746e-04\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5218e-04 - mse: 7.7680e-05 - NMSE: 7.0209e-04 - tot_time: 1h 5m 46.7s\n",
      "\n",
      "Epoch 144: val_NMSE did not improve from 0.00061\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.5218e-04 - mse: 7.7680e-05 - NMSE: 7.0209e-04 - val_loss: 2.4997e-04 - val_mse: 7.5669e-05 - val_NMSE: 6.8401e-04\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1904e-04 - mse: 4.4867e-05 - NMSE: 4.0558e-04 - tot_time: 1h 6m 5.8s\n",
      "\n",
      "Epoch 145: val_NMSE improved from 0.00061 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.1904e-04 - mse: 4.4867e-05 - NMSE: 4.0558e-04 - val_loss: 2.3989e-04 - val_mse: 6.5866e-05 - val_NMSE: 5.9541e-04\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1534e-04 - mse: 4.1427e-05 - NMSE: 3.7449e-04 - tot_time: 1h 6m 25.1s\n",
      "\n",
      "Epoch 146: val_NMSE improved from 0.00060 to 0.00058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.1534e-04 - mse: 4.1427e-05 - NMSE: 3.7449e-04 - val_loss: 2.3846e-04 - val_mse: 6.4681e-05 - val_NMSE: 5.8470e-04\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0627e-04 - mse: 1.3262e-04 - NMSE: 0.0012     - tot_time: 1h 6m 44.4s\n",
      "\n",
      "Epoch 147: val_NMSE did not improve from 0.00058\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.0627e-04 - mse: 1.3262e-04 - NMSE: 0.0012 - val_loss: 4.9624e-04 - val_mse: 3.2282e-04 - val_NMSE: 0.0029\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8698e-04 - mse: 1.1393e-04 - NMSE: 0.0010 - tot_time: 1h 7m 3.6s\n",
      "\n",
      "Epoch 148: val_NMSE did not improve from 0.00058\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.8698e-04 - mse: 1.1393e-04 - NMSE: 0.0010 - val_loss: 2.4482e-04 - val_mse: 7.2004e-05 - val_NMSE: 6.5089e-04\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 2.2195e-04 - mse: 4.9251e-05 - NMSE: 4.4520e-04 - tot_time: 1h 7m 22.9s\n",
      "\n",
      "Epoch 149: val_NMSE did not improve from 0.00058\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.2195e-04 - mse: 4.9251e-05 - NMSE: 4.4520e-04 - val_loss: 2.4141e-04 - val_mse: 6.8821e-05 - val_NMSE: 6.2213e-04\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1487e-04 - mse: 4.2358e-05 - NMSE: 3.8291e-04 - tot_time: 1h 7m 42.4s\n",
      "\n",
      "Epoch 150: val_NMSE did not improve from 0.00058\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.1487e-04 - mse: 4.2358e-05 - NMSE: 3.8291e-04 - val_loss: 2.3720e-04 - val_mse: 6.4770e-05 - val_NMSE: 5.8551e-04\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1263e-04 - mse: 4.0291e-05 - NMSE: 3.6422e-04 - tot_time: 1h 8m 1.4s\n",
      "\n",
      "Epoch 151: val_NMSE improved from 0.00058 to 0.00057, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.1263e-04 - mse: 4.0291e-05 - NMSE: 3.6422e-04 - val_loss: 2.3575e-04 - val_mse: 6.3526e-05 - val_NMSE: 5.7426e-04\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1170e-04 - mse: 3.9577e-05 - NMSE: 3.5777e-04 - tot_time: 1h 8m 22.6s\n",
      "\n",
      "Epoch 152: val_NMSE improved from 0.00057 to 0.00057, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.1170e-04 - mse: 3.9577e-05 - NMSE: 3.5777e-04 - val_loss: 2.3458e-04 - val_mse: 6.2587e-05 - val_NMSE: 5.6577e-04\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1770e-04 - mse: 4.5821e-05 - NMSE: 4.1422e-04 - tot_time: 1h 8m 43.9s\n",
      "\n",
      "Epoch 153: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.1770e-04 - mse: 4.5821e-05 - NMSE: 4.1422e-04 - val_loss: 3.1029e-04 - val_mse: 1.3856e-04 - val_NMSE: 0.0013\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1859e-04 - mse: 1.4711e-04 - NMSE: 0.0013 - tot_time: 1h 9m 5.0s\n",
      "\n",
      "Epoch 154: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 3.1859e-04 - mse: 1.4711e-04 - NMSE: 0.0013 - val_loss: 2.5035e-04 - val_mse: 7.9126e-05 - val_NMSE: 7.1530e-04\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2088e-04 - mse: 4.9776e-05 - NMSE: 4.4997e-04 - tot_time: 1h 9m 25.8s\n",
      "\n",
      "Epoch 155: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.2088e-04 - mse: 4.9776e-05 - NMSE: 4.4997e-04 - val_loss: 2.3869e-04 - val_mse: 6.7706e-05 - val_NMSE: 6.1205e-04\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1126e-04 - mse: 4.0382e-05 - NMSE: 3.6504e-04 - tot_time: 1h 9m 46.3s\n",
      "\n",
      "Epoch 156: val_NMSE improved from 0.00057 to 0.00056, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.1126e-04 - mse: 4.0382e-05 - NMSE: 3.6504e-04 - val_loss: 2.3235e-04 - val_mse: 6.1587e-05 - val_NMSE: 5.5673e-04\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0922e-04 - mse: 3.8556e-05 - NMSE: 3.4853e-04 - tot_time: 1h 10m 7.0s\n",
      "\n",
      "Epoch 157: val_NMSE improved from 0.00056 to 0.00055, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0922e-04 - mse: 3.8556e-05 - NMSE: 3.4853e-04 - val_loss: 2.3104e-04 - val_mse: 6.0497e-05 - val_NMSE: 5.4688e-04\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0787e-04 - mse: 3.7430e-05 - NMSE: 3.3836e-04 - tot_time: 1h 10m 27.5s\n",
      "\n",
      "Epoch 158: val_NMSE improved from 0.00055 to 0.00054, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.0787e-04 - mse: 3.7430e-05 - NMSE: 3.3836e-04 - val_loss: 2.2982e-04 - val_mse: 5.9517e-05 - val_NMSE: 5.3802e-04\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0718e-04 - mse: 3.6998e-05 - NMSE: 3.3445e-04 - tot_time: 1h 10m 48.2s\n",
      "\n",
      "Epoch 159: val_NMSE improved from 0.00054 to 0.00054, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0718e-04 - mse: 3.6998e-05 - NMSE: 3.3445e-04 - val_loss: 2.2949e-04 - val_mse: 5.9449e-05 - val_NMSE: 5.3740e-04\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1246e-04 - mse: 4.2550e-05 - NMSE: 3.8464e-04 - tot_time: 1h 11m 9.7s\n",
      "\n",
      "Epoch 160: val_NMSE did not improve from 0.00054\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.1246e-04 - mse: 4.2550e-05 - NMSE: 3.8464e-04 - val_loss: 2.6088e-04 - val_mse: 9.1133e-05 - val_NMSE: 8.2386e-04\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9107e-04 - mse: 1.2155e-04 - NMSE: 0.0011 - tot_time: 1h 11m 30.9s\n",
      "\n",
      "Epoch 161: val_NMSE did not improve from 0.00054\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.9107e-04 - mse: 1.2155e-04 - NMSE: 0.0011 - val_loss: 2.3870e-04 - val_mse: 6.9481e-05 - val_NMSE: 6.2810e-04\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1508e-04 - mse: 4.5996e-05 - NMSE: 4.1580e-04 - tot_time: 1h 11m 51.9s\n",
      "\n",
      "Epoch 162: val_NMSE did not improve from 0.00054\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.1508e-04 - mse: 4.5996e-05 - NMSE: 4.1580e-04 - val_loss: 2.2942e-04 - val_mse: 6.0487e-05 - val_NMSE: 5.4678e-04\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0611e-04 - mse: 3.7273e-05 - NMSE: 3.3694e-04 - tot_time: 1h 12m 12.9s\n",
      "\n",
      "Epoch 163: val_NMSE improved from 0.00054 to 0.00053, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0611e-04 - mse: 3.7273e-05 - NMSE: 3.3694e-04 - val_loss: 2.2784e-04 - val_mse: 5.9128e-05 - val_NMSE: 5.3450e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0471e-04 - mse: 3.6114e-05 - NMSE: 3.2646e-04 - tot_time: 1h 12m 33.9s\n",
      "\n",
      "Epoch 164: val_NMSE improved from 0.00053 to 0.00052, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0471e-04 - mse: 3.6114e-05 - NMSE: 3.2646e-04 - val_loss: 2.2641e-04 - val_mse: 5.7949e-05 - val_NMSE: 5.2385e-04\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0369e-04 - mse: 3.5360e-05 - NMSE: 3.1964e-04 - tot_time: 1h 12m 54.7s\n",
      "\n",
      "Epoch 165: val_NMSE improved from 0.00052 to 0.00050, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0369e-04 - mse: 3.5360e-05 - NMSE: 3.1964e-04 - val_loss: 2.2404e-04 - val_mse: 5.5853e-05 - val_NMSE: 5.0490e-04\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0565e-04 - mse: 3.7596e-05 - NMSE: 3.3984e-04 - tot_time: 1h 13m 15.9s\n",
      "\n",
      "Epoch 166: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0565e-04 - mse: 3.7596e-05 - NMSE: 3.3984e-04 - val_loss: 2.3348e-04 - val_mse: 6.5593e-05 - val_NMSE: 5.9290e-04\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5864e-04 - mse: 1.9102e-04 - NMSE: 0.0017 - tot_time: 1h 13m 36.9s\n",
      "\n",
      "Epoch 167: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 3.5864e-04 - mse: 1.9102e-04 - NMSE: 0.0017 - val_loss: 2.5457e-04 - val_mse: 8.7379e-05 - val_NMSE: 7.8980e-04\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3052e-04 - mse: 6.3531e-05 - NMSE: 5.7423e-04 - tot_time: 1h 13m 57.9s\n",
      "\n",
      "Epoch 168: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.3052e-04 - mse: 6.3531e-05 - NMSE: 5.7423e-04 - val_loss: 2.2727e-04 - val_mse: 6.0434e-05 - val_NMSE: 5.4630e-04\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0593e-04 - mse: 3.9174e-05 - NMSE: 3.5412e-04 - tot_time: 1h 14m 19.1s\n",
      "\n",
      "Epoch 169: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0593e-04 - mse: 3.9174e-05 - NMSE: 3.5412e-04 - val_loss: 2.2493e-04 - val_mse: 5.8274e-05 - val_NMSE: 5.2677e-04\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0258e-04 - mse: 3.6004e-05 - NMSE: 3.2546e-04 - tot_time: 1h 14m 40.2s\n",
      "\n",
      "Epoch 170: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0258e-04 - mse: 3.6004e-05 - NMSE: 3.2546e-04 - val_loss: 2.2294e-04 - val_mse: 5.6474e-05 - val_NMSE: 5.1050e-04\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0103e-04 - mse: 3.4664e-05 - NMSE: 3.1335e-04 - tot_time: 1h 15m 1.2s\n",
      "\n",
      "Epoch 171: val_NMSE improved from 0.00050 to 0.00050, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0103e-04 - mse: 3.4664e-05 - NMSE: 3.1335e-04 - val_loss: 2.2166e-04 - val_mse: 5.5425e-05 - val_NMSE: 5.0103e-04\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0024e-04 - mse: 3.4122e-05 - NMSE: 3.0845e-04 - tot_time: 1h 15m 22.0s\n",
      "\n",
      "Epoch 172: val_NMSE improved from 0.00050 to 0.00050, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0024e-04 - mse: 3.4122e-05 - NMSE: 3.0845e-04 - val_loss: 2.2077e-04 - val_mse: 5.4797e-05 - val_NMSE: 4.9535e-04\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9944e-04 - mse: 3.3594e-05 - NMSE: 3.0368e-04 - tot_time: 1h 15m 43.5s\n",
      "\n",
      "Epoch 173: val_NMSE improved from 0.00050 to 0.00049, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.9944e-04 - mse: 3.3594e-05 - NMSE: 3.0368e-04 - val_loss: 2.1950e-04 - val_mse: 5.3797e-05 - val_NMSE: 4.8630e-04\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9894e-04 - mse: 3.3374e-05 - NMSE: 3.0169e-04 - tot_time: 1h 16m 4.8s\n",
      "\n",
      "Epoch 174: val_NMSE did not improve from 0.00049\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9894e-04 - mse: 3.3374e-05 - NMSE: 3.0169e-04 - val_loss: 2.2031e-04 - val_mse: 5.4905e-05 - val_NMSE: 4.9633e-04\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3292e-04 - mse: 1.6768e-04 - NMSE: 0.0015 - tot_time: 1h 16m 25.8s\n",
      "\n",
      "Epoch 175: val_NMSE did not improve from 0.00049\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 3.3292e-04 - mse: 1.6768e-04 - NMSE: 0.0015 - val_loss: 4.1146e-04 - val_mse: 2.4654e-04 - val_NMSE: 0.0022\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5184e-04 - mse: 8.7111e-05 - NMSE: 7.8749e-04 - tot_time: 1h 16m 47.1s\n",
      "\n",
      "Epoch 176: val_NMSE did not improve from 0.00049\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.5184e-04 - mse: 8.7111e-05 - NMSE: 7.8749e-04 - val_loss: 2.2310e-04 - val_mse: 5.8532e-05 - val_NMSE: 5.2911e-04\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0465e-04 - mse: 4.0167e-05 - NMSE: 3.6310e-04 - tot_time: 1h 17m 8.2s\n",
      "\n",
      "Epoch 177: val_NMSE did not improve from 0.00049\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0465e-04 - mse: 4.0167e-05 - NMSE: 3.6310e-04 - val_loss: 2.1983e-04 - val_mse: 5.5464e-05 - val_NMSE: 5.0138e-04\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9881e-04 - mse: 3.4538e-05 - NMSE: 3.1221e-04 - tot_time: 1h 17m 29.6s\n",
      "\n",
      "Epoch 178: val_NMSE did not improve from 0.00049\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9881e-04 - mse: 3.4538e-05 - NMSE: 3.1221e-04 - val_loss: 2.1839e-04 - val_mse: 5.4239e-05 - val_NMSE: 4.9030e-04\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9725e-04 - mse: 3.3215e-05 - NMSE: 3.0025e-04 - tot_time: 1h 17m 50.6s\n",
      "\n",
      "Epoch 179: val_NMSE improved from 0.00049 to 0.00048, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9725e-04 - mse: 3.3215e-05 - NMSE: 3.0025e-04 - val_loss: 2.1736e-04 - val_mse: 5.3465e-05 - val_NMSE: 4.8330e-04\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9649e-04 - mse: 3.2722e-05 - NMSE: 2.9579e-04 - tot_time: 1h 18m 11.6s\n",
      "\n",
      "Epoch 180: val_NMSE improved from 0.00048 to 0.00047, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9649e-04 - mse: 3.2722e-05 - NMSE: 2.9579e-04 - val_loss: 2.1613e-04 - val_mse: 5.2506e-05 - val_NMSE: 4.7464e-04\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9567e-04 - mse: 3.2183e-05 - NMSE: 2.9092e-04 - tot_time: 1h 18m 33.1s\n",
      "\n",
      "Epoch 181: val_NMSE improved from 0.00047 to 0.00047, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.9567e-04 - mse: 3.2183e-05 - NMSE: 2.9092e-04 - val_loss: 2.1522e-04 - val_mse: 5.1891e-05 - val_NMSE: 4.6908e-04\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9490e-04 - mse: 3.1706e-05 - NMSE: 2.8661e-04 - tot_time: 1h 18m 54.3s\n",
      "\n",
      "Epoch 182: val_NMSE improved from 0.00047 to 0.00046, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9490e-04 - mse: 3.1706e-05 - NMSE: 2.8661e-04 - val_loss: 2.1416e-04 - val_mse: 5.1136e-05 - val_NMSE: 4.6225e-04\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9425e-04 - mse: 3.1374e-05 - NMSE: 2.8361e-04 - tot_time: 1h 19m 15.4s\n",
      "\n",
      "Epoch 183: val_NMSE improved from 0.00046 to 0.00046, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9425e-04 - mse: 3.1374e-05 - NMSE: 2.8361e-04 - val_loss: 2.1372e-04 - val_mse: 5.1018e-05 - val_NMSE: 4.6119e-04\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9497e-04 - mse: 3.2409e-05 - NMSE: 2.9297e-04 - tot_time: 1h 19m 36.6s\n",
      "\n",
      "Epoch 184: val_NMSE did not improve from 0.00046\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9497e-04 - mse: 3.2409e-05 - NMSE: 2.9297e-04 - val_loss: 2.2230e-04 - val_mse: 5.9927e-05 - val_NMSE: 5.4170e-04\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1413e-04 - mse: 1.5202e-04 - NMSE: 0.0014 - tot_time: 1h 19m 57.8s\n",
      "\n",
      "Epoch 185: val_NMSE did not improve from 0.00046\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 3.1413e-04 - mse: 1.5202e-04 - NMSE: 0.0014 - val_loss: 2.1908e-04 - val_mse: 5.7385e-05 - val_NMSE: 5.1874e-04\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1193e-04 - mse: 5.0410e-05 - NMSE: 4.5567e-04 - tot_time: 1h 20m 18.9s\n",
      "\n",
      "Epoch 186: val_NMSE did not improve from 0.00046\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.1193e-04 - mse: 5.0410e-05 - NMSE: 4.5567e-04 - val_loss: 2.1546e-04 - val_mse: 5.4117e-05 - val_NMSE: 4.8920e-04\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9686e-04 - mse: 3.5625e-05 - NMSE: 3.2203e-04 - tot_time: 1h 20m 39.2s\n",
      "\n",
      "Epoch 187: val_NMSE did not improve from 0.00046\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 1.9686e-04 - mse: 3.5625e-05 - NMSE: 3.2203e-04 - val_loss: 2.1328e-04 - val_mse: 5.2175e-05 - val_NMSE: 4.7164e-04\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9354e-04 - mse: 3.2529e-05 - NMSE: 2.9405e-04 - tot_time: 1h 21m 0.2s\n",
      "\n",
      "Epoch 188: val_NMSE did not improve from 0.00046\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9354e-04 - mse: 3.2529e-05 - NMSE: 2.9405e-04 - val_loss: 2.1295e-04 - val_mse: 5.2078e-05 - val_NMSE: 4.7076e-04\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9206e-04 - mse: 3.1313e-05 - NMSE: 2.8306e-04 - tot_time: 1h 21m 21.4s\n",
      "\n",
      "Epoch 189: val_NMSE improved from 0.00046 to 0.00046, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9206e-04 - mse: 3.1313e-05 - NMSE: 2.8306e-04 - val_loss: 2.1115e-04 - val_mse: 5.0553e-05 - val_NMSE: 4.5698e-04\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9133e-04 - mse: 3.0866e-05 - NMSE: 2.7902e-04 - tot_time: 1h 21m 42.9s\n",
      "\n",
      "Epoch 190: val_NMSE improved from 0.00046 to 0.00045, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.9133e-04 - mse: 3.0866e-05 - NMSE: 2.7902e-04 - val_loss: 2.1023e-04 - val_mse: 4.9921e-05 - val_NMSE: 4.5127e-04\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9078e-04 - mse: 3.0611e-05 - NMSE: 2.7671e-04 - tot_time: 1h 22m 4.1s\n",
      "\n",
      "Epoch 191: val_NMSE improved from 0.00045 to 0.00045, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9078e-04 - mse: 3.0611e-05 - NMSE: 2.7671e-04 - val_loss: 2.0939e-04 - val_mse: 4.9387e-05 - val_NMSE: 4.4644e-04\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2011e-04 - mse: 6.0266e-05 - NMSE: 5.4479e-04 - tot_time: 1h 22m 24.8s\n",
      "\n",
      "Epoch 192: val_NMSE did not improve from 0.00045\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.2011e-04 - mse: 6.0266e-05 - NMSE: 5.4479e-04 - val_loss: 3.6956e-04 - val_mse: 2.0991e-04 - val_NMSE: 0.0019\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2982e-04 - mse: 7.0364e-05 - NMSE: 6.3609e-04 - tot_time: 1h 22m 45.9s\n",
      "\n",
      "Epoch 193: val_NMSE did not improve from 0.00045\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.2982e-04 - mse: 7.0364e-05 - NMSE: 6.3609e-04 - val_loss: 2.1772e-04 - val_mse: 5.8469e-05 - val_NMSE: 5.2855e-04\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.9336e-04 - mse: 3.4249e-05 - NMSE: 3.0960e-04 - tot_time: 1h 23m 7.1s\n",
      "\n",
      "Epoch 194: val_NMSE did not improve from 0.00045\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9336e-04 - mse: 3.4249e-05 - NMSE: 3.0960e-04 - val_loss: 2.0858e-04 - val_mse: 4.9625e-05 - val_NMSE: 4.4859e-04\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8900e-04 - mse: 3.0182e-05 - NMSE: 2.7284e-04 - tot_time: 1h 23m 28.6s\n",
      "\n",
      "Epoch 195: val_NMSE improved from 0.00045 to 0.00044, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8900e-04 - mse: 3.0182e-05 - NMSE: 2.7284e-04 - val_loss: 2.0782e-04 - val_mse: 4.9167e-05 - val_NMSE: 4.4445e-04\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8836e-04 - mse: 2.9842e-05 - NMSE: 2.6976e-04 - tot_time: 1h 23m 49.8s\n",
      "\n",
      "Epoch 196: val_NMSE improved from 0.00044 to 0.00044, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8836e-04 - mse: 2.9842e-05 - NMSE: 2.6976e-04 - val_loss: 2.0675e-04 - val_mse: 4.8399e-05 - val_NMSE: 4.3751e-04\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0686e-04 - mse: 4.8661e-05 - NMSE: 4.3983e-04 - tot_time: 1h 24m 10.7s\n",
      "\n",
      "Epoch 197: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0686e-04 - mse: 4.8661e-05 - NMSE: 4.3983e-04 - val_loss: 4.2845e-04 - val_mse: 2.7044e-04 - val_NMSE: 0.0024\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2785e-04 - mse: 1.7019e-04 - NMSE: 0.0015 - tot_time: 1h 24m 32.0s\n",
      "\n",
      "Epoch 198: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 3.2785e-04 - mse: 1.7019e-04 - NMSE: 0.0015 - val_loss: 2.1619e-04 - val_mse: 5.8838e-05 - val_NMSE: 5.3187e-04\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0117e-04 - mse: 4.3947e-05 - NMSE: 3.9725e-04 - tot_time: 1h 24m 53.0s\n",
      "\n",
      "Epoch 199: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 2.0117e-04 - mse: 4.3947e-05 - NMSE: 3.9725e-04 - val_loss: 2.1033e-04 - val_mse: 5.3242e-05 - val_NMSE: 4.8128e-04\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9028e-04 - mse: 3.3270e-05 - NMSE: 3.0075e-04 - tot_time: 1h 25m 14.0s\n",
      "\n",
      "Epoch 200: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.9028e-04 - mse: 3.3270e-05 - NMSE: 3.0075e-04 - val_loss: 2.0703e-04 - val_mse: 5.0129e-05 - val_NMSE: 4.5315e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.0001 ----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8717e-04 - mse: 3.0287e-05 - NMSE: 2.7378e-04 - tot_time: 1h 25m 35.6s\n",
      "\n",
      "Epoch 1: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8717e-04 - mse: 3.0287e-05 - NMSE: 2.7378e-04 - val_loss: 2.0593e-04 - val_mse: 4.9058e-05 - val_NMSE: 4.4347e-04\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8675e-04 - mse: 2.9890e-05 - NMSE: 2.7020e-04 - tot_time: 1h 25m 57.1s\n",
      "\n",
      "Epoch 2: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8675e-04 - mse: 2.9890e-05 - NMSE: 2.7020e-04 - val_loss: 2.0576e-04 - val_mse: 4.8914e-05 - val_NMSE: 4.4216e-04\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8665e-04 - mse: 2.9808e-05 - NMSE: 2.6945e-04 - tot_time: 1h 26m 18.1s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8665e-04 - mse: 2.9808e-05 - NMSE: 2.6945e-04 - val_loss: 2.0560e-04 - val_mse: 4.8775e-05 - val_NMSE: 4.4090e-04\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8650e-04 - mse: 2.9682e-05 - NMSE: 2.6832e-04 - tot_time: 1h 26m 39.1s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8650e-04 - mse: 2.9682e-05 - NMSE: 2.6832e-04 - val_loss: 2.0549e-04 - val_mse: 4.8694e-05 - val_NMSE: 4.4017e-04\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8640e-04 - mse: 2.9612e-05 - NMSE: 2.6768e-04 - tot_time: 1h 27m 0.4s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8640e-04 - mse: 2.9612e-05 - NMSE: 2.6768e-04 - val_loss: 2.0535e-04 - val_mse: 4.8577e-05 - val_NMSE: 4.3912e-04\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8636e-04 - mse: 2.9599e-05 - NMSE: 2.6757e-04 - tot_time: 1h 27m 21.3s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8636e-04 - mse: 2.9599e-05 - NMSE: 2.6757e-04 - val_loss: 2.0528e-04 - val_mse: 4.8529e-05 - val_NMSE: 4.3869e-04\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8627e-04 - mse: 2.9537e-05 - NMSE: 2.6700e-04 - tot_time: 1h 27m 42.4s\n",
      "\n",
      "Epoch 7: val_NMSE improved from 0.00044 to 0.00044, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8627e-04 - mse: 2.9537e-05 - NMSE: 2.6700e-04 - val_loss: 2.0512e-04 - val_mse: 4.8399e-05 - val_NMSE: 4.3751e-04\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8612e-04 - mse: 2.9413e-05 - NMSE: 2.6588e-04 - tot_time: 1h 28m 3.1s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.00044 to 0.00044, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8612e-04 - mse: 2.9413e-05 - NMSE: 2.6588e-04 - val_loss: 2.0502e-04 - val_mse: 4.8326e-05 - val_NMSE: 4.3685e-04\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.8609e-04 - mse: 2.9415e-05 - NMSE: 2.6590e-04 - tot_time: 1h 28m 23.9s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.00044 to 0.00044, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8609e-04 - mse: 2.9415e-05 - NMSE: 2.6590e-04 - val_loss: 2.0495e-04 - val_mse: 4.8284e-05 - val_NMSE: 4.3647e-04\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8602e-04 - mse: 2.9370e-05 - NMSE: 2.6549e-04 - tot_time: 1h 28m 44.7s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.00044 to 0.00044, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8602e-04 - mse: 2.9370e-05 - NMSE: 2.6549e-04 - val_loss: 2.0485e-04 - val_mse: 4.8214e-05 - val_NMSE: 4.3583e-04\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8594e-04 - mse: 2.9320e-05 - NMSE: 2.6504e-04 - tot_time: 1h 29m 5.7s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00044 to 0.00044, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8594e-04 - mse: 2.9320e-05 - NMSE: 2.6504e-04 - val_loss: 2.0479e-04 - val_mse: 4.8187e-05 - val_NMSE: 4.3559e-04\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8575e-04 - mse: 2.9156e-05 - NMSE: 2.6356e-04 - tot_time: 1h 29m 26.8s\n",
      "\n",
      "Epoch 12: val_NMSE improved from 0.00044 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8575e-04 - mse: 2.9156e-05 - NMSE: 2.6356e-04 - val_loss: 2.0458e-04 - val_mse: 4.8004e-05 - val_NMSE: 4.3393e-04\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8574e-04 - mse: 2.9176e-05 - NMSE: 2.6374e-04 - tot_time: 1h 29m 47.9s\n",
      "\n",
      "Epoch 13: val_NMSE did not improve from 0.00043\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8574e-04 - mse: 2.9176e-05 - NMSE: 2.6374e-04 - val_loss: 2.0455e-04 - val_mse: 4.8005e-05 - val_NMSE: 4.3395e-04\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8570e-04 - mse: 2.9168e-05 - NMSE: 2.6366e-04 - tot_time: 1h 30m 8.7s\n",
      "\n",
      "Epoch 14: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8570e-04 - mse: 2.9168e-05 - NMSE: 2.6366e-04 - val_loss: 2.0440e-04 - val_mse: 4.7880e-05 - val_NMSE: 4.3281e-04\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8551e-04 - mse: 2.9003e-05 - NMSE: 2.6217e-04 - tot_time: 1h 30m 29.3s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8551e-04 - mse: 2.9003e-05 - NMSE: 2.6217e-04 - val_loss: 2.0425e-04 - val_mse: 4.7766e-05 - val_NMSE: 4.3178e-04\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8543e-04 - mse: 2.8959e-05 - NMSE: 2.6178e-04 - tot_time: 1h 30m 50.3s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8543e-04 - mse: 2.8959e-05 - NMSE: 2.6178e-04 - val_loss: 2.0416e-04 - val_mse: 4.7708e-05 - val_NMSE: 4.3126e-04\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8538e-04 - mse: 2.8941e-05 - NMSE: 2.6162e-04 - tot_time: 1h 31m 11.3s\n",
      "\n",
      "Epoch 17: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8538e-04 - mse: 2.8941e-05 - NMSE: 2.6162e-04 - val_loss: 2.0401e-04 - val_mse: 4.7590e-05 - val_NMSE: 4.3019e-04\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8537e-04 - mse: 2.8957e-05 - NMSE: 2.6176e-04 - tot_time: 1h 31m 32.5s\n",
      "\n",
      "Epoch 18: val_NMSE did not improve from 0.00043\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8537e-04 - mse: 2.8957e-05 - NMSE: 2.6176e-04 - val_loss: 2.0401e-04 - val_mse: 4.7612e-05 - val_NMSE: 4.3040e-04\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8518e-04 - mse: 2.8804e-05 - NMSE: 2.6038e-04 - tot_time: 1h 31m 53.3s\n",
      "\n",
      "Epoch 19: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8518e-04 - mse: 2.8804e-05 - NMSE: 2.6038e-04 - val_loss: 2.0385e-04 - val_mse: 4.7489e-05 - val_NMSE: 4.2928e-04\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8519e-04 - mse: 2.8841e-05 - NMSE: 2.6071e-04 - tot_time: 1h 32m 14.3s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8519e-04 - mse: 2.8841e-05 - NMSE: 2.6071e-04 - val_loss: 2.0372e-04 - val_mse: 4.7397e-05 - val_NMSE: 4.2845e-04\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8507e-04 - mse: 2.8758e-05 - NMSE: 2.5996e-04 - tot_time: 1h 32m 34.8s\n",
      "\n",
      "Epoch 21: val_NMSE did not improve from 0.00043\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8507e-04 - mse: 2.8758e-05 - NMSE: 2.5996e-04 - val_loss: 2.0369e-04 - val_mse: 4.7398e-05 - val_NMSE: 4.2846e-04\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8498e-04 - mse: 2.8706e-05 - NMSE: 2.5949e-04 - tot_time: 1h 32m 56.2s\n",
      "\n",
      "Epoch 22: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8498e-04 - mse: 2.8706e-05 - NMSE: 2.5949e-04 - val_loss: 2.0357e-04 - val_mse: 4.7311e-05 - val_NMSE: 4.2767e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8499e-04 - mse: 2.8746e-05 - NMSE: 2.5986e-04 - tot_time: 1h 33m 17.7s\n",
      "\n",
      "Epoch 23: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8499e-04 - mse: 2.8746e-05 - NMSE: 2.5986e-04 - val_loss: 2.0344e-04 - val_mse: 4.7219e-05 - val_NMSE: 4.2684e-04\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8485e-04 - mse: 2.8641e-05 - NMSE: 2.5890e-04 - tot_time: 1h 33m 38.9s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8485e-04 - mse: 2.8641e-05 - NMSE: 2.5890e-04 - val_loss: 2.0335e-04 - val_mse: 4.7166e-05 - val_NMSE: 4.2636e-04\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8476e-04 - mse: 2.8593e-05 - NMSE: 2.5847e-04 - tot_time: 1h 33m 60.0s\n",
      "\n",
      "Epoch 25: val_NMSE improved from 0.00043 to 0.00043, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8476e-04 - mse: 2.8593e-05 - NMSE: 2.5847e-04 - val_loss: 2.0320e-04 - val_mse: 4.7051e-05 - val_NMSE: 4.2532e-04\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8466e-04 - mse: 2.8524e-05 - NMSE: 2.5785e-04 - tot_time: 1h 34m 21.1s\n",
      "\n",
      "Epoch 26: val_NMSE improved from 0.00043 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8466e-04 - mse: 2.8524e-05 - NMSE: 2.5785e-04 - val_loss: 2.0312e-04 - val_mse: 4.6998e-05 - val_NMSE: 4.2484e-04\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8466e-04 - mse: 2.8558e-05 - NMSE: 2.5815e-04 - tot_time: 1h 34m 42.2s\n",
      "\n",
      "Epoch 27: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8466e-04 - mse: 2.8558e-05 - NMSE: 2.5815e-04 - val_loss: 2.0303e-04 - val_mse: 4.6947e-05 - val_NMSE: 4.2438e-04\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8451e-04 - mse: 2.8449e-05 - NMSE: 2.5717e-04 - tot_time: 1h 35m 3.6s\n",
      "\n",
      "Epoch 28: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8451e-04 - mse: 2.8449e-05 - NMSE: 2.5717e-04 - val_loss: 2.0293e-04 - val_mse: 4.6887e-05 - val_NMSE: 4.2384e-04\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8448e-04 - mse: 2.8456e-05 - NMSE: 2.5723e-04 - tot_time: 1h 35m 24.8s\n",
      "\n",
      "Epoch 29: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8448e-04 - mse: 2.8456e-05 - NMSE: 2.5723e-04 - val_loss: 2.0283e-04 - val_mse: 4.6820e-05 - val_NMSE: 4.2324e-04\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8437e-04 - mse: 2.8382e-05 - NMSE: 2.5656e-04 - tot_time: 1h 35m 45.2s\n",
      "\n",
      "Epoch 30: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 1.8437e-04 - mse: 2.8382e-05 - NMSE: 2.5656e-04 - val_loss: 2.0273e-04 - val_mse: 4.6762e-05 - val_NMSE: 4.2271e-04\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8432e-04 - mse: 2.8372e-05 - NMSE: 2.5647e-04 - tot_time: 1h 36m 6.0s\n",
      "\n",
      "Epoch 31: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8432e-04 - mse: 2.8372e-05 - NMSE: 2.5647e-04 - val_loss: 2.0261e-04 - val_mse: 4.6676e-05 - val_NMSE: 4.2193e-04\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8419e-04 - mse: 2.8277e-05 - NMSE: 2.5561e-04 - tot_time: 1h 36m 27.1s\n",
      "\n",
      "Epoch 32: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8419e-04 - mse: 2.8277e-05 - NMSE: 2.5561e-04 - val_loss: 2.0252e-04 - val_mse: 4.6633e-05 - val_NMSE: 4.2155e-04\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8413e-04 - mse: 2.8253e-05 - NMSE: 2.5539e-04 - tot_time: 1h 36m 48.3s\n",
      "\n",
      "Epoch 33: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8413e-04 - mse: 2.8253e-05 - NMSE: 2.5539e-04 - val_loss: 2.0231e-04 - val_mse: 4.6461e-05 - val_NMSE: 4.1999e-04\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8405e-04 - mse: 2.8215e-05 - NMSE: 2.5505e-04 - tot_time: 1h 37m 9.1s\n",
      "\n",
      "Epoch 34: val_NMSE did not improve from 0.00042\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8405e-04 - mse: 2.8215e-05 - NMSE: 2.5505e-04 - val_loss: 2.0233e-04 - val_mse: 4.6517e-05 - val_NMSE: 4.2050e-04\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8398e-04 - mse: 2.8183e-05 - NMSE: 2.5476e-04 - tot_time: 1h 37m 30.8s\n",
      "\n",
      "Epoch 35: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8398e-04 - mse: 2.8183e-05 - NMSE: 2.5476e-04 - val_loss: 2.0213e-04 - val_mse: 4.6363e-05 - val_NMSE: 4.1910e-04\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8383e-04 - mse: 2.8075e-05 - NMSE: 2.5379e-04 - tot_time: 1h 37m 51.4s\n",
      "\n",
      "Epoch 36: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8383e-04 - mse: 2.8075e-05 - NMSE: 2.5379e-04 - val_loss: 2.0208e-04 - val_mse: 4.6346e-05 - val_NMSE: 4.1895e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8386e-04 - mse: 2.8145e-05 - NMSE: 2.5442e-04 - tot_time: 1h 38m 12.1s\n",
      "\n",
      "Epoch 37: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8386e-04 - mse: 2.8145e-05 - NMSE: 2.5442e-04 - val_loss: 2.0195e-04 - val_mse: 4.6257e-05 - val_NMSE: 4.1814e-04\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8370e-04 - mse: 2.8028e-05 - NMSE: 2.5336e-04 - tot_time: 1h 38m 33.5s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8370e-04 - mse: 2.8028e-05 - NMSE: 2.5336e-04 - val_loss: 2.0181e-04 - val_mse: 4.6164e-05 - val_NMSE: 4.1730e-04\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8357e-04 - mse: 2.7946e-05 - NMSE: 2.5262e-04 - tot_time: 1h 38m 54.5s\n",
      "\n",
      "Epoch 39: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8357e-04 - mse: 2.7946e-05 - NMSE: 2.5262e-04 - val_loss: 2.0166e-04 - val_mse: 4.6058e-05 - val_NMSE: 4.1634e-04\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8354e-04 - mse: 2.7951e-05 - NMSE: 2.5267e-04 - tot_time: 1h 39m 15.9s\n",
      "\n",
      "Epoch 40: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8354e-04 - mse: 2.7951e-05 - NMSE: 2.5267e-04 - val_loss: 2.0161e-04 - val_mse: 4.6049e-05 - val_NMSE: 4.1627e-04\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8345e-04 - mse: 2.7905e-05 - NMSE: 2.5225e-04 - tot_time: 1h 39m 37.2s\n",
      "\n",
      "Epoch 41: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8345e-04 - mse: 2.7905e-05 - NMSE: 2.5225e-04 - val_loss: 2.0149e-04 - val_mse: 4.5969e-05 - val_NMSE: 4.1554e-04\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8335e-04 - mse: 2.7856e-05 - NMSE: 2.5181e-04 - tot_time: 1h 39m 58.1s\n",
      "\n",
      "Epoch 42: val_NMSE improved from 0.00042 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8335e-04 - mse: 2.7856e-05 - NMSE: 2.5181e-04 - val_loss: 2.0142e-04 - val_mse: 4.5947e-05 - val_NMSE: 4.1534e-04\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8328e-04 - mse: 2.7832e-05 - NMSE: 2.5159e-04 - tot_time: 1h 40m 18.7s\n",
      "\n",
      "Epoch 43: val_NMSE improved from 0.00042 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8328e-04 - mse: 2.7832e-05 - NMSE: 2.5159e-04 - val_loss: 2.0126e-04 - val_mse: 4.5830e-05 - val_NMSE: 4.1429e-04\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8321e-04 - mse: 2.7798e-05 - NMSE: 2.5129e-04 - tot_time: 1h 40m 39.4s\n",
      "\n",
      "Epoch 44: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8321e-04 - mse: 2.7798e-05 - NMSE: 2.5129e-04 - val_loss: 2.0117e-04 - val_mse: 4.5789e-05 - val_NMSE: 4.1391e-04\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8312e-04 - mse: 2.7759e-05 - NMSE: 2.5093e-04 - tot_time: 1h 41m 0.8s\n",
      "\n",
      "Epoch 45: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8312e-04 - mse: 2.7759e-05 - NMSE: 2.5093e-04 - val_loss: 2.0109e-04 - val_mse: 4.5755e-05 - val_NMSE: 4.1360e-04\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8297e-04 - mse: 2.7655e-05 - NMSE: 2.4999e-04 - tot_time: 1h 41m 22.9s\n",
      "\n",
      "Epoch 46: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8297e-04 - mse: 2.7655e-05 - NMSE: 2.4999e-04 - val_loss: 2.0093e-04 - val_mse: 4.5634e-05 - val_NMSE: 4.1252e-04\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8294e-04 - mse: 2.7673e-05 - NMSE: 2.5015e-04 - tot_time: 1h 41m 44.5s\n",
      "\n",
      "Epoch 47: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8294e-04 - mse: 2.7673e-05 - NMSE: 2.5015e-04 - val_loss: 2.0078e-04 - val_mse: 4.5536e-05 - val_NMSE: 4.1162e-04\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8288e-04 - mse: 2.7652e-05 - NMSE: 2.4996e-04 - tot_time: 1h 42m 5.9s\n",
      "\n",
      "Epoch 48: val_NMSE did not improve from 0.00041\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8288e-04 - mse: 2.7652e-05 - NMSE: 2.4996e-04 - val_loss: 2.0075e-04 - val_mse: 4.5546e-05 - val_NMSE: 4.1171e-04\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8280e-04 - mse: 2.7621e-05 - NMSE: 2.4968e-04 - tot_time: 1h 42m 26.8s\n",
      "\n",
      "Epoch 49: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8280e-04 - mse: 2.7621e-05 - NMSE: 2.4968e-04 - val_loss: 2.0061e-04 - val_mse: 4.5459e-05 - val_NMSE: 4.1093e-04\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8267e-04 - mse: 2.7541e-05 - NMSE: 2.4896e-04 - tot_time: 1h 42m 48.2s\n",
      "\n",
      "Epoch 50: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8267e-04 - mse: 2.7541e-05 - NMSE: 2.4896e-04 - val_loss: 2.0048e-04 - val_mse: 4.5375e-05 - val_NMSE: 4.1017e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8263e-04 - mse: 2.7551e-05 - NMSE: 2.4904e-04 - tot_time: 1h 43m 8.5s\n",
      "\n",
      "Epoch 51: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 1.8263e-04 - mse: 2.7551e-05 - NMSE: 2.4904e-04 - val_loss: 2.0034e-04 - val_mse: 4.5279e-05 - val_NMSE: 4.0931e-04\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8251e-04 - mse: 2.7476e-05 - NMSE: 2.4837e-04 - tot_time: 1h 43m 29.1s\n",
      "\n",
      "Epoch 52: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8251e-04 - mse: 2.7476e-05 - NMSE: 2.4837e-04 - val_loss: 2.0025e-04 - val_mse: 4.5240e-05 - val_NMSE: 4.0895e-04\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8248e-04 - mse: 2.7498e-05 - NMSE: 2.4857e-04 - tot_time: 1h 43m 50.7s\n",
      "\n",
      "Epoch 53: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8248e-04 - mse: 2.7498e-05 - NMSE: 2.4857e-04 - val_loss: 2.0011e-04 - val_mse: 4.5148e-05 - val_NMSE: 4.0812e-04\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8226e-04 - mse: 2.7327e-05 - NMSE: 2.4702e-04 - tot_time: 1h 44m 11.5s\n",
      "\n",
      "Epoch 54: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8226e-04 - mse: 2.7327e-05 - NMSE: 2.4702e-04 - val_loss: 2.0002e-04 - val_mse: 4.5112e-05 - val_NMSE: 4.0779e-04\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8223e-04 - mse: 2.7346e-05 - NMSE: 2.4719e-04 - tot_time: 1h 44m 32.8s\n",
      "\n",
      "Epoch 55: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8223e-04 - mse: 2.7346e-05 - NMSE: 2.4719e-04 - val_loss: 1.9990e-04 - val_mse: 4.5046e-05 - val_NMSE: 4.0720e-04\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8218e-04 - mse: 2.7347e-05 - NMSE: 2.4720e-04 - tot_time: 1h 44m 53.6s\n",
      "\n",
      "Epoch 56: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8218e-04 - mse: 2.7347e-05 - NMSE: 2.4720e-04 - val_loss: 1.9975e-04 - val_mse: 4.4947e-05 - val_NMSE: 4.0630e-04\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8208e-04 - mse: 2.7297e-05 - NMSE: 2.4675e-04 - tot_time: 1h 45m 14.8s\n",
      "\n",
      "Epoch 57: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8208e-04 - mse: 2.7297e-05 - NMSE: 2.4675e-04 - val_loss: 1.9964e-04 - val_mse: 4.4887e-05 - val_NMSE: 4.0576e-04\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8196e-04 - mse: 2.7229e-05 - NMSE: 2.4614e-04 - tot_time: 1h 45m 35.9s\n",
      "\n",
      "Epoch 58: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8196e-04 - mse: 2.7229e-05 - NMSE: 2.4614e-04 - val_loss: 1.9954e-04 - val_mse: 4.4841e-05 - val_NMSE: 4.0534e-04\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8185e-04 - mse: 2.7173e-05 - NMSE: 2.4563e-04 - tot_time: 1h 45m 56.3s\n",
      "\n",
      "Epoch 59: val_NMSE improved from 0.00041 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 1.8185e-04 - mse: 2.7173e-05 - NMSE: 2.4563e-04 - val_loss: 1.9944e-04 - val_mse: 4.4792e-05 - val_NMSE: 4.0490e-04\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8177e-04 - mse: 2.7147e-05 - NMSE: 2.4540e-04 - tot_time: 1h 46m 17.9s\n",
      "\n",
      "Epoch 60: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8177e-04 - mse: 2.7147e-05 - NMSE: 2.4540e-04 - val_loss: 1.9929e-04 - val_mse: 4.4697e-05 - val_NMSE: 4.0404e-04\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8169e-04 - mse: 2.7119e-05 - NMSE: 2.4515e-04 - tot_time: 1h 46m 38.8s\n",
      "\n",
      "Epoch 61: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8169e-04 - mse: 2.7119e-05 - NMSE: 2.4515e-04 - val_loss: 1.9912e-04 - val_mse: 4.4573e-05 - val_NMSE: 4.0292e-04\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8156e-04 - mse: 2.7040e-05 - NMSE: 2.4443e-04 - tot_time: 1h 46m 59.7s\n",
      "\n",
      "Epoch 62: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8156e-04 - mse: 2.7040e-05 - NMSE: 2.4443e-04 - val_loss: 1.9901e-04 - val_mse: 4.4523e-05 - val_NMSE: 4.0247e-04\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8152e-04 - mse: 2.7055e-05 - NMSE: 2.4456e-04 - tot_time: 1h 47m 21.1s\n",
      "\n",
      "Epoch 63: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8152e-04 - mse: 2.7055e-05 - NMSE: 2.4456e-04 - val_loss: 1.9893e-04 - val_mse: 4.4497e-05 - val_NMSE: 4.0223e-04\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8143e-04 - mse: 2.7021e-05 - NMSE: 2.4426e-04 - tot_time: 1h 47m 42.1s\n",
      "\n",
      "Epoch 64: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8143e-04 - mse: 2.7021e-05 - NMSE: 2.4426e-04 - val_loss: 1.9884e-04 - val_mse: 4.4457e-05 - val_NMSE: 4.0187e-04\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8125e-04 - mse: 2.6891e-05 - NMSE: 2.4309e-04 - tot_time: 1h 48m 3.4s\n",
      "\n",
      "Epoch 65: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8125e-04 - mse: 2.6891e-05 - NMSE: 2.4309e-04 - val_loss: 1.9867e-04 - val_mse: 4.4350e-05 - val_NMSE: 4.0090e-04\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8117e-04 - mse: 2.6870e-05 - NMSE: 2.4289e-04 - tot_time: 1h 48m 25.0s\n",
      "\n",
      "Epoch 66: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8117e-04 - mse: 2.6870e-05 - NMSE: 2.4289e-04 - val_loss: 1.9855e-04 - val_mse: 4.4285e-05 - val_NMSE: 4.0032e-04\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8113e-04 - mse: 2.6883e-05 - NMSE: 2.4301e-04 - tot_time: 1h 48m 46.3s\n",
      "\n",
      "Epoch 67: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8113e-04 - mse: 2.6883e-05 - NMSE: 2.4301e-04 - val_loss: 1.9837e-04 - val_mse: 4.4161e-05 - val_NMSE: 3.9920e-04\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8099e-04 - mse: 2.6800e-05 - NMSE: 2.4226e-04 - tot_time: 1h 49m 7.2s\n",
      "\n",
      "Epoch 68: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8099e-04 - mse: 2.6800e-05 - NMSE: 2.4226e-04 - val_loss: 1.9830e-04 - val_mse: 4.4143e-05 - val_NMSE: 3.9903e-04\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8095e-04 - mse: 2.6824e-05 - NMSE: 2.4247e-04 - tot_time: 1h 49m 28.0s\n",
      "\n",
      "Epoch 69: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8095e-04 - mse: 2.6824e-05 - NMSE: 2.4247e-04 - val_loss: 1.9819e-04 - val_mse: 4.4094e-05 - val_NMSE: 3.9859e-04\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8090e-04 - mse: 2.6828e-05 - NMSE: 2.4251e-04 - tot_time: 1h 49m 48.7s\n",
      "\n",
      "Epoch 70: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8090e-04 - mse: 2.6828e-05 - NMSE: 2.4251e-04 - val_loss: 1.9800e-04 - val_mse: 4.3960e-05 - val_NMSE: 3.9738e-04\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8069e-04 - mse: 2.6677e-05 - NMSE: 2.4115e-04 - tot_time: 1h 50m 9.2s\n",
      "\n",
      "Epoch 71: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8069e-04 - mse: 2.6677e-05 - NMSE: 2.4115e-04 - val_loss: 1.9790e-04 - val_mse: 4.3916e-05 - val_NMSE: 3.9698e-04\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8066e-04 - mse: 2.6705e-05 - NMSE: 2.4140e-04 - tot_time: 1h 50m 30.3s\n",
      "\n",
      "Epoch 72: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8066e-04 - mse: 2.6705e-05 - NMSE: 2.4140e-04 - val_loss: 1.9782e-04 - val_mse: 4.3898e-05 - val_NMSE: 3.9682e-04\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8051e-04 - mse: 2.6621e-05 - NMSE: 2.4064e-04 - tot_time: 1h 50m 51.2s\n",
      "\n",
      "Epoch 73: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8051e-04 - mse: 2.6621e-05 - NMSE: 2.4064e-04 - val_loss: 1.9766e-04 - val_mse: 4.3803e-05 - val_NMSE: 3.9596e-04\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8043e-04 - mse: 2.6593e-05 - NMSE: 2.4038e-04 - tot_time: 1h 51m 12.0s\n",
      "\n",
      "Epoch 74: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8043e-04 - mse: 2.6593e-05 - NMSE: 2.4038e-04 - val_loss: 1.9752e-04 - val_mse: 4.3716e-05 - val_NMSE: 3.9517e-04\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8037e-04 - mse: 2.6600e-05 - NMSE: 2.4045e-04 - tot_time: 1h 51m 32.9s\n",
      "\n",
      "Epoch 75: val_NMSE improved from 0.00040 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8037e-04 - mse: 2.6600e-05 - NMSE: 2.4045e-04 - val_loss: 1.9740e-04 - val_mse: 4.3661e-05 - val_NMSE: 3.9468e-04\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8029e-04 - mse: 2.6574e-05 - NMSE: 2.4022e-04 - tot_time: 1h 51m 54.4s\n",
      "\n",
      "Epoch 76: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.8029e-04 - mse: 2.6574e-05 - NMSE: 2.4022e-04 - val_loss: 1.9730e-04 - val_mse: 4.3622e-05 - val_NMSE: 3.9432e-04\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8015e-04 - mse: 2.6501e-05 - NMSE: 2.3956e-04 - tot_time: 1h 52m 15.4s\n",
      "\n",
      "Epoch 77: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8015e-04 - mse: 2.6501e-05 - NMSE: 2.3956e-04 - val_loss: 1.9713e-04 - val_mse: 4.3514e-05 - val_NMSE: 3.9335e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8007e-04 - mse: 2.6481e-05 - NMSE: 2.3938e-04 - tot_time: 1h 52m 36.9s\n",
      "\n",
      "Epoch 78: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.8007e-04 - mse: 2.6481e-05 - NMSE: 2.3938e-04 - val_loss: 1.9702e-04 - val_mse: 4.3465e-05 - val_NMSE: 3.9290e-04\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7990e-04 - mse: 2.6374e-05 - NMSE: 2.3841e-04 - tot_time: 1h 52m 58.3s\n",
      "\n",
      "Epoch 79: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7990e-04 - mse: 2.6374e-05 - NMSE: 2.3841e-04 - val_loss: 1.9693e-04 - val_mse: 4.3435e-05 - val_NMSE: 3.9264e-04\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7982e-04 - mse: 2.6353e-05 - NMSE: 2.3822e-04 - tot_time: 1h 53m 20.0s\n",
      "\n",
      "Epoch 80: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7982e-04 - mse: 2.6353e-05 - NMSE: 2.3822e-04 - val_loss: 1.9674e-04 - val_mse: 4.3312e-05 - val_NMSE: 3.9152e-04\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7978e-04 - mse: 2.6385e-05 - NMSE: 2.3851e-04 - tot_time: 1h 53m 41.1s\n",
      "\n",
      "Epoch 81: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7978e-04 - mse: 2.6385e-05 - NMSE: 2.3851e-04 - val_loss: 1.9660e-04 - val_mse: 4.3232e-05 - val_NMSE: 3.9080e-04\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7966e-04 - mse: 2.6328e-05 - NMSE: 2.3800e-04 - tot_time: 1h 54m 2.0s\n",
      "\n",
      "Epoch 82: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7966e-04 - mse: 2.6328e-05 - NMSE: 2.3800e-04 - val_loss: 1.9649e-04 - val_mse: 4.3188e-05 - val_NMSE: 3.9040e-04\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7953e-04 - mse: 2.6258e-05 - NMSE: 2.3736e-04 - tot_time: 1h 54m 23.3s\n",
      "\n",
      "Epoch 83: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7953e-04 - mse: 2.6258e-05 - NMSE: 2.3736e-04 - val_loss: 1.9631e-04 - val_mse: 4.3079e-05 - val_NMSE: 3.8942e-04\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7944e-04 - mse: 2.6233e-05 - NMSE: 2.3713e-04 - tot_time: 1h 54m 44.7s\n",
      "\n",
      "Epoch 84: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7944e-04 - mse: 2.6233e-05 - NMSE: 2.3713e-04 - val_loss: 1.9615e-04 - val_mse: 4.2985e-05 - val_NMSE: 3.8856e-04\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7941e-04 - mse: 2.6268e-05 - NMSE: 2.3745e-04 - tot_time: 1h 55m 6.8s\n",
      "\n",
      "Epoch 85: val_NMSE did not improve from 0.00039\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7941e-04 - mse: 2.6268e-05 - NMSE: 2.3745e-04 - val_loss: 1.9612e-04 - val_mse: 4.3014e-05 - val_NMSE: 3.8883e-04\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7923e-04 - mse: 2.6159e-05 - NMSE: 2.3646e-04 - tot_time: 1h 55m 28.6s\n",
      "\n",
      "Epoch 86: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7923e-04 - mse: 2.6159e-05 - NMSE: 2.3646e-04 - val_loss: 1.9593e-04 - val_mse: 4.2889e-05 - val_NMSE: 3.8770e-04\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7912e-04 - mse: 2.6113e-05 - NMSE: 2.3605e-04 - tot_time: 1h 55m 49.9s\n",
      "\n",
      "Epoch 87: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7912e-04 - mse: 2.6113e-05 - NMSE: 2.3605e-04 - val_loss: 1.9582e-04 - val_mse: 4.2853e-05 - val_NMSE: 3.8737e-04\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7908e-04 - mse: 2.6143e-05 - NMSE: 2.3632e-04 - tot_time: 1h 56m 11.0s\n",
      "\n",
      "Epoch 88: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7908e-04 - mse: 2.6143e-05 - NMSE: 2.3632e-04 - val_loss: 1.9562e-04 - val_mse: 4.2722e-05 - val_NMSE: 3.8619e-04\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7895e-04 - mse: 2.6077e-05 - NMSE: 2.3573e-04 - tot_time: 1h 56m 32.6s\n",
      "\n",
      "Epoch 89: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7895e-04 - mse: 2.6077e-05 - NMSE: 2.3573e-04 - val_loss: 1.9554e-04 - val_mse: 4.2709e-05 - val_NMSE: 3.8607e-04\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7879e-04 - mse: 2.5994e-05 - NMSE: 2.3497e-04 - tot_time: 1h 56m 54.0s\n",
      "\n",
      "Epoch 90: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7879e-04 - mse: 2.5994e-05 - NMSE: 2.3497e-04 - val_loss: 1.9538e-04 - val_mse: 4.2613e-05 - val_NMSE: 3.8520e-04\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7872e-04 - mse: 2.5988e-05 - NMSE: 2.3492e-04 - tot_time: 1h 57m 15.1s\n",
      "\n",
      "Epoch 91: val_NMSE improved from 0.00039 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7872e-04 - mse: 2.5988e-05 - NMSE: 2.3492e-04 - val_loss: 1.9520e-04 - val_mse: 4.2503e-05 - val_NMSE: 3.8420e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7857e-04 - mse: 2.5908e-05 - NMSE: 2.3420e-04 - tot_time: 1h 57m 36.2s\n",
      "\n",
      "Epoch 92: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7857e-04 - mse: 2.5908e-05 - NMSE: 2.3420e-04 - val_loss: 1.9510e-04 - val_mse: 4.2471e-05 - val_NMSE: 3.8392e-04\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7843e-04 - mse: 2.5841e-05 - NMSE: 2.3359e-04 - tot_time: 1h 57m 57.1s\n",
      "\n",
      "Epoch 93: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7843e-04 - mse: 2.5841e-05 - NMSE: 2.3359e-04 - val_loss: 1.9495e-04 - val_mse: 4.2399e-05 - val_NMSE: 3.8327e-04\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7838e-04 - mse: 2.5857e-05 - NMSE: 2.3374e-04 - tot_time: 1h 58m 17.9s\n",
      "\n",
      "Epoch 94: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7838e-04 - mse: 2.5857e-05 - NMSE: 2.3374e-04 - val_loss: 1.9476e-04 - val_mse: 4.2276e-05 - val_NMSE: 3.8216e-04\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7823e-04 - mse: 2.5776e-05 - NMSE: 2.3300e-04 - tot_time: 1h 58m 39.1s\n",
      "\n",
      "Epoch 95: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7823e-04 - mse: 2.5776e-05 - NMSE: 2.3300e-04 - val_loss: 1.9465e-04 - val_mse: 4.2234e-05 - val_NMSE: 3.8178e-04\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7815e-04 - mse: 2.5770e-05 - NMSE: 2.3294e-04 - tot_time: 1h 59m 0.8s\n",
      "\n",
      "Epoch 96: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7815e-04 - mse: 2.5770e-05 - NMSE: 2.3294e-04 - val_loss: 1.9451e-04 - val_mse: 4.2175e-05 - val_NMSE: 3.8125e-04\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7807e-04 - mse: 2.5767e-05 - NMSE: 2.3292e-04 - tot_time: 1h 59m 22.2s\n",
      "\n",
      "Epoch 97: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7807e-04 - mse: 2.5767e-05 - NMSE: 2.3292e-04 - val_loss: 1.9434e-04 - val_mse: 4.2068e-05 - val_NMSE: 3.8028e-04\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7792e-04 - mse: 2.5688e-05 - NMSE: 2.3220e-04 - tot_time: 1h 59m 43.5s\n",
      "\n",
      "Epoch 98: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7792e-04 - mse: 2.5688e-05 - NMSE: 2.3220e-04 - val_loss: 1.9425e-04 - val_mse: 4.2058e-05 - val_NMSE: 3.8018e-04\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7777e-04 - mse: 2.5612e-05 - NMSE: 2.3152e-04 - tot_time: 2h 0m 4.5s\n",
      "\n",
      "Epoch 99: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7777e-04 - mse: 2.5612e-05 - NMSE: 2.3152e-04 - val_loss: 1.9407e-04 - val_mse: 4.1946e-05 - val_NMSE: 3.7917e-04\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7774e-04 - mse: 2.5658e-05 - NMSE: 2.3193e-04 - tot_time: 2h 0m 25.6s\n",
      "\n",
      "Epoch 100: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7774e-04 - mse: 2.5658e-05 - NMSE: 2.3193e-04 - val_loss: 1.9389e-04 - val_mse: 4.1841e-05 - val_NMSE: 3.7822e-04\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7763e-04 - mse: 2.5616e-05 - NMSE: 2.3156e-04 - tot_time: 2h 0m 46.8s\n",
      "\n",
      "Epoch 101: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7763e-04 - mse: 2.5616e-05 - NMSE: 2.3156e-04 - val_loss: 1.9375e-04 - val_mse: 4.1775e-05 - val_NMSE: 3.7763e-04\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7749e-04 - mse: 2.5550e-05 - NMSE: 2.3096e-04 - tot_time: 2h 1m 8.2s\n",
      "\n",
      "Epoch 102: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7749e-04 - mse: 2.5550e-05 - NMSE: 2.3096e-04 - val_loss: 1.9360e-04 - val_mse: 4.1702e-05 - val_NMSE: 3.7696e-04\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7734e-04 - mse: 2.5481e-05 - NMSE: 2.3034e-04 - tot_time: 2h 1m 29.6s\n",
      "\n",
      "Epoch 103: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7734e-04 - mse: 2.5481e-05 - NMSE: 2.3034e-04 - val_loss: 1.9349e-04 - val_mse: 4.1674e-05 - val_NMSE: 3.7672e-04\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7731e-04 - mse: 2.5528e-05 - NMSE: 2.3076e-04 - tot_time: 2h 1m 50.9s\n",
      "\n",
      "Epoch 104: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7731e-04 - mse: 2.5528e-05 - NMSE: 2.3076e-04 - val_loss: 1.9332e-04 - val_mse: 4.1578e-05 - val_NMSE: 3.7585e-04\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7717e-04 - mse: 2.5458e-05 - NMSE: 2.3013e-04 - tot_time: 2h 2m 12.1s\n",
      "\n",
      "Epoch 105: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7717e-04 - mse: 2.5458e-05 - NMSE: 2.3013e-04 - val_loss: 1.9324e-04 - val_mse: 4.1574e-05 - val_NMSE: 3.7581e-04\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7698e-04 - mse: 2.5355e-05 - NMSE: 2.2920e-04 - tot_time: 2h 2m 33.5s\n",
      "\n",
      "Epoch 106: val_NMSE improved from 0.00038 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7698e-04 - mse: 2.5355e-05 - NMSE: 2.2920e-04 - val_loss: 1.9304e-04 - val_mse: 4.1454e-05 - val_NMSE: 3.7472e-04\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7693e-04 - mse: 2.5376e-05 - NMSE: 2.2939e-04 - tot_time: 2h 2m 54.7s\n",
      "\n",
      "Epoch 107: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7693e-04 - mse: 2.5376e-05 - NMSE: 2.2939e-04 - val_loss: 1.9287e-04 - val_mse: 4.1364e-05 - val_NMSE: 3.7391e-04\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7679e-04 - mse: 2.5320e-05 - NMSE: 2.2888e-04 - tot_time: 2h 3m 16.1s\n",
      "\n",
      "Epoch 108: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7679e-04 - mse: 2.5320e-05 - NMSE: 2.2888e-04 - val_loss: 1.9276e-04 - val_mse: 4.1332e-05 - val_NMSE: 3.7363e-04\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7671e-04 - mse: 2.5319e-05 - NMSE: 2.2887e-04 - tot_time: 2h 3m 37.2s\n",
      "\n",
      "Epoch 109: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7671e-04 - mse: 2.5319e-05 - NMSE: 2.2887e-04 - val_loss: 1.9260e-04 - val_mse: 4.1248e-05 - val_NMSE: 3.7287e-04\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7651e-04 - mse: 2.5193e-05 - NMSE: 2.2774e-04 - tot_time: 2h 3m 58.3s\n",
      "\n",
      "Epoch 110: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7651e-04 - mse: 2.5193e-05 - NMSE: 2.2774e-04 - val_loss: 1.9246e-04 - val_mse: 4.1186e-05 - val_NMSE: 3.7230e-04\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7647e-04 - mse: 2.5238e-05 - NMSE: 2.2814e-04 - tot_time: 2h 4m 19.5s\n",
      "\n",
      "Epoch 111: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7647e-04 - mse: 2.5238e-05 - NMSE: 2.2814e-04 - val_loss: 1.9228e-04 - val_mse: 4.1089e-05 - val_NMSE: 3.7143e-04\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7627e-04 - mse: 2.5117e-05 - NMSE: 2.2705e-04 - tot_time: 2h 4m 40.6s\n",
      "\n",
      "Epoch 112: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7627e-04 - mse: 2.5117e-05 - NMSE: 2.2705e-04 - val_loss: 1.9210e-04 - val_mse: 4.0989e-05 - val_NMSE: 3.7052e-04\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7622e-04 - mse: 2.5146e-05 - NMSE: 2.2731e-04 - tot_time: 2h 5m 1.6s\n",
      "\n",
      "Epoch 113: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7622e-04 - mse: 2.5146e-05 - NMSE: 2.2731e-04 - val_loss: 1.9193e-04 - val_mse: 4.0897e-05 - val_NMSE: 3.6969e-04\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7607e-04 - mse: 2.5083e-05 - NMSE: 2.2674e-04 - tot_time: 2h 5m 22.5s\n",
      "\n",
      "Epoch 114: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7607e-04 - mse: 2.5083e-05 - NMSE: 2.2674e-04 - val_loss: 1.9180e-04 - val_mse: 4.0852e-05 - val_NMSE: 3.6928e-04\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7587e-04 - mse: 2.4965e-05 - NMSE: 2.2567e-04 - tot_time: 2h 5m 43.4s\n",
      "\n",
      "Epoch 115: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7587e-04 - mse: 2.4965e-05 - NMSE: 2.2567e-04 - val_loss: 1.9157e-04 - val_mse: 4.0712e-05 - val_NMSE: 3.6801e-04\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7585e-04 - mse: 2.5022e-05 - NMSE: 2.2618e-04 - tot_time: 2h 6m 4.1s\n",
      "\n",
      "Epoch 116: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7585e-04 - mse: 2.5022e-05 - NMSE: 2.2618e-04 - val_loss: 1.9148e-04 - val_mse: 4.0696e-05 - val_NMSE: 3.6787e-04\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7565e-04 - mse: 2.4906e-05 - NMSE: 2.2514e-04 - tot_time: 2h 6m 25.0s\n",
      "\n",
      "Epoch 117: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7565e-04 - mse: 2.4906e-05 - NMSE: 2.2514e-04 - val_loss: 1.9128e-04 - val_mse: 4.0587e-05 - val_NMSE: 3.6689e-04\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7561e-04 - mse: 2.4957e-05 - NMSE: 2.2560e-04 - tot_time: 2h 6m 46.1s\n",
      "\n",
      "Epoch 118: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7561e-04 - mse: 2.4957e-05 - NMSE: 2.2560e-04 - val_loss: 1.9112e-04 - val_mse: 4.0512e-05 - val_NMSE: 3.6621e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7544e-04 - mse: 2.4871e-05 - NMSE: 2.2482e-04 - tot_time: 2h 7m 6.6s\n",
      "\n",
      "Epoch 119: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 1.7544e-04 - mse: 2.4871e-05 - NMSE: 2.2482e-04 - val_loss: 1.9103e-04 - val_mse: 4.0506e-05 - val_NMSE: 3.6616e-04\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7530e-04 - mse: 2.4811e-05 - NMSE: 2.2428e-04 - tot_time: 2h 7m 27.6s\n",
      "\n",
      "Epoch 120: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7530e-04 - mse: 2.4811e-05 - NMSE: 2.2428e-04 - val_loss: 1.9083e-04 - val_mse: 4.0386e-05 - val_NMSE: 3.6507e-04\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7522e-04 - mse: 2.4815e-05 - NMSE: 2.2432e-04 - tot_time: 2h 7m 48.7s\n",
      "\n",
      "Epoch 121: val_NMSE improved from 0.00037 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7522e-04 - mse: 2.4815e-05 - NMSE: 2.2432e-04 - val_loss: 1.9070e-04 - val_mse: 4.0348e-05 - val_NMSE: 3.6472e-04\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7507e-04 - mse: 2.4758e-05 - NMSE: 2.2380e-04 - tot_time: 2h 8m 10.0s\n",
      "\n",
      "Epoch 122: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7507e-04 - mse: 2.4758e-05 - NMSE: 2.2380e-04 - val_loss: 1.9052e-04 - val_mse: 4.0251e-05 - val_NMSE: 3.6385e-04\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7499e-04 - mse: 2.4757e-05 - NMSE: 2.2379e-04 - tot_time: 2h 8m 31.4s\n",
      "\n",
      "Epoch 123: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7499e-04 - mse: 2.4757e-05 - NMSE: 2.2379e-04 - val_loss: 1.9037e-04 - val_mse: 4.0182e-05 - val_NMSE: 3.6323e-04\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7481e-04 - mse: 2.4673e-05 - NMSE: 2.2303e-04 - tot_time: 2h 8m 52.3s\n",
      "\n",
      "Epoch 124: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7481e-04 - mse: 2.4673e-05 - NMSE: 2.2303e-04 - val_loss: 1.9018e-04 - val_mse: 4.0089e-05 - val_NMSE: 3.6239e-04\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7468e-04 - mse: 2.4627e-05 - NMSE: 2.2262e-04 - tot_time: 2h 9m 12.9s\n",
      "\n",
      "Epoch 125: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7468e-04 - mse: 2.4627e-05 - NMSE: 2.2262e-04 - val_loss: 1.9003e-04 - val_mse: 4.0024e-05 - val_NMSE: 3.6179e-04\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7456e-04 - mse: 2.4595e-05 - NMSE: 2.2233e-04 - tot_time: 2h 9m 34.2s\n",
      "\n",
      "Epoch 126: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7456e-04 - mse: 2.4595e-05 - NMSE: 2.2233e-04 - val_loss: 1.8983e-04 - val_mse: 3.9915e-05 - val_NMSE: 3.6081e-04\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7447e-04 - mse: 2.4598e-05 - NMSE: 2.2236e-04 - tot_time: 2h 9m 55.3s\n",
      "\n",
      "Epoch 127: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7447e-04 - mse: 2.4598e-05 - NMSE: 2.2236e-04 - val_loss: 1.8969e-04 - val_mse: 3.9863e-05 - val_NMSE: 3.6034e-04\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7436e-04 - mse: 2.4568e-05 - NMSE: 2.2208e-04 - tot_time: 2h 10m 16.4s\n",
      "\n",
      "Epoch 128: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7436e-04 - mse: 2.4568e-05 - NMSE: 2.2208e-04 - val_loss: 1.8955e-04 - val_mse: 3.9809e-05 - val_NMSE: 3.5985e-04\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7419e-04 - mse: 2.4496e-05 - NMSE: 2.2143e-04 - tot_time: 2h 10m 37.4s\n",
      "\n",
      "Epoch 129: val_NMSE did not improve from 0.00036\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7419e-04 - mse: 2.4496e-05 - NMSE: 2.2143e-04 - val_loss: 1.8949e-04 - val_mse: 3.9835e-05 - val_NMSE: 3.6009e-04\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7409e-04 - mse: 2.4478e-05 - NMSE: 2.2127e-04 - tot_time: 2h 10m 59.0s\n",
      "\n",
      "Epoch 130: val_NMSE did not improve from 0.00036\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7409e-04 - mse: 2.4478e-05 - NMSE: 2.2127e-04 - val_loss: 1.8937e-04 - val_mse: 3.9816e-05 - val_NMSE: 3.5991e-04\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7395e-04 - mse: 2.4432e-05 - NMSE: 2.2086e-04 - tot_time: 2h 11m 20.0s\n",
      "\n",
      "Epoch 131: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7395e-04 - mse: 2.4432e-05 - NMSE: 2.2086e-04 - val_loss: 1.8913e-04 - val_mse: 3.9662e-05 - val_NMSE: 3.5852e-04\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7380e-04 - mse: 2.4381e-05 - NMSE: 2.2039e-04 - tot_time: 2h 11m 40.8s\n",
      "\n",
      "Epoch 132: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7380e-04 - mse: 2.4381e-05 - NMSE: 2.2039e-04 - val_loss: 1.8898e-04 - val_mse: 3.9600e-05 - val_NMSE: 3.5797e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7375e-04 - mse: 2.4414e-05 - NMSE: 2.2069e-04 - tot_time: 2h 12m 2.1s\n",
      "\n",
      "Epoch 133: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7375e-04 - mse: 2.4414e-05 - NMSE: 2.2069e-04 - val_loss: 1.8879e-04 - val_mse: 3.9511e-05 - val_NMSE: 3.5716e-04\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7357e-04 - mse: 2.4326e-05 - NMSE: 2.1989e-04 - tot_time: 2h 12m 23.5s\n",
      "\n",
      "Epoch 134: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7357e-04 - mse: 2.4326e-05 - NMSE: 2.1989e-04 - val_loss: 1.8860e-04 - val_mse: 3.9410e-05 - val_NMSE: 3.5625e-04\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7342e-04 - mse: 2.4277e-05 - NMSE: 2.1945e-04 - tot_time: 2h 12m 44.2s\n",
      "\n",
      "Epoch 135: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7342e-04 - mse: 2.4277e-05 - NMSE: 2.1945e-04 - val_loss: 1.8842e-04 - val_mse: 3.9320e-05 - val_NMSE: 3.5543e-04\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7324e-04 - mse: 2.4188e-05 - NMSE: 2.1865e-04 - tot_time: 2h 13m 5.3s\n",
      "\n",
      "Epoch 136: val_NMSE improved from 0.00036 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7324e-04 - mse: 2.4188e-05 - NMSE: 2.1865e-04 - val_loss: 1.8826e-04 - val_mse: 3.9252e-05 - val_NMSE: 3.5482e-04\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7317e-04 - mse: 2.4210e-05 - NMSE: 2.1885e-04 - tot_time: 2h 13m 26.7s\n",
      "\n",
      "Epoch 137: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7317e-04 - mse: 2.4210e-05 - NMSE: 2.1885e-04 - val_loss: 1.8815e-04 - val_mse: 3.9242e-05 - val_NMSE: 3.5473e-04\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7305e-04 - mse: 2.4187e-05 - NMSE: 2.1864e-04 - tot_time: 2h 13m 47.7s\n",
      "\n",
      "Epoch 138: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7305e-04 - mse: 2.4187e-05 - NMSE: 2.1864e-04 - val_loss: 1.8797e-04 - val_mse: 3.9157e-05 - val_NMSE: 3.5396e-04\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7286e-04 - mse: 2.4089e-05 - NMSE: 2.1775e-04 - tot_time: 2h 14m 8.4s\n",
      "\n",
      "Epoch 139: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7286e-04 - mse: 2.4089e-05 - NMSE: 2.1775e-04 - val_loss: 1.8776e-04 - val_mse: 3.9039e-05 - val_NMSE: 3.5289e-04\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7272e-04 - mse: 2.4046e-05 - NMSE: 2.1736e-04 - tot_time: 2h 14m 29.5s\n",
      "\n",
      "Epoch 140: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7272e-04 - mse: 2.4046e-05 - NMSE: 2.1736e-04 - val_loss: 1.8763e-04 - val_mse: 3.9003e-05 - val_NMSE: 3.5257e-04\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7267e-04 - mse: 2.4096e-05 - NMSE: 2.1782e-04 - tot_time: 2h 14m 51.6s\n",
      "\n",
      "Epoch 141: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7267e-04 - mse: 2.4096e-05 - NMSE: 2.1782e-04 - val_loss: 1.8744e-04 - val_mse: 3.8915e-05 - val_NMSE: 3.5177e-04\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7253e-04 - mse: 2.4050e-05 - NMSE: 2.1740e-04 - tot_time: 2h 15m 12.6s\n",
      "\n",
      "Epoch 142: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7253e-04 - mse: 2.4050e-05 - NMSE: 2.1740e-04 - val_loss: 1.8730e-04 - val_mse: 3.8872e-05 - val_NMSE: 3.5139e-04\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7250e-04 - mse: 2.4112e-05 - NMSE: 2.1796e-04 - tot_time: 2h 15m 33.2s\n",
      "\n",
      "Epoch 143: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7250e-04 - mse: 2.4112e-05 - NMSE: 2.1796e-04 - val_loss: 1.8714e-04 - val_mse: 3.8807e-05 - val_NMSE: 3.5079e-04\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7229e-04 - mse: 2.4000e-05 - NMSE: 2.1695e-04 - tot_time: 2h 15m 53.8s\n",
      "\n",
      "Epoch 144: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7229e-04 - mse: 2.4000e-05 - NMSE: 2.1695e-04 - val_loss: 1.8700e-04 - val_mse: 3.8760e-05 - val_NMSE: 3.5037e-04\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7214e-04 - mse: 2.3954e-05 - NMSE: 2.1653e-04 - tot_time: 2h 16m 15.2s\n",
      "\n",
      "Epoch 145: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7214e-04 - mse: 2.3954e-05 - NMSE: 2.1653e-04 - val_loss: 1.8667e-04 - val_mse: 3.8532e-05 - val_NMSE: 3.4831e-04\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7199e-04 - mse: 2.3901e-05 - NMSE: 2.1605e-04 - tot_time: 2h 16m 36.6s\n",
      "\n",
      "Epoch 146: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7199e-04 - mse: 2.3901e-05 - NMSE: 2.1605e-04 - val_loss: 1.8645e-04 - val_mse: 3.8413e-05 - val_NMSE: 3.4723e-04\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7183e-04 - mse: 2.3839e-05 - NMSE: 2.1549e-04 - tot_time: 2h 16m 58.2s\n",
      "\n",
      "Epoch 147: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7183e-04 - mse: 2.3839e-05 - NMSE: 2.1549e-04 - val_loss: 1.8639e-04 - val_mse: 3.8446e-05 - val_NMSE: 3.4753e-04\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7163e-04 - mse: 2.3735e-05 - NMSE: 2.1455e-04 - tot_time: 2h 17m 19.1s\n",
      "\n",
      "Epoch 148: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7163e-04 - mse: 2.3735e-05 - NMSE: 2.1455e-04 - val_loss: 1.8619e-04 - val_mse: 3.8350e-05 - val_NMSE: 3.4667e-04\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7153e-04 - mse: 2.3739e-05 - NMSE: 2.1459e-04 - tot_time: 2h 17m 39.6s\n",
      "\n",
      "Epoch 149: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 1.7153e-04 - mse: 2.3739e-05 - NMSE: 2.1459e-04 - val_loss: 1.8621e-04 - val_mse: 3.8470e-05 - val_NMSE: 3.4775e-04\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7153e-04 - mse: 2.3836e-05 - NMSE: 2.1547e-04 - tot_time: 2h 18m 0.6s\n",
      "\n",
      "Epoch 150: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7153e-04 - mse: 2.3836e-05 - NMSE: 2.1547e-04 - val_loss: 1.8630e-04 - val_mse: 3.8654e-05 - val_NMSE: 3.4941e-04\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7152e-04 - mse: 2.3929e-05 - NMSE: 2.1631e-04 - tot_time: 2h 18m 21.2s\n",
      "\n",
      "Epoch 151: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7152e-04 - mse: 2.3929e-05 - NMSE: 2.1631e-04 - val_loss: 1.8648e-04 - val_mse: 3.8944e-05 - val_NMSE: 3.5203e-04\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7160e-04 - mse: 2.4104e-05 - NMSE: 2.1789e-04 - tot_time: 2h 18m 42.5s\n",
      "\n",
      "Epoch 152: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7160e-04 - mse: 2.4104e-05 - NMSE: 2.1789e-04 - val_loss: 1.8674e-04 - val_mse: 3.9304e-05 - val_NMSE: 3.5529e-04\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7133e-04 - mse: 2.3939e-05 - NMSE: 2.1640e-04 - tot_time: 2h 19m 3.9s\n",
      "\n",
      "Epoch 153: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7133e-04 - mse: 2.3939e-05 - NMSE: 2.1640e-04 - val_loss: 1.8603e-04 - val_mse: 3.8696e-05 - val_NMSE: 3.4979e-04\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7115e-04 - mse: 2.3859e-05 - NMSE: 2.1568e-04 - tot_time: 2h 19m 25.0s\n",
      "\n",
      "Epoch 154: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7115e-04 - mse: 2.3859e-05 - NMSE: 2.1568e-04 - val_loss: 1.8623e-04 - val_mse: 3.8990e-05 - val_NMSE: 3.5245e-04\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7115e-04 - mse: 2.3963e-05 - NMSE: 2.1661e-04 - tot_time: 2h 19m 45.5s\n",
      "\n",
      "Epoch 155: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 20s 1s/step - loss: 1.7115e-04 - mse: 2.3963e-05 - NMSE: 2.1661e-04 - val_loss: 1.8608e-04 - val_mse: 3.8942e-05 - val_NMSE: 3.5202e-04\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7112e-04 - mse: 2.4035e-05 - NMSE: 2.1726e-04Restoring model weights from the end of the best epoch: 146.\n",
      " - tot_time: 2h 20m 6.7s\n",
      "\n",
      "Epoch 156: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7112e-04 - mse: 2.4035e-05 - NMSE: 2.1726e-04 - val_loss: 1.8618e-04 - val_mse: 3.9150e-05 - val_NMSE: 3.5389e-04\n",
      "Epoch 156: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-05 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7207e-04 - mse: 2.4036e-05 - NMSE: 2.1727e-04 - tot_time: 2h 20m 27.7s\n",
      "\n",
      "Epoch 1: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7207e-04 - mse: 2.4036e-05 - NMSE: 2.1727e-04 - val_loss: 1.8635e-04 - val_mse: 3.8322e-05 - val_NMSE: 3.4641e-04\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7175e-04 - mse: 2.3726e-05 - NMSE: 2.1447e-04 - tot_time: 2h 20m 49.4s\n",
      "\n",
      "Epoch 2: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7175e-04 - mse: 2.3726e-05 - NMSE: 2.1447e-04 - val_loss: 1.8640e-04 - val_mse: 3.8382e-05 - val_NMSE: 3.4696e-04\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7173e-04 - mse: 2.3713e-05 - NMSE: 2.1435e-04 - tot_time: 2h 21m 10.3s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7173e-04 - mse: 2.3713e-05 - NMSE: 2.1435e-04 - val_loss: 1.8636e-04 - val_mse: 3.8352e-05 - val_NMSE: 3.4668e-04\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7174e-04 - mse: 2.3738e-05 - NMSE: 2.1458e-04 - tot_time: 2h 21m 31.0s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7174e-04 - mse: 2.3738e-05 - NMSE: 2.1458e-04 - val_loss: 1.8635e-04 - val_mse: 3.8351e-05 - val_NMSE: 3.4667e-04\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7170e-04 - mse: 2.3707e-05 - NMSE: 2.1430e-04 - tot_time: 2h 21m 52.3s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7170e-04 - mse: 2.3707e-05 - NMSE: 2.1430e-04 - val_loss: 1.8635e-04 - val_mse: 3.8360e-05 - val_NMSE: 3.4675e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7178e-04 - mse: 2.3792e-05 - NMSE: 2.1507e-04 - tot_time: 2h 22m 14.3s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 22s 1s/step - loss: 1.7178e-04 - mse: 2.3792e-05 - NMSE: 2.1507e-04 - val_loss: 1.8628e-04 - val_mse: 3.8306e-05 - val_NMSE: 3.4627e-04\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7164e-04 - mse: 2.3669e-05 - NMSE: 2.1396e-04 - tot_time: 2h 22m 35.5s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7164e-04 - mse: 2.3669e-05 - NMSE: 2.1396e-04 - val_loss: 1.8628e-04 - val_mse: 3.8317e-05 - val_NMSE: 3.4637e-04\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7173e-04 - mse: 2.3765e-05 - NMSE: 2.1482e-04 - tot_time: 2h 22m 57.0s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7173e-04 - mse: 2.3765e-05 - NMSE: 2.1482e-04 - val_loss: 1.8627e-04 - val_mse: 3.8311e-05 - val_NMSE: 3.4631e-04\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7167e-04 - mse: 2.3718e-05 - NMSE: 2.1440e-04 - tot_time: 2h 23m 17.7s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7167e-04 - mse: 2.3718e-05 - NMSE: 2.1440e-04 - val_loss: 1.8625e-04 - val_mse: 3.8309e-05 - val_NMSE: 3.4630e-04\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7163e-04 - mse: 2.3688e-05 - NMSE: 2.1413e-04 - tot_time: 2h 23m 38.6s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7163e-04 - mse: 2.3688e-05 - NMSE: 2.1413e-04 - val_loss: 1.8620e-04 - val_mse: 3.8271e-05 - val_NMSE: 3.4595e-04\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7165e-04 - mse: 2.3722e-05 - NMSE: 2.1444e-04Restoring model weights from the end of the best epoch: 1.\n",
      " - tot_time: 2h 24m 0.0s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00035\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_004/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 21s 1s/step - loss: 1.7165e-04 - mse: 2.3722e-05 - NMSE: 2.1444e-04 - val_loss: 1.8620e-04 - val_mse: 3.8272e-05 - val_NMSE: 3.4596e-04\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "# compiling the network\n",
    "rnn_net.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=['mse', NMSE(divisor_arr=time_stddev)],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # this loads the weights/attributes of the optimizer as well\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net.load_weights(wt_file)\n",
    "    else:\n",
    "        rnn_net.load_weights(wt_file)\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # implementing early stopping\n",
    "    baseline = None\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        baseline = np.min(val_loss_hist)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_NMSE',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=True,\n",
    "        min_delta=min_delta,\n",
    "        baseline=baseline\n",
    "    )\n",
    "    #** the two lines below are useless because wait is set to 0 in on_train_begin\n",
    "    # early_stopping_cb.wait = earlystopping_wait\n",
    "    # print('early_stopping_cb.wait : {}\\n'.format(early_stopping_cb.wait))\n",
    "\n",
    "    # time callback for each epoch\n",
    "    timekeeper_cb = mytimecallback()\n",
    "\n",
    "    # model checkpoint callback\n",
    "    dir_name_ckpt = dir_name_rnn+dir_sep+'checkpoints'\n",
    "    if not os.path.isdir(dir_name_ckpt):\n",
    "        os.makedirs(dir_name_ckpt)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dir_name_ckpt+dir_sep+'checkpoint',#+'/checkpoint--loss={loss:.4f}--vall_loss={val_loss:.4f}',\n",
    "        monitor='val_NMSE',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=2,\n",
    "        initial_value_threshold=baseline,\n",
    "        period=1  # saves every `period` epochs\n",
    "    )\n",
    "\n",
    "    # save losses callback\n",
    "    savelosses_cb = SaveLosses(\n",
    "        filepath=dir_name_ckpt+dir_sep+'LossHistoriesCheckpoint',\n",
    "        val_loss_arr=savelosses_cb_vallossarr,\n",
    "        train_loss_arr=savelosses_cb_trainlossarr,\n",
    "        total_epochs=epochs,\n",
    "        period=1)\n",
    "\n",
    "    for i in range(starting_lr_idx, len(learning_rate_list)):\n",
    "        learning_rate = learning_rate_list[i]\n",
    "        K.set_value(rnn_net.optimizer.lr, learning_rate)\n",
    "\n",
    "        savelosses_cb.update_lr_idx(i)\n",
    "\n",
    "        if i == starting_lr_idx:\n",
    "            EPOCHS = num_epochs_left\n",
    "            savelosses_cb.update_offset(epochs-num_epochs_left)\n",
    "        else:\n",
    "            EPOCHS = epochs\n",
    "            savelosses_cb.update_offset(0)\n",
    "\n",
    "        total_s_len = 80\n",
    "        sep_lr_s = ' LEARNING RATE : {} '.format(learning_rate)\n",
    "        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n",
    "        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n",
    "        print('\\n\\n' + '-'*len(sep_lr_s))\n",
    "        print('\\n' + sep_lr_s+'\\n')\n",
    "        print('-'*len(sep_lr_s) + '\\n\\n')\n",
    "        \n",
    "        history = rnn_net.fit(training_data_rnn_input, training_data_rnn_output,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=batch_size,\n",
    "#             validation_split=val_split/train_split,\n",
    "            validation_data=(val_data_rnn_input, val_data_rnn_output),\n",
    "            callbacks=[early_stopping_cb, timekeeper_cb, checkpoint_cb, savelosses_cb],\n",
    "            verbose=1,\n",
    "            shuffle=not stateful,\n",
    "        )\n",
    "\n",
    "        val_loss_hist.extend(history.history['val_loss'])\n",
    "        train_loss_hist.extend(history.history['loss'])\n",
    "        \n",
    "        val_NMSE_hist.extend(history.history['val_NMSE'])\n",
    "        train_NMSE_hist.extend(history.history['NMSE'])\n",
    "\n",
    "        val_MSE_hist.extend(history.history['val_mse'])\n",
    "        train_MSE_hist.extend(history.history['mse'])\n",
    "        \n",
    "        if i == starting_lr_idx:\n",
    "            lr_change[i+1] += len(history.history['val_loss'])\n",
    "        else:\n",
    "            lr_change.append(lr_change[i]+len(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 552, 2) (96, 552, 2)\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 3.5668e-04 - mse: 2.0865e-04 - NMSE: 0.0019\n"
     ]
    }
   ],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    for layer in rnn_net.rnn_list:\n",
    "        if layer.stateful == True:\n",
    "            layer.reset_states()\n",
    "    print(testing_data_rnn_input.shape, testing_data_rnn_output.shape)\n",
    "    eval_dict = rnn_net.evaluate(\n",
    "        testing_data_rnn_input, testing_data_rnn_output,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "\n",
    "    with open(save_path+dir_sep+'losses.txt', 'w') as f:\n",
    "        f.write(str({\n",
    "            'val_loss_hist':val_loss_hist,\n",
    "            'train_loss_hist':train_loss_hist,\n",
    "            'val_MSE_hist':val_MSE_hist,\n",
    "            'train_MSE_hist':train_MSE_hist,\n",
    "            'val_NMSE_hist':val_NMSE_hist,\n",
    "            'train_NMSE_hist':train_NMSE_hist,\n",
    "            'lr_change':lr_change,\n",
    "            'test_loss':eval_dict[0],\n",
    "            'test_MSE':eval_dict[1],\n",
    "            'test_NMSE':eval_dict[2],\n",
    "        }))\n",
    "        \n",
    "    if normalize_dataset == True:\n",
    "        with open(save_path+dir_sep+'rnn_normalization.txt', 'w') as f:\n",
    "            f.write(str({\n",
    "                'normalization_arr':normalization_arr\n",
    "            }))\n",
    "\n",
    "    rnn_net.save_everything(\n",
    "        file_name=save_path+dir_sep+'final_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_kwargs = {'fontsize':15}\n",
    "ylabel_kwargs = {'fontsize':15}\n",
    "legend_kwargs = {'fontsize':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting losses\n",
    "dir_name_plot = dir_name_rnn + '/plots'\n",
    "if not os.path.isdir(dir_name_plot):\n",
    "    os.makedirs(dir_name_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": [
    "# Visualize loss history\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_loss_hist,\n",
    "    val_loss=val_loss_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "\n",
    "plt.savefig(dir_name_plot + '{ds}loss_history.pdf'.format(ds=dir_sep), dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_MSE_hist,\n",
    "    val_loss=val_MSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training MSE', 'Validation MSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='MSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/MSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_NMSE_hist,\n",
    "    val_loss=val_NMSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training NMSE', 'Validation NMSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='NMSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/NMSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": [
    "def rescale_data(data, normalization_arr):\n",
    "    '''\n",
    "    data - [num_batches x num_timesteps x num_states]\n",
    "    normalization_arr = [2 x num_states]\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(data.shape[-1]):\n",
    "        new_data[:, i] -= normalization_arr[0, i]\n",
    "        new_data[:, i] /= normalization_arr[1, i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def norm_sq_time_average(data):\n",
    "    data_norm_sq = np.zeros(shape=data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_norm_sq[:] += data[:, i]**2\n",
    "    # integrating using the trapezoidal rule\n",
    "    norm_sq_time_avg = np.sum(data_norm_sq) - 0.5*(data_norm_sq[0]+data_norm_sq[-1])\n",
    "    norm_sq_time_avg /= data_norm_sq.shape[0]\n",
    "    return norm_sq_time_avg\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = np.empty_like(data)\n",
    "    shape = new_data.shape\n",
    "    # print(shape)\n",
    "    for i in range(shape[-1]):\n",
    "        if len(shape) == 2:\n",
    "            new_data[:, i] = data[:, i]\n",
    "            new_data[:, i] *= normalization_arr[1, i]\n",
    "            new_data[:, i] += normalization_arr[0, i]\n",
    "        elif len(shape) == 3:\n",
    "            new_data[:, :, i] = data[:, :, i]\n",
    "            new_data[:, :, i] *= normalization_arr[1, i]\n",
    "            new_data[:, :, i] += normalization_arr[0, i]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_rnn/rnn_004\n",
      "num_runs : 100\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.9248239567077842, median : 0.724641689878773\n",
      "ph_min : 0.09058021123484662, ph_max : 2.989146970749939\n",
      "stddev : 0.7123423850926088, IQR : 0.9963823235833129\n",
      "1st quartile : 0.3623208449393865, 3rd quartile : 1.3587031685226993\n",
      "analysis time : 13.329096794128418 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = AR_testing_data_rnn_input.shape[0]\n",
    "\n",
    "analysis_time = time.time()\n",
    "\n",
    "AR_rnn_net = AR_RNN(\n",
    "    load_file=save_path+'/final_net_class_dict.txt',\n",
    "    T_input=T_sample_input_AR,\n",
    "    T_output=T_sample_output_AR,\n",
    "    stddev=0.0,\n",
    "    batch_size=num_runs,\n",
    "    lambda_reg=lambda_reg,\n",
    ")\n",
    "AR_rnn_net.build(input_shape=tuple(AR_testing_data_rnn_input.shape[0:2]) + tuple(testing_data_rnn_input.shape[2:]))\n",
    "AR_rnn_net.load_weights_from_file(save_path+'/final_net_gru_weights.h5')\n",
    "\n",
    "AR_AERNN_net = AR_AERNN(\n",
    "    ae_net,\n",
    "    AR_rnn_net,\n",
    "    normalization_arr,\n",
    "    normalization_constant_arr_aedata,\n",
    "    covmat_lmda=0.0,\n",
    "    time_stddev_ogdata=time_stddev_ogdata,\n",
    "    time_mean_ogdata=time_mean_ogdata,\n",
    "    loss_weights=None,\n",
    "    clipnorm=None,\n",
    "    global_clipnorm=None\n",
    ")\n",
    "\n",
    "savefig_fname = 'pre_ARtraining-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "npsavedata_fname = '/prediction_horizons-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "plot_dir = '/plots'\n",
    "\n",
    "sidx1 = dir_name_rnn[::-1].index('/')\n",
    "sidx2 = dir_name_rnn[-sidx1-2::-1].index('/')\n",
    "print(dir_name_rnn[-(sidx1+sidx2+1):])\n",
    "print('num_runs :', num_runs)\n",
    "\n",
    "prediction_horizon_arr = np.empty(shape=num_runs)\n",
    "prediction = np.array(AR_AERNN_net(AR_testing_data_rnn_input, training=False))\n",
    "prediction = invert_normalization(prediction, normalization_constant_arr_aedata)\n",
    "\n",
    "data_in_og = AR_testing_data_rnn_input\n",
    "data_out_og = AR_testing_data_rnn_output\n",
    "\n",
    "energySpectrum_dataout = 0.0\n",
    "energySpectrum_pred = 0.0\n",
    "\n",
    "avg_time = 0.\n",
    "for i in range(num_runs):\n",
    "    run_time = time.time()\n",
    "    lyap_time = lyapunov_time_arr[0]\n",
    "\n",
    "    data_out = data_out_og[i]\n",
    "    data_out = invert_normalization(data_out, normalization_constant_arr_aedata)\n",
    "\n",
    "    ### Error and prediction horizon\n",
    "    # error = np.linalg.norm(data_out[:, :] - prediction[i, :, :], axis=1)\n",
    "    error = (data_out[:, :] - prediction[i, :, :])**2\n",
    "    # error /= norm_sq_time_average(data_out)**0.5\n",
    "    error = np.mean(np.divide(error, time_stddev_ogdata**2), axis=1)**0.5\n",
    "\n",
    "    predhor_idx = np.where(error >= error_threshold)[0]\n",
    "    if predhor_idx.shape[0] == 0:\n",
    "        predhor_idx = error.shape[0]\n",
    "    else:\n",
    "        predhor_idx = predhor_idx[0]\n",
    "\n",
    "    prediction_horizon_arr[i] = predhor_idx*dt_rnn/lyap_time\n",
    "\n",
    "    run_time = time.time() - run_time\n",
    "    avg_time = (avg_time*i + run_time)/(i+1)\n",
    "    eta = avg_time * (num_runs-1 - i)\n",
    "    # print('    {} / {} -- run_time : {:.2f} s -- eta : {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "    #     i+1,\n",
    "    #     num_runs,\n",
    "    #     run_time,\n",
    "    #     float(eta // 3600),\n",
    "    #     float((eta%3600)//60),\n",
    "    #     float((eta%3600)%60),\n",
    "    # ))\n",
    "\n",
    "median_idx = int(np.round(0.5*num_runs-1))\n",
    "quartile_1_idx = int(np.round(0.25*num_runs-1))\n",
    "quartile_3_idx = int(np.round(0.75*num_runs-1))\n",
    "\n",
    "prediction_horizon_arr.sort()\n",
    "\n",
    "median = prediction_horizon_arr[median_idx]\n",
    "quartile_1 = prediction_horizon_arr[quartile_1_idx]\n",
    "quartile_3 = prediction_horizon_arr[quartile_3_idx]\n",
    "IQR = quartile_3 - quartile_1\n",
    "\n",
    "prediction_horizon = np.mean(prediction_horizon_arr)\n",
    "stddev_ph = np.std(prediction_horizon_arr)\n",
    "\n",
    "s = 'error_threshold = {}\\n'.format(error_threshold)\n",
    "s += 'prediction_horizon : {}, median : {}\\n'.format(prediction_horizon, median)\n",
    "s += 'ph_min : {}, ph_max : {}\\n'.format(prediction_horizon_arr.min(), prediction_horizon_arr.max())\n",
    "s += 'stddev : {}, IQR : {}\\n'.format(stddev_ph, IQR)\n",
    "s += '1st quartile : {}, 3rd quartile : {}'.format(quartile_1, quartile_3)\n",
    "\n",
    "print('\\n'+s)\n",
    "\n",
    "plot_histogram_and_save(\n",
    "    prediction_horizon_arr, median,\n",
    "    save_dir=dir_name_rnn+plot_dir,\n",
    "    savefig_fname=savefig_fname,\n",
    ")\n",
    "\n",
    "np.savez(\n",
    "    dir_name_rnn+npsavedata_fname,\n",
    "    prediction_horizon_arr=prediction_horizon_arr,\n",
    "    error_threshold=error_threshold,\n",
    ")\n",
    "\n",
    "with open(dir_name_rnn+npsavedata_fname+'--statistics.txt', 'w') as fl:\n",
    "    fl.write(s)\n",
    "\n",
    "print('analysis time : {} s\\n'.format(time.time() - analysis_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'use_trainable_weights_with_reslayers' in rnn_net.__dict__.keys():\n",
    "    if use_trainable_weights_with_reslayers == True:\n",
    "        for i in range(rnn_net.num_skip_connections):\n",
    "            print('reslayer_factor_{} : {}'.format(i, rnn_net.reslayer_factor[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
