{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from keras.engine import data_adapter\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "array = np.array\n",
    "float32 = np.float32\n",
    "int32 = np.int32\n",
    "float64 = np.float64\n",
    "int64 = np.int64\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/new_lorenz\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data_KS, plot_latent_states_KS , readAndReturnLossHistories, sigmoidWarmupAndDecayLRSchedule\n",
    "from tools.ae_v2 import Autoencoder\n",
    "from tools.ESN_v2_ensembleAR import ESN_ensemble as AR_RNN\n",
    "from tools.AEESN_AR_v1 import AR_AERNN_ESN as AR_AERNN\n",
    "from tools.trainAEESN_ensemble import trainAERNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 22:42:23.385986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:24.039025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:24.039462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:24.041683: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-09 22:42:24.055627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:24.055863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:24.056210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:28.064181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:28.064619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:28.065004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-09 22:42:28.065291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23656 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 0\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "2.8.1\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print('')\n",
    "print(tf.config.list_logical_devices())\n",
    "print('')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_name_AR_AErnn: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009\n",
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_ESN_ensemble/ESN_ensemble_006\n",
      "dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_ae/ae_024\n",
      "data_dir_idx: 010\n",
      "normalize_flag_ogdata: False\n"
     ]
    }
   ],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_ESN_ensemble/ESN_ensemble_006'\n",
    "\n",
    "    # making AR-RNN save directory\n",
    "    dir_name_ARrnn = os.getcwd() + dir_sep + 'saved_AR_AEESN_rnn'\n",
    "    if not os.path.isdir(dir_name_ARrnn):\n",
    "        os.makedirs(dir_name_ARrnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'AR_ESN_ensemble_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_ARrnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_ARrnn = dir_name_ARrnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_ARrnn)\n",
    "    os.makedirs(dir_name_ARrnn+dir_sep+'plots')\n",
    "    \n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in rnn_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "        \n",
    "    \n",
    "    # training params\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "    try:\n",
    "        fRMS = tparams_dict['fRMS']\n",
    "    except:\n",
    "        fRMS = 0.0\n",
    "\n",
    "    loss_weights = 0.98\n",
    "else:\n",
    "    # AR-RNN directory\n",
    "    dir_name_ARrnn = os.getcwd()+'/saved_AR_AERNN_rnn/AR_rnn_014'\n",
    "\n",
    "    # reading AR-RNN parameters\n",
    "    with open(dir_name_ARrnn + '/AR_RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    params_AR_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dir_name_rnn = params_AR_rnn_dict['dir_name_rnn']\n",
    "    rnn_idx = dir_name_rnn[-3:]\n",
    "    dir_name_rnn = os.getcwd()+'/saved_ESN/ESN_'+rnn_idx\n",
    "\n",
    "    dt_rnn = params_AR_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_AR_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_AR_rnn_dict['T_sample_output']\n",
    "    T_offset = params_AR_rnn_dict['T_offset']\n",
    "    return_params_arr = params_AR_rnn_dict['return_params_arr']\n",
    "    params = params_AR_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_AR_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in AR_rnn_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_AR_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in AR_RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_AR_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in AR_RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        use_ae_data = params_AR_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in AR_RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "    try:\n",
    "        normalization_type = params_AR_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in AR_RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "\n",
    "    # training params\n",
    "    with open(dir_name_ARrnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "    covmat_lmda = tparams_dict['covmat_lmda']\n",
    "    try:\n",
    "        lambda_reg = tparams_dict['lambda_reg']\n",
    "    except:\n",
    "        lambda_reg = 1e-6\n",
    "    try:\n",
    "        fRMS = tparams_dict['fRMS']\n",
    "    except:\n",
    "        fRMS = 0.0\n",
    "    try:\n",
    "        loss_weights = tparams_dict['loss_weights']\n",
    "    except:\n",
    "        loss_weights = None\n",
    "    if 'freeze_layers' in tparams_dict.keys():\n",
    "        freeze_layers = tparams_dict['freeze_layers']\n",
    "    else:\n",
    "        freeze_layers = None\n",
    "    if 'clipnorm' in tparams_dict.keys():\n",
    "        clipnorm = tparams_dict['clipnorm']\n",
    "    else:\n",
    "        clipnorm = None\n",
    "    \n",
    "\n",
    "\n",
    "# reading stddev\n",
    "with open(dir_name_rnn + '/final_net/0_final_net_class_dict.txt') as f:\n",
    "    lines = f.readlines()\n",
    "finalnet_dict = eval(''.join(lines))\n",
    "stddev = finalnet_dict['stddev']\n",
    "# stddev = 0.0\n",
    "\n",
    "# reading RNN normalization constants\n",
    "normalization_arr_rnn = None\n",
    "if normalize_dataset == True:\n",
    "    with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    normarr_rnn_dict = eval(''.join(lines))\n",
    "    normalization_arr_rnn = normarr_rnn_dict['normalization_arr']\n",
    "\n",
    "if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_arr_rnn = fl['normalization_arr'][0]\n",
    "\n",
    "# reading AE directory\n",
    "with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "params_dict = eval(''.join(lines))\n",
    "\n",
    "dir_name_ae = params_dict['dir_name_ae']\n",
    "ae_idx = dir_name_ae[-3:]\n",
    "dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "try:\n",
    "    use_ae_data = params_dict['use_ae_data']\n",
    "except:\n",
    "    print(\"'use_ae_data' not present in sim_data_AE_params, set to True.\")\n",
    "    use_ae_data = True\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in ae_data, set to True.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "print('dir_name_AR_AErnn:', dir_name_ARrnn)\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state = params_dict['init_state']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data'].astype(FTYPE)\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']\n",
    "\n",
    "\n",
    "test_split = 1 - train_split - val_split\n",
    "\n",
    "# setting seed for PRNGs\n",
    "np.random.seed(prng_seed)\n",
    "tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case : 1, lyapunov exponent : 0.9058021372262592, lyapunov time : 1.1039938926696777s\n"
     ]
    }
   ],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delaing with normalizing the data before feeding into autoencoder\n",
    "num_params = params_mat.shape[1]\n",
    "og_vars = all_data.shape[1]\n",
    "if alldata_withparams_flag == True:\n",
    "    og_vars -= num_params\n",
    "\n",
    "# if use_ae_data == True:\n",
    "#     if ae_data_with_params == True and alldata_withparams_flag == False:\n",
    "#         new_all_data = np.empty(shape=(all_data.shape[0], og_vars+num_params), dtype=FTYPE)\n",
    "#         new_all_data[:, 0:og_vars] = all_data[:, 0:og_vars]\n",
    "#         del(all_data)\n",
    "#         all_data = new_all_data\n",
    "#         prev_idx = 0\n",
    "#         for i in range(boundary_idx_arr.shape[0]):\n",
    "#             all_data[prev_idx:boundary_idx_arr[i], num_params:] = params_mat[i]\n",
    "#             prev_idx = boundary_idx_arr[i]\n",
    "\n",
    "#     if normalizeforae_flag == True:\n",
    "#         for i in range(all_data.shape[1]):\n",
    "#             all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "#             all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "#     if ae_data_with_params == False:\n",
    "#         all_data = all_data[:, 0:og_vars]\n",
    "# else:\n",
    "#     # using raw data, neglecting the params attached (if any)\n",
    "#     all_data = all_data[:, 0:og_vars]\n",
    "\n",
    "if use_ae_data == True and ae_data_with_params == False:\n",
    "    all_data = all_data[:, 0:og_vars]\n",
    "else:\n",
    "    all_data = all_data[:, 0:og_vars]\n",
    "    \n",
    "normalization_constant_arr_aedata = normalization_constant_arr_aedata[:, 0:all_data.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 100001\n",
    "all_data = all_data[0:a]\n",
    "boundary_idx_arr = [all_data.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data.shape :  (100001, 3)\n",
      "all_data.dtype :  float32\n"
     ]
    }
   ],
   "source": [
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_data.dtype : ', all_data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "# if use_ae_data == True:\n",
    "#     load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "#     wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "# if use_ae_data == True:\n",
    "#     ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "#     ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    num_lyaptimesteps_totrain = np.array([\n",
    "        5,#10,\n",
    "        10,#16,\n",
    "        15,#23,\n",
    "        20,#30,\n",
    "        # 55,\n",
    "    ])*dt_rnn/np.mean(lyapunov_time_arr)\n",
    "    num_timesteps_warmup = 1*np.mean(lyapunov_time_arr)/dt_rnn\n",
    "    T_sample_input = num_timesteps_warmup*dt_rnn\n",
    "    T_sample_output = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_offset = T_sample_input\n",
    "    skip_intermediate = 'full sample'\n",
    "    stateful = True\n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "\n",
    "    # saving AR RNN specific data\n",
    "    AR_RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_lyaptimesteps_totrain':num_lyaptimesteps_totrain,\n",
    "        'num_timesteps_warmup':num_timesteps_warmup,\n",
    "        'dir_name_rnn':dir_name_rnn,\n",
    "        'dir_name_ae':dir_name_ae,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':AR_RNN.__module__,\n",
    "        'normalization_type':normalization_type,\n",
    "        'use_ae_data':use_ae_data,\n",
    "        'stateful':stateful,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_ARrnn+dir_sep+'AR_RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(AR_RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [\n",
    "        # [1e-3, 5e-4, 1e-4],\n",
    "        [1e-4, 5e-5, 1e-5],\n",
    "        [1e-5, 5e-6, 1e-6],\n",
    "        [1e-6, 5e-7, 1e-7],\n",
    "        [1e-6, 5e-7, 1e-7],\n",
    "    ]\n",
    "    epochs = [\n",
    "        [200]*len(learning_rate_list[0]),\n",
    "        [200]*len(learning_rate_list[1]),\n",
    "        [200]*len(learning_rate_list[2]),\n",
    "        [200]*len(learning_rate_list[3]),\n",
    "        # [1000],\n",
    "    ]\n",
    "    patience = [\n",
    "        [20]*len(learning_rate_list[0]),\n",
    "        [20]*len(learning_rate_list[1]),\n",
    "        [20]*len(learning_rate_list[2]),\n",
    "        [20]*len(learning_rate_list[3]),\n",
    "        # [50],\n",
    "    ] # parameter for early stopping\n",
    "    min_delta = 5e-6  # parameter for early stopping\n",
    "    lambda_reg = 7e-11  # weight for regularizer\n",
    "    covmat_lmda = 1e-3  # weight for the covmat loss\n",
    "\n",
    "    if loss_weights is None:\n",
    "        loss_weights = 1.0\n",
    "        \n",
    "    freeze_layers = [\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    ]\n",
    "    \n",
    "    clipnorm = None #1.0\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_alpha = [False]*len(learning_rate_list)\n",
    "    train_omega_in = [False]*len(learning_rate_list)\n",
    "    train_rho_res = [False]*len(learning_rate_list)\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'loss_weights':loss_weights,\n",
    "        'stddev':stddev,\n",
    "        'covmat_lmda':covmat_lmda,\n",
    "        'freeze_layers':freeze_layers,\n",
    "        'clipnorm':clipnorm,\n",
    "        'lambda_reg':lambda_reg,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_ARrnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_ARrnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr_rnn],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "rnn_kwargs = {}\n",
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    load_file_rnn = dir_name_rnn + '/final_net/final_net_class_dict.txt'\n",
    "    wt_file_rnn = dir_name_rnn+'/final_net/final_net_ESN_weights.hdf5'\n",
    "    \n",
    "    load_file_ae = dir_name_ae+'/final_net/final_net_class_dict.txt'\n",
    "    wt_file_ae = dir_name_ae+'/final_net/final_net_ae_weights.h5'\n",
    "    \n",
    "    rnn_kwargs = {\n",
    "        'train_alpha':train_alpha,\n",
    "        'train_omega_in':train_omega_in,\n",
    "        'train_rho_res':train_rho_res,\n",
    "        'wts_to_be_loaded':True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_return_load_wt_file_lists(\n",
    "        load_dir,\n",
    "        wt_matcher='weights.hdf5',\n",
    "        classdict_matcher='class_dict.txt',\n",
    "    ):\n",
    "    contents_load_dir = [f for f in os.listdir(load_dir) if os.path.isfile(os.path.join(load_dir, f))]\n",
    "    load_files_lst = [f for f in contents_load_dir if f.endswith(classdict_matcher)]\n",
    "    wt_files_lst = [f for f in contents_load_dir if f.endswith(wt_matcher)]\n",
    "\n",
    "    load_files_lst_startingidx = []\n",
    "    for i in range(len(load_files_lst)):\n",
    "        fn = load_files_lst[i]\n",
    "        idx = fn.find('_')\n",
    "        load_files_lst_startingidx.append(int(fn[0:idx]))\n",
    "\n",
    "    wt_files_lst_startingidx = []\n",
    "    for i in range(len(wt_files_lst)):\n",
    "        fn = wt_files_lst[i]\n",
    "        idx = fn.find('_')\n",
    "        wt_files_lst_startingidx.append(int(fn[0:idx]))\n",
    "\n",
    "    load_files_sortidx = np.argsort(load_files_lst_startingidx)\n",
    "    wt_files_sortidx = np.argsort(wt_files_lst_startingidx)\n",
    "\n",
    "    load_files_lst = np.array(load_files_lst)[load_files_sortidx]\n",
    "    wt_files_lst = np.array(wt_files_lst)[wt_files_sortidx]\n",
    "\n",
    "    load_file_rnn = [load_dir + '/' + fn for fn in load_files_lst]\n",
    "    wt_file_rnn = [load_dir + '/' +  fn for fn in wt_files_lst]\n",
    "    \n",
    "    return load_file_rnn, wt_file_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = dir_name_rnn + '/final_net'\n",
    "load_file_rnn, wt_file_rnn = find_and_return_load_wt_file_lists(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 5 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "clipnorm : None, global_clipnorm : 30\n",
      "2/2 [==============================] - 3s 23ms/step - loss: 0.0299 - mse: 0.0010 - NMSE: 0.0094 - NMSE_wt: 0.0088 - covmat_fro_loss: 0.0000e+00 - global_gradnorm: 0.0000e+00\n",
      "baseline : 8.8416E-03\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.0001 ----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0510 - mse: 0.0036 - NMSE: 0.0320 - NMSE_wt: 0.0300 - covmat_fro_loss: 8.5247e-04 - global_gradnorm: 14.6318 - tot_time: 0h 0m 5.5s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.00884\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 5s 38ms/step - loss: 0.0510 - mse: 0.0036 - NMSE: 0.0320 - NMSE_wt: 0.0300 - covmat_fro_loss: 8.4760e-04 - global_gradnorm: 14.2156 - val_loss: 0.0369 - val_mse: 0.0019 - val_NMSE: 0.0169 - val_NMSE_wt: 0.0159 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0643 - mse: 0.0051 - NMSE: 0.0463 - NMSE_wt: 0.0432 - covmat_fro_loss: 9.8264e-04 - global_gradnorm: 20.3675 - tot_time: 0h 0m 5.9s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.00884\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.0643 - mse: 0.0051 - NMSE: 0.0463 - NMSE_wt: 0.0432 - covmat_fro_loss: 9.6339e-04 - global_gradnorm: 20.0392 - val_loss: 0.0311 - val_mse: 0.0012 - val_NMSE: 0.0106 - val_NMSE_wt: 0.0101 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0712 - mse: 0.0060 - NMSE: 0.0537 - NMSE_wt: 0.0501 - covmat_fro_loss: 9.9652e-04 - global_gradnorm: 19.6008 - tot_time: 0h 0m 6.4s\n",
      "\n",
      "Epoch 3: val_NMSE_wt improved from 0.00884 to 0.00486, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-5_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0712 - mse: 0.0060 - NMSE: 0.0537 - NMSE_wt: 0.0501 - covmat_fro_loss: 9.7960e-04 - global_gradnorm: 20.2508 - val_loss: 0.0259 - val_mse: 5.6429e-04 - val_NMSE: 0.0051 - val_NMSE_wt: 0.0049 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0517 - mse: 0.0036 - NMSE: 0.0327 - NMSE_wt: 0.0307 - covmat_fro_loss: 9.4583e-04 - global_gradnorm: 19.0603 - tot_time: 0h 0m 7.1s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0525 - mse: 0.0037 - NMSE: 0.0336 - NMSE_wt: 0.0314 - covmat_fro_loss: 9.5320e-04 - global_gradnorm: 20.4278 - val_loss: 0.0349 - val_mse: 0.0016 - val_NMSE: 0.0147 - val_NMSE_wt: 0.0138 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0522 - mse: 0.0037 - NMSE: 0.0332 - NMSE_wt: 0.0311 - covmat_fro_loss: 7.9929e-04 - global_gradnorm: 17.2066 - tot_time: 0h 0m 7.6s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0515 - mse: 0.0036 - NMSE: 0.0325 - NMSE_wt: 0.0305 - covmat_fro_loss: 8.3362e-04 - global_gradnorm: 17.5702 - val_loss: 0.0577 - val_mse: 0.0044 - val_NMSE: 0.0393 - val_NMSE_wt: 0.0366 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0407 - mse: 0.0023 - NMSE: 0.0209 - NMSE_wt: 0.0196 - covmat_fro_loss: 7.6945e-04 - global_gradnorm: 13.8978 - tot_time: 0h 0m 8.0s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.0403 - mse: 0.0023 - NMSE: 0.0205 - NMSE_wt: 0.0192 - covmat_fro_loss: 7.8781e-04 - global_gradnorm: 13.9511 - val_loss: 0.0734 - val_mse: 0.0062 - val_NMSE: 0.0561 - val_NMSE_wt: 0.0524 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0614 - mse: 0.0048 - NMSE: 0.0432 - NMSE_wt: 0.0403 - covmat_fro_loss: 9.8439e-04 - global_gradnorm: 18.4588 - tot_time: 0h 0m 8.4s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0597 - mse: 0.0046 - NMSE: 0.0414 - NMSE_wt: 0.0386 - covmat_fro_loss: 9.3839e-04 - global_gradnorm: 17.3507 - val_loss: 0.0951 - val_mse: 0.0088 - val_NMSE: 0.0795 - val_NMSE_wt: 0.0741 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0546 - mse: 0.0040 - NMSE: 0.0358 - NMSE_wt: 0.0336 - covmat_fro_loss: 8.9224e-04 - global_gradnorm: 17.8585 - tot_time: 0h 0m 9.0s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0546 - mse: 0.0040 - NMSE: 0.0358 - NMSE_wt: 0.0336 - covmat_fro_loss: 9.0323e-04 - global_gradnorm: 18.4443 - val_loss: 0.0260 - val_mse: 5.7934e-04 - val_NMSE: 0.0052 - val_NMSE_wt: 0.0050 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0538 - mse: 0.0039 - NMSE: 0.0350 - NMSE_wt: 0.0327 - covmat_fro_loss: 8.0147e-04 - global_gradnorm: 13.9185 - tot_time: 0h 0m 9.5s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0538 - mse: 0.0039 - NMSE: 0.0350 - NMSE_wt: 0.0327 - covmat_fro_loss: 7.9801e-04 - global_gradnorm: 14.0717 - val_loss: 0.0305 - val_mse: 0.0011 - val_NMSE: 0.0100 - val_NMSE_wt: 0.0095 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0563 - mse: 0.0042 - NMSE: 0.0377 - NMSE_wt: 0.0352 - covmat_fro_loss: 9.4650e-04 - global_gradnorm: 14.2301 - tot_time: 0h 0m 10.1s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0563 - mse: 0.0042 - NMSE: 0.0377 - NMSE_wt: 0.0352 - covmat_fro_loss: 9.4343e-04 - global_gradnorm: 14.3985 - val_loss: 0.0276 - val_mse: 7.6698e-04 - val_NMSE: 0.0069 - val_NMSE_wt: 0.0065 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0498 - mse: 0.0034 - NMSE: 0.0308 - NMSE_wt: 0.0288 - covmat_fro_loss: 8.2467e-04 - global_gradnorm: 16.3085 - tot_time: 0h 0m 10.5s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.0491 - mse: 0.0033 - NMSE: 0.0300 - NMSE_wt: 0.0281 - covmat_fro_loss: 8.5516e-04 - global_gradnorm: 17.2753 - val_loss: 0.0639 - val_mse: 0.0051 - val_NMSE: 0.0457 - val_NMSE_wt: 0.0429 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0763 - mse: 0.0066 - NMSE: 0.0592 - NMSE_wt: 0.0553 - covmat_fro_loss: 0.0011 - global_gradnorm: 22.6697 - tot_time: 0h 0m 10.9s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.0733 - mse: 0.0062 - NMSE: 0.0560 - NMSE_wt: 0.0523 - covmat_fro_loss: 0.0010 - global_gradnorm: 20.4764 - val_loss: 0.0301 - val_mse: 0.0011 - val_NMSE: 0.0096 - val_NMSE_wt: 0.0091 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0616 - mse: 0.0048 - NMSE: 0.0434 - NMSE_wt: 0.0406 - covmat_fro_loss: 9.7695e-04 - global_gradnorm: 14.4259 - tot_time: 0h 0m 11.4s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0621 - mse: 0.0049 - NMSE: 0.0440 - NMSE_wt: 0.0411 - covmat_fro_loss: 9.6682e-04 - global_gradnorm: 16.3727 - val_loss: 0.0967 - val_mse: 0.0090 - val_NMSE: 0.0812 - val_NMSE_wt: 0.0757 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0462 - mse: 0.0030 - NMSE: 0.0269 - NMSE_wt: 0.0252 - covmat_fro_loss: 8.9438e-04 - global_gradnorm: 14.7155 - tot_time: 0h 0m 11.9s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0462 - mse: 0.0030 - NMSE: 0.0269 - NMSE_wt: 0.0252 - covmat_fro_loss: 8.7835e-04 - global_gradnorm: 15.6708 - val_loss: 0.0423 - val_mse: 0.0025 - val_NMSE: 0.0227 - val_NMSE_wt: 0.0212 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0509 - mse: 0.0036 - NMSE: 0.0320 - NMSE_wt: 0.0299 - covmat_fro_loss: 8.5026e-04 - global_gradnorm: 18.2485 - tot_time: 0h 0m 12.4s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.0501 - mse: 0.0035 - NMSE: 0.0311 - NMSE_wt: 0.0291 - covmat_fro_loss: 8.4005e-04 - global_gradnorm: 17.2115 - val_loss: 0.1280 - val_mse: 0.0127 - val_NMSE: 0.1147 - val_NMSE_wt: 0.1070 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0536 - mse: 0.0039 - NMSE: 0.0348 - NMSE_wt: 0.0326 - covmat_fro_loss: 9.9262e-04 - global_gradnorm: 19.3163 - tot_time: 0h 0m 12.8s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0516 - mse: 0.0036 - NMSE: 0.0327 - NMSE_wt: 0.0306 - covmat_fro_loss: 9.7074e-04 - global_gradnorm: 17.2685 - val_loss: 0.0336 - val_mse: 0.0015 - val_NMSE: 0.0134 - val_NMSE_wt: 0.0126 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0407 - mse: 0.0023 - NMSE: 0.0210 - NMSE_wt: 0.0197 - covmat_fro_loss: 7.4017e-04 - global_gradnorm: 15.0737 - tot_time: 0h 0m 13.3s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0403 - mse: 0.0023 - NMSE: 0.0206 - NMSE_wt: 0.0193 - covmat_fro_loss: 7.6630e-04 - global_gradnorm: 16.9395 - val_loss: 0.0346 - val_mse: 0.0016 - val_NMSE: 0.0144 - val_NMSE_wt: 0.0136 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0025 - NMSE: 0.0228 - NMSE_wt: 0.0214 - covmat_fro_loss: 6.7562e-04 - global_gradnorm: 13.5303 - tot_time: 0h 0m 13.8s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.00486\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0424 - mse: 0.0025 - NMSE: 0.0228 - NMSE_wt: 0.0214 - covmat_fro_loss: 6.5839e-04 - global_gradnorm: 12.7140 - val_loss: 0.0314 - val_mse: 0.0012 - val_NMSE: 0.0110 - val_NMSE_wt: 0.0103 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0312 - mse: 0.0012 - NMSE: 0.0108 - NMSE_wt: 0.0102 - covmat_fro_loss: 5.8867e-04 - global_gradnorm: 8.6053 - tot_time: 0h 0m 14.3s\n",
      "\n",
      "Epoch 19: val_NMSE_wt improved from 0.00486 to 0.00439, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-5_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 42ms/step - loss: 0.0312 - mse: 0.0012 - NMSE: 0.0108 - NMSE_wt: 0.0102 - covmat_fro_loss: 5.8316e-04 - global_gradnorm: 8.3735 - val_loss: 0.0254 - val_mse: 5.1135e-04 - val_NMSE: 0.0046 - val_NMSE_wt: 0.0044 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0024 - NMSE: 0.0218 - NMSE_wt: 0.0204 - covmat_fro_loss: 7.1338e-04 - global_gradnorm: 15.3041 - tot_time: 0h 0m 15.0s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0414 - mse: 0.0024 - NMSE: 0.0218 - NMSE_wt: 0.0204 - covmat_fro_loss: 7.2423e-04 - global_gradnorm: 15.7620 - val_loss: 0.0259 - val_mse: 5.6845e-04 - val_NMSE: 0.0051 - val_NMSE_wt: 0.0049 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0022 - NMSE: 0.0198 - NMSE_wt: 0.0185 - covmat_fro_loss: 7.1119e-04 - global_gradnorm: 11.5845 - tot_time: 0h 0m 15.4s\n",
      "\n",
      "Epoch 21: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.0396 - mse: 0.0022 - NMSE: 0.0198 - NMSE_wt: 0.0185 - covmat_fro_loss: 6.9689e-04 - global_gradnorm: 11.1744 - val_loss: 0.0268 - val_mse: 6.7813e-04 - val_NMSE: 0.0061 - val_NMSE_wt: 0.0058 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0552 - mse: 0.0041 - NMSE: 0.0366 - NMSE_wt: 0.0342 - covmat_fro_loss: 8.7287e-04 - global_gradnorm: 17.1956 - tot_time: 0h 0m 16.0s\n",
      "\n",
      "Epoch 22: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0552 - mse: 0.0041 - NMSE: 0.0366 - NMSE_wt: 0.0342 - covmat_fro_loss: 8.9436e-04 - global_gradnorm: 17.9958 - val_loss: 0.0548 - val_mse: 0.0040 - val_NMSE: 0.0361 - val_NMSE_wt: 0.0337 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 23/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0400 - mse: 0.0022 - NMSE: 0.0202 - NMSE_wt: 0.0190 - covmat_fro_loss: 8.0021e-04 - global_gradnorm: 12.5837 - tot_time: 0h 0m 16.5s\n",
      "\n",
      "Epoch 23: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0400 - mse: 0.0022 - NMSE: 0.0202 - NMSE_wt: 0.0190 - covmat_fro_loss: 8.0586e-04 - global_gradnorm: 12.1289 - val_loss: 0.0277 - val_mse: 7.8974e-04 - val_NMSE: 0.0071 - val_NMSE_wt: 0.0067 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 24/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0431 - mse: 0.0026 - NMSE: 0.0236 - NMSE_wt: 0.0221 - covmat_fro_loss: 7.6725e-04 - global_gradnorm: 14.9796 - tot_time: 0h 0m 17.0s\n",
      "\n",
      "Epoch 24: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0423 - mse: 0.0025 - NMSE: 0.0227 - NMSE_wt: 0.0213 - covmat_fro_loss: 7.4889e-04 - global_gradnorm: 13.2583 - val_loss: 0.0303 - val_mse: 0.0011 - val_NMSE: 0.0099 - val_NMSE_wt: 0.0093 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 25/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0401 - mse: 0.0023 - NMSE: 0.0204 - NMSE_wt: 0.0191 - covmat_fro_loss: 7.2260e-04 - global_gradnorm: 11.3355 - tot_time: 0h 0m 17.3s\n",
      "\n",
      "Epoch 25: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0395 - mse: 0.0022 - NMSE: 0.0198 - NMSE_wt: 0.0185 - covmat_fro_loss: 7.0259e-04 - global_gradnorm: 11.4578 - val_loss: 0.0413 - val_mse: 0.0024 - val_NMSE: 0.0217 - val_NMSE_wt: 0.0203 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0024 - NMSE: 0.0217 - NMSE_wt: 0.0204 - covmat_fro_loss: 6.7339e-04 - global_gradnorm: 10.9160 - tot_time: 0h 0m 17.7s\n",
      "\n",
      "Epoch 26: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.0414 - mse: 0.0024 - NMSE: 0.0217 - NMSE_wt: 0.0204 - covmat_fro_loss: 6.6417e-04 - global_gradnorm: 10.6039 - val_loss: 0.0390 - val_mse: 0.0021 - val_NMSE: 0.0191 - val_NMSE_wt: 0.0179 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0021 - NMSE: 0.0193 - NMSE_wt: 0.0182 - covmat_fro_loss: 7.0936e-04 - global_gradnorm: 13.3574 - tot_time: 0h 0m 18.2s\n",
      "\n",
      "Epoch 27: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0392 - mse: 0.0021 - NMSE: 0.0193 - NMSE_wt: 0.0182 - covmat_fro_loss: 6.9527e-04 - global_gradnorm: 12.7038 - val_loss: 0.0491 - val_mse: 0.0033 - val_NMSE: 0.0301 - val_NMSE_wt: 0.0281 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0512 - mse: 0.0036 - NMSE: 0.0322 - NMSE_wt: 0.0301 - covmat_fro_loss: 7.6703e-04 - global_gradnorm: 13.4059 - tot_time: 0h 0m 18.8s\n",
      "\n",
      "Epoch 28: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.0512 - mse: 0.0036 - NMSE: 0.0322 - NMSE_wt: 0.0301 - covmat_fro_loss: 7.8155e-04 - global_gradnorm: 14.3479 - val_loss: 0.1293 - val_mse: 0.0129 - val_NMSE: 0.1161 - val_NMSE_wt: 0.1083 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0486 - mse: 0.0033 - NMSE: 0.0295 - NMSE_wt: 0.0276 - covmat_fro_loss: 8.5739e-04 - global_gradnorm: 17.0016 - tot_time: 0h 0m 19.5s\n",
      "\n",
      "Epoch 29: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0486 - mse: 0.0033 - NMSE: 0.0295 - NMSE_wt: 0.0276 - covmat_fro_loss: 8.6504e-04 - global_gradnorm: 16.6194 - val_loss: 0.0549 - val_mse: 0.0040 - val_NMSE: 0.0364 - val_NMSE_wt: 0.0339 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 30/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0473 - mse: 0.0031 - NMSE: 0.0281 - NMSE_wt: 0.0263 - covmat_fro_loss: 8.0290e-04 - global_gradnorm: 15.3128 - tot_time: 0h 0m 19.9s\n",
      "\n",
      "Epoch 30: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.0460 - mse: 0.0030 - NMSE: 0.0267 - NMSE_wt: 0.0250 - covmat_fro_loss: 7.6257e-04 - global_gradnorm: 13.6983 - val_loss: 0.0435 - val_mse: 0.0027 - val_NMSE: 0.0240 - val_NMSE_wt: 0.0224 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 31/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0502 - mse: 0.0035 - NMSE: 0.0312 - NMSE_wt: 0.0291 - covmat_fro_loss: 7.4665e-04 - global_gradnorm: 14.1419 - tot_time: 0h 0m 20.2s\n",
      "\n",
      "Epoch 31: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.0487 - mse: 0.0033 - NMSE: 0.0296 - NMSE_wt: 0.0276 - covmat_fro_loss: 7.2030e-04 - global_gradnorm: 12.6063 - val_loss: 0.0393 - val_mse: 0.0022 - val_NMSE: 0.0195 - val_NMSE_wt: 0.0182 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0587 - mse: 0.0045 - NMSE: 0.0403 - NMSE_wt: 0.0377 - covmat_fro_loss: 8.3830e-04 - global_gradnorm: 17.2307 - tot_time: 0h 0m 20.6s\n",
      "\n",
      "Epoch 32: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.0587 - mse: 0.0045 - NMSE: 0.0403 - NMSE_wt: 0.0377 - covmat_fro_loss: 8.2791e-04 - global_gradnorm: 16.7549 - val_loss: 0.0635 - val_mse: 0.0051 - val_NMSE: 0.0455 - val_NMSE_wt: 0.0425 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0502 - mse: 0.0035 - NMSE: 0.0312 - NMSE_wt: 0.0292 - covmat_fro_loss: 8.4655e-04 - global_gradnorm: 12.9869 - tot_time: 0h 0m 21.2s\n",
      "\n",
      "Epoch 33: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0494 - mse: 0.0034 - NMSE: 0.0303 - NMSE_wt: 0.0283 - covmat_fro_loss: 8.5005e-04 - global_gradnorm: 12.9498 - val_loss: 0.0951 - val_mse: 0.0088 - val_NMSE: 0.0795 - val_NMSE_wt: 0.0740 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 34/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0646 - mse: 0.0052 - NMSE: 0.0466 - NMSE_wt: 0.0436 - covmat_fro_loss: 0.0011 - global_gradnorm: 19.0754    - tot_time: 0h 0m 21.7s\n",
      "\n",
      "Epoch 34: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0624 - mse: 0.0049 - NMSE: 0.0443 - NMSE_wt: 0.0414 - covmat_fro_loss: 0.0011 - global_gradnorm: 20.4410 - val_loss: 0.0944 - val_mse: 0.0088 - val_NMSE: 0.0788 - val_NMSE_wt: 0.0734 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0815 - mse: 0.0072 - NMSE: 0.0645 - NMSE_wt: 0.0604 - covmat_fro_loss: 0.0012 - global_gradnorm: 18.2496 - tot_time: 0h 0m 22.1s\n",
      "\n",
      "Epoch 35: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.0815 - mse: 0.0072 - NMSE: 0.0645 - NMSE_wt: 0.0604 - covmat_fro_loss: 0.0012 - global_gradnorm: 17.3729 - val_loss: 0.0508 - val_mse: 0.0035 - val_NMSE: 0.0317 - val_NMSE_wt: 0.0297 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0498 - mse: 0.0034 - NMSE: 0.0307 - NMSE_wt: 0.0287 - covmat_fro_loss: 0.0011 - global_gradnorm: 20.6629 - tot_time: 0h 0m 22.6s\n",
      "\n",
      "Epoch 36: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.0498 - mse: 0.0034 - NMSE: 0.0307 - NMSE_wt: 0.0287 - covmat_fro_loss: 0.0011 - global_gradnorm: 21.2465 - val_loss: 0.0534 - val_mse: 0.0038 - val_NMSE: 0.0345 - val_NMSE_wt: 0.0324 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 37/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0464 - mse: 0.0030 - NMSE: 0.0271 - NMSE_wt: 0.0254 - covmat_fro_loss: 9.0731e-04 - global_gradnorm: 17.7323 - tot_time: 0h 0m 23.1s\n",
      "\n",
      "Epoch 37: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0474 - mse: 0.0031 - NMSE: 0.0282 - NMSE_wt: 0.0264 - covmat_fro_loss: 9.3335e-04 - global_gradnorm: 17.5805 - val_loss: 0.0336 - val_mse: 0.0015 - val_NMSE: 0.0133 - val_NMSE_wt: 0.0126 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 38/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0422 - mse: 0.0025 - NMSE: 0.0226 - NMSE_wt: 0.0212 - covmat_fro_loss: 6.9392e-04 - global_gradnorm: 9.1840 - tot_time: 0h 0m 23.6s\n",
      "\n",
      "Epoch 38: val_NMSE_wt did not improve from 0.00439\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0488 - mse: 0.0033 - NMSE: 0.0296 - NMSE_wt: 0.0277 - covmat_fro_loss: 7.3242e-04 - global_gradnorm: 11.7860 - val_loss: 0.0262 - val_mse: 6.0061e-04 - val_NMSE: 0.0054 - val_NMSE_wt: 0.0051 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 39/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0444 - mse: 0.0028 - NMSE: 0.0250 - NMSE_wt: 0.0234 - covmat_fro_loss: 8.1294e-04 - global_gradnorm: 13.6324 - tot_time: 0h 0m 24.1s\n",
      "\n",
      "Epoch 39: val_NMSE_wt improved from 0.00439 to 0.00405, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-5_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.0435 - mse: 0.0027 - NMSE: 0.0240 - NMSE_wt: 0.0225 - covmat_fro_loss: 7.8817e-04 - global_gradnorm: 12.9048 - val_loss: 0.0251 - val_mse: 4.7048e-04 - val_NMSE: 0.0042 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 40/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0435 - mse: 0.0027 - NMSE: 0.0240 - NMSE_wt: 0.0225 - covmat_fro_loss: 6.9035e-04 - global_gradnorm: 13.7090 - tot_time: 0h 0m 24.6s\n",
      "\n",
      "Epoch 40: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.0425 - mse: 0.0025 - NMSE: 0.0229 - NMSE_wt: 0.0214 - covmat_fro_loss: 6.7651e-04 - global_gradnorm: 12.5482 - val_loss: 0.0303 - val_mse: 0.0011 - val_NMSE: 0.0099 - val_NMSE_wt: 0.0093 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 41/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0392 - mse: 0.0022 - NMSE: 0.0194 - NMSE_wt: 0.0182 - covmat_fro_loss: 7.1580e-04 - global_gradnorm: 11.7414 - tot_time: 0h 0m 25.1s\n",
      "\n",
      "Epoch 41: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.0387 - mse: 0.0021 - NMSE: 0.0188 - NMSE_wt: 0.0176 - covmat_fro_loss: 7.0495e-04 - global_gradnorm: 12.0647 - val_loss: 0.0279 - val_mse: 8.1313e-04 - val_NMSE: 0.0073 - val_NMSE_wt: 0.0069 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0451 - mse: 0.0029 - NMSE: 0.0258 - NMSE_wt: 0.0241 - covmat_fro_loss: 7.1027e-04 - global_gradnorm: 14.9980 - tot_time: 0h 0m 25.6s\n",
      "\n",
      "Epoch 42: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0451 - mse: 0.0029 - NMSE: 0.0258 - NMSE_wt: 0.0241 - covmat_fro_loss: 6.9397e-04 - global_gradnorm: 14.2358 - val_loss: 0.0261 - val_mse: 5.9583e-04 - val_NMSE: 0.0054 - val_NMSE_wt: 0.0051 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0408 - mse: 0.0023 - NMSE: 0.0211 - NMSE_wt: 0.0198 - covmat_fro_loss: 7.4209e-04 - global_gradnorm: 9.6902  - tot_time: 0h 0m 26.0s\n",
      "\n",
      "Epoch 43: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.0408 - mse: 0.0023 - NMSE: 0.0211 - NMSE_wt: 0.0198 - covmat_fro_loss: 7.3201e-04 - global_gradnorm: 9.4414 - val_loss: 0.0709 - val_mse: 0.0060 - val_NMSE: 0.0536 - val_NMSE_wt: 0.0499 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0497 - mse: 0.0034 - NMSE: 0.0307 - NMSE_wt: 0.0287 - covmat_fro_loss: 7.2581e-04 - global_gradnorm: 12.3194 - tot_time: 0h 0m 26.6s\n",
      "\n",
      "Epoch 44: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0497 - mse: 0.0034 - NMSE: 0.0307 - NMSE_wt: 0.0287 - covmat_fro_loss: 7.3228e-04 - global_gradnorm: 12.8204 - val_loss: 0.0660 - val_mse: 0.0054 - val_NMSE: 0.0482 - val_NMSE_wt: 0.0450 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0500 - mse: 0.0034 - NMSE: 0.0309 - NMSE_wt: 0.0289 - covmat_fro_loss: 8.1510e-04 - global_gradnorm: 14.2511 - tot_time: 0h 0m 26.9s\n",
      "\n",
      "Epoch 45: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.0500 - mse: 0.0034 - NMSE: 0.0309 - NMSE_wt: 0.0289 - covmat_fro_loss: 8.1711e-04 - global_gradnorm: 14.9006 - val_loss: 0.1059 - val_mse: 0.0101 - val_NMSE: 0.0911 - val_NMSE_wt: 0.0849 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 46/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0489 - mse: 0.0033 - NMSE: 0.0298 - NMSE_wt: 0.0279 - covmat_fro_loss: 8.1633e-04 - global_gradnorm: 12.9084 - tot_time: 0h 0m 27.3s\n",
      "\n",
      "Epoch 46: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.0469 - mse: 0.0031 - NMSE: 0.0276 - NMSE_wt: 0.0258 - covmat_fro_loss: 7.9025e-04 - global_gradnorm: 14.7308 - val_loss: 0.0309 - val_mse: 0.0012 - val_NMSE: 0.0104 - val_NMSE_wt: 0.0098 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 47/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0566 - mse: 0.0042 - NMSE: 0.0381 - NMSE_wt: 0.0356 - covmat_fro_loss: 0.0010 - global_gradnorm: 17.8250     - tot_time: 0h 0m 27.9s\n",
      "\n",
      "Epoch 47: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0602 - mse: 0.0047 - NMSE: 0.0419 - NMSE_wt: 0.0392 - covmat_fro_loss: 0.0011 - global_gradnorm: 19.3469 - val_loss: 0.0394 - val_mse: 0.0022 - val_NMSE: 0.0195 - val_NMSE_wt: 0.0183 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 48/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0911 - mse: 0.0083 - NMSE: 0.0749 - NMSE_wt: 0.0701 - covmat_fro_loss: 0.0011 - global_gradnorm: 23.7268  - tot_time: 0h 0m 28.4s\n",
      "\n",
      "Epoch 48: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0879 - mse: 0.0079 - NMSE: 0.0715 - NMSE_wt: 0.0669 - covmat_fro_loss: 0.0011 - global_gradnorm: 22.1179 - val_loss: 0.0642 - val_mse: 0.0051 - val_NMSE: 0.0459 - val_NMSE_wt: 0.0431 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0024 - NMSE: 0.0213 - NMSE_wt: 0.0201 - covmat_fro_loss: 8.5451e-04 - global_gradnorm: 12.9273 - tot_time: 0h 0m 29.0s\n",
      "\n",
      "Epoch 49: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.0411 - mse: 0.0024 - NMSE: 0.0213 - NMSE_wt: 0.0201 - covmat_fro_loss: 8.4824e-04 - global_gradnorm: 13.2345 - val_loss: 0.0253 - val_mse: 4.9636e-04 - val_NMSE: 0.0045 - val_NMSE_wt: 0.0043 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0462 - mse: 0.0030 - NMSE: 0.0269 - NMSE_wt: 0.0252 - covmat_fro_loss: 7.9043e-04 - global_gradnorm: 12.5023 - tot_time: 0h 0m 29.4s\n",
      "\n",
      "Epoch 50: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.0462 - mse: 0.0030 - NMSE: 0.0269 - NMSE_wt: 0.0252 - covmat_fro_loss: 7.8324e-04 - global_gradnorm: 12.0051 - val_loss: 0.0285 - val_mse: 8.7617e-04 - val_NMSE: 0.0079 - val_NMSE_wt: 0.0074 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0018 - NMSE: 0.0164 - NMSE_wt: 0.0154 - covmat_fro_loss: 6.8541e-04 - global_gradnorm: 10.6009 - tot_time: 0h 0m 29.7s\n",
      "\n",
      "Epoch 51: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.0365 - mse: 0.0018 - NMSE: 0.0164 - NMSE_wt: 0.0154 - covmat_fro_loss: 6.9125e-04 - global_gradnorm: 10.4173 - val_loss: 0.0383 - val_mse: 0.0020 - val_NMSE: 0.0183 - val_NMSE_wt: 0.0173 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0586 - mse: 0.0045 - NMSE: 0.0402 - NMSE_wt: 0.0376 - covmat_fro_loss: 9.2383e-04 - global_gradnorm: 19.7116 - tot_time: 0h 0m 30.2s\n",
      "\n",
      "Epoch 52: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.0586 - mse: 0.0045 - NMSE: 0.0402 - NMSE_wt: 0.0376 - covmat_fro_loss: 9.1396e-04 - global_gradnorm: 19.4878 - val_loss: 0.0277 - val_mse: 7.7896e-04 - val_NMSE: 0.0070 - val_NMSE_wt: 0.0067 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0341 - mse: 0.0015 - NMSE: 0.0139 - NMSE_wt: 0.0131 - covmat_fro_loss: 6.9488e-04 - global_gradnorm: 8.7171 - tot_time: 0h 0m 30.8s\n",
      "\n",
      "Epoch 53: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0341 - mse: 0.0015 - NMSE: 0.0139 - NMSE_wt: 0.0131 - covmat_fro_loss: 6.9618e-04 - global_gradnorm: 8.5094 - val_loss: 0.0254 - val_mse: 5.1289e-04 - val_NMSE: 0.0046 - val_NMSE_wt: 0.0044 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 54/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0506 - mse: 0.0035 - NMSE: 0.0316 - NMSE_wt: 0.0296 - covmat_fro_loss: 7.3579e-04 - global_gradnorm: 9.9949 - tot_time: 0h 0m 31.3s\n",
      "\n",
      "Epoch 54: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0500 - mse: 0.0034 - NMSE: 0.0309 - NMSE_wt: 0.0290 - covmat_fro_loss: 7.5140e-04 - global_gradnorm: 9.4254 - val_loss: 0.0363 - val_mse: 0.0018 - val_NMSE: 0.0164 - val_NMSE_wt: 0.0153 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0023 - NMSE: 0.0205 - NMSE_wt: 0.0192 - covmat_fro_loss: 7.7127e-04 - global_gradnorm: 13.4595 - tot_time: 0h 0m 31.7s\n",
      "\n",
      "Epoch 55: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.0402 - mse: 0.0023 - NMSE: 0.0205 - NMSE_wt: 0.0192 - covmat_fro_loss: 7.6799e-04 - global_gradnorm: 13.6042 - val_loss: 0.0425 - val_mse: 0.0026 - val_NMSE: 0.0230 - val_NMSE_wt: 0.0215 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 56/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0422 - mse: 0.0025 - NMSE: 0.0226 - NMSE_wt: 0.0212 - covmat_fro_loss: 7.2737e-04 - global_gradnorm: 14.1696 - tot_time: 0h 0m 32.2s\n",
      "\n",
      "Epoch 56: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0412 - mse: 0.0024 - NMSE: 0.0216 - NMSE_wt: 0.0202 - covmat_fro_loss: 6.9426e-04 - global_gradnorm: 12.8910 - val_loss: 0.0280 - val_mse: 8.1632e-04 - val_NMSE: 0.0073 - val_NMSE_wt: 0.0070 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0341 - mse: 0.0015 - NMSE: 0.0139 - NMSE_wt: 0.0130 - covmat_fro_loss: 6.3904e-04 - global_gradnorm: 6.8836 - tot_time: 0h 0m 32.7s\n",
      "\n",
      "Epoch 57: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.0341 - mse: 0.0015 - NMSE: 0.0139 - NMSE_wt: 0.0130 - covmat_fro_loss: 6.3034e-04 - global_gradnorm: 6.6046 - val_loss: 0.0275 - val_mse: 7.5992e-04 - val_NMSE: 0.0068 - val_NMSE_wt: 0.0065 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 58/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0330 - mse: 0.0014 - NMSE: 0.0128 - NMSE_wt: 0.0120 - covmat_fro_loss: 6.3560e-04 - global_gradnorm: 10.6959 - tot_time: 0h 0m 33.1s\n",
      "\n",
      "Epoch 58: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.0344 - mse: 0.0016 - NMSE: 0.0142 - NMSE_wt: 0.0134 - covmat_fro_loss: 6.3783e-04 - global_gradnorm: 10.9492 - val_loss: 0.0337 - val_mse: 0.0015 - val_NMSE: 0.0135 - val_NMSE_wt: 0.0126 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0564 - mse: 0.0042 - NMSE: 0.0378 - NMSE_wt: 0.0353 - covmat_fro_loss: 9.0767e-04 - global_gradnorm: 18.2474Restoring model weights from the end of the best epoch: 39.\n",
      " - tot_time: 0h 0m 33.6s\n",
      "\n",
      "Epoch 59: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.0564 - mse: 0.0042 - NMSE: 0.0378 - NMSE_wt: 0.0353 - covmat_fro_loss: 9.0733e-04 - global_gradnorm: 18.9819 - val_loss: 0.0341 - val_mse: 0.0015 - val_NMSE: 0.0139 - val_NMSE_wt: 0.0131 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 59: early stopping\n",
      "\n",
      "baseline : 4.0486E-03\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 5e-05 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0400 - mse: 0.0023 - NMSE: 0.0203 - NMSE_wt: 0.0190 - covmat_fro_loss: 6.9877e-04 - global_gradnorm: 10.8471 - tot_time: 0h 0m 34.0s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0391 - mse: 0.0021 - NMSE: 0.0193 - NMSE_wt: 0.0181 - covmat_fro_loss: 6.8124e-04 - global_gradnorm: 9.6582 - val_loss: 0.0312 - val_mse: 0.0012 - val_NMSE: 0.0108 - val_NMSE_wt: 0.0102 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0471 - mse: 0.0031 - NMSE: 0.0279 - NMSE_wt: 0.0260 - covmat_fro_loss: 8.0051e-04 - global_gradnorm: 15.3458 - tot_time: 0h 0m 34.5s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0464 - mse: 0.0030 - NMSE: 0.0271 - NMSE_wt: 0.0254 - covmat_fro_loss: 7.8656e-04 - global_gradnorm: 17.1776 - val_loss: 0.0425 - val_mse: 0.0026 - val_NMSE: 0.0230 - val_NMSE_wt: 0.0215 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0440 - mse: 0.0027 - NMSE: 0.0245 - NMSE_wt: 0.0230 - covmat_fro_loss: 6.8975e-04 - global_gradnorm: 14.4229 - tot_time: 0h 0m 35.1s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0440 - mse: 0.0027 - NMSE: 0.0245 - NMSE_wt: 0.0230 - covmat_fro_loss: 6.8035e-04 - global_gradnorm: 13.8335 - val_loss: 0.0272 - val_mse: 7.2862e-04 - val_NMSE: 0.0066 - val_NMSE_wt: 0.0062 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0383 - mse: 0.0020 - NMSE: 0.0184 - NMSE_wt: 0.0172 - covmat_fro_loss: 7.6122e-04 - global_gradnorm: 14.8080 - tot_time: 0h 0m 35.6s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0389 - mse: 0.0021 - NMSE: 0.0191 - NMSE_wt: 0.0179 - covmat_fro_loss: 7.8384e-04 - global_gradnorm: 16.0539 - val_loss: 0.0306 - val_mse: 0.0011 - val_NMSE: 0.0101 - val_NMSE_wt: 0.0096 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0022 - NMSE: 0.0197 - NMSE_wt: 0.0184 - covmat_fro_loss: 7.4051e-04 - global_gradnorm: 12.4787 - tot_time: 0h 0m 36.1s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0394 - mse: 0.0022 - NMSE: 0.0197 - NMSE_wt: 0.0184 - covmat_fro_loss: 7.5650e-04 - global_gradnorm: 13.3666 - val_loss: 0.0836 - val_mse: 0.0075 - val_NMSE: 0.0672 - val_NMSE_wt: 0.0626 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0485 - mse: 0.0033 - NMSE: 0.0294 - NMSE_wt: 0.0275 - covmat_fro_loss: 8.0036e-04 - global_gradnorm: 15.8772 - tot_time: 0h 0m 36.4s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.0481 - mse: 0.0032 - NMSE: 0.0289 - NMSE_wt: 0.0271 - covmat_fro_loss: 8.0716e-04 - global_gradnorm: 16.9215 - val_loss: 0.0319 - val_mse: 0.0013 - val_NMSE: 0.0116 - val_NMSE_wt: 0.0109 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0551 - mse: 0.0041 - NMSE: 0.0365 - NMSE_wt: 0.0341 - covmat_fro_loss: 7.4573e-04 - global_gradnorm: 17.1588 - tot_time: 0h 0m 36.9s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0551 - mse: 0.0041 - NMSE: 0.0365 - NMSE_wt: 0.0341 - covmat_fro_loss: 7.4637e-04 - global_gradnorm: 16.4862 - val_loss: 0.0430 - val_mse: 0.0026 - val_NMSE: 0.0233 - val_NMSE_wt: 0.0219 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0472 - mse: 0.0031 - NMSE: 0.0279 - NMSE_wt: 0.0262 - covmat_fro_loss: 7.9509e-04 - global_gradnorm: 17.3004 - tot_time: 0h 0m 37.4s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0472 - mse: 0.0031 - NMSE: 0.0279 - NMSE_wt: 0.0262 - covmat_fro_loss: 8.0233e-04 - global_gradnorm: 17.0603 - val_loss: 0.0333 - val_mse: 0.0015 - val_NMSE: 0.0131 - val_NMSE_wt: 0.0123 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0019 - NMSE: 0.0174 - NMSE_wt: 0.0163 - covmat_fro_loss: 6.7385e-04 - global_gradnorm: 12.9840 - tot_time: 0h 0m 38.0s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0373 - mse: 0.0019 - NMSE: 0.0174 - NMSE_wt: 0.0163 - covmat_fro_loss: 6.6746e-04 - global_gradnorm: 13.4796 - val_loss: 0.0478 - val_mse: 0.0032 - val_NMSE: 0.0286 - val_NMSE_wt: 0.0267 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0022 - NMSE: 0.0195 - NMSE_wt: 0.0182 - covmat_fro_loss: 6.9377e-04 - global_gradnorm: 12.8152 - tot_time: 0h 0m 38.5s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0393 - mse: 0.0022 - NMSE: 0.0195 - NMSE_wt: 0.0182 - covmat_fro_loss: 6.8285e-04 - global_gradnorm: 12.1291 - val_loss: 0.0493 - val_mse: 0.0034 - val_NMSE: 0.0303 - val_NMSE_wt: 0.0283 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0486 - mse: 0.0033 - NMSE: 0.0295 - NMSE_wt: 0.0276 - covmat_fro_loss: 7.3128e-04 - global_gradnorm: 14.8436 - tot_time: 0h 0m 38.8s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0462 - mse: 0.0030 - NMSE: 0.0269 - NMSE_wt: 0.0252 - covmat_fro_loss: 7.1845e-04 - global_gradnorm: 12.7486 - val_loss: 0.0267 - val_mse: 6.5826e-04 - val_NMSE: 0.0059 - val_NMSE_wt: 0.0056 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0418 - mse: 0.0025 - NMSE: 0.0221 - NMSE_wt: 0.0207 - covmat_fro_loss: 7.2943e-04 - global_gradnorm: 14.9097 - tot_time: 0h 0m 39.2s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0413 - mse: 0.0024 - NMSE: 0.0216 - NMSE_wt: 0.0202 - covmat_fro_loss: 7.0251e-04 - global_gradnorm: 14.2024 - val_loss: 0.0287 - val_mse: 9.0476e-04 - val_NMSE: 0.0081 - val_NMSE_wt: 0.0077 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0024 - NMSE: 0.0214 - NMSE_wt: 0.0201 - covmat_fro_loss: 7.0267e-04 - global_gradnorm: 11.8697 - tot_time: 0h 0m 39.7s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0411 - mse: 0.0024 - NMSE: 0.0214 - NMSE_wt: 0.0201 - covmat_fro_loss: 6.9132e-04 - global_gradnorm: 12.0077 - val_loss: 0.0381 - val_mse: 0.0020 - val_NMSE: 0.0183 - val_NMSE_wt: 0.0171 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0458 - mse: 0.0029 - NMSE: 0.0265 - NMSE_wt: 0.0248 - covmat_fro_loss: 7.4088e-04 - global_gradnorm: 14.6285 - tot_time: 0h 0m 40.3s\n",
      "\n",
      "Epoch 14: val_NMSE_wt improved from 0.00405 to 0.00381, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-5_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.0458 - mse: 0.0029 - NMSE: 0.0265 - NMSE_wt: 0.0248 - covmat_fro_loss: 7.4274e-04 - global_gradnorm: 15.5892 - val_loss: 0.0248 - val_mse: 4.4143e-04 - val_NMSE: 0.0040 - val_NMSE_wt: 0.0038 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0379 - mse: 0.0020 - NMSE: 0.0180 - NMSE_wt: 0.0169 - covmat_fro_loss: 7.0163e-04 - global_gradnorm: 13.1809 - tot_time: 0h 0m 40.9s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.0380 - mse: 0.0020 - NMSE: 0.0181 - NMSE_wt: 0.0170 - covmat_fro_loss: 6.8702e-04 - global_gradnorm: 13.6792 - val_loss: 0.0349 - val_mse: 0.0016 - val_NMSE: 0.0147 - val_NMSE_wt: 0.0138 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0028 - NMSE: 0.0251 - NMSE_wt: 0.0235 - covmat_fro_loss: 7.0851e-04 - global_gradnorm: 13.5408 - tot_time: 0h 0m 41.2s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.0446 - mse: 0.0028 - NMSE: 0.0251 - NMSE_wt: 0.0235 - covmat_fro_loss: 7.2609e-04 - global_gradnorm: 14.1050 - val_loss: 0.0300 - val_mse: 0.0011 - val_NMSE: 0.0096 - val_NMSE_wt: 0.0090 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0023 - NMSE: 0.0208 - NMSE_wt: 0.0194 - covmat_fro_loss: 6.4506e-04 - global_gradnorm: 11.9838 - tot_time: 0h 0m 41.7s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0405 - mse: 0.0023 - NMSE: 0.0208 - NMSE_wt: 0.0194 - covmat_fro_loss: 6.6894e-04 - global_gradnorm: 13.1098 - val_loss: 0.0526 - val_mse: 0.0038 - val_NMSE: 0.0338 - val_NMSE_wt: 0.0316 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0433 - mse: 0.0026 - NMSE: 0.0238 - NMSE_wt: 0.0223 - covmat_fro_loss: 7.5614e-04 - global_gradnorm: 13.9119 - tot_time: 0h 0m 42.1s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0420 - mse: 0.0025 - NMSE: 0.0224 - NMSE_wt: 0.0210 - covmat_fro_loss: 7.1003e-04 - global_gradnorm: 12.2185 - val_loss: 0.0304 - val_mse: 0.0011 - val_NMSE: 0.0100 - val_NMSE_wt: 0.0094 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0357 - mse: 0.0017 - NMSE: 0.0156 - NMSE_wt: 0.0146 - covmat_fro_loss: 7.0417e-04 - global_gradnorm: 11.0505 - tot_time: 0h 0m 42.7s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0357 - mse: 0.0017 - NMSE: 0.0156 - NMSE_wt: 0.0146 - covmat_fro_loss: 6.9501e-04 - global_gradnorm: 11.1943 - val_loss: 0.0284 - val_mse: 8.6816e-04 - val_NMSE: 0.0078 - val_NMSE_wt: 0.0074 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0358 - mse: 0.0018 - NMSE: 0.0158 - NMSE_wt: 0.0148 - covmat_fro_loss: 6.2833e-04 - global_gradnorm: 12.0691 - tot_time: 0h 0m 43.2s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0358 - mse: 0.0018 - NMSE: 0.0158 - NMSE_wt: 0.0148 - covmat_fro_loss: 6.4700e-04 - global_gradnorm: 13.1898 - val_loss: 0.0266 - val_mse: 6.4751e-04 - val_NMSE: 0.0058 - val_NMSE_wt: 0.0055 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 21/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0409 - mse: 0.0024 - NMSE: 0.0213 - NMSE_wt: 0.0199 - covmat_fro_loss: 6.7950e-04 - global_gradnorm: 13.7494 - tot_time: 0h 0m 43.6s\n",
      "\n",
      "Epoch 21: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.0399 - mse: 0.0022 - NMSE: 0.0202 - NMSE_wt: 0.0189 - covmat_fro_loss: 6.4616e-04 - global_gradnorm: 12.3291 - val_loss: 0.0254 - val_mse: 5.0957e-04 - val_NMSE: 0.0046 - val_NMSE_wt: 0.0044 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 22/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0361 - mse: 0.0018 - NMSE: 0.0160 - NMSE_wt: 0.0150 - covmat_fro_loss: 6.4956e-04 - global_gradnorm: 11.3495 - tot_time: 0h 0m 44.1s\n",
      "\n",
      "Epoch 22: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0383 - mse: 0.0020 - NMSE: 0.0184 - NMSE_wt: 0.0172 - covmat_fro_loss: 6.8593e-04 - global_gradnorm: 13.6808 - val_loss: 0.0402 - val_mse: 0.0023 - val_NMSE: 0.0205 - val_NMSE_wt: 0.0192 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0017 - NMSE: 0.0150 - NMSE_wt: 0.0141 - covmat_fro_loss: 6.2533e-04 - global_gradnorm: 9.5519  - tot_time: 0h 0m 44.6s\n",
      "\n",
      "Epoch 23: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0352 - mse: 0.0017 - NMSE: 0.0150 - NMSE_wt: 0.0141 - covmat_fro_loss: 6.3243e-04 - global_gradnorm: 9.0945 - val_loss: 0.0260 - val_mse: 5.8142e-04 - val_NMSE: 0.0052 - val_NMSE_wt: 0.0050 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0022 - NMSE: 0.0199 - NMSE_wt: 0.0186 - covmat_fro_loss: 6.5517e-04 - global_gradnorm: 14.0786 - tot_time: 0h 0m 45.2s\n",
      "\n",
      "Epoch 24: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0396 - mse: 0.0022 - NMSE: 0.0199 - NMSE_wt: 0.0186 - covmat_fro_loss: 6.5424e-04 - global_gradnorm: 13.8336 - val_loss: 0.0263 - val_mse: 6.1700e-04 - val_NMSE: 0.0056 - val_NMSE_wt: 0.0053 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 25/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0410 - mse: 0.0024 - NMSE: 0.0213 - NMSE_wt: 0.0199 - covmat_fro_loss: 6.7675e-04 - global_gradnorm: 13.2620 - tot_time: 0h 0m 45.7s\n",
      "\n",
      "Epoch 25: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0400 - mse: 0.0022 - NMSE: 0.0202 - NMSE_wt: 0.0189 - covmat_fro_loss: 6.5662e-04 - global_gradnorm: 12.0018 - val_loss: 0.0256 - val_mse: 5.2974e-04 - val_NMSE: 0.0048 - val_NMSE_wt: 0.0045 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 26/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0360 - mse: 0.0018 - NMSE: 0.0159 - NMSE_wt: 0.0150 - covmat_fro_loss: 6.1824e-04 - global_gradnorm: 10.4080  - tot_time: 0h 0m 46.0s\n",
      "\n",
      "Epoch 26: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0392 - mse: 0.0022 - NMSE: 0.0194 - NMSE_wt: 0.0182 - covmat_fro_loss: 6.2474e-04 - global_gradnorm: 10.7817 - val_loss: 0.0260 - val_mse: 5.8046e-04 - val_NMSE: 0.0052 - val_NMSE_wt: 0.0050 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0441 - mse: 0.0027 - NMSE: 0.0247 - NMSE_wt: 0.0231 - covmat_fro_loss: 7.6448e-04 - global_gradnorm: 14.7348 - tot_time: 0h 0m 46.4s\n",
      "\n",
      "Epoch 27: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0441 - mse: 0.0027 - NMSE: 0.0247 - NMSE_wt: 0.0231 - covmat_fro_loss: 7.5114e-04 - global_gradnorm: 14.1383 - val_loss: 0.1150 - val_mse: 0.0112 - val_NMSE: 0.1008 - val_NMSE_wt: 0.0940 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 28/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0649 - mse: 0.0052 - NMSE: 0.0470 - NMSE_wt: 0.0439 - covmat_fro_loss: 9.1882e-04 - global_gradnorm: 18.6785 - tot_time: 0h 0m 46.9s\n",
      "\n",
      "Epoch 28: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.0623 - mse: 0.0049 - NMSE: 0.0442 - NMSE_wt: 0.0412 - covmat_fro_loss: 8.6838e-04 - global_gradnorm: 16.4556 - val_loss: 0.0306 - val_mse: 0.0011 - val_NMSE: 0.0102 - val_NMSE_wt: 0.0096 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - ETA: 0s - loss: 0.0447 - mse: 0.0028 - NMSE: 0.0253 - NMSE_wt: 0.0237 - covmat_fro_loss: 7.4222e-04 - global_gradnorm: 14.9332 - tot_time: 0h 0m 47.4s\n",
      "\n",
      "Epoch 29: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0447 - mse: 0.0028 - NMSE: 0.0253 - NMSE_wt: 0.0237 - covmat_fro_loss: 7.5402e-04 - global_gradnorm: 15.2688 - val_loss: 0.0279 - val_mse: 8.0801e-04 - val_NMSE: 0.0073 - val_NMSE_wt: 0.0069 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0466 - mse: 0.0030 - NMSE: 0.0273 - NMSE_wt: 0.0256 - covmat_fro_loss: 8.2337e-04 - global_gradnorm: 14.8506 - tot_time: 0h 0m 47.9s\n",
      "\n",
      "Epoch 30: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0466 - mse: 0.0030 - NMSE: 0.0273 - NMSE_wt: 0.0256 - covmat_fro_loss: 8.0173e-04 - global_gradnorm: 14.1509 - val_loss: 0.0381 - val_mse: 0.0020 - val_NMSE: 0.0183 - val_NMSE_wt: 0.0171 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 31/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0384 - mse: 0.0021 - NMSE: 0.0186 - NMSE_wt: 0.0174 - covmat_fro_loss: 6.5907e-04 - global_gradnorm: 11.0599 - tot_time: 0h 0m 48.3s\n",
      "\n",
      "Epoch 31: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.0377 - mse: 0.0020 - NMSE: 0.0178 - NMSE_wt: 0.0166 - covmat_fro_loss: 6.3424e-04 - global_gradnorm: 9.8826 - val_loss: 0.0319 - val_mse: 0.0013 - val_NMSE: 0.0116 - val_NMSE_wt: 0.0109 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 32/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0359 - mse: 0.0018 - NMSE: 0.0158 - NMSE_wt: 0.0149 - covmat_fro_loss: 6.3456e-04 - global_gradnorm: 11.9103 - tot_time: 0h 0m 48.8s\n",
      "\n",
      "Epoch 32: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0352 - mse: 0.0017 - NMSE: 0.0151 - NMSE_wt: 0.0142 - covmat_fro_loss: 6.1535e-04 - global_gradnorm: 10.5936 - val_loss: 0.0412 - val_mse: 0.0024 - val_NMSE: 0.0216 - val_NMSE_wt: 0.0202 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 33/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0510 - mse: 0.0036 - NMSE: 0.0320 - NMSE_wt: 0.0299 - covmat_fro_loss: 7.0861e-04 - global_gradnorm: 14.2806 - tot_time: 0h 0m 49.2s\n",
      "\n",
      "Epoch 33: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.0496 - mse: 0.0034 - NMSE: 0.0305 - NMSE_wt: 0.0285 - covmat_fro_loss: 6.9414e-04 - global_gradnorm: 13.5254 - val_loss: 0.0250 - val_mse: 4.6279e-04 - val_NMSE: 0.0042 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0019 - NMSE: 0.0168 - NMSE_wt: 0.0158 - covmat_fro_loss: 6.8632e-04 - global_gradnorm: 12.0324Restoring model weights from the end of the best epoch: 14.\n",
      " - tot_time: 0h 0m 49.7s\n",
      "\n",
      "Epoch 34: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0368 - mse: 0.0019 - NMSE: 0.0168 - NMSE_wt: 0.0158 - covmat_fro_loss: 6.7707e-04 - global_gradnorm: 12.3424 - val_loss: 0.0251 - val_mse: 4.6829e-04 - val_NMSE: 0.0042 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 34: early stopping\n",
      "\n",
      "baseline : 3.8095E-03\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-05 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0018 - NMSE: 0.0166 - NMSE_wt: 0.0156 - covmat_fro_loss: 6.4951e-04 - global_gradnorm: 11.2426 - tot_time: 0h 0m 50.2s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.0366 - mse: 0.0018 - NMSE: 0.0166 - NMSE_wt: 0.0156 - covmat_fro_loss: 6.5234e-04 - global_gradnorm: 11.0211 - val_loss: 0.0317 - val_mse: 0.0013 - val_NMSE: 0.0114 - val_NMSE_wt: 0.0107 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0411 - mse: 0.0024 - NMSE: 0.0215 - NMSE_wt: 0.0201 - covmat_fro_loss: 6.7959e-04 - global_gradnorm: 9.7574  - tot_time: 0h 0m 50.6s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0406 - mse: 0.0023 - NMSE: 0.0209 - NMSE_wt: 0.0196 - covmat_fro_loss: 6.7086e-04 - global_gradnorm: 10.2273 - val_loss: 0.0257 - val_mse: 5.4491e-04 - val_NMSE: 0.0049 - val_NMSE_wt: 0.0047 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0310 - mse: 0.0012 - NMSE: 0.0106 - NMSE_wt: 0.0100 - covmat_fro_loss: 6.1551e-04 - global_gradnorm: 9.4266 - tot_time: 0h 0m 51.1s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0310 - mse: 0.0012 - NMSE: 0.0106 - NMSE_wt: 0.0100 - covmat_fro_loss: 6.1021e-04 - global_gradnorm: 9.7899 - val_loss: 0.0282 - val_mse: 8.4227e-04 - val_NMSE: 0.0076 - val_NMSE_wt: 0.0072 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0318 - mse: 0.0013 - NMSE: 0.0114 - NMSE_wt: 0.0108 - covmat_fro_loss: 6.0516e-04 - global_gradnorm: 7.7030 - tot_time: 0h 0m 51.6s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0318 - mse: 0.0013 - NMSE: 0.0114 - NMSE_wt: 0.0108 - covmat_fro_loss: 6.1038e-04 - global_gradnorm: 8.3021 - val_loss: 0.0249 - val_mse: 4.4764e-04 - val_NMSE: 0.0040 - val_NMSE_wt: 0.0039 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0325 - mse: 0.0014 - NMSE: 0.0122 - NMSE_wt: 0.0115 - covmat_fro_loss: 5.8012e-04 - global_gradnorm: 8.4690 - tot_time: 0h 0m 52.1s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0325 - mse: 0.0014 - NMSE: 0.0122 - NMSE_wt: 0.0115 - covmat_fro_loss: 5.9126e-04 - global_gradnorm: 8.3481 - val_loss: 0.0249 - val_mse: 4.4851e-04 - val_NMSE: 0.0040 - val_NMSE_wt: 0.0039 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0028 - NMSE: 0.0252 - NMSE_wt: 0.0236 - covmat_fro_loss: 6.6874e-04 - global_gradnorm: 11.8665 - tot_time: 0h 0m 52.6s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0446 - mse: 0.0028 - NMSE: 0.0252 - NMSE_wt: 0.0236 - covmat_fro_loss: 6.6257e-04 - global_gradnorm: 11.6573 - val_loss: 0.0293 - val_mse: 9.8038e-04 - val_NMSE: 0.0088 - val_NMSE_wt: 0.0083 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0366 - mse: 0.0018 - NMSE: 0.0166 - NMSE_wt: 0.0156 - covmat_fro_loss: 6.3053e-04 - global_gradnorm: 8.2446 - tot_time: 0h 0m 53.0s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0359 - mse: 0.0018 - NMSE: 0.0159 - NMSE_wt: 0.0149 - covmat_fro_loss: 6.1717e-04 - global_gradnorm: 7.2593 - val_loss: 0.0278 - val_mse: 7.9338e-04 - val_NMSE: 0.0071 - val_NMSE_wt: 0.0067 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0355 - mse: 0.0017 - NMSE: 0.0154 - NMSE_wt: 0.0145 - covmat_fro_loss: 6.2041e-04 - global_gradnorm: 9.9961 - tot_time: 0h 0m 53.4s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.0350 - mse: 0.0017 - NMSE: 0.0149 - NMSE_wt: 0.0140 - covmat_fro_loss: 6.3337e-04 - global_gradnorm: 9.2820 - val_loss: 0.0250 - val_mse: 4.6473e-04 - val_NMSE: 0.0042 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0024 - NMSE: 0.0215 - NMSE_wt: 0.0201 - covmat_fro_loss: 6.9185e-04 - global_gradnorm: 12.3164 - tot_time: 0h 0m 54.0s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0411 - mse: 0.0024 - NMSE: 0.0215 - NMSE_wt: 0.0201 - covmat_fro_loss: 6.8543e-04 - global_gradnorm: 11.7300 - val_loss: 0.0250 - val_mse: 4.6460e-04 - val_NMSE: 0.0042 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0378 - mse: 0.0020 - NMSE: 0.0179 - NMSE_wt: 0.0168 - covmat_fro_loss: 6.4516e-04 - global_gradnorm: 9.9935 - tot_time: 0h 0m 54.4s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.00381\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.0372 - mse: 0.0019 - NMSE: 0.0172 - NMSE_wt: 0.0162 - covmat_fro_loss: 6.3120e-04 - global_gradnorm: 9.1470 - val_loss: 0.0261 - val_mse: 5.9529e-04 - val_NMSE: 0.0054 - val_NMSE_wt: 0.0051 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0020 - NMSE: 0.0184 - NMSE_wt: 0.0173 - covmat_fro_loss: 6.5551e-04 - global_gradnorm: 11.9586 - tot_time: 0h 0m 55.0s\n",
      "\n",
      "Epoch 11: val_NMSE_wt improved from 0.00381 to 0.00378, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-5_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.0383 - mse: 0.0020 - NMSE: 0.0184 - NMSE_wt: 0.0173 - covmat_fro_loss: 6.5236e-04 - global_gradnorm: 11.3018 - val_loss: 0.0248 - val_mse: 4.3913e-04 - val_NMSE: 0.0040 - val_NMSE_wt: 0.0038 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0024 - NMSE: 0.0212 - NMSE_wt: 0.0199 - covmat_fro_loss: 6.8429e-04 - global_gradnorm: 9.4677  - tot_time: 0h 0m 55.5s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.0409 - mse: 0.0024 - NMSE: 0.0212 - NMSE_wt: 0.0199 - covmat_fro_loss: 6.7317e-04 - global_gradnorm: 8.9170 - val_loss: 0.0248 - val_mse: 4.3892e-04 - val_NMSE: 0.0040 - val_NMSE_wt: 0.0038 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0364 - mse: 0.0018 - NMSE: 0.0164 - NMSE_wt: 0.0154 - covmat_fro_loss: 6.8533e-04 - global_gradnorm: 9.2771  - tot_time: 0h 0m 55.8s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.0357 - mse: 0.0017 - NMSE: 0.0156 - NMSE_wt: 0.0147 - covmat_fro_loss: 6.6058e-04 - global_gradnorm: 8.9406 - val_loss: 0.0303 - val_mse: 0.0011 - val_NMSE: 0.0099 - val_NMSE_wt: 0.0093 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0021 - NMSE: 0.0187 - NMSE_wt: 0.0175 - covmat_fro_loss: 6.9991e-04 - global_gradnorm: 9.8959  - tot_time: 0h 0m 56.4s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0385 - mse: 0.0021 - NMSE: 0.0187 - NMSE_wt: 0.0175 - covmat_fro_loss: 7.0216e-04 - global_gradnorm: 10.0610 - val_loss: 0.0260 - val_mse: 5.8709e-04 - val_NMSE: 0.0053 - val_NMSE_wt: 0.0050 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0347 - mse: 0.0016 - NMSE: 0.0146 - NMSE_wt: 0.0137 - covmat_fro_loss: 6.0717e-04 - global_gradnorm: 8.9806 - tot_time: 0h 0m 56.9s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0347 - mse: 0.0016 - NMSE: 0.0146 - NMSE_wt: 0.0137 - covmat_fro_loss: 5.9683e-04 - global_gradnorm: 8.7820 - val_loss: 0.0256 - val_mse: 5.3662e-04 - val_NMSE: 0.0048 - val_NMSE_wt: 0.0046 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0337 - mse: 0.0015 - NMSE: 0.0134 - NMSE_wt: 0.0126 - covmat_fro_loss: 6.2950e-04 - global_gradnorm: 10.0577 - tot_time: 0h 0m 57.4s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0337 - mse: 0.0015 - NMSE: 0.0134 - NMSE_wt: 0.0126 - covmat_fro_loss: 6.3190e-04 - global_gradnorm: 10.1758 - val_loss: 0.0251 - val_mse: 4.7233e-04 - val_NMSE: 0.0043 - val_NMSE_wt: 0.0041 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0349 - mse: 0.0016 - NMSE: 0.0147 - NMSE_wt: 0.0138 - covmat_fro_loss: 6.2939e-04 - global_gradnorm: 11.6197  - tot_time: 0h 0m 57.7s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0368 - mse: 0.0019 - NMSE: 0.0168 - NMSE_wt: 0.0157 - covmat_fro_loss: 6.6607e-04 - global_gradnorm: 15.0660 - val_loss: 0.0302 - val_mse: 0.0011 - val_NMSE: 0.0098 - val_NMSE_wt: 0.0092 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0390 - mse: 0.0021 - NMSE: 0.0192 - NMSE_wt: 0.0180 - covmat_fro_loss: 6.5901e-04 - global_gradnorm: 12.5424 - tot_time: 0h 0m 58.2s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.0380 - mse: 0.0020 - NMSE: 0.0181 - NMSE_wt: 0.0170 - covmat_fro_loss: 6.1809e-04 - global_gradnorm: 11.0179 - val_loss: 0.0254 - val_mse: 5.0608e-04 - val_NMSE: 0.0046 - val_NMSE_wt: 0.0043 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0307 - mse: 0.0011 - NMSE: 0.0103 - NMSE_wt: 0.0097 - covmat_fro_loss: 6.0295e-04 - global_gradnorm: 9.3808 - tot_time: 0h 0m 58.7s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0307 - mse: 0.0011 - NMSE: 0.0103 - NMSE_wt: 0.0097 - covmat_fro_loss: 5.9952e-04 - global_gradnorm: 9.4711 - val_loss: 0.0298 - val_mse: 0.0010 - val_NMSE: 0.0093 - val_NMSE_wt: 0.0088 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0018 - NMSE: 0.0165 - NMSE_wt: 0.0156 - covmat_fro_loss: 6.2746e-04 - global_gradnorm: 13.1260 - tot_time: 0h 0m 59.2s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.0366 - mse: 0.0018 - NMSE: 0.0165 - NMSE_wt: 0.0156 - covmat_fro_loss: 6.4054e-04 - global_gradnorm: 14.1807 - val_loss: 0.0250 - val_mse: 4.6646e-04 - val_NMSE: 0.0042 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 21/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0340 - mse: 0.0015 - NMSE: 0.0138 - NMSE_wt: 0.0129 - covmat_fro_loss: 6.0540e-04 - global_gradnorm: 10.7909 - tot_time: 0h 0m 59.7s\n",
      "\n",
      "Epoch 21: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0334 - mse: 0.0015 - NMSE: 0.0131 - NMSE_wt: 0.0123 - covmat_fro_loss: 5.7945e-04 - global_gradnorm: 9.6553 - val_loss: 0.0263 - val_mse: 6.2277e-04 - val_NMSE: 0.0056 - val_NMSE_wt: 0.0053 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 22/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0359 - mse: 0.0018 - NMSE: 0.0158 - NMSE_wt: 0.0149 - covmat_fro_loss: 6.4119e-04 - global_gradnorm: 11.4247 - tot_time: 0h 1m 0.1s\n",
      "\n",
      "Epoch 22: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.0358 - mse: 0.0018 - NMSE: 0.0158 - NMSE_wt: 0.0148 - covmat_fro_loss: 6.3096e-04 - global_gradnorm: 11.0735 - val_loss: 0.0249 - val_mse: 4.4642e-04 - val_NMSE: 0.0040 - val_NMSE_wt: 0.0038 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0456 - mse: 0.0029 - NMSE: 0.0263 - NMSE_wt: 0.0246 - covmat_fro_loss: 6.7031e-04 - global_gradnorm: 10.9049 - tot_time: 0h 1m 0.6s\n",
      "\n",
      "Epoch 23: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 34ms/step - loss: 0.0456 - mse: 0.0029 - NMSE: 0.0263 - NMSE_wt: 0.0246 - covmat_fro_loss: 6.7997e-04 - global_gradnorm: 10.9666 - val_loss: 0.0285 - val_mse: 8.7919e-04 - val_NMSE: 0.0079 - val_NMSE_wt: 0.0075 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 24/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0407 - mse: 0.0023 - NMSE: 0.0210 - NMSE_wt: 0.0197 - covmat_fro_loss: 5.7004e-04 - global_gradnorm: 10.7425 - tot_time: 0h 1m 1.0s\n",
      "\n",
      "Epoch 24: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.0401 - mse: 0.0023 - NMSE: 0.0203 - NMSE_wt: 0.0191 - covmat_fro_loss: 5.8297e-04 - global_gradnorm: 9.9147 - val_loss: 0.0309 - val_mse: 0.0012 - val_NMSE: 0.0105 - val_NMSE_wt: 0.0098 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 25/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0450 - mse: 0.0029 - NMSE: 0.0257 - NMSE_wt: 0.0240 - covmat_fro_loss: 6.9753e-04 - global_gradnorm: 10.5100 - tot_time: 0h 1m 1.5s\n",
      "\n",
      "Epoch 25: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 0.0445 - mse: 0.0028 - NMSE: 0.0251 - NMSE_wt: 0.0235 - covmat_fro_loss: 6.8018e-04 - global_gradnorm: 11.2017 - val_loss: 0.0319 - val_mse: 0.0013 - val_NMSE: 0.0115 - val_NMSE_wt: 0.0108 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0020 - NMSE: 0.0184 - NMSE_wt: 0.0173 - covmat_fro_loss: 6.3610e-04 - global_gradnorm: 10.9036 - tot_time: 0h 1m 2.1s\n",
      "\n",
      "Epoch 26: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0383 - mse: 0.0020 - NMSE: 0.0184 - NMSE_wt: 0.0173 - covmat_fro_loss: 6.2509e-04 - global_gradnorm: 10.7065 - val_loss: 0.0277 - val_mse: 7.7905e-04 - val_NMSE: 0.0070 - val_NMSE_wt: 0.0066 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 27/200\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.0352 - mse: 0.0017 - NMSE: 0.0151 - NMSE_wt: 0.0142 - covmat_fro_loss: 6.3273e-04 - global_gradnorm: 7.7844 - tot_time: 0h 1m 2.5s\n",
      "\n",
      "Epoch 27: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.0339 - mse: 0.0015 - NMSE: 0.0137 - NMSE_wt: 0.0129 - covmat_fro_loss: 6.0321e-04 - global_gradnorm: 6.6358 - val_loss: 0.0277 - val_mse: 7.8818e-04 - val_NMSE: 0.0071 - val_NMSE_wt: 0.0067 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0316 - mse: 0.0012 - NMSE: 0.0112 - NMSE_wt: 0.0106 - covmat_fro_loss: 6.2307e-04 - global_gradnorm: 10.1516 - tot_time: 0h 1m 2.9s\n",
      "\n",
      "Epoch 28: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.0315 - mse: 0.0012 - NMSE: 0.0111 - NMSE_wt: 0.0105 - covmat_fro_loss: 6.3165e-04 - global_gradnorm: 10.5190 - val_loss: 0.0258 - val_mse: 5.5850e-04 - val_NMSE: 0.0050 - val_NMSE_wt: 0.0048 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 29/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0334 - mse: 0.0015 - NMSE: 0.0131 - NMSE_wt: 0.0123 - covmat_fro_loss: 6.1921e-04 - global_gradnorm: 9.7685 - tot_time: 0h 1m 3.4s\n",
      "\n",
      "Epoch 29: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.0331 - mse: 0.0014 - NMSE: 0.0128 - NMSE_wt: 0.0120 - covmat_fro_loss: 6.1769e-04 - global_gradnorm: 9.9573 - val_loss: 0.0250 - val_mse: 4.6562e-04 - val_NMSE: 0.0042 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 30/200\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.0325 - mse: 0.0013 - NMSE: 0.0121 - NMSE_wt: 0.0114 - covmat_fro_loss: 6.1797e-04 - global_gradnorm: 7.0474 - tot_time: 0h 1m 3.9s\n",
      "\n",
      "Epoch 30: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.0321 - mse: 0.0013 - NMSE: 0.0117 - NMSE_wt: 0.0110 - covmat_fro_loss: 5.9073e-04 - global_gradnorm: 6.2050 - val_loss: 0.0249 - val_mse: 4.4878e-04 - val_NMSE: 0.0040 - val_NMSE_wt: 0.0039 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0023 - NMSE: 0.0208 - NMSE_wt: 0.0195 - covmat_fro_loss: 6.6005e-04 - global_gradnorm: 10.7370Restoring model weights from the end of the best epoch: 11.\n",
      " - tot_time: 0h 1m 4.4s\n",
      "\n",
      "Epoch 31: val_NMSE_wt did not improve from 0.00378\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-5_outsteps\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0405 - mse: 0.0023 - NMSE: 0.0208 - NMSE_wt: 0.0195 - covmat_fro_loss: 6.5257e-04 - global_gradnorm: 10.2329 - val_loss: 0.0303 - val_mse: 0.0011 - val_NMSE: 0.0099 - val_NMSE_wt: 0.0093 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 31: early stopping\n",
      "\n",
      "baseline : 3.7833E-03\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0254 - mse: 5.0723e-04 - NMSE: 0.0046 - NMSE_wt: 0.0044 - covmat_fro_loss: 0.0000e+00 - global_gradnorm: 0.0000e+00\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 10 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "clipnorm : None, global_clipnorm : 17.7\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0371 - mse: 0.0020 - NMSE: 0.0179 - NMSE_wt: 0.0161 - covmat_fro_loss: 0.0000e+00 - global_gradnorm: 0.0000e+00\n",
      "baseline : 1.6112E-02\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-05 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0506 - mse: 0.0037 - NMSE: 0.0331 - NMSE_wt: 0.0295 - covmat_fro_loss: 9.9028e-04 - global_gradnorm: 6.6714 - tot_time: 0h 0m 8.2s\n",
      "\n",
      "Epoch 1: val_NMSE_wt improved from 0.01611 to 0.00852, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-10_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 8s 54ms/step - loss: 0.0506 - mse: 0.0037 - NMSE: 0.0331 - NMSE_wt: 0.0295 - covmat_fro_loss: 9.7138e-04 - global_gradnorm: 6.3432 - val_loss: 0.0296 - val_mse: 0.0011 - val_NMSE: 0.0095 - val_NMSE_wt: 0.0085 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0591 - mse: 0.0048 - NMSE: 0.0430 - NMSE_wt: 0.0380 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.2068     - tot_time: 0h 0m 8.8s\n",
      "\n",
      "Epoch 2: val_NMSE_wt improved from 0.00852 to 0.00671, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-10_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 69ms/step - loss: 0.0564 - mse: 0.0044 - NMSE: 0.0400 - NMSE_wt: 0.0354 - covmat_fro_loss: 9.8514e-04 - global_gradnorm: 9.8597 - val_loss: 0.0277 - val_mse: 8.3251e-04 - val_NMSE: 0.0075 - val_NMSE_wt: 0.0067 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0533 - mse: 0.0040 - NMSE: 0.0364 - NMSE_wt: 0.0323 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.2400 - tot_time: 0h 0m 9.6s\n",
      "\n",
      "Epoch 3: val_NMSE_wt improved from 0.00671 to 0.00405, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-10_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 0.0533 - mse: 0.0040 - NMSE: 0.0364 - NMSE_wt: 0.0323 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.0172 - val_loss: 0.0251 - val_mse: 5.0249e-04 - val_NMSE: 0.0045 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0557 - mse: 0.0043 - NMSE: 0.0390 - NMSE_wt: 0.0346 - covmat_fro_loss: 0.0010 - global_gradnorm: 12.5956     - tot_time: 0h 0m 10.1s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0557 - mse: 0.0043 - NMSE: 0.0390 - NMSE_wt: 0.0346 - covmat_fro_loss: 0.0011 - global_gradnorm: 13.0209 - val_loss: 0.0268 - val_mse: 7.2211e-04 - val_NMSE: 0.0065 - val_NMSE_wt: 0.0057 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0539 - mse: 0.0042 - NMSE: 0.0374 - NMSE_wt: 0.0329 - covmat_fro_loss: 0.0010 - global_gradnorm: 13.2386    - tot_time: 0h 0m 10.6s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.0539 - mse: 0.0042 - NMSE: 0.0374 - NMSE_wt: 0.0329 - covmat_fro_loss: 0.0010 - global_gradnorm: 13.6103 - val_loss: 0.0284 - val_mse: 9.0825e-04 - val_NMSE: 0.0082 - val_NMSE_wt: 0.0073 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0634 - mse: 0.0053 - NMSE: 0.0478 - NMSE_wt: 0.0424 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.3650 - tot_time: 0h 0m 11.1s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 45ms/step - loss: 0.0634 - mse: 0.0053 - NMSE: 0.0478 - NMSE_wt: 0.0424 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.8930 - val_loss: 0.0255 - val_mse: 5.5089e-04 - val_NMSE: 0.0050 - val_NMSE_wt: 0.0044 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0726 - mse: 0.0065 - NMSE: 0.0583 - NMSE_wt: 0.0516 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.5625    - tot_time: 0h 0m 11.6s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.0726 - mse: 0.0065 - NMSE: 0.0583 - NMSE_wt: 0.0516 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.9834 - val_loss: 0.0458 - val_mse: 0.0030 - val_NMSE: 0.0274 - val_NMSE_wt: 0.0248 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0506 - mse: 0.0037 - NMSE: 0.0336 - NMSE_wt: 0.0296 - covmat_fro_loss: 0.0010 - global_gradnorm: 10.3319     - tot_time: 0h 0m 12.1s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 47ms/step - loss: 0.0564 - mse: 0.0044 - NMSE: 0.0400 - NMSE_wt: 0.0354 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.5599 - val_loss: 0.0256 - val_mse: 5.6426e-04 - val_NMSE: 0.0051 - val_NMSE_wt: 0.0045 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0554 - mse: 0.0043 - NMSE: 0.0390 - NMSE_wt: 0.0343 - covmat_fro_loss: 0.0010 - global_gradnorm: 7.6378 - tot_time: 0h 0m 12.5s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.0554 - mse: 0.0043 - NMSE: 0.0390 - NMSE_wt: 0.0343 - covmat_fro_loss: 9.8965e-04 - global_gradnorm: 7.1214 - val_loss: 0.0257 - val_mse: 5.8506e-04 - val_NMSE: 0.0053 - val_NMSE_wt: 0.0047 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0651 - mse: 0.0055 - NMSE: 0.0496 - NMSE_wt: 0.0441 - covmat_fro_loss: 9.9923e-04 - global_gradnorm: 8.6748 - tot_time: 0h 0m 12.9s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 0.0651 - mse: 0.0055 - NMSE: 0.0496 - NMSE_wt: 0.0441 - covmat_fro_loss: 9.8170e-04 - global_gradnorm: 8.0515 - val_loss: 0.0258 - val_mse: 5.9249e-04 - val_NMSE: 0.0053 - val_NMSE_wt: 0.0048 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0506 - mse: 0.0037 - NMSE: 0.0337 - NMSE_wt: 0.0296 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.4599 - tot_time: 0h 0m 13.5s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 48ms/step - loss: 0.0506 - mse: 0.0037 - NMSE: 0.0337 - NMSE_wt: 0.0296 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.0633 - val_loss: 0.0311 - val_mse: 0.0012 - val_NMSE: 0.0112 - val_NMSE_wt: 0.0100 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0505 - mse: 0.0037 - NMSE: 0.0333 - NMSE_wt: 0.0295 - covmat_fro_loss: 0.0010 - global_gradnorm: 10.2609     - tot_time: 0h 0m 14.0s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 0.0505 - mse: 0.0037 - NMSE: 0.0333 - NMSE_wt: 0.0295 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.6124 - val_loss: 0.0269 - val_mse: 7.3217e-04 - val_NMSE: 0.0066 - val_NMSE_wt: 0.0058 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0542 - mse: 0.0041 - NMSE: 0.0372 - NMSE_wt: 0.0332 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.0076   - tot_time: 0h 0m 14.5s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 46ms/step - loss: 0.0542 - mse: 0.0041 - NMSE: 0.0372 - NMSE_wt: 0.0332 - covmat_fro_loss: 0.0010 - global_gradnorm: 10.9013 - val_loss: 0.0276 - val_mse: 8.2279e-04 - val_NMSE: 0.0074 - val_NMSE_wt: 0.0066 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0579 - mse: 0.0046 - NMSE: 0.0418 - NMSE_wt: 0.0369 - covmat_fro_loss: 0.0011 - global_gradnorm: 9.8361  - tot_time: 0h 0m 14.9s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 0.0579 - mse: 0.0046 - NMSE: 0.0418 - NMSE_wt: 0.0369 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.6688 - val_loss: 0.0306 - val_mse: 0.0012 - val_NMSE: 0.0109 - val_NMSE_wt: 0.0096 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0547 - mse: 0.0042 - NMSE: 0.0379 - NMSE_wt: 0.0337 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.4610 - tot_time: 0h 0m 15.4s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 48ms/step - loss: 0.0541 - mse: 0.0041 - NMSE: 0.0372 - NMSE_wt: 0.0331 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.6675 - val_loss: 0.0317 - val_mse: 0.0013 - val_NMSE: 0.0118 - val_NMSE_wt: 0.0107 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0490 - mse: 0.0035 - NMSE: 0.0317 - NMSE_wt: 0.0279 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.4699  - tot_time: 0h 0m 15.8s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 0.0490 - mse: 0.0035 - NMSE: 0.0317 - NMSE_wt: 0.0279 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.7974 - val_loss: 0.0296 - val_mse: 0.0011 - val_NMSE: 0.0097 - val_NMSE_wt: 0.0085 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 0.0651 - mse: 0.0055 - NMSE: 0.0495 - NMSE_wt: 0.0441 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.0884 - tot_time: 0h 0m 16.3s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 0.0651 - mse: 0.0055 - NMSE: 0.0495 - NMSE_wt: 0.0441 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.5514 - val_loss: 0.0267 - val_mse: 7.1023e-04 - val_NMSE: 0.0064 - val_NMSE_wt: 0.0056 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0502 - mse: 0.0037 - NMSE: 0.0331 - NMSE_wt: 0.0292 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.4784 - tot_time: 0h 0m 16.8s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 49ms/step - loss: 0.0502 - mse: 0.0037 - NMSE: 0.0331 - NMSE_wt: 0.0292 - covmat_fro_loss: 0.0010 - global_gradnorm: 10.8739 - val_loss: 0.0273 - val_mse: 7.7833e-04 - val_NMSE: 0.0070 - val_NMSE_wt: 0.0063 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 0.0466 - mse: 0.0032 - NMSE: 0.0288 - NMSE_wt: 0.0256 - covmat_fro_loss: 9.3427e-04 - global_gradnorm: 7.8469 - tot_time: 0h 0m 17.2s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.0521 - mse: 0.0039 - NMSE: 0.0349 - NMSE_wt: 0.0311 - covmat_fro_loss: 9.6733e-04 - global_gradnorm: 8.6803 - val_loss: 0.0324 - val_mse: 0.0014 - val_NMSE: 0.0126 - val_NMSE_wt: 0.0113 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0564 - mse: 0.0044 - NMSE: 0.0399 - NMSE_wt: 0.0354 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.2922 - tot_time: 0h 0m 17.7s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 45ms/step - loss: 0.0537 - mse: 0.0041 - NMSE: 0.0368 - NMSE_wt: 0.0327 - covmat_fro_loss: 9.8504e-04 - global_gradnorm: 7.0112 - val_loss: 0.0302 - val_mse: 0.0011 - val_NMSE: 0.0103 - val_NMSE_wt: 0.0092 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0500 - mse: 0.0036 - NMSE: 0.0325 - NMSE_wt: 0.0290 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.3223 - tot_time: 0h 0m 18.2s\n",
      "\n",
      "Epoch 21: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.0500 - mse: 0.0036 - NMSE: 0.0325 - NMSE_wt: 0.0290 - covmat_fro_loss: 0.0010 - global_gradnorm: 7.9484 - val_loss: 0.0261 - val_mse: 6.3980e-04 - val_NMSE: 0.0058 - val_NMSE_wt: 0.0051 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 22/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0529 - mse: 0.0040 - NMSE: 0.0358 - NMSE_wt: 0.0319 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.7655   - tot_time: 0h 0m 18.8s\n",
      "\n",
      "Epoch 22: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.0519 - mse: 0.0038 - NMSE: 0.0346 - NMSE_wt: 0.0309 - covmat_fro_loss: 9.9137e-04 - global_gradnorm: 9.7863 - val_loss: 0.0333 - val_mse: 0.0015 - val_NMSE: 0.0136 - val_NMSE_wt: 0.0123 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0599 - mse: 0.0049 - NMSE: 0.0437 - NMSE_wt: 0.0388 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.5862Restoring model weights from the end of the best epoch: 3.\n",
      " - tot_time: 0h 0m 19.3s\n",
      "\n",
      "Epoch 23: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.0599 - mse: 0.0049 - NMSE: 0.0437 - NMSE_wt: 0.0388 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.1790 - val_loss: 0.0270 - val_mse: 7.4712e-04 - val_NMSE: 0.0067 - val_NMSE_wt: 0.0060 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 23: early stopping\n",
      "\n",
      "baseline : 4.0491E-03\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 5e-06 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0428 - mse: 0.0027 - NMSE: 0.0244 - NMSE_wt: 0.0218 - covmat_fro_loss: 9.5350e-04 - global_gradnorm: 6.3627 - tot_time: 0h 0m 19.9s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 46ms/step - loss: 0.0439 - mse: 0.0029 - NMSE: 0.0258 - NMSE_wt: 0.0229 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.2522 - val_loss: 0.0259 - val_mse: 6.1187e-04 - val_NMSE: 0.0055 - val_NMSE_wt: 0.0049 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0501 - mse: 0.0036 - NMSE: 0.0327 - NMSE_wt: 0.0291 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.4567   - tot_time: 0h 0m 20.4s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.0501 - mse: 0.0036 - NMSE: 0.0327 - NMSE_wt: 0.0291 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.2780 - val_loss: 0.0258 - val_mse: 5.9529e-04 - val_NMSE: 0.0054 - val_NMSE_wt: 0.0048 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0513 - mse: 0.0038 - NMSE: 0.0342 - NMSE_wt: 0.0303 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.4985 - tot_time: 0h 0m 20.9s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 0.0503 - mse: 0.0037 - NMSE: 0.0330 - NMSE_wt: 0.0292 - covmat_fro_loss: 0.0010 - global_gradnorm: 12.5314 - val_loss: 0.0297 - val_mse: 0.0011 - val_NMSE: 0.0097 - val_NMSE_wt: 0.0087 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0614 - mse: 0.0051 - NMSE: 0.0457 - NMSE_wt: 0.0404 - covmat_fro_loss: 0.0011 - global_gradnorm: 12.9949    - tot_time: 0h 0m 21.4s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 46ms/step - loss: 0.0614 - mse: 0.0051 - NMSE: 0.0457 - NMSE_wt: 0.0404 - covmat_fro_loss: 0.0011 - global_gradnorm: 13.3870 - val_loss: 0.0516 - val_mse: 0.0037 - val_NMSE: 0.0337 - val_NMSE_wt: 0.0305 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0610 - mse: 0.0050 - NMSE: 0.0446 - NMSE_wt: 0.0399 - covmat_fro_loss: 0.0011 - global_gradnorm: 9.3192 - tot_time: 0h 0m 21.8s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0613 - mse: 0.0050 - NMSE: 0.0452 - NMSE_wt: 0.0403 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.7160 - val_loss: 0.0261 - val_mse: 6.3666e-04 - val_NMSE: 0.0057 - val_NMSE_wt: 0.0051 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0551 - mse: 0.0043 - NMSE: 0.0383 - NMSE_wt: 0.0340 - covmat_fro_loss: 0.0010 - global_gradnorm: 7.3422 - tot_time: 0h 0m 22.2s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 0.0528 - mse: 0.0040 - NMSE: 0.0357 - NMSE_wt: 0.0317 - covmat_fro_loss: 0.0010 - global_gradnorm: 6.5846 - val_loss: 0.0295 - val_mse: 0.0010 - val_NMSE: 0.0094 - val_NMSE_wt: 0.0085 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0792 - mse: 0.0073 - NMSE: 0.0657 - NMSE_wt: 0.0582 - covmat_fro_loss: 0.0012 - global_gradnorm: 10.1991 - tot_time: 0h 0m 22.8s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 0.0750 - mse: 0.0068 - NMSE: 0.0609 - NMSE_wt: 0.0539 - covmat_fro_loss: 0.0011 - global_gradnorm: 8.6784 - val_loss: 0.0371 - val_mse: 0.0020 - val_NMSE: 0.0180 - val_NMSE_wt: 0.0161 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0467 - mse: 0.0032 - NMSE: 0.0288 - NMSE_wt: 0.0257 - covmat_fro_loss: 9.8431e-04 - global_gradnorm: 8.2220 - tot_time: 0h 0m 23.3s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.0467 - mse: 0.0032 - NMSE: 0.0288 - NMSE_wt: 0.0257 - covmat_fro_loss: 9.8136e-04 - global_gradnorm: 9.0119 - val_loss: 0.0258 - val_mse: 5.9974e-04 - val_NMSE: 0.0054 - val_NMSE_wt: 0.0048 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0396 - mse: 0.0023 - NMSE: 0.0210 - NMSE_wt: 0.0186 - covmat_fro_loss: 9.2247e-04 - global_gradnorm: 8.9688 - tot_time: 0h 0m 23.8s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 0.0409 - mse: 0.0025 - NMSE: 0.0224 - NMSE_wt: 0.0199 - covmat_fro_loss: 9.1304e-04 - global_gradnorm: 9.7553 - val_loss: 0.0347 - val_mse: 0.0017 - val_NMSE: 0.0151 - val_NMSE_wt: 0.0137 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0674 - mse: 0.0058 - NMSE: 0.0523 - NMSE_wt: 0.0464 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.3860 - tot_time: 0h 0m 24.1s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0638 - mse: 0.0054 - NMSE: 0.0482 - NMSE_wt: 0.0428 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.4629 - val_loss: 0.0384 - val_mse: 0.0021 - val_NMSE: 0.0193 - val_NMSE_wt: 0.0174 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0603 - mse: 0.0049 - NMSE: 0.0441 - NMSE_wt: 0.0393 - covmat_fro_loss: 0.0010 - global_gradnorm: 12.3417     - tot_time: 0h 0m 24.5s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.0578 - mse: 0.0046 - NMSE: 0.0413 - NMSE_wt: 0.0368 - covmat_fro_loss: 9.9294e-04 - global_gradnorm: 11.6189 - val_loss: 0.0260 - val_mse: 6.2111e-04 - val_NMSE: 0.0056 - val_NMSE_wt: 0.0050 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0517 - mse: 0.0039 - NMSE: 0.0347 - NMSE_wt: 0.0307 - covmat_fro_loss: 0.0010 - global_gradnorm: 7.3654   - tot_time: 0h 0m 25.0s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 46ms/step - loss: 0.0517 - mse: 0.0039 - NMSE: 0.0347 - NMSE_wt: 0.0307 - covmat_fro_loss: 9.9787e-04 - global_gradnorm: 6.7996 - val_loss: 0.0276 - val_mse: 8.2492e-04 - val_NMSE: 0.0074 - val_NMSE_wt: 0.0066 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0604 - mse: 0.0050 - NMSE: 0.0447 - NMSE_wt: 0.0394 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.5825 - tot_time: 0h 0m 25.4s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 0.0604 - mse: 0.0050 - NMSE: 0.0447 - NMSE_wt: 0.0394 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.1757 - val_loss: 0.0304 - val_mse: 0.0012 - val_NMSE: 0.0106 - val_NMSE_wt: 0.0094 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0654 - mse: 0.0056 - NMSE: 0.0501 - NMSE_wt: 0.0443 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.0285 - tot_time: 0h 0m 25.9s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 47ms/step - loss: 0.0654 - mse: 0.0056 - NMSE: 0.0501 - NMSE_wt: 0.0443 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.3696 - val_loss: 0.0338 - val_mse: 0.0016 - val_NMSE: 0.0145 - val_NMSE_wt: 0.0128 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0504 - mse: 0.0037 - NMSE: 0.0332 - NMSE_wt: 0.0294 - covmat_fro_loss: 0.0010 - global_gradnorm: 12.2531     - tot_time: 0h 0m 26.4s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 44ms/step - loss: 0.0488 - mse: 0.0035 - NMSE: 0.0313 - NMSE_wt: 0.0277 - covmat_fro_loss: 9.9188e-04 - global_gradnorm: 11.8294 - val_loss: 0.0280 - val_mse: 8.7394e-04 - val_NMSE: 0.0079 - val_NMSE_wt: 0.0070 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 0.0599 - mse: 0.0049 - NMSE: 0.0438 - NMSE_wt: 0.0389 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.9167 - tot_time: 0h 0m 26.8s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.0599 - mse: 0.0049 - NMSE: 0.0438 - NMSE_wt: 0.0389 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.4820 - val_loss: 0.0279 - val_mse: 8.6637e-04 - val_NMSE: 0.0078 - val_NMSE_wt: 0.0069 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0506 - mse: 0.0037 - NMSE: 0.0335 - NMSE_wt: 0.0296 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.0730     - tot_time: 0h 0m 27.3s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 0.0506 - mse: 0.0037 - NMSE: 0.0335 - NMSE_wt: 0.0296 - covmat_fro_loss: 0.0011 - global_gradnorm: 9.7920 - val_loss: 0.0298 - val_mse: 0.0011 - val_NMSE: 0.0099 - val_NMSE_wt: 0.0088 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0581 - mse: 0.0046 - NMSE: 0.0417 - NMSE_wt: 0.0371 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.4881 - tot_time: 0h 0m 27.7s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 0.0560 - mse: 0.0044 - NMSE: 0.0394 - NMSE_wt: 0.0350 - covmat_fro_loss: 0.0011 - global_gradnorm: 9.5887 - val_loss: 0.0287 - val_mse: 9.6893e-04 - val_NMSE: 0.0087 - val_NMSE_wt: 0.0076 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0386 - mse: 0.0022 - NMSE: 0.0199 - NMSE_wt: 0.0176 - covmat_fro_loss: 9.8041e-04 - global_gradnorm: 11.0447 - tot_time: 0h 0m 28.2s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 46ms/step - loss: 0.0387 - mse: 0.0022 - NMSE: 0.0199 - NMSE_wt: 0.0176 - covmat_fro_loss: 9.5757e-04 - global_gradnorm: 11.7613 - val_loss: 0.0301 - val_mse: 0.0011 - val_NMSE: 0.0101 - val_NMSE_wt: 0.0091 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0572 - mse: 0.0045 - NMSE: 0.0406 - NMSE_wt: 0.0362 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.6093Restoring model weights from the end of the best epoch: 2.\n",
      " - tot_time: 0h 0m 28.8s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 48ms/step - loss: 0.0572 - mse: 0.0045 - NMSE: 0.0406 - NMSE_wt: 0.0362 - covmat_fro_loss: 0.0011 - global_gradnorm: 9.9212 - val_loss: 0.0294 - val_mse: 0.0010 - val_NMSE: 0.0094 - val_NMSE_wt: 0.0084 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20: early stopping\n",
      "\n",
      "baseline : 4.0491E-03\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-06 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0562 - mse: 0.0044 - NMSE: 0.0397 - NMSE_wt: 0.0352 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.0574  - tot_time: 0h 0m 29.3s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 44ms/step - loss: 0.0562 - mse: 0.0044 - NMSE: 0.0397 - NMSE_wt: 0.0352 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.9941 - val_loss: 0.0254 - val_mse: 5.4617e-04 - val_NMSE: 0.0049 - val_NMSE_wt: 0.0044 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0540 - mse: 0.0041 - NMSE: 0.0370 - NMSE_wt: 0.0329 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.2775  - tot_time: 0h 0m 29.8s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 46ms/step - loss: 0.0540 - mse: 0.0041 - NMSE: 0.0370 - NMSE_wt: 0.0329 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.0902 - val_loss: 0.0264 - val_mse: 6.7512e-04 - val_NMSE: 0.0061 - val_NMSE_wt: 0.0054 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0441 - mse: 0.0029 - NMSE: 0.0259 - NMSE_wt: 0.0231 - covmat_fro_loss: 9.5821e-04 - global_gradnorm: 8.9208 - tot_time: 0h 0m 30.3s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 49ms/step - loss: 0.0441 - mse: 0.0029 - NMSE: 0.0259 - NMSE_wt: 0.0231 - covmat_fro_loss: 9.7346e-04 - global_gradnorm: 9.6524 - val_loss: 0.0255 - val_mse: 5.5171e-04 - val_NMSE: 0.0050 - val_NMSE_wt: 0.0044 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0511 - mse: 0.0037 - NMSE: 0.0337 - NMSE_wt: 0.0301 - covmat_fro_loss: 9.8093e-04 - global_gradnorm: 9.3652 - tot_time: 0h 0m 30.8s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.00405\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 46ms/step - loss: 0.0511 - mse: 0.0037 - NMSE: 0.0337 - NMSE_wt: 0.0301 - covmat_fro_loss: 9.8954e-04 - global_gradnorm: 8.8266 - val_loss: 0.0277 - val_mse: 8.3179e-04 - val_NMSE: 0.0075 - val_NMSE_wt: 0.0067 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0520 - mse: 0.0039 - NMSE: 0.0349 - NMSE_wt: 0.0310 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.0255  - tot_time: 0h 0m 31.2s\n",
      "\n",
      "Epoch 5: val_NMSE_wt improved from 0.00405 to 0.00397, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-10_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 0.0520 - mse: 0.0039 - NMSE: 0.0349 - NMSE_wt: 0.0310 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.5817 - val_loss: 0.0250 - val_mse: 4.9575e-04 - val_NMSE: 0.0045 - val_NMSE_wt: 0.0040 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0463 - mse: 0.0032 - NMSE: 0.0284 - NMSE_wt: 0.0252 - covmat_fro_loss: 9.8522e-04 - global_gradnorm: 9.3420  - tot_time: 0h 0m 31.7s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 0.0463 - mse: 0.0032 - NMSE: 0.0284 - NMSE_wt: 0.0252 - covmat_fro_loss: 9.5992e-04 - global_gradnorm: 8.6181 - val_loss: 0.0264 - val_mse: 6.6294e-04 - val_NMSE: 0.0060 - val_NMSE_wt: 0.0054 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0530 - mse: 0.0040 - NMSE: 0.0362 - NMSE_wt: 0.0320 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.7027 - tot_time: 0h 0m 32.2s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 49ms/step - loss: 0.0530 - mse: 0.0040 - NMSE: 0.0362 - NMSE_wt: 0.0320 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.0803 - val_loss: 0.0431 - val_mse: 0.0027 - val_NMSE: 0.0245 - val_NMSE_wt: 0.0221 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0500 - mse: 0.0037 - NMSE: 0.0330 - NMSE_wt: 0.0290 - covmat_fro_loss: 0.0010 - global_gradnorm: 10.1282 - tot_time: 0h 0m 32.7s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 49ms/step - loss: 0.0507 - mse: 0.0037 - NMSE: 0.0336 - NMSE_wt: 0.0296 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.3902 - val_loss: 0.0258 - val_mse: 5.9309e-04 - val_NMSE: 0.0053 - val_NMSE_wt: 0.0048 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0542 - mse: 0.0041 - NMSE: 0.0373 - NMSE_wt: 0.0332 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.9044 - tot_time: 0h 0m 33.2s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 44ms/step - loss: 0.0542 - mse: 0.0041 - NMSE: 0.0373 - NMSE_wt: 0.0332 - covmat_fro_loss: 0.0010 - global_gradnorm: 12.3873 - val_loss: 0.0251 - val_mse: 5.1310e-04 - val_NMSE: 0.0046 - val_NMSE_wt: 0.0041 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0439 - mse: 0.0029 - NMSE: 0.0261 - NMSE_wt: 0.0229 - covmat_fro_loss: 9.8199e-04 - global_gradnorm: 8.1090 - tot_time: 0h 0m 33.7s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 46ms/step - loss: 0.0427 - mse: 0.0027 - NMSE: 0.0247 - NMSE_wt: 0.0217 - covmat_fro_loss: 9.4008e-04 - global_gradnorm: 8.2266 - val_loss: 0.0257 - val_mse: 5.8441e-04 - val_NMSE: 0.0053 - val_NMSE_wt: 0.0047 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0522 - mse: 0.0039 - NMSE: 0.0352 - NMSE_wt: 0.0312 - covmat_fro_loss: 9.6041e-04 - global_gradnorm: 9.1356 - tot_time: 0h 0m 34.2s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 48ms/step - loss: 0.0522 - mse: 0.0039 - NMSE: 0.0352 - NMSE_wt: 0.0312 - covmat_fro_loss: 9.5469e-04 - global_gradnorm: 9.4169 - val_loss: 0.0256 - val_mse: 5.6930e-04 - val_NMSE: 0.0051 - val_NMSE_wt: 0.0046 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0736 - mse: 0.0066 - NMSE: 0.0592 - NMSE_wt: 0.0526 - covmat_fro_loss: 0.0011 - global_gradnorm: 12.5085 - tot_time: 0h 0m 34.7s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 48ms/step - loss: 0.0697 - mse: 0.0061 - NMSE: 0.0547 - NMSE_wt: 0.0487 - covmat_fro_loss: 0.0010 - global_gradnorm: 10.7646 - val_loss: 0.0259 - val_mse: 6.1427e-04 - val_NMSE: 0.0055 - val_NMSE_wt: 0.0049 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0566 - mse: 0.0044 - NMSE: 0.0399 - NMSE_wt: 0.0356 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.7802 - tot_time: 0h 0m 35.3s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 48ms/step - loss: 0.0566 - mse: 0.0044 - NMSE: 0.0399 - NMSE_wt: 0.0356 - covmat_fro_loss: 0.0010 - global_gradnorm: 8.6446 - val_loss: 0.0270 - val_mse: 7.4799e-04 - val_NMSE: 0.0067 - val_NMSE_wt: 0.0060 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0650 - mse: 0.0055 - NMSE: 0.0496 - NMSE_wt: 0.0439 - covmat_fro_loss: 0.0011 - global_gradnorm: 7.2227 - tot_time: 0h 0m 35.8s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 0.0650 - mse: 0.0055 - NMSE: 0.0496 - NMSE_wt: 0.0439 - covmat_fro_loss: 0.0011 - global_gradnorm: 7.3863 - val_loss: 0.0357 - val_mse: 0.0018 - val_NMSE: 0.0164 - val_NMSE_wt: 0.0147 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0616 - mse: 0.0051 - NMSE: 0.0462 - NMSE_wt: 0.0405 - covmat_fro_loss: 0.0011 - global_gradnorm: 13.1708 - tot_time: 0h 0m 36.2s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0588 - mse: 0.0048 - NMSE: 0.0430 - NMSE_wt: 0.0378 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.2509 - val_loss: 0.0259 - val_mse: 6.1145e-04 - val_NMSE: 0.0055 - val_NMSE_wt: 0.0049 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0571 - mse: 0.0045 - NMSE: 0.0408 - NMSE_wt: 0.0361 - covmat_fro_loss: 0.0010 - global_gradnorm: 10.9618 - tot_time: 0h 0m 36.7s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 49ms/step - loss: 0.0571 - mse: 0.0045 - NMSE: 0.0408 - NMSE_wt: 0.0361 - covmat_fro_loss: 0.0011 - global_gradnorm: 10.8667 - val_loss: 0.0329 - val_mse: 0.0015 - val_NMSE: 0.0132 - val_NMSE_wt: 0.0118 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0618 - mse: 0.0051 - NMSE: 0.0462 - NMSE_wt: 0.0407 - covmat_fro_loss: 9.8777e-04 - global_gradnorm: 10.9240 - tot_time: 0h 0m 37.3s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 52ms/step - loss: 0.0618 - mse: 0.0051 - NMSE: 0.0462 - NMSE_wt: 0.0407 - covmat_fro_loss: 9.7666e-04 - global_gradnorm: 10.1329 - val_loss: 0.0264 - val_mse: 6.7702e-04 - val_NMSE: 0.0061 - val_NMSE_wt: 0.0054 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 0.0510 - mse: 0.0038 - NMSE: 0.0342 - NMSE_wt: 0.0299 - covmat_fro_loss: 9.9529e-04 - global_gradnorm: 9.0298 - tot_time: 0h 0m 37.9s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 50ms/step - loss: 0.0510 - mse: 0.0038 - NMSE: 0.0342 - NMSE_wt: 0.0299 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.5712 - val_loss: 0.0268 - val_mse: 7.1926e-04 - val_NMSE: 0.0065 - val_NMSE_wt: 0.0058 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0575 - mse: 0.0046 - NMSE: 0.0412 - NMSE_wt: 0.0365 - covmat_fro_loss: 0.0011 - global_gradnorm: 8.7207  - tot_time: 0h 0m 38.3s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 0.0575 - mse: 0.0046 - NMSE: 0.0412 - NMSE_wt: 0.0365 - covmat_fro_loss: 0.0011 - global_gradnorm: 8.0627 - val_loss: 0.0271 - val_mse: 7.4908e-04 - val_NMSE: 0.0067 - val_NMSE_wt: 0.0060 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0026 - NMSE: 0.0236 - NMSE_wt: 0.0210 - covmat_fro_loss: 9.9862e-04 - global_gradnorm: 11.0907 - tot_time: 0h 0m 38.7s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 0.0420 - mse: 0.0026 - NMSE: 0.0236 - NMSE_wt: 0.0210 - covmat_fro_loss: 9.7979e-04 - global_gradnorm: 10.4223 - val_loss: 0.0269 - val_mse: 7.3959e-04 - val_NMSE: 0.0067 - val_NMSE_wt: 0.0059 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0528 - mse: 0.0040 - NMSE: 0.0360 - NMSE_wt: 0.0317 - covmat_fro_loss: 9.8794e-04 - global_gradnorm: 9.1640 - tot_time: 0h 0m 39.1s\n",
      "\n",
      "Epoch 21: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.0528 - mse: 0.0040 - NMSE: 0.0360 - NMSE_wt: 0.0317 - covmat_fro_loss: 9.8050e-04 - global_gradnorm: 9.0545 - val_loss: 0.0315 - val_mse: 0.0013 - val_NMSE: 0.0117 - val_NMSE_wt: 0.0105 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 22/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0464 - mse: 0.0032 - NMSE: 0.0287 - NMSE_wt: 0.0254 - covmat_fro_loss: 0.0010 - global_gradnorm: 9.7814      - tot_time: 0h 0m 39.7s\n",
      "\n",
      "Epoch 22: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 1s 50ms/step - loss: 0.0515 - mse: 0.0038 - NMSE: 0.0343 - NMSE_wt: 0.0305 - covmat_fro_loss: 9.9750e-04 - global_gradnorm: 9.8001 - val_loss: 0.0428 - val_mse: 0.0027 - val_NMSE: 0.0241 - val_NMSE_wt: 0.0218 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0504 - mse: 0.0037 - NMSE: 0.0334 - NMSE_wt: 0.0294 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.6902   - tot_time: 0h 0m 40.1s\n",
      "\n",
      "Epoch 23: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 0.0504 - mse: 0.0037 - NMSE: 0.0334 - NMSE_wt: 0.0294 - covmat_fro_loss: 0.0010 - global_gradnorm: 12.1910 - val_loss: 0.0335 - val_mse: 0.0015 - val_NMSE: 0.0139 - val_NMSE_wt: 0.0125 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 24/200\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.0511 - mse: 0.0038 - NMSE: 0.0338 - NMSE_wt: 0.0300 - covmat_fro_loss: 0.0011 - global_gradnorm: 11.1968 - tot_time: 0h 0m 40.6s\n",
      "\n",
      "Epoch 24: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 44ms/step - loss: 0.0496 - mse: 0.0036 - NMSE: 0.0322 - NMSE_wt: 0.0286 - covmat_fro_loss: 0.0010 - global_gradnorm: 11.6113 - val_loss: 0.0261 - val_mse: 6.3054e-04 - val_NMSE: 0.0057 - val_NMSE_wt: 0.0051 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0577 - mse: 0.0046 - NMSE: 0.0412 - NMSE_wt: 0.0367 - covmat_fro_loss: 0.0010 - global_gradnorm: 7.9570Restoring model weights from the end of the best epoch: 5.\n",
      " - tot_time: 0h 0m 41.0s\n",
      "\n",
      "Epoch 25: val_NMSE_wt did not improve from 0.00397\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-10_outsteps\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0577 - mse: 0.0046 - NMSE: 0.0412 - NMSE_wt: 0.0367 - covmat_fro_loss: 0.0010 - global_gradnorm: 7.5510 - val_loss: 0.0290 - val_mse: 9.9821e-04 - val_NMSE: 0.0090 - val_NMSE_wt: 0.0079 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 25: early stopping\n",
      "\n",
      "baseline : 3.9699E-03\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0265 - mse: 6.8203e-04 - NMSE: 0.0061 - NMSE_wt: 0.0055 - covmat_fro_loss: 0.0000e+00 - global_gradnorm: 0.0000e+00\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 15 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "clipnorm : None, global_clipnorm : 11.8\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0520 - mse: 0.0042 - NMSE: 0.0380 - NMSE_wt: 0.0309 - covmat_fro_loss: 0.0000e+00 - global_gradnorm: 0.0000e+00\n",
      "baseline : 3.0929E-02\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-06 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0787 - mse: 0.0079 - NMSE: 0.0707 - NMSE_wt: 0.0577 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.4735 - tot_time: 0h 0m 11.3s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 11s 72ms/step - loss: 0.0825 - mse: 0.0084 - NMSE: 0.0757 - NMSE_wt: 0.0615 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.8965 - val_loss: 0.1131 - val_mse: 0.0124 - val_NMSE: 0.1115 - val_NMSE_wt: 0.0920 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0721 - mse: 0.0069 - NMSE: 0.0624 - NMSE_wt: 0.0510 - covmat_fro_loss: 0.0015 - global_gradnorm: 7.4042 - tot_time: 0h 0m 11.9s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.0721 - mse: 0.0069 - NMSE: 0.0624 - NMSE_wt: 0.0510 - covmat_fro_loss: 0.0015 - global_gradnorm: 6.9579 - val_loss: 0.0997 - val_mse: 0.0105 - val_NMSE: 0.0948 - val_NMSE_wt: 0.0786 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0880 - mse: 0.0091 - NMSE: 0.0817 - NMSE_wt: 0.0670 - covmat_fro_loss: 0.0016 - global_gradnorm: 11.3035 - tot_time: 0h 0m 12.3s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0880 - mse: 0.0091 - NMSE: 0.0817 - NMSE_wt: 0.0670 - covmat_fro_loss: 0.0015 - global_gradnorm: 11.3486 - val_loss: 0.1328 - val_mse: 0.0152 - val_NMSE: 0.1364 - val_NMSE_wt: 0.1118 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0915 - mse: 0.0095 - NMSE: 0.0857 - NMSE_wt: 0.0705 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.5742 - tot_time: 0h 0m 12.9s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.0915 - mse: 0.0095 - NMSE: 0.0857 - NMSE_wt: 0.0705 - covmat_fro_loss: 0.0016 - global_gradnorm: 8.8675 - val_loss: 0.1106 - val_mse: 0.0120 - val_NMSE: 0.1084 - val_NMSE_wt: 0.0896 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0924 - mse: 0.0098 - NMSE: 0.0879 - NMSE_wt: 0.0714 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.0089 - tot_time: 0h 0m 13.4s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.0924 - mse: 0.0098 - NMSE: 0.0879 - NMSE_wt: 0.0714 - covmat_fro_loss: 0.0017 - global_gradnorm: 8.7612 - val_loss: 0.0536 - val_mse: 0.0045 - val_NMSE: 0.0402 - val_NMSE_wt: 0.0326 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0638 - mse: 0.0058 - NMSE: 0.0526 - NMSE_wt: 0.0428 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.1157 - tot_time: 0h 0m 14.0s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.0638 - mse: 0.0058 - NMSE: 0.0526 - NMSE_wt: 0.0428 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.4507 - val_loss: 0.1035 - val_mse: 0.0111 - val_NMSE: 0.0995 - val_NMSE_wt: 0.0825 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0767 - mse: 0.0075 - NMSE: 0.0678 - NMSE_wt: 0.0556 - covmat_fro_loss: 0.0016 - global_gradnorm: 8.9197 - tot_time: 0h 0m 14.4s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0731 - mse: 0.0070 - NMSE: 0.0633 - NMSE_wt: 0.0520 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.6223 - val_loss: 0.0977 - val_mse: 0.0102 - val_NMSE: 0.0921 - val_NMSE_wt: 0.0766 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0732 - mse: 0.0070 - NMSE: 0.0633 - NMSE_wt: 0.0522 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.6374 - tot_time: 0h 0m 15.0s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 61ms/step - loss: 0.0732 - mse: 0.0070 - NMSE: 0.0633 - NMSE_wt: 0.0522 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.0689 - val_loss: 0.1075 - val_mse: 0.0116 - val_NMSE: 0.1047 - val_NMSE_wt: 0.0864 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0720 - mse: 0.0069 - NMSE: 0.0625 - NMSE_wt: 0.0509 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.5570 - tot_time: 0h 0m 15.5s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.0720 - mse: 0.0069 - NMSE: 0.0625 - NMSE_wt: 0.0509 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.7609 - val_loss: 0.0541 - val_mse: 0.0045 - val_NMSE: 0.0408 - val_NMSE_wt: 0.0331 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0891 - mse: 0.0091 - NMSE: 0.0821 - NMSE_wt: 0.0680 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.2248 - tot_time: 0h 0m 16.0s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.0832 - mse: 0.0083 - NMSE: 0.0750 - NMSE_wt: 0.0622 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.8797 - val_loss: 0.1066 - val_mse: 0.0115 - val_NMSE: 0.1034 - val_NMSE_wt: 0.0856 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0835 - mse: 0.0084 - NMSE: 0.0759 - NMSE_wt: 0.0625 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.6232 - tot_time: 0h 0m 16.6s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.0835 - mse: 0.0084 - NMSE: 0.0759 - NMSE_wt: 0.0625 - covmat_fro_loss: 0.0016 - global_gradnorm: 10.7302 - val_loss: 0.1346 - val_mse: 0.0154 - val_NMSE: 0.1384 - val_NMSE_wt: 0.1135 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1025 - mse: 0.0109 - NMSE: 0.0977 - NMSE_wt: 0.0814 - covmat_fro_loss: 0.0016 - global_gradnorm: 7.8953 - tot_time: 0h 0m 17.0s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.1025 - mse: 0.0109 - NMSE: 0.0977 - NMSE_wt: 0.0814 - covmat_fro_loss: 0.0016 - global_gradnorm: 7.8087 - val_loss: 0.0523 - val_mse: 0.0043 - val_NMSE: 0.0386 - val_NMSE_wt: 0.0312 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0695 - mse: 0.0066 - NMSE: 0.0597 - NMSE_wt: 0.0485 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.1994 - tot_time: 0h 0m 17.5s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 50ms/step - loss: 0.0740 - mse: 0.0072 - NMSE: 0.0646 - NMSE_wt: 0.0530 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.2693 - val_loss: 0.0716 - val_mse: 0.0069 - val_NMSE: 0.0617 - val_NMSE_wt: 0.0506 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 0.0892 - mse: 0.0092 - NMSE: 0.0832 - NMSE_wt: 0.0681 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.5126 - tot_time: 0h 0m 18.0s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 50ms/step - loss: 0.0892 - mse: 0.0092 - NMSE: 0.0832 - NMSE_wt: 0.0681 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.6296 - val_loss: 0.0981 - val_mse: 0.0103 - val_NMSE: 0.0928 - val_NMSE_wt: 0.0771 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0755 - mse: 0.0074 - NMSE: 0.0662 - NMSE_wt: 0.0545 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.1742 - tot_time: 0h 0m 18.5s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0765 - mse: 0.0075 - NMSE: 0.0676 - NMSE_wt: 0.0555 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.9387 - val_loss: 0.0708 - val_mse: 0.0067 - val_NMSE: 0.0602 - val_NMSE_wt: 0.0498 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0696 - mse: 0.0066 - NMSE: 0.0592 - NMSE_wt: 0.0485 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.5164 - tot_time: 0h 0m 19.1s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.0668 - mse: 0.0062 - NMSE: 0.0558 - NMSE_wt: 0.0457 - covmat_fro_loss: 0.0014 - global_gradnorm: 7.7438 - val_loss: 0.0603 - val_mse: 0.0053 - val_NMSE: 0.0475 - val_NMSE_wt: 0.0393 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0655 - mse: 0.0061 - NMSE: 0.0553 - NMSE_wt: 0.0445 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.6715 - tot_time: 0h 0m 19.6s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.0655 - mse: 0.0061 - NMSE: 0.0553 - NMSE_wt: 0.0445 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.3389 - val_loss: 0.1143 - val_mse: 0.0126 - val_NMSE: 0.1135 - val_NMSE_wt: 0.0933 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0773 - mse: 0.0077 - NMSE: 0.0692 - NMSE_wt: 0.0563 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.6963 - tot_time: 0h 0m 20.2s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.0773 - mse: 0.0077 - NMSE: 0.0692 - NMSE_wt: 0.0563 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.8876 - val_loss: 0.0546 - val_mse: 0.0046 - val_NMSE: 0.0411 - val_NMSE_wt: 0.0336 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0941 - mse: 0.0098 - NMSE: 0.0882 - NMSE_wt: 0.0730 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.9275  - tot_time: 0h 0m 20.6s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0932 - mse: 0.0097 - NMSE: 0.0873 - NMSE_wt: 0.0721 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.2680 - val_loss: 0.0686 - val_mse: 0.0064 - val_NMSE: 0.0578 - val_NMSE_wt: 0.0476 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0911 - mse: 0.0095 - NMSE: 0.0855 - NMSE_wt: 0.0701 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.3717Restoring model weights from the end of the best epoch: 12.\n",
      " - tot_time: 0h 0m 21.2s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.0895 - mse: 0.0093 - NMSE: 0.0837 - NMSE_wt: 0.0685 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.2400 - val_loss: 0.1174 - val_mse: 0.0130 - val_NMSE: 0.1171 - val_NMSE_wt: 0.0964 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20: early stopping\n",
      "\n",
      "baseline : 3.0929E-02\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 5e-07 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0700 - mse: 0.0067 - NMSE: 0.0599 - NMSE_wt: 0.0490 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.5832 - tot_time: 0h 0m 21.7s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0781 - mse: 0.0078 - NMSE: 0.0698 - NMSE_wt: 0.0571 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.1681 - val_loss: 0.1203 - val_mse: 0.0134 - val_NMSE: 0.1202 - val_NMSE_wt: 0.0993 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0843 - mse: 0.0085 - NMSE: 0.0763 - NMSE_wt: 0.0633 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.5291 - tot_time: 0h 0m 22.2s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0843 - mse: 0.0085 - NMSE: 0.0763 - NMSE_wt: 0.0633 - covmat_fro_loss: 0.0015 - global_gradnorm: 7.9846 - val_loss: 0.0625 - val_mse: 0.0056 - val_NMSE: 0.0505 - val_NMSE_wt: 0.0415 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0904 - mse: 0.0094 - NMSE: 0.0850 - NMSE_wt: 0.0693 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.4967 - tot_time: 0h 0m 22.8s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.0904 - mse: 0.0094 - NMSE: 0.0850 - NMSE_wt: 0.0693 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.3233 - val_loss: 0.1290 - val_mse: 0.0146 - val_NMSE: 0.1318 - val_NMSE_wt: 0.1080 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0951 - mse: 0.0100 - NMSE: 0.0901 - NMSE_wt: 0.0741 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.5600 - tot_time: 0h 0m 23.3s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.03093\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.0951 - mse: 0.0100 - NMSE: 0.0901 - NMSE_wt: 0.0741 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.7636 - val_loss: 0.1063 - val_mse: 0.0115 - val_NMSE: 0.1032 - val_NMSE_wt: 0.0853 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0806 - mse: 0.0081 - NMSE: 0.0733 - NMSE_wt: 0.0596 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.4055 - tot_time: 0h 0m 23.8s\n",
      "\n",
      "Epoch 5: val_NMSE_wt improved from 0.03093 to 0.02933, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-15_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.0806 - mse: 0.0081 - NMSE: 0.0733 - NMSE_wt: 0.0596 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.7141 - val_loss: 0.0504 - val_mse: 0.0040 - val_NMSE: 0.0362 - val_NMSE_wt: 0.0293 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0856 - mse: 0.0088 - NMSE: 0.0790 - NMSE_wt: 0.0646 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.0468 - tot_time: 0h 0m 24.5s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.0856 - mse: 0.0088 - NMSE: 0.0790 - NMSE_wt: 0.0646 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.2971 - val_loss: 0.0994 - val_mse: 0.0105 - val_NMSE: 0.0942 - val_NMSE_wt: 0.0783 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0776 - mse: 0.0076 - NMSE: 0.0684 - NMSE_wt: 0.0566 - covmat_fro_loss: 0.0016 - global_gradnorm: 10.3297 - tot_time: 0h 0m 25.0s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.0774 - mse: 0.0075 - NMSE: 0.0679 - NMSE_wt: 0.0564 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.5970 - val_loss: 0.1014 - val_mse: 0.0108 - val_NMSE: 0.0968 - val_NMSE_wt: 0.0804 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0754 - mse: 0.0074 - NMSE: 0.0668 - NMSE_wt: 0.0543 - covmat_fro_loss: 0.0015 - global_gradnorm: 7.3710 - tot_time: 0h 0m 25.5s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.0754 - mse: 0.0074 - NMSE: 0.0668 - NMSE_wt: 0.0543 - covmat_fro_loss: 0.0014 - global_gradnorm: 7.7737 - val_loss: 0.0968 - val_mse: 0.0101 - val_NMSE: 0.0912 - val_NMSE_wt: 0.0758 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0716 - mse: 0.0069 - NMSE: 0.0618 - NMSE_wt: 0.0506 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.5771 - tot_time: 0h 0m 26.1s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.0691 - mse: 0.0065 - NMSE: 0.0587 - NMSE_wt: 0.0481 - covmat_fro_loss: 0.0014 - global_gradnorm: 9.1335 - val_loss: 0.0584 - val_mse: 0.0051 - val_NMSE: 0.0457 - val_NMSE_wt: 0.0373 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0921 - mse: 0.0098 - NMSE: 0.0879 - NMSE_wt: 0.0710 - covmat_fro_loss: 0.0016 - global_gradnorm: 10.5263 - tot_time: 0h 0m 26.5s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0864 - mse: 0.0090 - NMSE: 0.0808 - NMSE_wt: 0.0654 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.3258 - val_loss: 0.0616 - val_mse: 0.0055 - val_NMSE: 0.0494 - val_NMSE_wt: 0.0406 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0764 - mse: 0.0075 - NMSE: 0.0672 - NMSE_wt: 0.0554 - covmat_fro_loss: 0.0014 - global_gradnorm: 7.9505 - tot_time: 0h 0m 27.2s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.0764 - mse: 0.0075 - NMSE: 0.0672 - NMSE_wt: 0.0554 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.3005 - val_loss: 0.0905 - val_mse: 0.0094 - val_NMSE: 0.0845 - val_NMSE_wt: 0.0695 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1018 - mse: 0.0109 - NMSE: 0.0980 - NMSE_wt: 0.0808 - covmat_fro_loss: 0.0016 - global_gradnorm: 7.5542 - tot_time: 0h 0m 27.8s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.1018 - mse: 0.0109 - NMSE: 0.0980 - NMSE_wt: 0.0808 - covmat_fro_loss: 0.0015 - global_gradnorm: 7.4675 - val_loss: 0.0515 - val_mse: 0.0041 - val_NMSE: 0.0373 - val_NMSE_wt: 0.0304 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0759 - mse: 0.0075 - NMSE: 0.0671 - NMSE_wt: 0.0549 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.4387 - tot_time: 0h 0m 28.3s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.0759 - mse: 0.0075 - NMSE: 0.0671 - NMSE_wt: 0.0549 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.7442 - val_loss: 0.0527 - val_mse: 0.0044 - val_NMSE: 0.0392 - val_NMSE_wt: 0.0317 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0959 - mse: 0.0102 - NMSE: 0.0916 - NMSE_wt: 0.0749 - covmat_fro_loss: 0.0016 - global_gradnorm: 11.2230 - tot_time: 0h 0m 28.8s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 49ms/step - loss: 0.0959 - mse: 0.0102 - NMSE: 0.0916 - NMSE_wt: 0.0749 - covmat_fro_loss: 0.0016 - global_gradnorm: 11.2755 - val_loss: 0.0884 - val_mse: 0.0090 - val_NMSE: 0.0810 - val_NMSE_wt: 0.0673 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0770 - mse: 0.0076 - NMSE: 0.0682 - NMSE_wt: 0.0560 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.8098  - tot_time: 0h 0m 29.3s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.0770 - mse: 0.0076 - NMSE: 0.0682 - NMSE_wt: 0.0560 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.8268 - val_loss: 0.0829 - val_mse: 0.0083 - val_NMSE: 0.0747 - val_NMSE_wt: 0.0618 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 0.0886 - mse: 0.0091 - NMSE: 0.0819 - NMSE_wt: 0.0675 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.9224 - tot_time: 0h 0m 29.9s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.0886 - mse: 0.0091 - NMSE: 0.0819 - NMSE_wt: 0.0675 - covmat_fro_loss: 0.0016 - global_gradnorm: 10.0931 - val_loss: 0.0575 - val_mse: 0.0049 - val_NMSE: 0.0444 - val_NMSE_wt: 0.0365 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0627 - mse: 0.0057 - NMSE: 0.0513 - NMSE_wt: 0.0416 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.9521 - tot_time: 0h 0m 30.5s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.0749 - mse: 0.0074 - NMSE: 0.0664 - NMSE_wt: 0.0539 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.4699 - val_loss: 0.1142 - val_mse: 0.0126 - val_NMSE: 0.1133 - val_NMSE_wt: 0.0932 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0935 - mse: 0.0098 - NMSE: 0.0883 - NMSE_wt: 0.0724 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.1755 - tot_time: 0h 0m 31.0s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0935 - mse: 0.0098 - NMSE: 0.0883 - NMSE_wt: 0.0724 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.4141 - val_loss: 0.0560 - val_mse: 0.0047 - val_NMSE: 0.0427 - val_NMSE_wt: 0.0349 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0870 - mse: 0.0090 - NMSE: 0.0810 - NMSE_wt: 0.0660 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.7568 - tot_time: 0h 0m 31.6s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.0870 - mse: 0.0090 - NMSE: 0.0810 - NMSE_wt: 0.0660 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.9425 - val_loss: 0.0665 - val_mse: 0.0061 - val_NMSE: 0.0551 - val_NMSE_wt: 0.0454 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0865 - mse: 0.0088 - NMSE: 0.0791 - NMSE_wt: 0.0655 - covmat_fro_loss: 0.0015 - global_gradnorm: 7.8690 - tot_time: 0h 0m 32.3s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0865 - mse: 0.0088 - NMSE: 0.0791 - NMSE_wt: 0.0655 - covmat_fro_loss: 0.0016 - global_gradnorm: 7.6678 - val_loss: 0.1136 - val_mse: 0.0125 - val_NMSE: 0.1123 - val_NMSE_wt: 0.0925 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0816 - mse: 0.0083 - NMSE: 0.0744 - NMSE_wt: 0.0605 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.7841 - tot_time: 0h 0m 32.8s\n",
      "\n",
      "Epoch 21: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0816 - mse: 0.0083 - NMSE: 0.0744 - NMSE_wt: 0.0605 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.0583 - val_loss: 0.0680 - val_mse: 0.0063 - val_NMSE: 0.0568 - val_NMSE_wt: 0.0470 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0813 - mse: 0.0082 - NMSE: 0.0738 - NMSE_wt: 0.0603 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.2301 - tot_time: 0h 0m 33.3s\n",
      "\n",
      "Epoch 22: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.0813 - mse: 0.0082 - NMSE: 0.0738 - NMSE_wt: 0.0603 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.0461 - val_loss: 0.0617 - val_mse: 0.0055 - val_NMSE: 0.0491 - val_NMSE_wt: 0.0407 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0687 - mse: 0.0064 - NMSE: 0.0579 - NMSE_wt: 0.0477 - covmat_fro_loss: 0.0014 - global_gradnorm: 7.1296 - tot_time: 0h 0m 33.9s\n",
      "\n",
      "Epoch 23: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 58ms/step - loss: 0.0687 - mse: 0.0064 - NMSE: 0.0579 - NMSE_wt: 0.0477 - covmat_fro_loss: 0.0014 - global_gradnorm: 6.6787 - val_loss: 0.0600 - val_mse: 0.0053 - val_NMSE: 0.0479 - val_NMSE_wt: 0.0390 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0919 - mse: 0.0095 - NMSE: 0.0858 - NMSE_wt: 0.0709 - covmat_fro_loss: 0.0016 - global_gradnorm: 10.1479 - tot_time: 0h 0m 34.5s\n",
      "\n",
      "Epoch 24: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.0919 - mse: 0.0095 - NMSE: 0.0858 - NMSE_wt: 0.0709 - covmat_fro_loss: 0.0016 - global_gradnorm: 10.2981 - val_loss: 0.1071 - val_mse: 0.0116 - val_NMSE: 0.1043 - val_NMSE_wt: 0.0861 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0755 - mse: 0.0074 - NMSE: 0.0663 - NMSE_wt: 0.0544 - covmat_fro_loss: 0.0014 - global_gradnorm: 7.2271Restoring model weights from the end of the best epoch: 5.\n",
      " - tot_time: 0h 0m 35.1s\n",
      "\n",
      "Epoch 25: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.0755 - mse: 0.0074 - NMSE: 0.0663 - NMSE_wt: 0.0544 - covmat_fro_loss: 0.0014 - global_gradnorm: 7.1891 - val_loss: 0.1211 - val_mse: 0.0135 - val_NMSE: 0.1216 - val_NMSE_wt: 0.1001 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 25: early stopping\n",
      "\n",
      "baseline : 2.9325E-02\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-07 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0826 - mse: 0.0083 - NMSE: 0.0750 - NMSE_wt: 0.0616 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.4812 - tot_time: 0h 0m 35.7s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.0821 - mse: 0.0083 - NMSE: 0.0747 - NMSE_wt: 0.0611 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.0846 - val_loss: 0.1122 - val_mse: 0.0123 - val_NMSE: 0.1105 - val_NMSE_wt: 0.0911 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0807 - mse: 0.0081 - NMSE: 0.0729 - NMSE_wt: 0.0597 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.3357 - tot_time: 0h 0m 36.2s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.0807 - mse: 0.0081 - NMSE: 0.0729 - NMSE_wt: 0.0597 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.5597 - val_loss: 0.0570 - val_mse: 0.0049 - val_NMSE: 0.0440 - val_NMSE_wt: 0.0360 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0997 - mse: 0.0106 - NMSE: 0.0953 - NMSE_wt: 0.0787 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.4338 - tot_time: 0h 0m 36.7s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 50ms/step - loss: 0.0997 - mse: 0.0106 - NMSE: 0.0953 - NMSE_wt: 0.0787 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.7398 - val_loss: 0.1048 - val_mse: 0.0113 - val_NMSE: 0.1021 - val_NMSE_wt: 0.0838 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0862 - mse: 0.0088 - NMSE: 0.0795 - NMSE_wt: 0.0651 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.0053 - tot_time: 0h 0m 37.4s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.0862 - mse: 0.0088 - NMSE: 0.0795 - NMSE_wt: 0.0651 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.2594 - val_loss: 0.1105 - val_mse: 0.0120 - val_NMSE: 0.1081 - val_NMSE_wt: 0.0895 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0753 - mse: 0.0073 - NMSE: 0.0660 - NMSE_wt: 0.0543 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.5444 - tot_time: 0h 0m 37.9s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.0786 - mse: 0.0078 - NMSE: 0.0702 - NMSE_wt: 0.0575 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.5929 - val_loss: 0.0541 - val_mse: 0.0045 - val_NMSE: 0.0408 - val_NMSE_wt: 0.0330 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0818 - mse: 0.0082 - NMSE: 0.0740 - NMSE_wt: 0.0608 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.9889 - tot_time: 0h 0m 38.4s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.0818 - mse: 0.0082 - NMSE: 0.0740 - NMSE_wt: 0.0608 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.2445 - val_loss: 0.1020 - val_mse: 0.0108 - val_NMSE: 0.0976 - val_NMSE_wt: 0.0809 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0879 - mse: 0.0091 - NMSE: 0.0823 - NMSE_wt: 0.0668 - covmat_fro_loss: 0.0015 - global_gradnorm: 10.6090 - tot_time: 0h 0m 39.0s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.0879 - mse: 0.0091 - NMSE: 0.0823 - NMSE_wt: 0.0668 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.7494 - val_loss: 0.1123 - val_mse: 0.0123 - val_NMSE: 0.1107 - val_NMSE_wt: 0.0912 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0885 - mse: 0.0091 - NMSE: 0.0822 - NMSE_wt: 0.0675 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.8193  - tot_time: 0h 0m 39.6s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.0885 - mse: 0.0091 - NMSE: 0.0822 - NMSE_wt: 0.0675 - covmat_fro_loss: 0.0016 - global_gradnorm: 9.9993 - val_loss: 0.1031 - val_mse: 0.0110 - val_NMSE: 0.0993 - val_NMSE_wt: 0.0821 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0815 - mse: 0.0082 - NMSE: 0.0738 - NMSE_wt: 0.0605 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.2518 - tot_time: 0h 0m 40.2s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.0815 - mse: 0.0082 - NMSE: 0.0738 - NMSE_wt: 0.0605 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.4835 - val_loss: 0.0585 - val_mse: 0.0051 - val_NMSE: 0.0459 - val_NMSE_wt: 0.0375 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0973 - mse: 0.0103 - NMSE: 0.0929 - NMSE_wt: 0.0763 - covmat_fro_loss: 0.0017 - global_gradnorm: 10.6313 - tot_time: 0h 0m 40.6s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.0907 - mse: 0.0094 - NMSE: 0.0848 - NMSE_wt: 0.0697 - covmat_fro_loss: 0.0016 - global_gradnorm: 8.9199 - val_loss: 0.0615 - val_mse: 0.0055 - val_NMSE: 0.0492 - val_NMSE_wt: 0.0405 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0836 - mse: 0.0085 - NMSE: 0.0761 - NMSE_wt: 0.0626 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.1883 - tot_time: 0h 0m 41.2s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.0836 - mse: 0.0085 - NMSE: 0.0761 - NMSE_wt: 0.0626 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.5166 - val_loss: 0.1052 - val_mse: 0.0113 - val_NMSE: 0.1021 - val_NMSE_wt: 0.0841 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0839 - mse: 0.0084 - NMSE: 0.0760 - NMSE_wt: 0.0629 - covmat_fro_loss: 0.0015 - global_gradnorm: 7.8785 - tot_time: 0h 0m 41.7s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 0.0839 - mse: 0.0084 - NMSE: 0.0760 - NMSE_wt: 0.0629 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.1885 - val_loss: 0.0536 - val_mse: 0.0045 - val_NMSE: 0.0401 - val_NMSE_wt: 0.0326 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 0.0811 - mse: 0.0081 - NMSE: 0.0729 - NMSE_wt: 0.0600 - covmat_fro_loss: 0.0015 - global_gradnorm: 11.2004 - tot_time: 0h 0m 42.2s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0811 - mse: 0.0081 - NMSE: 0.0729 - NMSE_wt: 0.0600 - covmat_fro_loss: 0.0015 - global_gradnorm: 11.2549 - val_loss: 0.0532 - val_mse: 0.0044 - val_NMSE: 0.0399 - val_NMSE_wt: 0.0322 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0758 - mse: 0.0074 - NMSE: 0.0669 - NMSE_wt: 0.0548 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.8910  - tot_time: 0h 0m 42.6s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0758 - mse: 0.0074 - NMSE: 0.0669 - NMSE_wt: 0.0548 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.4565 - val_loss: 0.0631 - val_mse: 0.0057 - val_NMSE: 0.0510 - val_NMSE_wt: 0.0421 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0910 - mse: 0.0095 - NMSE: 0.0857 - NMSE_wt: 0.0700 - covmat_fro_loss: 0.0016 - global_gradnorm: 7.9860 - tot_time: 0h 0m 43.1s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0891 - mse: 0.0093 - NMSE: 0.0836 - NMSE_wt: 0.0681 - covmat_fro_loss: 0.0016 - global_gradnorm: 8.6795 - val_loss: 0.0958 - val_mse: 0.0102 - val_NMSE: 0.0922 - val_NMSE_wt: 0.0747 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0843 - mse: 0.0086 - NMSE: 0.0774 - NMSE_wt: 0.0633 - covmat_fro_loss: 0.0015 - global_gradnorm: 7.2680 - tot_time: 0h 0m 43.7s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 60ms/step - loss: 0.0843 - mse: 0.0086 - NMSE: 0.0774 - NMSE_wt: 0.0633 - covmat_fro_loss: 0.0015 - global_gradnorm: 7.6800 - val_loss: 0.0578 - val_mse: 0.0050 - val_NMSE: 0.0446 - val_NMSE_wt: 0.0368 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0898 - mse: 0.0093 - NMSE: 0.0834 - NMSE_wt: 0.0688 - covmat_fro_loss: 0.0016 - global_gradnorm: 8.4303 - tot_time: 0h 0m 44.1s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0898 - mse: 0.0093 - NMSE: 0.0834 - NMSE_wt: 0.0688 - covmat_fro_loss: 0.0017 - global_gradnorm: 8.0834 - val_loss: 0.1189 - val_mse: 0.0132 - val_NMSE: 0.1190 - val_NMSE_wt: 0.0979 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0731 - mse: 0.0070 - NMSE: 0.0631 - NMSE_wt: 0.0521 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.9298 - tot_time: 0h 0m 44.7s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0731 - mse: 0.0070 - NMSE: 0.0631 - NMSE_wt: 0.0521 - covmat_fro_loss: 0.0014 - global_gradnorm: 8.6364 - val_loss: 0.0575 - val_mse: 0.0050 - val_NMSE: 0.0446 - val_NMSE_wt: 0.0365 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0783 - mse: 0.0078 - NMSE: 0.0699 - NMSE_wt: 0.0573 - covmat_fro_loss: 0.0015 - global_gradnorm: 9.3443 - tot_time: 0h 0m 45.1s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0783 - mse: 0.0078 - NMSE: 0.0699 - NMSE_wt: 0.0573 - covmat_fro_loss: 0.0015 - global_gradnorm: 8.7474 - val_loss: 0.0645 - val_mse: 0.0059 - val_NMSE: 0.0527 - val_NMSE_wt: 0.0434 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0923 - mse: 0.0097 - NMSE: 0.0870 - NMSE_wt: 0.0712 - covmat_fro_loss: 0.0016 - global_gradnorm: 10.3737Restoring model weights from the end of the best epoch: 13.\n",
      " - tot_time: 0h 0m 45.7s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.02933\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-15_outsteps\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.0923 - mse: 0.0097 - NMSE: 0.0870 - NMSE_wt: 0.0712 - covmat_fro_loss: 0.0016 - global_gradnorm: 10.3165 - val_loss: 0.1228 - val_mse: 0.0138 - val_NMSE: 0.1239 - val_NMSE_wt: 0.1018 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20: early stopping\n",
      "\n",
      "baseline : 2.9325E-02\n",
      "reverting to original rnn and ae weights\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0299 - mse: 0.0012 - NMSE: 0.0107 - NMSE_wt: 0.0089 - covmat_fro_loss: 0.0000e+00 - global_gradnorm: 0.0000e+00\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 20 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "clipnorm : None, global_clipnorm : 11.0\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0524 - mse: 0.0044 - NMSE: 0.0398 - NMSE_wt: 0.0314 - covmat_fro_loss: 0.0000e+00 - global_gradnorm: 0.0000e+00\n",
      "baseline : 3.1384E-02\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-06 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0977 - mse: 0.0112 - NMSE: 0.1012 - NMSE_wt: 0.0767 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.8111 - tot_time: 0h 0m 15.1s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.03138\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 15s 81ms/step - loss: 0.0977 - mse: 0.0112 - NMSE: 0.1012 - NMSE_wt: 0.0767 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.0543 - val_loss: 0.0963 - val_mse: 0.0106 - val_NMSE: 0.0952 - val_NMSE_wt: 0.0752 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1184 - mse: 0.0144 - NMSE: 0.1294 - NMSE_wt: 0.0974 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.8668  - tot_time: 0h 0m 15.5s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.03138\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.1184 - mse: 0.0144 - NMSE: 0.1294 - NMSE_wt: 0.0974 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.9927 - val_loss: 0.0719 - val_mse: 0.0072 - val_NMSE: 0.0652 - val_NMSE_wt: 0.0508 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1231 - mse: 0.0149 - NMSE: 0.1342 - NMSE_wt: 0.1021 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.0265 - tot_time: 0h 0m 16.0s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.03138\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 65ms/step - loss: 0.1231 - mse: 0.0149 - NMSE: 0.1342 - NMSE_wt: 0.1021 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.0713 - val_loss: 0.0631 - val_mse: 0.0061 - val_NMSE: 0.0545 - val_NMSE_wt: 0.0420 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1265 - mse: 0.0156 - NMSE: 0.1407 - NMSE_wt: 0.1055 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.2095 - tot_time: 0h 0m 16.6s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.03138\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.1265 - mse: 0.0156 - NMSE: 0.1407 - NMSE_wt: 0.1055 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.4084 - val_loss: 0.0710 - val_mse: 0.0069 - val_NMSE: 0.0622 - val_NMSE_wt: 0.0500 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1338 - mse: 0.0166 - NMSE: 0.1490 - NMSE_wt: 0.1127 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.7637 - tot_time: 0h 0m 17.2s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.03138\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.1338 - mse: 0.0166 - NMSE: 0.1490 - NMSE_wt: 0.1127 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.0121 - val_loss: 0.0849 - val_mse: 0.0089 - val_NMSE: 0.0798 - val_NMSE_wt: 0.0639 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1084 - mse: 0.0128 - NMSE: 0.1156 - NMSE_wt: 0.0874 - covmat_fro_loss: 0.0021 - global_gradnorm: 6.5361 - tot_time: 0h 0m 17.6s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.03138\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1100 - mse: 0.0131 - NMSE: 0.1181 - NMSE_wt: 0.0890 - covmat_fro_loss: 0.0020 - global_gradnorm: 7.5281 - val_loss: 0.0554 - val_mse: 0.0050 - val_NMSE: 0.0448 - val_NMSE_wt: 0.0344 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0947 - mse: 0.0110 - NMSE: 0.0987 - NMSE_wt: 0.0736 - covmat_fro_loss: 0.0019 - global_gradnorm: 9.9519 - tot_time: 0h 0m 18.1s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.03138\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0947 - mse: 0.0110 - NMSE: 0.0987 - NMSE_wt: 0.0736 - covmat_fro_loss: 0.0020 - global_gradnorm: 10.0683 - val_loss: 0.1071 - val_mse: 0.0121 - val_NMSE: 0.1089 - val_NMSE_wt: 0.0861 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1173 - mse: 0.0141 - NMSE: 0.1270 - NMSE_wt: 0.0962 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.1650 - tot_time: 0h 0m 18.7s\n",
      "\n",
      "Epoch 8: val_NMSE_wt improved from 0.03138 to 0.02739, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-20_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.1173 - mse: 0.0141 - NMSE: 0.1270 - NMSE_wt: 0.0962 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.4800 - val_loss: 0.0484 - val_mse: 0.0040 - val_NMSE: 0.0360 - val_NMSE_wt: 0.0274 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1018 - mse: 0.0120 - NMSE: 0.1078 - NMSE_wt: 0.0808 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.4085 - tot_time: 0h 0m 19.4s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.02739\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.1018 - mse: 0.0120 - NMSE: 0.1078 - NMSE_wt: 0.0808 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.6964 - val_loss: 0.1049 - val_mse: 0.0117 - val_NMSE: 0.1057 - val_NMSE_wt: 0.0839 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1355 - mse: 0.0167 - NMSE: 0.1506 - NMSE_wt: 0.1145 - covmat_fro_loss: 0.0023 - global_gradnorm: 8.6313 - tot_time: 0h 0m 19.9s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.02739\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.1392 - mse: 0.0173 - NMSE: 0.1556 - NMSE_wt: 0.1182 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.1577 - val_loss: 0.0672 - val_mse: 0.0064 - val_NMSE: 0.0574 - val_NMSE_wt: 0.0462 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1070 - mse: 0.0127 - NMSE: 0.1143 - NMSE_wt: 0.0859 - covmat_fro_loss: 0.0022 - global_gradnorm: 10.6472 - tot_time: 0h 0m 20.3s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.02739\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.1148 - mse: 0.0138 - NMSE: 0.1246 - NMSE_wt: 0.0938 - covmat_fro_loss: 0.0022 - global_gradnorm: 10.7256 - val_loss: 0.0797 - val_mse: 0.0087 - val_NMSE: 0.0781 - val_NMSE_wt: 0.0587 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1051 - mse: 0.0126 - NMSE: 0.1131 - NMSE_wt: 0.0841 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.5782 - tot_time: 0h 0m 20.8s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.02739\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.1051 - mse: 0.0126 - NMSE: 0.1131 - NMSE_wt: 0.0841 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.4470 - val_loss: 0.0641 - val_mse: 0.0060 - val_NMSE: 0.0544 - val_NMSE_wt: 0.0430 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1281 - mse: 0.0156 - NMSE: 0.1406 - NMSE_wt: 0.1071 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.5315  - tot_time: 0h 0m 21.4s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.02739\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.1281 - mse: 0.0156 - NMSE: 0.1406 - NMSE_wt: 0.1071 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.1028 - val_loss: 0.0540 - val_mse: 0.0047 - val_NMSE: 0.0419 - val_NMSE_wt: 0.0330 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - ETA: 0s - loss: 0.1107 - mse: 0.0133 - NMSE: 0.1193 - NMSE_wt: 0.0897 - covmat_fro_loss: 0.0020 - global_gradnorm: 7.7377 - tot_time: 0h 0m 22.0s\n",
      "\n",
      "Epoch 14: val_NMSE_wt improved from 0.02739 to 0.02092, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-20_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 0.1107 - mse: 0.0133 - NMSE: 0.1193 - NMSE_wt: 0.0897 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.1002 - val_loss: 0.0420 - val_mse: 0.0030 - val_NMSE: 0.0272 - val_NMSE_wt: 0.0209 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1139 - mse: 0.0138 - NMSE: 0.1238 - NMSE_wt: 0.0929 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.9508 - tot_time: 0h 0m 22.6s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.02092\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.1095 - mse: 0.0131 - NMSE: 0.1180 - NMSE_wt: 0.0885 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.4062 - val_loss: 0.0452 - val_mse: 0.0035 - val_NMSE: 0.0316 - val_NMSE_wt: 0.0242 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1147 - mse: 0.0137 - NMSE: 0.1237 - NMSE_wt: 0.0937 - covmat_fro_loss: 0.0021 - global_gradnorm: 10.7766 - tot_time: 0h 0m 23.1s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.02092\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.1147 - mse: 0.0137 - NMSE: 0.1237 - NMSE_wt: 0.0937 - covmat_fro_loss: 0.0022 - global_gradnorm: 10.8015 - val_loss: 0.0942 - val_mse: 0.0106 - val_NMSE: 0.0951 - val_NMSE_wt: 0.0732 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1093 - mse: 0.0130 - NMSE: 0.1171 - NMSE_wt: 0.0883 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.3055 - tot_time: 0h 0m 23.7s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.02092\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.1093 - mse: 0.0130 - NMSE: 0.1171 - NMSE_wt: 0.0883 - covmat_fro_loss: 0.0021 - global_gradnorm: 6.9769 - val_loss: 0.1065 - val_mse: 0.0120 - val_NMSE: 0.1080 - val_NMSE_wt: 0.0854 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1130 - mse: 0.0136 - NMSE: 0.1223 - NMSE_wt: 0.0920 - covmat_fro_loss: 0.0022 - global_gradnorm: 11.0000 - tot_time: 0h 0m 24.2s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.02092\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1130 - mse: 0.0136 - NMSE: 0.1223 - NMSE_wt: 0.0920 - covmat_fro_loss: 0.0022 - global_gradnorm: 11.0000 - val_loss: 0.0739 - val_mse: 0.0079 - val_NMSE: 0.0707 - val_NMSE_wt: 0.0529 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1248 - mse: 0.0152 - NMSE: 0.1371 - NMSE_wt: 0.1038 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.8954 - tot_time: 0h 0m 24.6s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.02092\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1248 - mse: 0.0152 - NMSE: 0.1371 - NMSE_wt: 0.1038 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.0294 - val_loss: 0.1371 - val_mse: 0.0166 - val_NMSE: 0.1490 - val_NMSE_wt: 0.1161 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1368 - mse: 0.0172 - NMSE: 0.1547 - NMSE_wt: 0.1157 - covmat_fro_loss: 0.0024 - global_gradnorm: 10.2184 - tot_time: 0h 0m 25.0s\n",
      "\n",
      "Epoch 20: val_NMSE_wt improved from 0.02092 to 0.01573, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-20_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.1278 - mse: 0.0158 - NMSE: 0.1426 - NMSE_wt: 0.1067 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.4913 - val_loss: 0.0368 - val_mse: 0.0023 - val_NMSE: 0.0204 - val_NMSE_wt: 0.0157 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1099 - mse: 0.0132 - NMSE: 0.1186 - NMSE_wt: 0.0889 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.2908 - tot_time: 0h 0m 25.7s\n",
      "\n",
      "Epoch 21: val_NMSE_wt did not improve from 0.01573\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.1099 - mse: 0.0132 - NMSE: 0.1186 - NMSE_wt: 0.0889 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.4807 - val_loss: 0.0657 - val_mse: 0.0063 - val_NMSE: 0.0567 - val_NMSE_wt: 0.0447 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1060 - mse: 0.0125 - NMSE: 0.1129 - NMSE_wt: 0.0849 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.0979 - tot_time: 0h 0m 26.2s\n",
      "\n",
      "Epoch 22: val_NMSE_wt did not improve from 0.01573\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.1060 - mse: 0.0125 - NMSE: 0.1129 - NMSE_wt: 0.0849 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.3092 - val_loss: 0.0491 - val_mse: 0.0040 - val_NMSE: 0.0363 - val_NMSE_wt: 0.0281 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1251 - mse: 0.0154 - NMSE: 0.1386 - NMSE_wt: 0.1040 - covmat_fro_loss: 0.0021 - global_gradnorm: 6.9288 - tot_time: 0h 0m 26.8s\n",
      "\n",
      "Epoch 23: val_NMSE_wt did not improve from 0.01573\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.1251 - mse: 0.0154 - NMSE: 0.1386 - NMSE_wt: 0.1040 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.3812 - val_loss: 0.0669 - val_mse: 0.0063 - val_NMSE: 0.0570 - val_NMSE_wt: 0.0458 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1140 - mse: 0.0138 - NMSE: 0.1241 - NMSE_wt: 0.0930 - covmat_fro_loss: 0.0021 - global_gradnorm: 10.0098 - tot_time: 0h 0m 27.3s\n",
      "\n",
      "Epoch 24: val_NMSE_wt did not improve from 0.01573\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.1140 - mse: 0.0138 - NMSE: 0.1241 - NMSE_wt: 0.0930 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.6211 - val_loss: 0.0762 - val_mse: 0.0081 - val_NMSE: 0.0729 - val_NMSE_wt: 0.0552 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1251 - mse: 0.0153 - NMSE: 0.1377 - NMSE_wt: 0.1040 - covmat_fro_loss: 0.0022 - global_gradnorm: 7.3853 - tot_time: 0h 0m 27.8s\n",
      "\n",
      "Epoch 25: val_NMSE_wt improved from 0.01573 to 0.01534, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/checkpoint-20_outsteps\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.1251 - mse: 0.0153 - NMSE: 0.1377 - NMSE_wt: 0.1040 - covmat_fro_loss: 0.0022 - global_gradnorm: 6.6848 - val_loss: 0.0364 - val_mse: 0.0022 - val_NMSE: 0.0202 - val_NMSE_wt: 0.0153 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1188 - mse: 0.0145 - NMSE: 0.1305 - NMSE_wt: 0.0978 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.0123 - tot_time: 0h 0m 28.5s\n",
      "\n",
      "Epoch 26: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.1188 - mse: 0.0145 - NMSE: 0.1305 - NMSE_wt: 0.0978 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.2331 - val_loss: 0.0889 - val_mse: 0.0096 - val_NMSE: 0.0863 - val_NMSE_wt: 0.0678 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1224 - mse: 0.0149 - NMSE: 0.1341 - NMSE_wt: 0.1013 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.1138 - tot_time: 0h 0m 29.1s\n",
      "\n",
      "Epoch 27: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.1224 - mse: 0.0149 - NMSE: 0.1341 - NMSE_wt: 0.1013 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.3234 - val_loss: 0.0471 - val_mse: 0.0037 - val_NMSE: 0.0336 - val_NMSE_wt: 0.0261 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0998 - mse: 0.0117 - NMSE: 0.1054 - NMSE_wt: 0.0788 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.5397 - tot_time: 0h 0m 29.6s\n",
      "\n",
      "Epoch 28: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.0998 - mse: 0.0117 - NMSE: 0.1054 - NMSE_wt: 0.0788 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.7020 - val_loss: 0.1035 - val_mse: 0.0115 - val_NMSE: 0.1037 - val_NMSE_wt: 0.0824 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1095 - mse: 0.0131 - NMSE: 0.1179 - NMSE_wt: 0.0884 - covmat_fro_loss: 0.0020 - global_gradnorm: 7.4672 - tot_time: 0h 0m 30.2s\n",
      "\n",
      "Epoch 29: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.1095 - mse: 0.0131 - NMSE: 0.1179 - NMSE_wt: 0.0884 - covmat_fro_loss: 0.0019 - global_gradnorm: 6.8402 - val_loss: 0.1070 - val_mse: 0.0120 - val_NMSE: 0.1082 - val_NMSE_wt: 0.0860 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1021 - mse: 0.0120 - NMSE: 0.1078 - NMSE_wt: 0.0811 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.3117 - tot_time: 0h 0m 30.7s\n",
      "\n",
      "Epoch 30: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1021 - mse: 0.0120 - NMSE: 0.1078 - NMSE_wt: 0.0811 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.6104 - val_loss: 0.0499 - val_mse: 0.0042 - val_NMSE: 0.0379 - val_NMSE_wt: 0.0289 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1208 - mse: 0.0147 - NMSE: 0.1325 - NMSE_wt: 0.0998 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.6585 - tot_time: 0h 0m 31.3s\n",
      "\n",
      "Epoch 31: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.1208 - mse: 0.0147 - NMSE: 0.1325 - NMSE_wt: 0.0998 - covmat_fro_loss: 0.0020 - global_gradnorm: 7.3116 - val_loss: 0.1080 - val_mse: 0.0122 - val_NMSE: 0.1099 - val_NMSE_wt: 0.0869 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 32/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1282 - mse: 0.0158 - NMSE: 0.1426 - NMSE_wt: 0.1072 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.4387 - tot_time: 0h 0m 31.8s\n",
      "\n",
      "Epoch 32: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.1312 - mse: 0.0162 - NMSE: 0.1462 - NMSE_wt: 0.1101 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.7857 - val_loss: 0.0664 - val_mse: 0.0064 - val_NMSE: 0.0576 - val_NMSE_wt: 0.0453 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1171 - mse: 0.0141 - NMSE: 0.1270 - NMSE_wt: 0.0961 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.1565 - tot_time: 0h 0m 32.3s\n",
      "\n",
      "Epoch 33: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.1171 - mse: 0.0141 - NMSE: 0.1270 - NMSE_wt: 0.0961 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.8344 - val_loss: 0.0443 - val_mse: 0.0034 - val_NMSE: 0.0303 - val_NMSE_wt: 0.0233 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1225 - mse: 0.0149 - NMSE: 0.1338 - NMSE_wt: 0.1014 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.7603 - tot_time: 0h 0m 32.8s\n",
      "\n",
      "Epoch 34: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1225 - mse: 0.0149 - NMSE: 0.1338 - NMSE_wt: 0.1014 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.8980 - val_loss: 0.1174 - val_mse: 0.0134 - val_NMSE: 0.1208 - val_NMSE_wt: 0.0964 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1147 - mse: 0.0137 - NMSE: 0.1232 - NMSE_wt: 0.0937 - covmat_fro_loss: 0.0020 - global_gradnorm: 10.8892 - tot_time: 0h 0m 33.4s\n",
      "\n",
      "Epoch 35: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.1147 - mse: 0.0137 - NMSE: 0.1232 - NMSE_wt: 0.0937 - covmat_fro_loss: 0.0020 - global_gradnorm: 10.9016 - val_loss: 0.0628 - val_mse: 0.0063 - val_NMSE: 0.0567 - val_NMSE_wt: 0.0417 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - ETA: 0s - loss: 0.1238 - mse: 0.0150 - NMSE: 0.1354 - NMSE_wt: 0.1028 - covmat_fro_loss: 0.0022 - global_gradnorm: 10.7419 - tot_time: 0h 0m 34.0s\n",
      "\n",
      "Epoch 36: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.1238 - mse: 0.0150 - NMSE: 0.1354 - NMSE_wt: 0.1028 - covmat_fro_loss: 0.0024 - global_gradnorm: 10.7705 - val_loss: 0.1234 - val_mse: 0.0145 - val_NMSE: 0.1303 - val_NMSE_wt: 0.1023 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1210 - mse: 0.0148 - NMSE: 0.1330 - NMSE_wt: 0.1000 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.1251 - tot_time: 0h 0m 34.4s\n",
      "\n",
      "Epoch 37: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1210 - mse: 0.0148 - NMSE: 0.1330 - NMSE_wt: 0.1000 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.3334 - val_loss: 0.0813 - val_mse: 0.0084 - val_NMSE: 0.0754 - val_NMSE_wt: 0.0603 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1259 - mse: 0.0156 - NMSE: 0.1400 - NMSE_wt: 0.1049 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.8450 - tot_time: 0h 0m 34.9s\n",
      "\n",
      "Epoch 38: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1259 - mse: 0.0156 - NMSE: 0.1400 - NMSE_wt: 0.1049 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.9734 - val_loss: 0.0520 - val_mse: 0.0045 - val_NMSE: 0.0405 - val_NMSE_wt: 0.0310 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1131 - mse: 0.0135 - NMSE: 0.1216 - NMSE_wt: 0.0921 - covmat_fro_loss: 0.0021 - global_gradnorm: 10.1197 - tot_time: 0h 0m 35.5s\n",
      "\n",
      "Epoch 39: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.1131 - mse: 0.0135 - NMSE: 0.1216 - NMSE_wt: 0.0921 - covmat_fro_loss: 0.0021 - global_gradnorm: 10.2175 - val_loss: 0.0383 - val_mse: 0.0025 - val_NMSE: 0.0224 - val_NMSE_wt: 0.0173 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1417 - mse: 0.0177 - NMSE: 0.1591 - NMSE_wt: 0.1206 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.7452 - tot_time: 0h 0m 35.9s\n",
      "\n",
      "Epoch 40: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.1417 - mse: 0.0177 - NMSE: 0.1591 - NMSE_wt: 0.1206 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.8846 - val_loss: 0.1102 - val_mse: 0.0123 - val_NMSE: 0.1103 - val_NMSE_wt: 0.0891 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1110 - mse: 0.0133 - NMSE: 0.1200 - NMSE_wt: 0.0900 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.7864 - tot_time: 0h 0m 36.4s\n",
      "\n",
      "Epoch 41: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.1110 - mse: 0.0133 - NMSE: 0.1200 - NMSE_wt: 0.0900 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.0323 - val_loss: 0.0636 - val_mse: 0.0060 - val_NMSE: 0.0536 - val_NMSE_wt: 0.0425 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 42/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1050 - mse: 0.0125 - NMSE: 0.1123 - NMSE_wt: 0.0840 - covmat_fro_loss: 0.0020 - global_gradnorm: 7.4505 - tot_time: 0h 0m 36.8s\n",
      "\n",
      "Epoch 42: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1168 - mse: 0.0143 - NMSE: 0.1283 - NMSE_wt: 0.0957 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.8951 - val_loss: 0.0661 - val_mse: 0.0068 - val_NMSE: 0.0611 - val_NMSE_wt: 0.0451 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1438 - mse: 0.0180 - NMSE: 0.1618 - NMSE_wt: 0.1228 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.7232  - tot_time: 0h 0m 37.4s\n",
      "\n",
      "Epoch 43: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.1438 - mse: 0.0180 - NMSE: 0.1618 - NMSE_wt: 0.1228 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.2794 - val_loss: 0.0880 - val_mse: 0.0092 - val_NMSE: 0.0830 - val_NMSE_wt: 0.0670 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1269 - mse: 0.0156 - NMSE: 0.1402 - NMSE_wt: 0.1058 - covmat_fro_loss: 0.0022 - global_gradnorm: 10.5732 - tot_time: 0h 0m 37.9s\n",
      "\n",
      "Epoch 44: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.1269 - mse: 0.0156 - NMSE: 0.1402 - NMSE_wt: 0.1058 - covmat_fro_loss: 0.0021 - global_gradnorm: 10.2538 - val_loss: 0.0468 - val_mse: 0.0036 - val_NMSE: 0.0326 - val_NMSE_wt: 0.0258 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1464 - mse: 0.0183 - NMSE: 0.1643 - NMSE_wt: 0.1254 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.5944Restoring model weights from the end of the best epoch: 25.\n",
      " - tot_time: 0h 0m 38.4s\n",
      "\n",
      "Epoch 45: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.1464 - mse: 0.0183 - NMSE: 0.1643 - NMSE_wt: 0.1254 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.6389 - val_loss: 0.0534 - val_mse: 0.0046 - val_NMSE: 0.0411 - val_NMSE_wt: 0.0324 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 45: early stopping\n",
      "\n",
      "baseline : 1.5343E-02\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 5e-07 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1046 - mse: 0.0124 - NMSE: 0.1116 - NMSE_wt: 0.0836 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.4927 - tot_time: 0h 0m 39.0s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.1046 - mse: 0.0124 - NMSE: 0.1116 - NMSE_wt: 0.0836 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.6601 - val_loss: 0.0747 - val_mse: 0.0075 - val_NMSE: 0.0671 - val_NMSE_wt: 0.0536 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1167 - mse: 0.0142 - NMSE: 0.1274 - NMSE_wt: 0.0956 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.1548 - tot_time: 0h 0m 39.5s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1167 - mse: 0.0142 - NMSE: 0.1274 - NMSE_wt: 0.0956 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.4080 - val_loss: 0.0756 - val_mse: 0.0081 - val_NMSE: 0.0725 - val_NMSE_wt: 0.0546 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1173 - mse: 0.0141 - NMSE: 0.1267 - NMSE_wt: 0.0962 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.7293 - tot_time: 0h 0m 40.0s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.1173 - mse: 0.0141 - NMSE: 0.1267 - NMSE_wt: 0.0962 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.5852 - val_loss: 0.0456 - val_mse: 0.0035 - val_NMSE: 0.0317 - val_NMSE_wt: 0.0245 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1183 - mse: 0.0145 - NMSE: 0.1303 - NMSE_wt: 0.0973 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.9759 - tot_time: 0h 0m 40.6s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.1183 - mse: 0.0145 - NMSE: 0.1303 - NMSE_wt: 0.0973 - covmat_fro_loss: 0.0021 - global_gradnorm: 10.0897 - val_loss: 0.1045 - val_mse: 0.0117 - val_NMSE: 0.1051 - val_NMSE_wt: 0.0834 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1112 - mse: 0.0133 - NMSE: 0.1199 - NMSE_wt: 0.0902 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.7173 - tot_time: 0h 0m 41.1s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.1112 - mse: 0.0133 - NMSE: 0.1199 - NMSE_wt: 0.0902 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.9198 - val_loss: 0.1338 - val_mse: 0.0162 - val_NMSE: 0.1454 - val_NMSE_wt: 0.1127 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1375 - mse: 0.0171 - NMSE: 0.1536 - NMSE_wt: 0.1165 - covmat_fro_loss: 0.0024 - global_gradnorm: 7.8329 - tot_time: 0h 0m 41.5s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1309 - mse: 0.0161 - NMSE: 0.1453 - NMSE_wt: 0.1099 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.5367 - val_loss: 0.0371 - val_mse: 0.0024 - val_NMSE: 0.0212 - val_NMSE_wt: 0.0161 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1175 - mse: 0.0142 - NMSE: 0.1278 - NMSE_wt: 0.0965 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.9830 - tot_time: 0h 0m 42.0s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.1175 - mse: 0.0142 - NMSE: 0.1278 - NMSE_wt: 0.0965 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.2071 - val_loss: 0.1294 - val_mse: 0.0156 - val_NMSE: 0.1403 - val_NMSE_wt: 0.1084 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1205 - mse: 0.0148 - NMSE: 0.1328 - NMSE_wt: 0.0995 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.3516  - tot_time: 0h 0m 42.6s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.1205 - mse: 0.0148 - NMSE: 0.1328 - NMSE_wt: 0.0995 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.7371 - val_loss: 0.0663 - val_mse: 0.0068 - val_NMSE: 0.0608 - val_NMSE_wt: 0.0453 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1311 - mse: 0.0163 - NMSE: 0.1469 - NMSE_wt: 0.1100 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.9779  - tot_time: 0h 0m 43.0s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1311 - mse: 0.0163 - NMSE: 0.1469 - NMSE_wt: 0.1100 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.4684 - val_loss: 0.1262 - val_mse: 0.0151 - val_NMSE: 0.1362 - val_NMSE_wt: 0.1052 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1278 - mse: 0.0157 - NMSE: 0.1414 - NMSE_wt: 0.1068 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.1451 - tot_time: 0h 0m 43.6s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.1278 - mse: 0.0157 - NMSE: 0.1414 - NMSE_wt: 0.1068 - covmat_fro_loss: 0.0023 - global_gradnorm: 8.9271 - val_loss: 0.1181 - val_mse: 0.0137 - val_NMSE: 0.1235 - val_NMSE_wt: 0.0970 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1074 - mse: 0.0127 - NMSE: 0.1142 - NMSE_wt: 0.0864 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.5804 - tot_time: 0h 0m 44.0s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1074 - mse: 0.0127 - NMSE: 0.1142 - NMSE_wt: 0.0864 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.7381 - val_loss: 0.0704 - val_mse: 0.0073 - val_NMSE: 0.0657 - val_NMSE_wt: 0.0493 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1139 - mse: 0.0139 - NMSE: 0.1249 - NMSE_wt: 0.0928 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.1753 - tot_time: 0h 0m 44.6s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1139 - mse: 0.0139 - NMSE: 0.1249 - NMSE_wt: 0.0928 - covmat_fro_loss: 0.0022 - global_gradnorm: 7.6743 - val_loss: 0.0720 - val_mse: 0.0076 - val_NMSE: 0.0682 - val_NMSE_wt: 0.0509 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1165 - mse: 0.0140 - NMSE: 0.1264 - NMSE_wt: 0.0954 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.0336 - tot_time: 0h 0m 45.2s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.01534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.1165 - mse: 0.0140 - NMSE: 0.1264 - NMSE_wt: 0.0954 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.2521 - val_loss: 0.0759 - val_mse: 0.0080 - val_NMSE: 0.0723 - val_NMSE_wt: 0.0548 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1192 - mse: 0.0144 - NMSE: 0.1294 - NMSE_wt: 0.0981 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.7980 - tot_time: 0h 0m 45.7s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.1192 - mse: 0.0144 - NMSE: 0.1294 - NMSE_wt: 0.0981 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.9315 - val_loss: 0.0673 - val_mse: 0.0070 - val_NMSE: 0.0631 - val_NMSE_wt: 0.0462 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1135 - mse: 0.0136 - NMSE: 0.1223 - NMSE_wt: 0.0925 - covmat_fro_loss: 0.0021 - global_gradnorm: 10.1469 - tot_time: 0h 0m 46.2s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1135 - mse: 0.0136 - NMSE: 0.1223 - NMSE_wt: 0.0925 - covmat_fro_loss: 0.0021 - global_gradnorm: 10.2417 - val_loss: 0.0425 - val_mse: 0.0031 - val_NMSE: 0.0277 - val_NMSE_wt: 0.0214 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1204 - mse: 0.0145 - NMSE: 0.1309 - NMSE_wt: 0.0993 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.1529 - tot_time: 0h 0m 46.7s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.1204 - mse: 0.0145 - NMSE: 0.1309 - NMSE_wt: 0.0993 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.3581 - val_loss: 0.1264 - val_mse: 0.0149 - val_NMSE: 0.1337 - val_NMSE_wt: 0.1054 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1279 - mse: 0.0156 - NMSE: 0.1408 - NMSE_wt: 0.1069 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.0627 - tot_time: 0h 0m 47.2s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1279 - mse: 0.0156 - NMSE: 0.1408 - NMSE_wt: 0.1069 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.7709 - val_loss: 0.1108 - val_mse: 0.0126 - val_NMSE: 0.1134 - val_NMSE_wt: 0.0898 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0989 - mse: 0.0114 - NMSE: 0.1027 - NMSE_wt: 0.0779 - covmat_fro_loss: 0.0020 - global_gradnorm: 10.7295 - tot_time: 0h 0m 47.8s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.0989 - mse: 0.0114 - NMSE: 0.1027 - NMSE_wt: 0.0779 - covmat_fro_loss: 0.0019 - global_gradnorm: 10.7595 - val_loss: 0.0609 - val_mse: 0.0058 - val_NMSE: 0.0526 - val_NMSE_wt: 0.0399 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1214 - mse: 0.0148 - NMSE: 0.1336 - NMSE_wt: 0.1003 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.0253 - tot_time: 0h 0m 48.3s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.1207 - mse: 0.0147 - NMSE: 0.1322 - NMSE_wt: 0.0996 - covmat_fro_loss: 0.0022 - global_gradnorm: 6.9678 - val_loss: 0.1393 - val_mse: 0.0169 - val_NMSE: 0.1520 - val_NMSE_wt: 0.1182 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1253 - mse: 0.0154 - NMSE: 0.1387 - NMSE_wt: 0.1042 - covmat_fro_loss: 0.0021 - global_gradnorm: 6.9482Restoring model weights from the end of the best epoch: 6.\n",
      " - tot_time: 0h 0m 48.7s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.1203 - mse: 0.0147 - NMSE: 0.1319 - NMSE_wt: 0.0993 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.8486 - val_loss: 0.0408 - val_mse: 0.0029 - val_NMSE: 0.0258 - val_NMSE_wt: 0.0198 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20: early stopping\n",
      "\n",
      "baseline : 1.5343E-02\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-07 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1408 - mse: 0.0175 - NMSE: 0.1579 - NMSE_wt: 0.1197 - covmat_fro_loss: 0.0023 - global_gradnorm: 8.6090 - tot_time: 0h 0m 49.4s\n",
      "\n",
      "Epoch 1: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.1408 - mse: 0.0175 - NMSE: 0.1579 - NMSE_wt: 0.1197 - covmat_fro_loss: 0.0024 - global_gradnorm: 8.8746 - val_loss: 0.0689 - val_mse: 0.0066 - val_NMSE: 0.0593 - val_NMSE_wt: 0.0479 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1225 - mse: 0.0148 - NMSE: 0.1333 - NMSE_wt: 0.1014 - covmat_fro_loss: 0.0022 - global_gradnorm: 7.3100 - tot_time: 0h 0m 50.0s\n",
      "\n",
      "Epoch 2: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.1225 - mse: 0.0148 - NMSE: 0.1333 - NMSE_wt: 0.1014 - covmat_fro_loss: 0.0021 - global_gradnorm: 7.4535 - val_loss: 0.0744 - val_mse: 0.0079 - val_NMSE: 0.0710 - val_NMSE_wt: 0.0534 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1139 - mse: 0.0136 - NMSE: 0.1225 - NMSE_wt: 0.0928 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.0836 - tot_time: 0h 0m 50.4s\n",
      "\n",
      "Epoch 3: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.1139 - mse: 0.0136 - NMSE: 0.1225 - NMSE_wt: 0.0928 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.7303 - val_loss: 0.0473 - val_mse: 0.0038 - val_NMSE: 0.0344 - val_NMSE_wt: 0.0263 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1012 - mse: 0.0118 - NMSE: 0.1058 - NMSE_wt: 0.0802 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.4914 - tot_time: 0h 0m 50.9s\n",
      "\n",
      "Epoch 4: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1055 - mse: 0.0124 - NMSE: 0.1114 - NMSE_wt: 0.0845 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.8266 - val_loss: 0.0855 - val_mse: 0.0089 - val_NMSE: 0.0798 - val_NMSE_wt: 0.0645 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1611 - mse: 0.0203 - NMSE: 0.1825 - NMSE_wt: 0.1400 - covmat_fro_loss: 0.0024 - global_gradnorm: 9.6801 - tot_time: 0h 0m 51.3s\n",
      "\n",
      "Epoch 5: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.1611 - mse: 0.0203 - NMSE: 0.1825 - NMSE_wt: 0.1400 - covmat_fro_loss: 0.0024 - global_gradnorm: 9.8267 - val_loss: 0.1364 - val_mse: 0.0165 - val_NMSE: 0.1487 - val_NMSE_wt: 0.1154 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1250 - mse: 0.0153 - NMSE: 0.1376 - NMSE_wt: 0.1040 - covmat_fro_loss: 0.0022 - global_gradnorm: 7.6725 - tot_time: 0h 0m 51.9s\n",
      "\n",
      "Epoch 6: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.1250 - mse: 0.0153 - NMSE: 0.1376 - NMSE_wt: 0.1040 - covmat_fro_loss: 0.0022 - global_gradnorm: 7.1132 - val_loss: 0.0379 - val_mse: 0.0025 - val_NMSE: 0.0222 - val_NMSE_wt: 0.0169 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1282 - mse: 0.0159 - NMSE: 0.1430 - NMSE_wt: 0.1072 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.8823 - tot_time: 0h 0m 52.5s\n",
      "\n",
      "Epoch 7: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.1282 - mse: 0.0159 - NMSE: 0.1430 - NMSE_wt: 0.1072 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.1176 - val_loss: 0.1208 - val_mse: 0.0143 - val_NMSE: 0.1285 - val_NMSE_wt: 0.0998 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1292 - mse: 0.0161 - NMSE: 0.1449 - NMSE_wt: 0.1082 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.7462 - tot_time: 0h 0m 53.1s\n",
      "\n",
      "Epoch 8: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.1292 - mse: 0.0161 - NMSE: 0.1449 - NMSE_wt: 0.1082 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.9966 - val_loss: 0.0671 - val_mse: 0.0069 - val_NMSE: 0.0620 - val_NMSE_wt: 0.0461 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1281 - mse: 0.0157 - NMSE: 0.1417 - NMSE_wt: 0.1071 - covmat_fro_loss: 0.0022 - global_gradnorm: 10.0714 - tot_time: 0h 0m 53.5s\n",
      "\n",
      "Epoch 9: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.1281 - mse: 0.0157 - NMSE: 0.1417 - NMSE_wt: 0.1071 - covmat_fro_loss: 0.0022 - global_gradnorm: 10.0080 - val_loss: 0.1240 - val_mse: 0.0148 - val_NMSE: 0.1334 - val_NMSE_wt: 0.1030 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1114 - mse: 0.0134 - NMSE: 0.1205 - NMSE_wt: 0.0903 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.3328 - tot_time: 0h 0m 54.0s\n",
      "\n",
      "Epoch 10: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1114 - mse: 0.0134 - NMSE: 0.1205 - NMSE_wt: 0.0903 - covmat_fro_loss: 0.0020 - global_gradnorm: 7.8371 - val_loss: 0.0944 - val_mse: 0.0105 - val_NMSE: 0.0945 - val_NMSE_wt: 0.0733 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 11/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1187 - mse: 0.0142 - NMSE: 0.1279 - NMSE_wt: 0.0977 - covmat_fro_loss: 0.0021 - global_gradnorm: 9.0725 - tot_time: 0h 0m 54.5s\n",
      "\n",
      "Epoch 11: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1294 - mse: 0.0158 - NMSE: 0.1418 - NMSE_wt: 0.1084 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.5008 - val_loss: 0.0721 - val_mse: 0.0076 - val_NMSE: 0.0683 - val_NMSE_wt: 0.0511 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1004 - mse: 0.0116 - NMSE: 0.1046 - NMSE_wt: 0.0794 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.1461 - tot_time: 0h 0m 55.1s\n",
      "\n",
      "Epoch 12: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.1004 - mse: 0.0116 - NMSE: 0.1046 - NMSE_wt: 0.0794 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.3521 - val_loss: 0.0722 - val_mse: 0.0076 - val_NMSE: 0.0685 - val_NMSE_wt: 0.0512 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 13/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1114 - mse: 0.0134 - NMSE: 0.1208 - NMSE_wt: 0.0904 - covmat_fro_loss: 0.0020 - global_gradnorm: 7.7247 - tot_time: 0h 0m 55.6s\n",
      "\n",
      "Epoch 13: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1073 - mse: 0.0128 - NMSE: 0.1152 - NMSE_wt: 0.0863 - covmat_fro_loss: 0.0020 - global_gradnorm: 8.4525 - val_loss: 0.0725 - val_mse: 0.0076 - val_NMSE: 0.0682 - val_NMSE_wt: 0.0515 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1338 - mse: 0.0166 - NMSE: 0.1494 - NMSE_wt: 0.1127 - covmat_fro_loss: 0.0023 - global_gradnorm: 7.7092 - tot_time: 0h 0m 56.1s\n",
      "\n",
      "Epoch 14: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.1338 - mse: 0.0166 - NMSE: 0.1494 - NMSE_wt: 0.1127 - covmat_fro_loss: 0.0024 - global_gradnorm: 7.6253 - val_loss: 0.0612 - val_mse: 0.0059 - val_NMSE: 0.0529 - val_NMSE_wt: 0.0401 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1612 - mse: 0.0207 - NMSE: 0.1861 - NMSE_wt: 0.1402 - covmat_fro_loss: 0.0023 - global_gradnorm: 10.4560 - tot_time: 0h 0m 56.6s\n",
      "\n",
      "Epoch 15: val_NMSE_wt did not improve from 0.01534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.1612 - mse: 0.0207 - NMSE: 0.1861 - NMSE_wt: 0.1402 - covmat_fro_loss: 0.0023 - global_gradnorm: 10.5164 - val_loss: 0.0488 - val_mse: 0.0039 - val_NMSE: 0.0353 - val_NMSE_wt: 0.0278 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 16/200\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1334 - mse: 0.0163 - NMSE: 0.1468 - NMSE_wt: 0.1123 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.8064 - tot_time: 0h 0m 57.1s\n",
      "\n",
      "Epoch 16: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1271 - mse: 0.0155 - NMSE: 0.1391 - NMSE_wt: 0.1061 - covmat_fro_loss: 0.0020 - global_gradnorm: 9.2939 - val_loss: 0.1263 - val_mse: 0.0148 - val_NMSE: 0.1334 - val_NMSE_wt: 0.1052 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1239 - mse: 0.0153 - NMSE: 0.1379 - NMSE_wt: 0.1029 - covmat_fro_loss: 0.0023 - global_gradnorm: 9.0990 - tot_time: 0h 0m 57.7s\n",
      "\n",
      "Epoch 17: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.1239 - mse: 0.0153 - NMSE: 0.1379 - NMSE_wt: 0.1029 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.3102 - val_loss: 0.1096 - val_mse: 0.0124 - val_NMSE: 0.1119 - val_NMSE_wt: 0.0885 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1375 - mse: 0.0170 - NMSE: 0.1527 - NMSE_wt: 0.1165 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.8049 - tot_time: 0h 0m 58.1s\n",
      "\n",
      "Epoch 18: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1375 - mse: 0.0170 - NMSE: 0.1527 - NMSE_wt: 0.1165 - covmat_fro_loss: 0.0022 - global_gradnorm: 9.9377 - val_loss: 0.0767 - val_mse: 0.0083 - val_NMSE: 0.0751 - val_NMSE_wt: 0.0557 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1169 - mse: 0.0141 - NMSE: 0.1267 - NMSE_wt: 0.0958 - covmat_fro_loss: 0.0022 - global_gradnorm: 8.5674 - tot_time: 0h 0m 58.7s\n",
      "\n",
      "Epoch 19: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.1169 - mse: 0.0141 - NMSE: 0.1267 - NMSE_wt: 0.0958 - covmat_fro_loss: 0.0021 - global_gradnorm: 8.8377 - val_loss: 0.1385 - val_mse: 0.0168 - val_NMSE: 0.1509 - val_NMSE_wt: 0.1175 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1573 - mse: 0.0201 - NMSE: 0.1806 - NMSE_wt: 0.1363 - covmat_fro_loss: 0.0024 - global_gradnorm: 8.9879Restoring model weights from the end of the best epoch: 6.\n",
      " - tot_time: 0h 0m 59.4s\n",
      "\n",
      "Epoch 20: val_NMSE_wt did not improve from 0.01534\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_009/checkpoints/LossHistoriesCheckpoint-20_outsteps\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.1573 - mse: 0.0201 - NMSE: 0.1806 - NMSE_wt: 0.1363 - covmat_fro_loss: 0.0024 - global_gradnorm: 9.2114 - val_loss: 0.0646 - val_mse: 0.0065 - val_NMSE: 0.0588 - val_NMSE_wt: 0.0435 - val_covmat_fro_loss: 0.0000e+00 - val_global_gradnorm: 0.0000e+00\n",
      "Epoch 20: early stopping\n",
      "\n",
      "baseline : 1.5343E-02\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0787 - mse: 0.0086 - NMSE: 0.0770 - NMSE_wt: 0.0576 - covmat_fro_loss: 0.0000e+00 - global_gradnorm: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "global_clipnorm = 30\n",
    "for kk in range(len(T_sample_output)):\n",
    "\n",
    "    num_outsteps = int((T_sample_output[kk] + 0.5*dt_rnn)//dt_rnn)\n",
    "    if type(freeze_layers) == type(None):\n",
    "        freeze_layers_thisoutstep = []\n",
    "    else:\n",
    "        if kk > len(freeze_layers) - 1:\n",
    "            freeze_layers_thisoutstep = freeze_layers[-1]\n",
    "        else:\n",
    "            freeze_layers_thisoutstep = freeze_layers[kk]\n",
    "        \n",
    "        if type(freeze_layers_thisoutstep) == type(None):\n",
    "            freeze_layers_thisoutstep = []\n",
    "\n",
    "    total_s_len = 80\n",
    "    sep_lr_s = ' num_outsteps : {} '.format(num_outsteps)\n",
    "    sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'>' + sep_lr_s\n",
    "    sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'<'\n",
    "    print('\\n\\n' + '*'*len(sep_lr_s))\n",
    "    print('' + sep_lr_s+'')\n",
    "    print('*'*len(sep_lr_s) + '\\n\\n')\n",
    "\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        if kk < len(T_sample_output) - 1:\n",
    "            temp = int((T_sample_output[kk+1] + 0.5*dt_rnn)//dt_rnn)\n",
    "        else:\n",
    "            temp = num_outsteps\n",
    "        checkfile1 = dir_name_ARrnn+'/final_net/final_net-{}_outsteps_rnn_weights.hdf5'.format(temp)\n",
    "        checkfile2 = dir_name_ARrnn+'/final_net/final_net-{}_outsteps_ae_weights.h5'.format(temp)\n",
    "        check1 = os.path.exists(checkfile1)\n",
    "        check2 = os.path.exists(checkfile2)\n",
    "        if check1 and check2:\n",
    "            # move on to checking the next time-step\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    print('clipnorm : {}, global_clipnorm : {}'.format(clipnorm, global_clipnorm))\n",
    "    \n",
    "    trainAERNN(\n",
    "        create_data_for_RNN,\n",
    "        Autoencoder,\n",
    "        AR_RNN,\n",
    "        all_data,\n",
    "        AR_AERNN,\n",
    "        dt_rnn=dt_rnn,\n",
    "        T_sample_input=T_sample_input,\n",
    "        T_sample_output=T_sample_output[kk],\n",
    "        T_offset=T_offset,\n",
    "        boundary_idx_arr=boundary_idx_arr,\n",
    "        delta_t=delta_t,\n",
    "        params=params,\n",
    "        normalize_dataset=normalize_dataset,\n",
    "        stddev_multiplier=stddev_multiplier,\n",
    "        skip_intermediate=skip_intermediate,\n",
    "        normalization_type=normalization_type,\n",
    "        normalization_constant_arr_aedata=normalization_constant_arr_aedata,\n",
    "        normalization_constant_arr_rnndata=normalization_arr_rnn,\n",
    "        learning_rate_list=learning_rate_list[kk],\n",
    "        epochs=epochs[kk],\n",
    "        patience=patience[kk],\n",
    "        loss_weights=loss_weights,\n",
    "        min_delta=min_delta,\n",
    "        lambda_reg=lambda_reg,\n",
    "        stddev_rnn=stddev,\n",
    "        stateful=False,\n",
    "        behaviour=behaviour,\n",
    "        strategy=strategy,\n",
    "        dir_name_rnn=dir_name_rnn,\n",
    "        dir_name_AR_AErnn=dir_name_ARrnn,\n",
    "        batch_size=batch_size,\n",
    "        load_file_rnn=load_file_rnn,\n",
    "        wt_file_rnn=wt_file_rnn,\n",
    "        load_file_ae=load_file_ae,\n",
    "        wt_file_ae=wt_file_ae,\n",
    "        covmat_lmda=covmat_lmda,\n",
    "        readAndReturnLossHistories=readAndReturnLossHistories,\n",
    "        mytimecallback=mytimecallback,\n",
    "        plot_losses=plot_losses,\n",
    "        SaveLosses=SaveLosses,\n",
    "        train_split=train_split,\n",
    "        test_split=test_split,\n",
    "        val_split=val_split,\n",
    "        freeze_layers=freeze_layers_thisoutstep,\n",
    "        clipnorm=clipnorm,\n",
    "        global_clipnorm=global_clipnorm,\n",
    "        ESN_flag=True,\n",
    "        rnn_kwargs=rnn_kwargs,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    load_dir = dir_name_ARrnn+'/final_net/{}_outsteps'.format(num_outsteps)\n",
    "    load_file_rnn, wt_file_rnn = find_and_return_load_wt_file_lists(\n",
    "        load_dir,\n",
    "        wt_matcher='ESN_weights.hdf5',\n",
    "        classdict_matcher='ESN_class_dict.txt'\n",
    "    )\n",
    "    \n",
    "    load_file_ae = load_dir + '/final_net-{}_outsteps_ae_class_dict.txt'.format(num_outsteps)\n",
    "    wt_file_ae = load_dir + '/final_net-{}_outsteps_ae_weights.h5'.format(num_outsteps)\n",
    "    \n",
    "    with open(load_dir+'/losses-{}_outsteps.txt'.format(num_outsteps), 'r') as fl:\n",
    "        lines = fl.readlines()\n",
    "\n",
    "    loss_dict = eval(''.join(lines))\n",
    "    train_global_gradnorm_hist = loss_dict['train_global_gradnorm_hist']\n",
    "    # lr_change = loss_dict['lr_change']\n",
    "    # trained_epochs = len(train_global_gradnorm_hist)\n",
    "    # if lr_change[-1] - lr_change[-2] == epochs[kk][-1]:\n",
    "    #     global_clipnorm = train_global_gradnorm_hist[-1]\n",
    "    # else:\n",
    "    #     global_clipnorm = train_global_gradnorm_hist[-patience[kk][-1]]\n",
    "\n",
    "    alpha1 = 0.9\n",
    "    alpha2 = 0.1\n",
    "    # global_clipnorm = train_global_gradnorm_hist[0]\n",
    "    for i in range(1, len(train_global_gradnorm_hist)):\n",
    "        train_global_gradnorm_hist[i] = alpha1*train_global_gradnorm_hist[i-1] + alpha2*train_global_gradnorm_hist[i]\n",
    "\n",
    "    idxs_to_ignore = 1\n",
    "    global_clipnorm_min = 3.0\n",
    "    global_clipnorm = np.max(train_global_gradnorm_hist[idxs_to_ignore:])\n",
    "    global_clipnorm = 0.1 * np.round(10*global_clipnorm)\n",
    "    global_clipnorm = max(global_clipnorm, global_clipnorm_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the combined AE-RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
