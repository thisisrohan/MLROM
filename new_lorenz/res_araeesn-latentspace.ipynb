{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583faddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg, fft\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from keras.engine import data_adapter\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\"\n",
    "})\n",
    "\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = np.empty_like(data)\n",
    "    new_data[:] = data[:]\n",
    "    new_data *= normalization_arr[1]\n",
    "    new_data += normalization_arr[0]\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def find_and_return_load_wt_file_lists(\n",
    "        load_dir,\n",
    "        wt_matcher='weights.hdf5',\n",
    "        classdict_matcher='class_dict.txt',\n",
    "    ):\n",
    "    contents_load_dir = [f for f in os.listdir(load_dir) if os.path.isfile(os.path.join(load_dir, f))]\n",
    "    load_files_lst = [f for f in contents_load_dir if f.endswith(classdict_matcher)]\n",
    "    wt_files_lst = [f for f in contents_load_dir if f.endswith(wt_matcher)]\n",
    "\n",
    "    load_files_lst_startingidx = []\n",
    "    for i in range(len(load_files_lst)):\n",
    "        fn = load_files_lst[i]\n",
    "        idx = fn.find('_')\n",
    "        load_files_lst_startingidx.append(int(fn[0:idx]))\n",
    "\n",
    "    wt_files_lst_startingidx = []\n",
    "    for i in range(len(wt_files_lst)):\n",
    "        fn = wt_files_lst[i]\n",
    "        idx = fn.find('_')\n",
    "        wt_files_lst_startingidx.append(int(fn[0:idx]))\n",
    "\n",
    "    load_files_sortidx = np.argsort(load_files_lst_startingidx)\n",
    "    wt_files_sortidx = np.argsort(wt_files_lst_startingidx)\n",
    "\n",
    "    load_files_lst = np.array(load_files_lst)[load_files_sortidx]\n",
    "    wt_files_lst = np.array(wt_files_lst)[wt_files_sortidx]\n",
    "\n",
    "    load_file_rnn = [load_dir + '/' + fn for fn in load_files_lst]\n",
    "    wt_file_rnn = [load_dir + '/' +  fn for fn in wt_files_lst]\n",
    "    \n",
    "    return load_file_rnn, wt_file_rnn\n",
    "\n",
    "\n",
    "def prediction_horizons(**kwargs):\n",
    "    num_outsteps = kwargs['num_outsteps']\n",
    "    dir_name_AR_AErnn = kwargs['dir_name_AR_AErnn']\n",
    "    Autoencoder = kwargs['Autoencoder']\n",
    "    # all_data = kwargs['all_data']\n",
    "    data_rnn_input = kwargs['data_rnn_input']\n",
    "    data_rnn_output = kwargs['data_rnn_output']\n",
    "    AR_RNN = kwargs['AR_RNN']\n",
    "    T_sample_input_rnn = kwargs['T_sample_input_rnn']\n",
    "    T_sample_output_rnn = kwargs['T_sample_output_rnn']\n",
    "    AR_AERNN = kwargs['AR_AERNN']\n",
    "    normalization_constant_arr_rnn = kwargs['normalization_constant_arr_rnn']\n",
    "    normalization_constant_arr_aedata = kwargs['normalization_constant_arr_aedata']\n",
    "    time_stddev_ogdata = kwargs['time_stddev_ogdata']\n",
    "    time_mean_ogdata = kwargs['time_mean_ogdata']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    num_runs = kwargs.pop('num_runs', 100)\n",
    "    error_threshold = kwargs.pop('error_threshold', 0.5)\n",
    "    rnn_data_boundary_idx_arr = kwargs['rnn_data_boundary_idx_arr']\n",
    "    lyapunov_time_arr = kwargs['lyapunov_time_arr']\n",
    "    savefig_fname = kwargs['savefig_fname']\n",
    "    data_to_consider = kwargs['data_to_consider']\n",
    "    bin_width = kwargs.pop('bin_width', 0.05)\n",
    "    bin_begin = kwargs.pop('bin_begin', 0.0)\n",
    "    density = kwargs.pop('hist_pdf_flag', True)\n",
    "    rnn_wt_extension = kwargs.pop('rnn_wt_extension', 'h5')\n",
    "    ae_load_file = kwargs.pop('ae_load_file', None)\n",
    "    ae_wt_file = kwargs.pop('ae_wt_file', None)\n",
    "    rnn_load_file = kwargs.pop('rnn_load_file', None)\n",
    "    rnn_wt_file = kwargs.pop('rnn_wt_file', None)\n",
    "    use_ae_data = kwargs.pop('use_ae_data', True)\n",
    "    xlabel_kwargs = kwargs.pop('xlabel_kwargs', {'fontsize':15})\n",
    "    ylabel_kwargs = kwargs.pop('ylabel_kwargs', {'fontsize':15})\n",
    "    title_kwargs = kwargs.pop('title_kwargs', {'fontsize':18})\n",
    "    legend_kwargs = kwargs.pop('legend_kwargs', {'fontsize':12})\n",
    "    all_data = kwargs.pop('all_data')\n",
    "    \n",
    "    if use_ae_data == True:\n",
    "        ae_net = Autoencoder(data_rnn_input.shape[-1], load_file=ae_load_file)\n",
    "        ae_net.load_weights_from_file(ae_wt_file)\n",
    "    else:\n",
    "        ae_net = None\n",
    "        normalization_constant_arr_aedata = normalization_constant_arr_rnn\n",
    "        normalization_constant_arr_rnn = None\n",
    "\n",
    "    data_rnn_input_shape = data_rnn_input.shape\n",
    "    data_rnn_input = np.reshape(data_rnn_input, (-1, data_rnn_input.shape[-1]))\n",
    "    data_rnn_input = ae_net.encoder_net(data_rnn_input, training=False)\n",
    "    data_rnn_input -= normalization_constant_arr_rnn[0, :]\n",
    "    data_rnn_input /= normalization_constant_arr_rnn[1, :]    \n",
    "    data_rnn_input = np.reshape(data_rnn_input, tuple(data_rnn_input_shape)[0:2]+(-1,))\n",
    "    \n",
    "    data_rnn_output_shape = data_rnn_output.shape\n",
    "    data_rnn_output = np.reshape(data_rnn_output, (-1, data_rnn_output.shape[-1]))\n",
    "    data_rnn_output = ae_net.encoder_net(data_rnn_output, training=False)\n",
    "    data_rnn_output = np.reshape(data_rnn_output, tuple(data_rnn_output_shape)[0:2]+(-1,))\n",
    "    \n",
    "    new_data = np.empty_like(all_data)\n",
    "    new_data[:] = all_data[:]\n",
    "    new_data -= normalization_constant_arr_aedata[0, :]\n",
    "    new_data /= normalization_constant_arr_aedata[1, :]\n",
    "    latent_states = np.array(ae_net.encoder_net(new_data, training=False))\n",
    "    time_stddev_ls = np.std(latent_states, axis=0)\n",
    "\n",
    "    data_in_og = data_rnn_input\n",
    "    data_out_og = data_rnn_output\n",
    "    num_runs = np.min([num_runs, data_in_og.shape[0]])\n",
    "    print('num_runs :', num_runs)\n",
    "\n",
    "    rnn_net = AR_RNN(\n",
    "        load_file=rnn_load_file,\n",
    "        T_input=T_sample_input_rnn,\n",
    "        T_output=T_sample_output_rnn,\n",
    "        stddev=0.0,\n",
    "        batch_size=num_runs,\n",
    "        # stateful=stateful,\n",
    "    )\n",
    "        \n",
    "    rnn_net.build(input_shape=(num_runs, data_rnn_input.shape[1], rnn_net.data_dim[0]))\n",
    "    rnn_net.load_weights_from_file(rnn_wt_file)\n",
    "    dt_rnn = rnn_net.dt_rnn\n",
    "    try:\n",
    "        dt_rnn = dt_rnn[0]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    data_idx_arr = np.linspace(0, data_in_og.shape[0]-1, num_runs, dtype=np.int32)\n",
    "\n",
    "    prediction_horizon_arr = np.empty(shape=num_runs)\n",
    "#     pod_eigvals_dataout_arr = np.empty(shape=(num_runs, data_out_og.shape[-1]))\n",
    "#     pod_eigvals_pred_arr = np.empty(shape=(num_runs, data_out_og.shape[-1]))\n",
    "#     pod_covmat_dataout = np.zeros(shape=(data_out_og.shape[-1], data_out_og.shape[-1]))\n",
    "#     pod_covmat_pred = np.zeros(shape=(data_out_og.shape[-1], data_out_og.shape[-1]))\n",
    "\n",
    "    prediction = rnn_net(data_in_og[data_idx_arr[0:num_runs], :, :], training=False).numpy()\n",
    "    prediction = invert_normalization(prediction, normalization_constant_arr_rnn)\n",
    "\n",
    "    energySpectrum_dataout = 0.0\n",
    "    energySpectrum_pred = 0.0\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        data_idx = data_idx_arr[i]\n",
    "\n",
    "        for j in range(len(rnn_data_boundary_idx_arr)):\n",
    "            if data_idx < rnn_data_boundary_idx_arr[j]:\n",
    "                case_idx = j\n",
    "                break\n",
    "        lyap_time = lyapunov_time_arr[j]\n",
    "\n",
    "        data_out = data_out_og[data_idx]\n",
    "        # data_out = invert_normalization(data_out, normalization_constant_arr_aedata)\n",
    "\n",
    "#         pod_dataout = data_out - np.mean(data_out, axis=0)\n",
    "#         pod_dataout = np.matmul(pod_dataout.transpose(), pod_dataout) / (pod_dataout.shape[0] - 1)\n",
    "#         pod_covmat_dataout += pod_dataout\n",
    "#         pod_dataout = np.abs(np.linalg.eigvals(pod_dataout))\n",
    "#         pod_dataout = np.sort(pod_dataout)\n",
    "#         pod_dataout = pod_dataout[::-1]\n",
    "#         pod_eigvals_dataout_arr[i, :] = pod_dataout\n",
    "        \n",
    "#         pod_prediction = prediction[i, :, :] - np.mean(prediction[i, :, :], axis=0)\n",
    "#         pod_prediction = np.matmul(pod_prediction.transpose(), pod_prediction) / (pod_prediction.shape[0] - 1)\n",
    "#         pod_covmat_pred += pod_prediction\n",
    "#         pod_prediction = np.abs(np.linalg.eigvals(pod_prediction))\n",
    "#         pod_prediction = np.sort(pod_prediction)\n",
    "#         pod_prediction = pod_prediction[::-1]\n",
    "#         pod_eigvals_pred_arr[i, :] = pod_prediction\n",
    "\n",
    "        ### Error and prediction horizon\n",
    "        error = (data_out - prediction[i])**2\n",
    "        error = np.mean(np.divide(error, time_stddev_ls**2), axis=1)**0.5\n",
    "\n",
    "        predhor_idx = np.where(error >= error_threshold)[0]\n",
    "        if predhor_idx.shape[0] == 0:\n",
    "            predhor_idx = error.shape[0]\n",
    "        else:\n",
    "            predhor_idx = predhor_idx[0]\n",
    "\n",
    "        prediction_horizon_arr[i] = predhor_idx*dt_rnn/lyap_time\n",
    "\n",
    "#     pod_eigvals_dataout_arr_mean = np.mean(pod_eigvals_dataout_arr, axis=0)\n",
    "#     pod_eigvals_pred_arr_mean = np.mean(pod_eigvals_pred_arr, axis=0)\n",
    "    \n",
    "#     pod_covmat_dataout /= num_runs\n",
    "#     pod_covmat_pred /= num_runs\n",
    "\n",
    "    median_idx = int(np.round(0.5*num_runs-1))\n",
    "    quartile_1_idx = int(np.round(0.25*num_runs-1))\n",
    "    quartile_3_idx = int(np.round(0.75*num_runs-1))\n",
    "\n",
    "    prediction_horizon_arr.sort()\n",
    "\n",
    "    median = prediction_horizon_arr[median_idx]\n",
    "    quartile_1 = prediction_horizon_arr[quartile_1_idx]\n",
    "    quartile_3 = prediction_horizon_arr[quartile_3_idx]\n",
    "    IQR = quartile_3 - quartile_1\n",
    "\n",
    "    prediction_horizon = np.mean(prediction_horizon_arr)\n",
    "    stddev_ph = np.std(prediction_horizon_arr)\n",
    "\n",
    "    ph_mean = np.mean(prediction_horizon_arr)\n",
    "    ph_max = np.max(prediction_horizon_arr)\n",
    "    ph_min = np.min(prediction_horizon_arr)\n",
    "    ph_stddev = np.std(prediction_horizon_arr)\n",
    "    \n",
    "    s = 'error_threshold = {}, num_runs : {}\\n'.format(error_threshold, num_runs)\n",
    "    s += 'prediction_horizon : {}, median : {}\\n'.format(prediction_horizon, median)\n",
    "    s += 'ph_min : {}, ph_max : {}\\n'.format(prediction_horizon_arr.min(), prediction_horizon_arr.max())\n",
    "    s += 'stddev : {}, IQR : {}\\n'.format(stddev_ph, IQR)\n",
    "    s += '1st quartile : {}, 3rd quartile : {}'.format(quartile_1, quartile_3)\n",
    "\n",
    "    print('\\n'+s)\n",
    "    \n",
    "    if savefig_fname != None:\n",
    "        npsavedata_fname = '/prediction_horizons-'+data_to_consider+'data--latentspace--{}outsteps'.format(num_outsteps)\n",
    "        np.savez(\n",
    "            dir_name_AR_AErnn+npsavedata_fname,\n",
    "            prediction_horizon_arr=prediction_horizon_arr,\n",
    "            error_threshold=error_threshold,\n",
    "        )\n",
    "\n",
    "        with open(dir_name_AR_AErnn+npsavedata_fname+'--latentspace--statistics.txt', 'w') as fl:\n",
    "            fl.write(s)\n",
    "#         npsavepod_fname = '/pod_eigvals-'+data_to_consider+'data--{}outsteps'.format(num_outsteps)\n",
    "#         np.savez(\n",
    "#             dir_name_AR_AErnn+npsavepod_fname,\n",
    "#             pod_eigvals_dataout_arr=pod_eigvals_dataout_arr,\n",
    "#             pod_eigvals_pred_arr=pod_eigvals_pred_arr,\n",
    "#         )\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    bin_end = bin_width*np.round((np.max(prediction_horizon_arr)+0.5*bin_width)//bin_width)\n",
    "    nbins = max(1, int(np.round(bin_end/bin_width)))\n",
    "\n",
    "    ax.hist(prediction_horizon_arr, bins=nbins, range = [bin_begin, bin_end], density=density)\n",
    "    ax.axvline(prediction_horizon, linewidth=0.9, linestyle='--', color='k')\n",
    "\n",
    "    ax.set_xlabel('Prediction Horizon (Lyapunov times)', **xlabel_kwargs)\n",
    "    ax.set_ylabel('PDF', **ylabel_kwargs)\n",
    "\n",
    "    ax.grid(True)\n",
    "    # ax.set_axisbelow(True)\n",
    "\n",
    "    ax.text(\n",
    "        0.01 + ax.transAxes.inverted().transform(ax.transData.transform([prediction_horizon, 0]))[0],\n",
    "        0.8,\n",
    "        'mean',\n",
    "        rotation=90,\n",
    "        verticalalignment='bottom',\n",
    "        horizontalalignment='left',\n",
    "        bbox=dict(facecolor=np.array([255,255,153])/255, alpha=0.6, boxstyle='square,pad=0.2'),\n",
    "        transform=ax.transAxes\n",
    "    )\n",
    "\n",
    "    text_xy = [0.95, 0.95]\n",
    "    ax.text(\n",
    "        text_xy[0],\n",
    "        text_xy[1],\n",
    "        'mean : {:.4f}\\nmedian : {:.4f}\\nmax : {:.4f}\\nmin : {:.4f}\\nstddev : {:.4f}'.format(\n",
    "            ph_mean,\n",
    "            median,\n",
    "            ph_max,\n",
    "            ph_min,\n",
    "            ph_stddev,\n",
    "        ),\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round\",\n",
    "            ec=(0.6, 0.6, 1),\n",
    "            fc=(0.9, 0.9, 1),\n",
    "            alpha=0.6,\n",
    "        ),\n",
    "        # bbox=dict(facecolor='C0', alpha=0.5, boxstyle='round,pad=0.2'),\n",
    "        horizontalalignment='right',\n",
    "        verticalalignment='top',\n",
    "        **legend_kwargs\n",
    "    )\n",
    "\n",
    "    ax.set_title('nbins = {}'.format(nbins), **title_kwargs)\n",
    "\n",
    "    if savefig_fname is not None:\n",
    "        fig.savefig(\n",
    "            dir_name_AR_AErnn+'/plots/'+savefig_fname+'--{}outsteps.pdf'.format(num_outsteps),\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')\n",
    "        fig.clear()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        print('')\n",
    "\n",
    "#     fig_eigvals, ax_eigvals = plt.subplots()\n",
    "#     ax_eigvals.semilogy(pod_eigvals_dataout_arr_mean, linestyle='--', marker='s', linewidth=0.9, markersize=2)\n",
    "#     ax_eigvals.semilogy(pod_eigvals_pred_arr_mean, linestyle='--', marker='^', linewidth=0.9, markersize=2)\n",
    "#     ax_eigvals.grid(True, which='both', axis='y')\n",
    "#     ax_eigvals.grid(True, which='major', axis='x')    \n",
    "#     ax_eigvals.legend([r'True Data', r'Predicted Data'], **legend_kwargs)\n",
    "#     ax_eigvals.set_axisbelow(True)\n",
    "#     ax_eigvals.set_title('Eigenvalues of the covariance matrix', **title_kwargs)\n",
    "#     if savefig_fname is not None:\n",
    "#         fig_eigvals.savefig(\n",
    "#             dir_name_AR_AErnn+'/plots/'+savefig_fname+'--eigvals--{}outsteps.pdf'.format(num_outsteps),\n",
    "#             dpi=300,\n",
    "#             bbox_inches='tight')\n",
    "#         fig_eigvals.clear()\n",
    "#         plt.close()\n",
    "#     else:\n",
    "#         plt.show()\n",
    "#         print('')\n",
    "\n",
    "        \n",
    "\n",
    "#     fig_covmat = plt.figure(figsize=(5.0*3, 5.0*1))\n",
    "#     subplot1 = 1\n",
    "#     subplot2 = subplot1 + 1\n",
    "    \n",
    "#     pod_covmat_dataout = np.divide(np.transpose(pod_covmat_dataout), np.diag(pod_covmat_dataout)).transpose()\n",
    "#     pod_covmat_pred = np.divide(np.transpose(pod_covmat_pred), np.diag(pod_covmat_dataout)).transpose()\n",
    "\n",
    "#     vmin_snap = 0.5\n",
    "#     vmax_snap = 0.5\n",
    "#     vmin = np.min([\n",
    "#         pod_covmat_dataout.min(),\n",
    "#         pod_covmat_pred.min()\n",
    "#     ])\n",
    "#     vmin = min(vmin, -1.0)\n",
    "#     vmin = -vmin_snap*np.round(-vmin/vmin_snap + 0.5)\n",
    "#     vmax = np.max([\n",
    "#         pod_covmat_dataout.max(),\n",
    "#         pod_covmat_pred.max()\n",
    "#     ])\n",
    "#     vmax = max(vmax, 1.0)\n",
    "#     vmax = vmax_snap*np.round(vmax/vmax_snap + 0.5)\n",
    "\n",
    "#     # plotting the original data\n",
    "#     ax_covmat_orig = fig_covmat.add_subplot(1, 3, subplot1)\n",
    "#     im_orig = ax_covmat_orig.imshow(\n",
    "#         pod_covmat_dataout,\n",
    "#         aspect='equal',\n",
    "#         origin='upper',\n",
    "#         vmin=vmin,\n",
    "#         vmax=vmax\n",
    "#     )\n",
    "#     ax_covmat_orig.set_title('Covariance Matrix (True Data)', **title_kwargs)\n",
    "#     # xticks = np.arange(0, N, int((xticks_snapto+0.5*delta_t)//delta_t))\n",
    "#     # ax_covmat_orig.set_xticks(ticks=xticks)\n",
    "#     # ax_covmat_orig.set_xticklabels(np.round(xticks*delta_t, 1))\n",
    "#     # ax_covmat_orig.tick_params(axis='x', rotation=270+45)\n",
    "#     # yticks = np.linspace(0, 1, num_yticks)*(len(xgrid)-1)\n",
    "#     # yticklabels = np.round(xgrid[0]+np.linspace(0, 1, yticks.shape[0])*(xgrid[-1]-xgrid[0]), 2)\n",
    "#     # ax_covmat_orig.set_yticks(ticks=yticks)\n",
    "#     # ax_covmat_orig.set_yticklabels(yticklabels)\n",
    "#     # ax_covmat_orig.set_xlabel(xlabel)\n",
    "#     # ax_covmat_orig.set_ylabel(ylabel)\n",
    "\n",
    "#     # plotting the predicted data\n",
    "#     ax_covmat_predict = fig_covmat.add_subplot(1, 3, subplot2, sharey=ax_covmat_orig, sharex=ax_covmat_orig)\n",
    "#     im_predict = ax_covmat_predict.imshow(\n",
    "#         pod_covmat_pred,\n",
    "#         aspect='equal',\n",
    "#         origin='upper',\n",
    "#         vmin=vmin,\n",
    "#         vmax=vmax\n",
    "#     )\n",
    "#     ax_covmat_predict.set_title('Covariance Matrix (Predicted Data)', **title_kwargs)\n",
    "#     # ax_covmat_predict.tick_params(axis='x', rotation=270+45)\n",
    "#     # ax_covmat_predict.set_xlabel(xlabel)\n",
    "#     # ax_covmat_predict.set_ylabel(ylabel)\n",
    "\n",
    "#     # subplots adjustment to account for colorbars\n",
    "#     fig_covmat.subplots_adjust(\n",
    "#         bottom=0.2,\n",
    "#         left=0.1,\n",
    "#     )\n",
    "\n",
    "#     # original data and recon data colorbar\n",
    "#     cb_xbegin = ax_covmat_orig.transData.transform([0, 0])\n",
    "#     cb_xbegin = fig_covmat.transFigure.inverted().transform(cb_xbegin)[0]\n",
    "#     cb_xend = ax_covmat_predict.transData.transform([pod_covmat_dataout.shape[-1], 0])\n",
    "#     cb_xend = fig_covmat.transFigure.inverted().transform(cb_xend)[0]\n",
    "\n",
    "#     cb_ax = fig_covmat.add_axes([cb_xbegin, 0.0, cb_xend-cb_xbegin, 0.025])\n",
    "#     cbar = fig_covmat.colorbar(im_predict, cax=cb_ax, orientation='horizontal')\n",
    "\n",
    "#     # computing the normalized error\n",
    "#     subplot3 = subplot2+1\n",
    "#     error = np.abs(pod_covmat_pred-pod_covmat_dataout)\n",
    "#     vmax_error_snap = 0.8\n",
    "#     vmax_error = np.max(error)\n",
    "#     vmax_error = vmax_error_snap*np.round(vmax_error/vmax_error_snap + 0.5)\n",
    "#     # error = 100*error / np.abs(pod_covmat_dataout)\n",
    "#     # plotting the normalized error\n",
    "#     ax_covmat_error = fig_covmat.add_subplot(1, 3, subplot3, sharey=ax_covmat_orig, sharex=ax_covmat_orig)\n",
    "#     im_error = ax_covmat_error.imshow(\n",
    "#         error,\n",
    "#         aspect='equal',\n",
    "#         origin='upper',\n",
    "#         vmin=0.0,\n",
    "#         vmax=vmax_error,\n",
    "#     )\n",
    "#     ax_covmat_error.set_title(r'Error', **title_kwargs)\n",
    "#     # ax_error.tick_params(axis='x', rotation=270+45)\n",
    "#     # ax_error.set_xlabel(xlabel)\n",
    "#     # ax_error.set_ylabel(ylabel)\n",
    "\n",
    "#     # error colorbar\n",
    "#     cbe_xbegin = ax_covmat_error.transData.transform([0, 0])\n",
    "#     cbe_xbegin = fig_covmat.transFigure.inverted().transform(cbe_xbegin)[0]\n",
    "#     cbe_xend = ax_covmat_error.transData.transform([pod_covmat_dataout.shape[-1], 0])\n",
    "#     cbe_xend = fig_covmat.transFigure.inverted().transform(cbe_xend)[0]\n",
    "#     error_cb_ax = fig_covmat.add_axes([cbe_xbegin, 0.0, cbe_xend-cbe_xbegin, 0.025])\n",
    "#     cbar_error = fig_covmat.colorbar(im_error, cax=error_cb_ax, orientation='horizontal')\n",
    "    \n",
    "#     if savefig_fname is not None:\n",
    "#         fig_covmat.savefig(\n",
    "#             dir_name_AR_AErnn+'/plots/'+savefig_fname+'--covmat--{}outsteps.pdf'.format(num_outsteps),\n",
    "#             dpi=300,\n",
    "#             bbox_inches='tight')\n",
    "#         fig_covmat.clear()\n",
    "#         plt.close()\n",
    "#     else:\n",
    "#         plt.show()\n",
    "#         print('')\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "from numpy import *\n",
    "def main(esn_dir_idx, gpu_to_use):\n",
    "    strategy = None\n",
    "\n",
    "    current_sys = platform.system()\n",
    "    if current_sys == 'Windows':\n",
    "        dir_sep = '\\\\'\n",
    "    else:\n",
    "        dir_sep = '/'\n",
    "\n",
    "    print(os.getcwd())\n",
    "\n",
    "    from tools.misc_tools import create_data_for_RNN, readAndReturnLossHistories\n",
    "    from tools.ae_v2 import Autoencoder\n",
    "    from tools.ESN_v1_ensembleAR import ESN_ensemble as AR_RNN\n",
    "    from tools.AEESN_AR_v1 import AR_AERNN_ESN as AR_AERNN\n",
    "\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(gpus)\n",
    "\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            # gpu_to_use = 0\n",
    "            # tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "            tf.config.set_visible_devices([], 'GPU') # no gpu used\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)\n",
    "\n",
    "    print(tf.config.list_physical_devices())\n",
    "    print('')\n",
    "    print(tf.config.list_logical_devices())\n",
    "    print('')\n",
    "    print(tf.__version__)\n",
    "\n",
    "\n",
    "    ###--- CDV System ---###\n",
    "    ### setting up params (and saving, if applicable)\n",
    "\n",
    "    ### AR AE-RNN directory\n",
    "    esn_dir_idx = '{:3d}'.format(esn_dir_idx)\n",
    "    esn_dir_idx = esn_dir_idx.replace(' ', '0')\n",
    "    dir_name_AR_AErnn = os.getcwd()+'/saved_AR_AEESN_rnn/AR_ESN_ensemble_' + esn_dir_idx\n",
    "\n",
    "    ### reading AR-RNN parameters\n",
    "    with open(dir_name_AR_AErnn + '/AR_RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_AR_AErnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dir_name_rnn = params_AR_AErnn_dict['dir_name_rnn']\n",
    "    idx1 = dir_name_rnn[::-1].find('/')\n",
    "    idx2 = dir_name_rnn[:-idx1-1][::-1].find('/')\n",
    "    dir_name_rnn = os.getcwd() + dir_name_rnn[-idx1-idx2-1-1:]\n",
    "\n",
    "    dir_name_ae = params_AR_AErnn_dict['dir_name_ae']\n",
    "    ae_idx = dir_name_ae[-3:]\n",
    "    dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "\n",
    "    dt_rnn = params_AR_AErnn_dict['dt_rnn']\n",
    "    # T_sample_input = params_AR_AErnn_dict['T_sample_input']\n",
    "    T_sample_output = params_AR_AErnn_dict['T_sample_output']\n",
    "    if type(T_sample_output) != type(np.array([])):\n",
    "        if type(T_sample_output) != type([]):\n",
    "            T_sample_output = [T_sample_output]\n",
    "        T_sample_output = np.array(T_sample_output)\n",
    "    num_outsteps = np.int32(np.round(T_sample_output/dt_rnn))\n",
    "    # T_offset = params_AR_AErnn_dict['T_offset']\n",
    "    return_params_arr = params_AR_AErnn_dict['return_params_arr']\n",
    "    params = params_AR_AErnn_dict['params']\n",
    "    try:\n",
    "        # this is the normalization flag for the data fed into the rnn\n",
    "        normalize_dataset = params_AR_AErnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in AR_rnn_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        use_ae_data = params_AR_AErnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in AR_rnn_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "\n",
    "    ### reading RNN normalization constants\n",
    "    normalization_arr_rnn = None\n",
    "    # if normalize_dataset == True:\n",
    "    #     with open(dir_name_AR_AErnn + '/final_net/rnn_normalization.txt') as f:\n",
    "    #         lines = f.readlines()\n",
    "    #     normarr_rnn_dict = eval(''.join(lines))\n",
    "    #     normalization_arr_rnn = normarr_rnn_dict['normalization_arr']\n",
    "\n",
    "    if os.path.exists(dir_name_AR_AErnn+'/normalization_data.npz'):\n",
    "        with np.load(dir_name_AR_AErnn+'/normalization_data.npz', allow_pickle=True) as fl:\n",
    "            normalization_arr_rnn = fl['normalization_arr'][0]\n",
    "\n",
    "    ### training params\n",
    "    with open(dir_name_AR_AErnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "\n",
    "    ### reading simulation parameters\n",
    "    with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    params_dict = eval(''.join(lines))\n",
    "    data_dir_idx = params_dict['data_dir_idx']\n",
    "    normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "    normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "    try:\n",
    "        ae_data_with_params = params_dict['ae_data_with_params']\n",
    "    except:\n",
    "        print(\"'ae_data_with_params' not present in ae_data, set to 'True'.\")\n",
    "        ae_data_with_params = True\n",
    "\n",
    "    print('dir_name_AR_AErnn:', dir_name_AR_AErnn)\n",
    "    print('dir_name_rnn:', dir_name_rnn)\n",
    "    print('dir_name_ae:', dir_name_ae)\n",
    "    print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "    ### loading data\n",
    "    dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "        \n",
    "    with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    params_dict = eval(''.join(lines))\n",
    "    params_mat = params_dict['params_mat']\n",
    "    # init_state = params_dict['init_state']\n",
    "    t0 = params_dict['t0']\n",
    "    T = params_dict['T']\n",
    "    delta_t = params_dict['delta_t']\n",
    "    return_params_arr = params_dict['return_params_arr']\n",
    "    normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "    print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "    alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "    with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "        all_data = fl['all_data'].astype(FTYPE)\n",
    "        boundary_idx_arr = fl['boundary_idx_arr']\n",
    "        normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "        initial_t0 = fl['initial_t0']\n",
    "        init_state_mat = fl['init_state_mat']\n",
    "\n",
    "        lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']\n",
    "\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # setting seed for PRNGs\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)\n",
    "\n",
    "    # set which data to use for plotting histogram\n",
    "    data_to_consider = 'training' # could be 'all', 'testing', 'training', 'val'\n",
    "\n",
    "    lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "    for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "        lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "        print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))\n",
    "\n",
    "    # delaing with normalizing the data before feeding into autoencoder\n",
    "    num_params = params_mat.shape[1]\n",
    "    og_vars = all_data.shape[1]\n",
    "    if alldata_withparams_flag == True:\n",
    "        og_vars -= num_params\n",
    "\n",
    "    if use_ae_data == True and ae_data_with_params == False:\n",
    "        all_data = all_data[:, 0:og_vars]\n",
    "    else:\n",
    "        all_data = all_data[:, 0:og_vars]\n",
    "        \n",
    "    normalization_constant_arr_aedata = normalization_constant_arr_aedata[:, 0:all_data.shape[1]]\n",
    "\n",
    "    print('all_data.shape : ', all_data.shape)\n",
    "    print('all_data.dtype : ', all_data.dtype)\n",
    "\n",
    "    ###--- Create Data ---###\n",
    "    T_sample_input_cd_AR = 1\n",
    "    T_sample_output_cd_AR = 10\n",
    "    T_sample_input_cd = T_sample_input_cd_AR*np.mean(lyapunov_time_arr)#50.1*dt_rnn\n",
    "    T_sample_output_cd = T_sample_output_cd_AR*np.mean(lyapunov_time_arr)\n",
    "    T_offset_cd = T_sample_input_cd\n",
    "\n",
    "    skip_intermediate_cd = T_sample_input_cd_AR/(T_sample_input_cd_AR+T_sample_output_cd_AR)\n",
    "\n",
    "    time_mean_ogdata = np.mean(all_data, axis=0)\n",
    "    time_stddev_ogdata = np.std(all_data, axis=0)\n",
    "\n",
    "    rnn_res_dict = create_data_for_RNN(\n",
    "        all_data,\n",
    "        dt_rnn,\n",
    "        T_sample_input_cd,\n",
    "        T_sample_output_cd,\n",
    "        T_offset_cd,\n",
    "        None,\n",
    "        boundary_idx_arr,\n",
    "        delta_t,\n",
    "        params=params,\n",
    "        return_numsamples=True,\n",
    "        normalize_dataset=normalizeforae_flag,\n",
    "        stddev_multiplier=3.0,\n",
    "        skip_intermediate=skip_intermediate_cd,\n",
    "        return_OrgDataIdxArr=False,\n",
    "        normalization_arr_external=normalization_constant_arr_aedata,\n",
    "        normalization_type='stddev')\n",
    "\n",
    "    data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "    data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "    org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "    org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "    num_samples = rnn_res_dict['num_samples']\n",
    "    normalization_arr = rnn_res_dict['normalization_arr']\n",
    "    rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']\n",
    "\n",
    "    temp = np.divide(all_data-normalization_arr[0], normalization_arr[1])\n",
    "    time_stddev = np.std(temp, axis=0)\n",
    "    timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "    del(org_data_idx_arr_input)\n",
    "    del(org_data_idx_arr_output)\n",
    "    del(temp)\n",
    "\n",
    "    if data_to_consider != 'all':\n",
    "        cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "        num_train = 0\n",
    "        num_val = 0\n",
    "        begin_idx = 0\n",
    "        for i in range(len(boundary_idx_arr)):\n",
    "            num_samples = rnn_data_boundary_idx_arr[i] - begin_idx\n",
    "            num_train += int( np.round(train_split*num_samples) )\n",
    "            num_val += int( np.round(val_split*num_samples) )\n",
    "            begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "        # defining shapes\n",
    "        training_input_shape = [num_train]\n",
    "        training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "        training_output_shape = [num_train]\n",
    "        training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "        val_input_shape = [num_val]\n",
    "        val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "        val_output_shape = [num_train]\n",
    "        val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "        testing_input_shape = [cum_samples-num_train-num_val]\n",
    "        testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "        testing_output_shape = [cum_samples-num_train-num_val]\n",
    "        testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "        shape_to_use = eval(data_to_consider+'_input_shape')\n",
    "        rnn_data_idx = np.empty(shape=shape_to_use[0], dtype=np.int32)\n",
    "        \n",
    "        begin_idx = 0\n",
    "        training_data_rolling_count = 0\n",
    "        val_data_rolling_count = 0\n",
    "        testing_data_rolling_count = 0\n",
    "        for i in range(len(boundary_idx_arr)):\n",
    "            num_samples = rnn_data_boundary_idx_arr[i] - begin_idx\n",
    "            num_train = int( np.round(train_split*num_samples) )\n",
    "            num_val = int( np.round(val_split*num_samples) )\n",
    "            num_test = num_samples-num_train-num_val+1\n",
    "\n",
    "            if data_to_consider == 'training':\n",
    "                rnn_data_idx[training_data_rolling_count:training_data_rolling_count+num_train] = np.arange(begin_idx, begin_idx+num_train)\n",
    "            elif data_to_consider == 'val':\n",
    "                rnn_data_idx[val_data_rolling_count:val_data_rolling_count+num_val] = np.arange(begin_idx+num_train, begin_idx+num_train+num_val)\n",
    "            elif data_to_consider == 'testing':\n",
    "                rnn_data_idx[testing_data_rolling_count:testing_data_rolling_count+num_test] = np.arange(begin_idx+num_train+num_val, rnn_data_boundary_idx_arr[i])\n",
    "\n",
    "            training_data_rolling_count += num_train\n",
    "            val_data_rolling_count += num_val\n",
    "            testing_data_rolling_count += num_test\n",
    "\n",
    "            begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "        # shuffling\n",
    "        np.random.shuffle(rnn_data_idx)\n",
    "        data_rnn_input = data_rnn_input[rnn_data_idx]\n",
    "        data_rnn_output = data_rnn_output[rnn_data_idx]\n",
    "        del(rnn_data_idx)\n",
    "\n",
    "    print(' data_rnn_input.shape :', data_rnn_input.shape)\n",
    "    print('data_rnn_output.shape :', data_rnn_output.shape)\n",
    "\n",
    "    ###--- Prediction Horizon ---###\n",
    "\n",
    "    num_runs = 100\n",
    "    num_runs = min(num_runs, data_rnn_input.shape[0])\n",
    "\n",
    "    \n",
    "    return_lst = [\n",
    "        [\n",
    "            dir_name_AR_AErnn, data_rnn_input, data_rnn_output,\n",
    "            normalization_constant_arr_aedata, time_stddev_ogdata,\n",
    "            num_outsteps, lyapunov_time_arr, dt_rnn\n",
    "        ]\n",
    "    ]\n",
    "    for kk in range(num_outsteps.shape[0]+1):    \n",
    "        total_s_len = 80\n",
    "        \n",
    "        if kk == 0:\n",
    "            num_outsteps_kk = 'ZERO'\n",
    "            load_dir = dir_name_rnn + '/final_net/'\n",
    "            load_file_rnn, wt_file_rnn = find_and_return_load_wt_file_lists(load_dir)\n",
    "\n",
    "            load_file_ae = dir_name_ae+'/final_net/final_net_class_dict.txt'\n",
    "            wt_file_ae = dir_name_ae+'/final_net/final_net_ae_weights.h5'\n",
    "        else:\n",
    "            num_outsteps_kk = num_outsteps[kk-1]\n",
    "            \n",
    "            load_dir = dir_name_AR_AErnn + '/final_net/{}_outsteps'.format(num_outsteps_kk)\n",
    "            load_file_rnn, wt_file_rnn = find_and_return_load_wt_file_lists(\n",
    "                load_dir,\n",
    "                wt_matcher='ESN_weights.hdf5',\n",
    "                classdict_matcher='ESN_class_dict.txt',\n",
    "            )\n",
    "\n",
    "            load_file_ae = dir_name_AR_AErnn+'/final_net/{}_outsteps/final_net-{}_outsteps_ae_class_dict.txt'.format(\n",
    "                num_outsteps_kk, num_outsteps_kk)\n",
    "            wt_file_ae = dir_name_AR_AErnn+'/final_net/{}_outsteps/final_net-{}_outsteps_ae_weights.h5'.format(\n",
    "                num_outsteps_kk, num_outsteps_kk)    \n",
    "        \n",
    "        sep_lr_s = ' num_outsteps : {} '.format(num_outsteps_kk)\n",
    "        \n",
    "        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'>' + sep_lr_s\n",
    "        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'<'\n",
    "        print('\\n\\n' + '*'*len(sep_lr_s))\n",
    "        print('' + sep_lr_s+'')\n",
    "        print('*'*len(sep_lr_s) + '\\n\\n')\n",
    "\n",
    "        preds = prediction_horizons(\n",
    "            num_outsteps=num_outsteps_kk,\n",
    "            dir_name_AR_AErnn=dir_name_AR_AErnn,\n",
    "            Autoencoder=Autoencoder,\n",
    "            data_rnn_input=data_rnn_input,\n",
    "            data_rnn_output=data_rnn_output,\n",
    "            AR_RNN=AR_RNN,\n",
    "            T_sample_input_rnn=T_sample_input_cd,\n",
    "            T_sample_output_rnn=T_sample_output_cd,\n",
    "            AR_AERNN=AR_AERNN,\n",
    "            normalization_constant_arr_rnn=normalization_arr_rnn,\n",
    "            normalization_constant_arr_aedata=normalization_constant_arr_aedata,\n",
    "            time_stddev_ogdata=time_stddev_ogdata,\n",
    "            time_mean_ogdata=time_mean_ogdata,\n",
    "            batch_size=num_runs,\n",
    "            num_runs=num_runs,\n",
    "            error_threshold=0.5,\n",
    "            rnn_data_boundary_idx_arr=rnn_data_boundary_idx_arr,\n",
    "            lyapunov_time_arr=lyapunov_time_arr,\n",
    "            savefig_fname='post-ARtraining'+'_'+data_to_consider+'data--latentspace',\n",
    "            data_to_consider=data_to_consider,\n",
    "            bin_width=0.05,\n",
    "            bin_begin=0.0,\n",
    "            rnn_wt_extension='hdf5', # 'h5' for tf saved rnns, 'hdf5' for my ESNs\n",
    "            rnn_load_file=load_file_rnn,\n",
    "            rnn_wt_file=wt_file_rnn,\n",
    "            ae_load_file=load_file_ae,\n",
    "            ae_wt_file=wt_file_ae,\n",
    "            use_ae_data=use_ae_data,\n",
    "            all_data=all_data,\n",
    "        )\n",
    "        \n",
    "        return_lst.append(preds)\n",
    "    \n",
    "    return return_lst\n",
    "        \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('gpu_to_use', type=int)\n",
    "#     parser.add_argument('esn_dir_idx', type=int)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     print('gpu_to_use : {}, esn_dir_idx : {}'.format(args.gpu_to_use, args.esn_dir_idx))\n",
    "    \n",
    "#     main(args.esn_dir_idx, args.gpu_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c25bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_kwargs={\"fontsize\":15}\n",
    "ylabel_kwargs={\"fontsize\":15}\n",
    "title_kwargs={\"fontsize\":17}\n",
    "legend_kwargs={\"fontsize\":12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9922f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/new_lorenz\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "\n",
      "2.8.1\n",
      "dir_name_AR_AErnn: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_AR_AEESN_rnn/AR_ESN_ensemble_005\n",
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_ESN_ensemble/ESN_ensemble_002\n",
      "dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_ae/ae_024\n",
      "data_dir_idx: 010\n",
      "normalize_flag_ogdata: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 00:46:57.967902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 00:46:58.030361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 00:46:58.030647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 00:46:58.031765: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case : 1, lyapunov exponent : 0.9058021372262592, lyapunov time : 1.1039938926696777s\n",
      "all_data.shape :  (4200001, 3)\n",
      "all_data.dtype :  float32\n",
      " data_rnn_input.shape : (30537, 11, 3)\n",
      "data_rnn_output.shape : (30537, 110, 3)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : ZERO <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "num_runs : 100\n",
      "\n",
      "error_threshold = 0.5, num_runs : 100\n",
      "prediction_horizon : 3.1848002270172078, median : 2.989146970749939\n",
      "ph_min : 0.3623208449393865, ph_max : 8.061638799901349\n",
      "stddev : 1.4611640966022716, IQR : 1.9927646471666254\n",
      "1st quartile : 2.1739250696363195, 3rd quartile : 4.166689716802945\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 5 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "num_runs : 100\n",
      "\n",
      "error_threshold = 0.5, num_runs : 100\n",
      "prediction_horizon : 1.5308055698689083, median : 1.3587031685226993\n",
      "ph_min : 0.18116042246969324, ph_max : 5.525392885325645\n",
      "stddev : 0.9900628411915261, IQR : 1.1775427460530064\n",
      "1st quartile : 0.8152219011136196, 3rd quartile : 1.9927646471666258\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 10 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "num_runs : 100\n",
      "\n",
      "error_threshold = 0.5, num_runs : 100\n",
      "prediction_horizon : 1.6032697388567854, median : 1.449283379757546\n",
      "ph_min : 0.27174063370453994, ph_max : 4.800751195446872\n",
      "stddev : 0.9978223318220804, IQR : 1.1775427460530064\n",
      "1st quartile : 0.9058021123484662, 3rd quartile : 2.0833448584014724\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 15 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "num_runs : 100\n",
      "\n",
      "error_threshold = 0.5, num_runs : 100\n",
      "prediction_horizon : 1.617762572654361, median : 1.449283379757546\n",
      "ph_min : 0.27174063370453994, ph_max : 4.891331406681718\n",
      "stddev : 1.0376887342805678, IQR : 1.2681229572878534\n",
      "1st quartile : 0.9058021123484662, 3rd quartile : 2.1739250696363195\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>> num_outsteps : 20 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Autoencoder.call at 0x7fdd487b7370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "num_runs : 100\n",
      "\n",
      "error_threshold = 0.5, num_runs : 100\n",
      "prediction_horizon : 1.5987407282950432, median : 1.449283379757546\n",
      "ph_min : 0.27174063370453994, ph_max : 4.800751195446872\n",
      "stddev : 0.9635942284837884, IQR : 1.2681229572878534\n",
      "1st quartile : 0.9058021123484662, 3rd quartile : 2.1739250696363195\n"
     ]
    }
   ],
   "source": [
    "main_tuple = main(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feace9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_tuple), len(main_tuple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a4e8d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (141193013.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    \\=\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "\\="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b064c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dir_name_AR_AErnn = main_tuple[0][0]\n",
    "data_rnn_input = main_tuple[0][1]\n",
    "data_rnn_output = main_tuple[0][2]\n",
    "normalization_constant_arr_aedata = main_tuple[0][3]\n",
    "time_stddev_ogdata = main_tuple[0][4]\n",
    "num_outsteps = main_tuple[0][5]\n",
    "lyapunov_time_arr = main_tuple[0][6]\n",
    "dt_rnn = main_tuple[0][7]\n",
    "\n",
    "lyap_time = np.mean(lyapunov_time_arr)\n",
    "data_idx_arr = np.linspace(0, data_rnn_input.shape[0]-1, main_tuple[1].shape[0], dtype=np.int32)\n",
    "AR_testing_data_rnn_input = data_rnn_input[data_idx_arr]\n",
    "AR_testing_data_rnn_output = data_rnn_output[data_idx_arr]\n",
    "\n",
    "d_idx = np.random.randint(low=0, high=AR_testing_data_rnn_input.shape[0])\n",
    "\n",
    "for i_mt in range(1, len(main_tuple)):\n",
    "    prediction = main_tuple[i_mt]\n",
    "    # d_idx = np.random.randint(low=0, high=AR_testing_data_rnn_input.shape[0])\n",
    "    # d_idx = 7 # 1, 7, 11, 86, 99\n",
    "    print('d_idx : {}'.format(d_idx))\n",
    "\n",
    "    num_sample_input_AR = prediction.shape[-2]\n",
    "    num_sample_input_AR = AR_testing_data_rnn_input.shape[-2]\n",
    "\n",
    "    n = 1\n",
    "    num_latent_states = prediction.shape[-1]\n",
    "    N = prediction.shape[-2]#num_output_timesteps\n",
    "\n",
    "    num_cols = 1\n",
    "    num_rows = n*num_latent_states+1\n",
    "\n",
    "    ax_ylabels = [r'$x_{'+str(i+1)+'}$' for i in range(num_latent_states)]\n",
    "\n",
    "    fig, ax = plt.subplots(num_latent_states+1, 1, sharex=True, figsize=np.array([7.5*num_cols, 2.5*num_rows])*0.75)\n",
    "    if num_latent_states == 1:\n",
    "        ax = [ax]\n",
    "\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 2*n)]\n",
    "\n",
    "    prev_idx = 0\n",
    "\n",
    "    time_arr_warmup1 = (np.arange(num_sample_input_AR+1) - num_sample_input_AR)*dt_rnn\n",
    "    # time_arr_warmup2 = time_arr_warmup1 + dt_rnn\n",
    "\n",
    "    time_arr_warmup1 /= lyap_time\n",
    "    # time_arr_warmup2 /= lyap_time\n",
    "\n",
    "    input_time = time_arr_warmup1[-1]+np.arange(N, dtype=float)*dt_rnn/lyap_time\n",
    "\n",
    "    din_shape = np.array(AR_testing_data_rnn_input[d_idx].shape)\n",
    "    din_shape[0] += 1\n",
    "    data_in_plot = np.empty(shape=din_shape)\n",
    "    data_in_plot[:-1] = AR_testing_data_rnn_input[d_idx]\n",
    "    data_in_plot[-1] = AR_testing_data_rnn_output[d_idx, 0]\n",
    "    data_out_plot = np.empty_like(AR_testing_data_rnn_output[d_idx])\n",
    "    data_out_plot[:] = AR_testing_data_rnn_output[d_idx]\n",
    "\n",
    "    data_in_plot = invert_normalization(data_in_plot, normalization_constant_arr_aedata)\n",
    "    data_out_plot = invert_normalization(data_out_plot, normalization_constant_arr_aedata)\n",
    "\n",
    "\n",
    "    for j in range(num_latent_states):\n",
    "        for i in range(n):\n",
    "            ax[j].plot(time_arr_warmup1, data_in_plot[:, j], linestyle='--', linewidth=1, color=colors[2*i], label='__nolabel__')#'actual warmup data')#.format(i+1))\n",
    "            # ax[j].plot(time_arr_warmup2, input_preds[:, j], linestyle='--', linewidth=1, color=colors[2*i+1], label='Case {} - predicted warmup data'.format(i+1))\n",
    "            ax[j].plot(input_time, data_out_plot[:, j], linewidth=1, color=colors[2*i], label='True data')# .format(i+1))\n",
    "            ax[j].plot(input_time, prediction[d_idx, :, j], linewidth=1, color=colors[2*i+1], label='Predicted data')#.format(i+1))\n",
    "        ax[j].set_ylabel(ax_ylabels[j], **ylabel_kwargs)\n",
    "        ax[j].grid(True)\n",
    "        ax[j].set_axisbelow(True)\n",
    "        # ax[j].set_ylim([-1, 1])\n",
    "        ax[j].set_xlim([input_time[0] - 0.5, input_time[-1]])\n",
    "\n",
    "\n",
    "    ax[-2].set_xlabel('Time$^+$', **xlabel_kwargs)\n",
    "\n",
    "    max_rows = 10\n",
    "    max_rows = float(max_rows)\n",
    "    # ncols = int(np.ceil(len(boundary_idx_arr) / max_rows))\n",
    "    ncols = int(np.ceil(1 / max_rows))\n",
    "    ax[0].legend(\n",
    "        loc=1,\n",
    "        ncol=ncols,\n",
    "        **legend_kwargs,\n",
    "    )\n",
    "    ax[0].set_title(r\"Lorenz $'63$ (ESN)\", **title_kwargs)\n",
    "\n",
    "\n",
    "    ### error computation\n",
    "    error = (data_out_plot - prediction[d_idx])**2\n",
    "    # error /= norm_sq_time_average(data_out)**0.5\n",
    "    error = np.mean(np.divide(error, time_stddev_ogdata**2), axis=1)**0.5\n",
    "\n",
    "    # print(norm_sq_time_average(data_out)**0.5)\n",
    "\n",
    "    # fig2, ax2 = plt.subplots(1, 1, figsize=(7.5, 2.5))\n",
    "    ax2 = ax[-1]\n",
    "    ax2.plot(input_time, error)\n",
    "    ax2.grid(True)\n",
    "    ax2.set_axisbelow(True)\n",
    "    ax2.set_xlabel('Time$^+$', **xlabel_kwargs)\n",
    "    ax2.set_ylabel('NRMSE', **ylabel_kwargs)\n",
    "\n",
    "    error_threshold = 0.5\n",
    "\n",
    "    predhor_idx = np.where(error >= error_threshold)[0]\n",
    "    if len(predhor_idx.shape) == 0 or predhor_idx.shape[0] == 0:\n",
    "        predhor_idx = error.shape[0]-1\n",
    "    else:\n",
    "        predhor_idx = np.sort(predhor_idx)[0]\n",
    "    ax2.plot(input_time[predhor_idx], error[predhor_idx], 'o', color='k')\n",
    "    ax2.axhline(error[predhor_idx], linewidth=0.9, linestyle='--', color='k')\n",
    "    # ax2.plot(input_time[predhor_idx], error_threshold, 'o', color='k')\n",
    "    # ax2.axhline(error_threshold, linewidth=0.9, linestyle='--', color='k')\n",
    "\n",
    "    for i in range(len(ax)):\n",
    "        ax[i].axvline(input_time[predhor_idx], linewidth=0.9, linestyle='--', color='k')\n",
    "    ax2.set_ylim([0, 2])\n",
    "\n",
    "    prediction_horizon = predhor_idx*dt_rnn/lyap_time\n",
    "    print(prediction_horizon)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_idx = '{:04d}'.format(num_outsteps[i_mt-2]) if i_mt >= 2 else 'ZERO'\n",
    "    plt.savefig(dir_name_AR_AErnn+'/plots/time_prediction--{}outsteps.pdf'.format(plot_idx), dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    np.savez(\n",
    "        dir_name_AR_AErnn+'/plots/time_prediction--{}outsteps'.format(plot_idx),\n",
    "        data_in_plot=data_in_plot,\n",
    "        data_out_plot=data_out_plot,\n",
    "        prediction=prediction[d_idx],\n",
    "        error=error,\n",
    "        normalization_constant_arr_aedata=normalization_constant_arr_aedata,\n",
    "        dt_rnn=dt_rnn,\n",
    "        lyapunov_time_arr=lyapunov_time_arr,\n",
    "        num_outsteps=[plot_idx],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3eb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac25571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe97ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ab3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9339f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
