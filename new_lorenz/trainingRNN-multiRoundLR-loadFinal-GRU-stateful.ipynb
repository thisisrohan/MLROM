{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "array = np.array\n",
    "float32 = np.float32\n",
    "int32 = np.int32\n",
    "float64 = np.float64\n",
    "int64 = np.int64\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/new_lorenz\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, readAndReturnLossHistories, plot_histogram_and_save\n",
    "\n",
    "from tools.ae_v2 import Autoencoder\n",
    "\n",
    "from tools.GRU_SingleStep_v1 import RNN_GRU as RNN_SingleStep\n",
    "# from tools.LSTM_SingleStep_v1 import RNN_LSTM as RNN_SingleStep\n",
    "# from tools.SimpleRNN_SingleStep_v1 import RNN_SimpleRNN as RNN_SingleStep\n",
    "\n",
    "from tools.GRU_AR_v1 import AR_RNN_GRU as AR_RNN\n",
    "# from tools.LSTM_AR_v1 import AR_RNN_LSTM as AR_RNN\n",
    "# from tools.SimpleRNN_AR_v1 import AR_RNN_SimpleRNN as AR_RNN\n",
    "\n",
    "from tools.AEGRU_AR_v1 import AR_AERNN_GRU as AR_AERNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": [
    "# setting seed for PRNGs\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    prng_seed = 42\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 07:43:53.473023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.518930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.519143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.520143: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-14 07:43:53.520691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.520854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.520999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.902900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.903069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.903217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-14 07:43:53.903330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23656 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 0\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "2.8.1\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print('')\n",
    "print(tf.config.list_logical_devices())\n",
    "print('')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008\n",
      "dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_ae/ae_024\n",
      "data_dir_idx: 010\n",
      "normalize_flag_ogdata: False\n"
     ]
    }
   ],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # making RNN save directory\n",
    "    dir_name_rnn = os.getcwd() + dir_sep + 'saved_rnn'\n",
    "    if not os.path.isdir(dir_name_rnn):\n",
    "        os.makedirs(dir_name_rnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'rnn_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_rnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_rnn = dir_name_rnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_rnn)\n",
    "    os.makedirs(dir_name_rnn+dir_sep+'plots')\n",
    "\n",
    "    # whether to use AE data or just work on raw data\n",
    "    use_ae_data = True # if false, specifying ae_idx will only show which dataset to use\n",
    "\n",
    "    # autoencoder directory\n",
    "    ae_idx = '024'\n",
    "    dir_name_ae = os.getcwd()+'{ds}saved_ae{ds}ae_'.format(ds=dir_sep)+ae_idx\n",
    "else:\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_rnn/rnn_015'\n",
    "\n",
    "    # reading AE directory\n",
    "    with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_dict = eval(''.join(lines))\n",
    "\n",
    "    try:\n",
    "        use_ae_data = params_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in sim_data_AE_params, set to True.\")\n",
    "        normalize_dataset = True\n",
    "    \n",
    "    dir_name_ae = params_dict['dir_name_ae']\n",
    "    ae_idx = dir_name_ae[-3:]\n",
    "    dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "\n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_rnn_dict['T_sample_output']\n",
    "    T_offset = params_rnn_dict['T_offset']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in RNN_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "    try:\n",
    "        dense_layer_act_func = params_rnn_dict['dense_layer_act_func']\n",
    "    except:\n",
    "        print(\"'dense_layer_act_func' not present in RNN_specific_data, set to 'linear'.\")\n",
    "        dense_layer_act_func = 'linear'\n",
    "    try:\n",
    "        stateful = params_rnn_dict['stateful']\n",
    "    except:\n",
    "        print(\"'stateful' not present in RNN_specific_data, set to True.\")\n",
    "        stateful = True\n",
    "    try:\n",
    "        use_learnable_state = params_rnn_dict['use_learnable_state']\n",
    "    except:\n",
    "        print(\"'use_learnable_state' not present in RNN_specific_data, set to False.\")\n",
    "        use_learnable_state = False\n",
    "    try:\n",
    "        use_weights_post_dense = params_rnn_dict['use_weights_post_dense']\n",
    "    except:\n",
    "        print(\"'use_weights_post_dense' not present in RNN_specific_data, set to False.\")\n",
    "        use_weights_post_dense = False\n",
    "    try:\n",
    "        use_ae_data = params_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "\n",
    "    \n",
    "\n",
    "    normalization_arr = None\n",
    "    try:\n",
    "        with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        rnn_norm_arr_dict = eval(lines)\n",
    "        normalization_arr = rnn_norm_arr_dict['normalization_arr']\n",
    "    except:\n",
    "        pass\n",
    "    if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "        with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "            normalization_arr = fl['normalization_arr'][0]\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in ae_data, set to 'True'.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state = params_dict['init_state']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data'].astype(FTYPE)\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case : 1, lyapunov exponent : 0.9058021372262592, lyapunov time : 1.1039938926696777s\n"
     ]
    }
   ],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delaing with normalizing the data before feeding into autoencoder\n",
    "num_params = params_mat.shape[1]\n",
    "og_vars = all_data.shape[1]\n",
    "if alldata_withparams_flag == True:\n",
    "    og_vars -= num_params\n",
    "\n",
    "time_stddev_ogdata = np.std(all_data[:, 0:og_vars], axis=0)\n",
    "time_mean_ogdata = np.mean(all_data[:, 0:og_vars], axis=0)\n",
    "    \n",
    "if use_ae_data == True:\n",
    "    if ae_data_with_params == True and alldata_withparams_flag == False:\n",
    "        new_all_data = np.empty(shape=(all_data.shape[0], og_vars+num_params), dtype=FTYPE)\n",
    "        new_all_data[:, 0:og_vars] = all_data[:, 0:og_vars]\n",
    "        del(all_data)\n",
    "        all_data = new_all_data\n",
    "        prev_idx = 0\n",
    "        for i in range(boundary_idx_arr.shape[0]):\n",
    "            all_data[prev_idx:boundary_idx_arr[i], num_params:] = params_mat[i]\n",
    "            prev_idx = boundary_idx_arr[i]\n",
    "\n",
    "    if normalizeforae_flag == True:\n",
    "        for i in range(all_data.shape[1]):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "    if ae_data_with_params == False:\n",
    "        all_data = all_data[:, 0:og_vars]\n",
    "else:\n",
    "    # using raw data, neglecting the params attached (if any)\n",
    "    all_data = all_data[:, 0:og_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [],
   "source": [
    "# a = 1000000\n",
    "# all_data = all_data[0:a]\n",
    "# boundary_idx_arr = [all_data.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "    wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "    ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Zl6ZvgtNtA_u",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776554,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lXpoaKRIneQc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1667868777509,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Q3a8HHyvneQc",
    "outputId": "51084913-6faf-4bb5-db69-2cbea705dd28"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "if use_ae_data == True:\n",
    "    latent_states_all = ae_net.encoder_net.predict(all_data)\n",
    "    # del(all_data)\n",
    "else:\n",
    "    latent_states_all = all_data\n",
    "num_latent_states = latent_states_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1667868778304,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wjgPNitSrt5p",
    "outputId": "0c916524-33ec-47bf-a16a-51e53d2e25f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868778305,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wnLnqg0Jrt5t"
   },
   "outputs": [],
   "source": [
    "# for i in range(ae_net.layers):\n",
    "#     tf.keras.utils.plot_model(\n",
    "#         ae_net.layers[i],\n",
    "#         to_file=dir_name_ae+'/plots/netlayer_{}.png'.format(i),\n",
    "#         show_shapes=True,\n",
    "#         dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BOJE8vREtque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fwjcsAxKneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "aFd7XgwVneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    num_lyaptimesteps_totrain = 50 # int(5000/np.mean(lyapunov_time_arr))#\n",
    "    dt_rnn = 0.1\n",
    "    T_sample_input = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_sample_output = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_offset = dt_rnn\n",
    "    normalize_dataset = True # whether the data for the RNN should be normalized by the dataset's mean and std\n",
    "    normalization_arr = None\n",
    "    skip_intermediate = 'full sample'\n",
    "    noise_type = 'normal' # can be 'uniform' or 'normal'\n",
    "\n",
    "    # can be 'minmax', 'minmax2', 'stddev', or a list with\n",
    "    # sequential order of any of these; if it is 'minmax'\n",
    "    # then stddev_multiplier has no effect\n",
    "    normalization_type = 'stddev'\n",
    "    stddev_multiplier = 3\n",
    "\n",
    "    dense_layer_act_func = ['tanh']\n",
    "    use_weights_post_dense = True\n",
    "    stateful = True\n",
    "    use_learnable_state = False\n",
    "    use_trainable_weights_with_reslayers = False\n",
    "        \n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "        \n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "\n",
    "    # saving simulation data\n",
    "    sim_data = {\n",
    "        'params_mat':params_mat,\n",
    "        'init_state_mat':init_state_mat,\n",
    "        't0':t0,\n",
    "        'T':T,\n",
    "        'delta_t':delta_t,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'dir_name_ae':dir_name_ae,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'use_ae_data':use_ae_data,\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'sim_data_AE_params.txt', 'w') as f:\n",
    "        f.write(str(sim_data))\n",
    "        \n",
    "    # saving RNN specific data\n",
    "    RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_lyaptimesteps_totrain':num_lyaptimesteps_totrain,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':RNN_SingleStep.__module__,\n",
    "        'noise_type':noise_type,\n",
    "        'normalization_type':normalization_type,\n",
    "        'dense_layer_act_func':dense_layer_act_func,\n",
    "        'stateful':stateful,\n",
    "        'use_learnable_state':use_learnable_state,\n",
    "        'use_weights_post_dense':use_weights_post_dense,\n",
    "        'use_trainable_weights_with_reslayers':use_trainable_weights_with_reslayers,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "UGnj8uQQ83-y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "0t2_8mzI1fhX"
   },
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    latent_states_all,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=normalize_dataset,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type)\n",
    "    \n",
    "data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "num_samples = rnn_res_dict['num_samples']\n",
    "normalization_arr = rnn_res_dict['normalization_arr']\n",
    "rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "pIsWCXkbr7ws"
   },
   "outputs": [],
   "source": [
    "temp = np.divide(latent_states_all-normalization_arr[0], normalization_arr[1])\n",
    "time_stddev = np.std(temp, axis=0)\n",
    "timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "del(org_data_idx_arr_input)\n",
    "del(org_data_idx_arr_output)\n",
    "del(latent_states_all)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    all_data,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=False,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type,\n",
    "    FTYPE=FTYPE,\n",
    "    ITYPE=ITYPE)\n",
    "    \n",
    "AR_data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "AR_data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "AR_org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "AR_org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "AR_num_samples = rnn_res_dict['num_samples']\n",
    "AR_normalization_arr = rnn_res_dict['normalization_arr']\n",
    "AR_rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']\n",
    "\n",
    "del(all_data)\n",
    "del(AR_org_data_idx_arr_input)\n",
    "del(AR_org_data_idx_arr_output)\n",
    "del(AR_rnn_data_boundary_idx_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Hem_9PUqneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778791,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uskBAAXpneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "\n",
    "# ph computation parameters\n",
    "num_runs = 100\n",
    "T_sample_input_AR_ratio = 1\n",
    "T_sample_output_AR_ratio = 5\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    epochs = 200\n",
    "    patience = 10 # parameter for early stopping\n",
    "    min_delta = 1e-6  # parameter for early stopping\n",
    "    lambda_reg = 3.72759372e-07  # weight for regularizer\n",
    "    train_split = 0.8\n",
    "    val_split = 0.1\n",
    "    test_split = 1 - train_split - val_split\n",
    "    batch_size = 32\n",
    "    fRMS = 5.17947468e-03\n",
    "    zoneout_rate = 0.0\n",
    "    rnncell_dropout_rate = 0.0\n",
    "    denselayer_dropout_rate = 0.0\n",
    "    \n",
    "\n",
    "    stddev = fRMS*np.mean(time_stddev[0:og_vars])\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'timeMeanofSpaceRMS':timeMeanofSpaceRMS,\n",
    "        'stddev':stddev,\n",
    "        'zoneout_rate':zoneout_rate,\n",
    "        'rnncell_dropout_rate':rnncell_dropout_rate,\n",
    "        'denselayer_dropout_rate':denselayer_dropout_rate,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_rnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr],\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    # dir_name_rnn_og = dir_name_rnn\n",
    "    # dir_name_rnn_temp = '/home/rkaushik/Documents/Thesis/MLROM/CDV/saved_rnn/rnn_'+dir_name_rnn_og[-3:]\n",
    "    # dir_name_rnn = dir_name_rnn_temp\n",
    "\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # setting seed for PRNGs\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "# idx = np.arange(data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# boundary = int(np.round(train_split*data_rnn_input.shape[0]))\n",
    "\n",
    "# training_data_rnn_input = data_rnn_input[idx[0:boundary]]\n",
    "# training_data_rnn_output = data_rnn_output[idx[0:boundary]]\n",
    "\n",
    "# testing_data_rnn_input = data_rnn_input[idx[boundary:]]\n",
    "# testing_data_rnn_output = data_rnn_output[idx[boundary:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1667868779601,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "EENXaWqcKW7j"
   },
   "outputs": [],
   "source": [
    "cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "# idx = np.arange(cum_samples)\n",
    "# np.random.shuffle(idx)\n",
    "num_train_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_val_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_test_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_samples_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "begin_idx = 0\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    num_samples = batch_size * int((rnn_data_boundary_idx_arr[i] - begin_idx) // batch_size)\n",
    "    num_train_arr[i] = batch_size * int( np.round(train_split*num_samples/batch_size) )\n",
    "    num_val_arr[i] = batch_size * int( np.round(val_split*num_samples/batch_size) )\n",
    "    num_test_arr[i] = batch_size * int( np.round((num_samples - num_train_arr[i] - num_val_arr[i])/batch_size) )\n",
    "    num_samples_arr[i] = num_train_arr[i] + num_val_arr[i] + num_test_arr[i]\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# defining shapes\n",
    "training_input_shape = [np.sum(num_train_arr)]\n",
    "training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "training_output_shape = [np.sum(num_train_arr)]\n",
    "training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "val_input_shape = [np.sum(num_val_arr)]\n",
    "val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "val_output_shape = [np.sum(num_val_arr)]\n",
    "val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "testing_input_shape = [np.sum(num_test_arr)]\n",
    "testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "testing_output_shape = [np.sum(num_test_arr)]\n",
    "testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "# defining required arrays\n",
    "training_data_rnn_input = np.empty(shape=training_input_shape, dtype=FTYPE)\n",
    "training_data_rnn_output = np.empty(shape=training_output_shape, dtype=FTYPE)\n",
    "\n",
    "val_data_rnn_input = np.empty(shape=val_input_shape, dtype=FTYPE)\n",
    "val_data_rnn_output = np.empty(shape=val_output_shape, dtype=FTYPE)\n",
    "\n",
    "testing_data_rnn_input = np.empty(shape=testing_input_shape, dtype=FTYPE)\n",
    "testing_data_rnn_output = np.empty(shape=testing_output_shape, dtype=FTYPE)\n",
    "\n",
    "AR_testing_data_rnn_input = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "AR_testing_data_rnn_output = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "\n",
    "begin_idx = 0\n",
    "training_data_rolling_count = 0\n",
    "val_data_rolling_count = 0\n",
    "testing_data_rolling_count = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    idx = np.arange(begin_idx, rnn_data_boundary_idx_arr[i])\n",
    "    # np.random.shuffle(idx)\n",
    "    # num_samples = idx.shape[0]\n",
    "    # num_train = int( np.round(train_split*num_samples/batch_size) )*batch_size\n",
    "    # num_val = int( np.round(val_split*num_samples/batch_size) )*batch_size\n",
    "    \n",
    "    num_samples = num_samples_arr[i]\n",
    "    num_train = num_train_arr[i]\n",
    "    num_val = num_val_arr[i]\n",
    "    num_test = num_test_arr[i]\n",
    "    \n",
    "    nbatches_train = num_train // batch_size\n",
    "    nbatches_val = num_val // batch_size\n",
    "    nbatches_test = num_test // batch_size\n",
    "\n",
    "    for j in range(batch_size):\n",
    "        training_data_rnn_input[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_input[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        training_data_rnn_output[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_output[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        \n",
    "        val_data_rnn_input[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_input[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "        val_data_rnn_output[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_output[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "\n",
    "        testing_data_rnn_input[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_input[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        testing_data_rnn_output[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_output[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "\n",
    "    AR_testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = AR_data_rnn_input[idx[num_train+num_val:num_samples]]\n",
    "    AR_testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = AR_data_rnn_output[idx[num_train+num_val:num_samples]]\n",
    "\n",
    "    # training_data_rnn_input[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_input[idx[0:num_train]]\n",
    "    # training_data_rnn_output[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_output[idx[0:num_train]]\n",
    "    training_data_rolling_count += num_train\n",
    "\n",
    "    # val_data_rnn_input[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_input[idx[num_train:num_train+num_val]]\n",
    "    # val_data_rnn_output[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_output[idx[num_train:num_train+num_val]]\n",
    "    val_data_rolling_count += num_val\n",
    "\n",
    "    # num_test = num_samples-num_train-num_val+1\n",
    "    # testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_input[idx[num_train+num_val:]]\n",
    "    # testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_output[idx[num_train+num_val:]]\n",
    "    testing_data_rolling_count += num_test\n",
    "\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# cleaning up\n",
    "del(data_rnn_input)\n",
    "del(data_rnn_output)\n",
    "del(AR_data_rnn_input)\n",
    "del(AR_data_rnn_output)\n",
    "\n",
    "# further shuffling\n",
    "if stateful == False:\n",
    "    idx = np.arange(0, training_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    training_data_rnn_input = training_data_rnn_input[idx]\n",
    "    training_data_rnn_output = training_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, val_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    val_data_rnn_input = val_data_rnn_input[idx]\n",
    "    val_data_rnn_output = val_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, testing_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    testing_data_rnn_input = testing_data_rnn_input[idx]\n",
    "    testing_data_rnn_output = testing_data_rnn_output[idx]\n",
    "\n",
    "    del(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868779603,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8isZN1tYBifp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_runs :  100\n"
     ]
    }
   ],
   "source": [
    "s_in = AR_testing_data_rnn_input.shape\n",
    "AR_testing_data_rnn_input = AR_testing_data_rnn_input.reshape((1, s_in[0]*s_in[1]) + s_in[2:])\n",
    "\n",
    "s_out = AR_testing_data_rnn_output.shape\n",
    "AR_testing_data_rnn_output = AR_testing_data_rnn_output.reshape((1, s_out[0]*s_out[1]) + s_out[2:])\n",
    "\n",
    "T_sample_input_AR = T_sample_input_AR_ratio*np.mean(lyapunov_time_arr)#50.1*dt_rnn\n",
    "num_sample_input_AR = int((T_sample_input_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "T_sample_output_AR = T_sample_output_AR_ratio*np.mean(lyapunov_time_arr)\n",
    "num_sample_output_AR = int((T_sample_output_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "num_offset_AR = num_sample_input_AR\n",
    "T_offset_AR = num_offset_AR*dt_rnn\n",
    "\n",
    "batch_idx = np.random.randint(low=0, high=AR_testing_data_rnn_input.shape[0])\n",
    "maxpossible_num_runs = AR_testing_data_rnn_input.shape[1]-(num_sample_input_AR+num_sample_output_AR)\n",
    "\n",
    "num_runs = np.min([num_runs, maxpossible_num_runs])\n",
    "\n",
    "print('num_runs : ', num_runs)\n",
    "\n",
    "data_idx_arr = np.linspace(0, maxpossible_num_runs-1, num_runs, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779605,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "x3KglJsgneQj"
   },
   "outputs": [],
   "source": [
    "AR_data_in = np.empty(shape=(num_runs, num_sample_input_AR)+tuple(s_in[2:]))\n",
    "AR_data_out = np.empty(shape=(num_runs, num_sample_output_AR)+tuple(s_out[2:]))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    d_idx = data_idx_arr[i]\n",
    "    AR_data_in[i] = AR_testing_data_rnn_input[0, d_idx:d_idx+num_sample_input_AR]\n",
    "    AR_data_out[i] = AR_testing_data_rnn_input[0, d_idx+num_sample_input_AR:d_idx+num_sample_input_AR+num_sample_output_AR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ixetsZHjCMKO"
   },
   "outputs": [],
   "source": [
    "del(AR_testing_data_rnn_input)\n",
    "del(AR_testing_data_rnn_output)\n",
    "AR_testing_data_rnn_input = AR_data_in\n",
    "AR_testing_data_rnn_output = AR_data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   training_data_rnn_input.shape :  (576, 552, 2)\n",
      "  training_data_rnn_output.shape :  (576, 552, 2)\n",
      "    testing_data_rnn_input.shape :  (96, 552, 2)\n",
      "   testing_data_rnn_output.shape :  (96, 552, 2)\n",
      "        val_data_rnn_input.shape :  (64, 552, 2)\n",
      "       val_data_rnn_output.shape :  (64, 552, 2)\n",
      "\n",
      " AR_testing_data_rnn_input.shape :  (100, 11, 3)\n",
      "AR_testing_data_rnn_output.shape :  (100, 55, 3)\n"
     ]
    }
   ],
   "source": [
    "print('   training_data_rnn_input.shape : ', training_data_rnn_input.shape)\n",
    "print('  training_data_rnn_output.shape : ', training_data_rnn_output.shape)\n",
    "print('    testing_data_rnn_input.shape : ', testing_data_rnn_input.shape)\n",
    "print('   testing_data_rnn_output.shape : ', testing_data_rnn_output.shape)\n",
    "print('        val_data_rnn_input.shape : ', val_data_rnn_input.shape)\n",
    "print('       val_data_rnn_output.shape : ', val_data_rnn_output.shape)\n",
    "print('')\n",
    "print(' AR_testing_data_rnn_input.shape : ', AR_testing_data_rnn_input.shape)\n",
    "print('AR_testing_data_rnn_output.shape : ', AR_testing_data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_NSTtZuyneQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeMeanofSpaceRMS : 0.2933353\n",
      "stddev : 0.0017228189150698256\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "#     rnn_layers_units = [500]*3\n",
    "    scalar_weights = None\n",
    "#     scalar_weights = [\n",
    "#         1.0,\n",
    "#     ] # Euler\n",
    "#     scalar_weights = [\n",
    "#         0.5, \n",
    "#         0.0, 0.5,\n",
    "#         0.0, 0.0, 1.0,\n",
    "#         1/6, 1/3, 1/3, 1/6\n",
    "#     ] # RK4\n",
    "    # scalar_weights = [\n",
    "    #     1.0,\n",
    "    #     0.25, 0.25,\n",
    "    #     1/6, 1/6, 2/3\n",
    "    # ] # TVD RK3\n",
    "#     scalar_weights = [\n",
    "#         1.0,\n",
    "#         0.5, 0.5\n",
    "#     ] # TVD RK2\n",
    "    num_rnn_layers = 1\n",
    "    if not isinstance(scalar_weights, type(None)):\n",
    "        num_rnn_layers += int( ((8*len(scalar_weights)+1)**0.5 - 1)/2 )\n",
    "    rnn_layers_units = [50*num_latent_states]*num_rnn_layers\n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "    print('timeMeanofSpaceRMS :', timeMeanofSpaceRMS)\n",
    "    print('stddev :', stddev)\n",
    "    if return_params_arr != False:\n",
    "        data_dim = num_latent_states + 3\n",
    "    else:\n",
    "        data_dim = num_latent_states\n",
    "\n",
    "    dense_dim = [rnn_layers_units[-1]]*(len(dense_layer_act_func)-1)\n",
    "    dense_dim.append(data_dim)\n",
    "        \n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_SingleStep(\n",
    "                data_dim=data_dim,\n",
    "            #     in_steps=int(T_sample_input // dt_rnn),\n",
    "            #     out_steps=int(T_sample_output // dt_rnn),\n",
    "                dt_rnn=dt_rnn,\n",
    "                lambda_reg=lambda_reg,\n",
    "                reg_name='L2',\n",
    "                rnn_layers_units=rnn_layers_units,\n",
    "                dense_layer_act_func=dense_layer_act_func,\n",
    "                load_file=None,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                stddev=stddev,\n",
    "                noise_type=noise_type,\n",
    "                dense_dim=dense_dim,\n",
    "                use_learnable_state=use_learnable_state,\n",
    "                stateful=stateful,\n",
    "                zoneout_rate=zoneout_rate,\n",
    "                batch_size=batch_size,\n",
    "                use_weights_post_dense=use_weights_post_dense,\n",
    "                rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "                denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "                scalar_weights=scalar_weights, # corresponding to RK4\n",
    "                use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_SingleStep(\n",
    "            data_dim=data_dim,\n",
    "        #     in_steps=int(T_sample_input // dt_rnn),\n",
    "        #     out_steps=int(T_sample_output // dt_rnn),\n",
    "            dt_rnn=dt_rnn,\n",
    "            lambda_reg=lambda_reg,\n",
    "            reg_name='L2',\n",
    "            rnn_layers_units=rnn_layers_units,\n",
    "            dense_layer_act_func=dense_layer_act_func,\n",
    "            load_file=None,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            stddev=stddev,\n",
    "            noise_type=noise_type,\n",
    "            dense_dim=dense_dim,\n",
    "            use_learnable_state=use_learnable_state,\n",
    "            stateful=stateful,\n",
    "            zoneout_rate=zoneout_rate,\n",
    "            batch_size=batch_size,\n",
    "            use_weights_post_dense=use_weights_post_dense,\n",
    "            rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "            denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "            scalar_weights=scalar_weights, # corresponding to RK4\n",
    "            use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "        )\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    rnn_net.save_class_dict(save_path+dir_sep+'final_net_class_dict.txt')\n",
    "else:\n",
    "    load_file = dir_name_rnn + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_SingleStep(\n",
    "                load_file=load_file,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                batch_size=batch_size,\n",
    "                \n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_SingleStep(\n",
    "            load_file=load_file,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    rnn_net.build(input_shape=(batch_size, None, num_latent_states))\n",
    "    \n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        wt_file = tf.train.latest_checkpoint(dir_name_rnn+dir_sep+'checkpoints')\n",
    "    elif behaviour == 'loadFinalNetAndPlot':\n",
    "        wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'final_net_gru_weights.h5'\n",
    "        # wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'f2'#+dir_sep+'saved_model.pb'\n",
    "        rnn_net.load_weights_from_file(wt_file)\n",
    "    \n",
    "    # this forces the model to initialize its kernel weights/biases\n",
    "    # temp = rnn_net.predict(tf.ones(shape=[batch_size, int(T_sample_input//dt_rnn), rnn_net.data_dim]))\n",
    "    # this loads just the kernel wieghts and biases of the model\n",
    "#     rnn_net.load_weights_from_file(wt_file)\n",
    "\n",
    "    # rnn_net = tf.keras.models.load_model(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    val_loss_hist = []\n",
    "    train_loss_hist = []\n",
    "    lr_change=[0, 0]\n",
    "    savelosses_cb_vallossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    savelosses_cb_trainlossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    starting_lr_idx = 0\n",
    "    num_epochs_left = epochs\n",
    "    earlystopping_wait = 0\n",
    "elif behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt, earlystopping_wait = readAndReturnLossHistories(\n",
    "        dir_name_ae=dir_name_rnn,\n",
    "        dir_sep=dir_sep,\n",
    "        epochs=epochs,\n",
    "        learning_rate_list=learning_rate_list,\n",
    "        return_earlystopping_wait=True)\n",
    "    savelosses_cb_vallossarr = val_loss_arr_fromckpt\n",
    "    savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n",
    "elif behaviour == 'loadFinalNetAndPlot':\n",
    "    with open(dir_name_rnn+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    losses_dict = eval(''.join(lines))\n",
    "\n",
    "    val_loss_hist = losses_dict['val_loss_hist']\n",
    "    train_loss_hist = losses_dict['train_loss_hist']\n",
    "    lr_change = losses_dict['lr_change']\n",
    "    test_loss = losses_dict['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NMSE_hist = []\n",
    "val_NMSE_hist = []\n",
    "\n",
    "train_MSE_hist = []\n",
    "val_MSE_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMSE(tf.keras.metrics.MeanSquaredError):\n",
    "    def __init__(self, divisor_arr, name='NMSE', **kwargs):\n",
    "        super(NMSE, self).__init__(name, **kwargs)\n",
    "        self.divisor_arr = divisor_arr\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = y_true / self.divisor_arr\n",
    "        y_pred = y_pred / self.divisor_arr\n",
    "        return super(NMSE, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------- LEARNING RATE : 0.01 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0444 - mse: 0.0444 - NMSE: 0.4011 - tot_time: 0h 0m 12.2s\n",
      "\n",
      "Epoch 1: val_NMSE improved from inf to 0.15708, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 12s 594ms/step - loss: 0.0444 - mse: 0.0444 - NMSE: 0.4011 - val_loss: 0.0175 - val_mse: 0.0174 - val_NMSE: 0.1571\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0138 - mse: 0.0136 - NMSE: 0.1234 - tot_time: 0h 0m 22.1s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.15708 to 0.08786, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 0.0138 - mse: 0.0136 - NMSE: 0.1234 - val_loss: 0.0099 - val_mse: 0.0097 - val_NMSE: 0.0879\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0081 - mse: 0.0079 - NMSE: 0.0712 - tot_time: 0h 0m 32.0s\n",
      "\n",
      "Epoch 3: val_NMSE improved from 0.08786 to 0.05923, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 0.0081 - mse: 0.0079 - NMSE: 0.0712 - val_loss: 0.0068 - val_mse: 0.0066 - val_NMSE: 0.0592\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0059 - mse: 0.0056 - NMSE: 0.0506 - tot_time: 0h 0m 41.9s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.05923 to 0.04438, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 0.0059 - mse: 0.0056 - NMSE: 0.0506 - val_loss: 0.0052 - val_mse: 0.0049 - val_NMSE: 0.0444\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0044 - mse: 0.0041 - NMSE: 0.0369 - tot_time: 0h 0m 52.0s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.04438 to 0.03224, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 566ms/step - loss: 0.0044 - mse: 0.0041 - NMSE: 0.0369 - val_loss: 0.0039 - val_mse: 0.0036 - val_NMSE: 0.0322\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0032 - mse: 0.0029 - NMSE: 0.0259 - tot_time: 0h 1m 1.6s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.03224 to 0.02300, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 535ms/step - loss: 0.0032 - mse: 0.0029 - NMSE: 0.0259 - val_loss: 0.0029 - val_mse: 0.0025 - val_NMSE: 0.0230\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0020 - NMSE: 0.0178 - tot_time: 0h 1m 11.5s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.02300\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 0.0023 - mse: 0.0020 - NMSE: 0.0178 - val_loss: 0.0039 - val_mse: 0.0035 - val_NMSE: 0.0320\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.0022 - NMSE: 0.0197 - tot_time: 0h 1m 21.5s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.02300 to 0.01435, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 560ms/step - loss: 0.0026 - mse: 0.0022 - NMSE: 0.0197 - val_loss: 0.0020 - val_mse: 0.0016 - val_NMSE: 0.0144\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.0012 - NMSE: 0.0109 - tot_time: 0h 1m 30.5s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.01435 to 0.01058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 506ms/step - loss: 0.0016 - mse: 0.0012 - NMSE: 0.0109 - val_loss: 0.0016 - val_mse: 0.0012 - val_NMSE: 0.0106\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0013 - mse: 8.7012e-04 - NMSE: 0.0079 - tot_time: 0h 1m 39.9s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.01058 to 0.00889, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 517ms/step - loss: 0.0013 - mse: 8.7012e-04 - NMSE: 0.0079 - val_loss: 0.0014 - val_mse: 9.8343e-04 - val_NMSE: 0.0089\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0011 - mse: 7.4316e-04 - NMSE: 0.0067 - tot_time: 0h 1m 49.1s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00889 to 0.00731, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 504ms/step - loss: 0.0011 - mse: 7.4316e-04 - NMSE: 0.0067 - val_loss: 0.0012 - val_mse: 8.0866e-04 - val_NMSE: 0.0073\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 9.9821e-04 - mse: 5.9814e-04 - NMSE: 0.0054 - tot_time: 0h 1m 58.0s\n",
      "\n",
      "Epoch 12: val_NMSE improved from 0.00731 to 0.00624, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 493ms/step - loss: 9.9821e-04 - mse: 5.9814e-04 - NMSE: 0.0054 - val_loss: 0.0011 - val_mse: 6.9087e-04 - val_NMSE: 0.0062\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0012 - mse: 8.5235e-04 - NMSE: 0.0077 - tot_time: 0h 2m 7.6s\n",
      "\n",
      "Epoch 13: val_NMSE did not improve from 0.00624\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 529ms/step - loss: 0.0012 - mse: 8.5235e-04 - NMSE: 0.0077 - val_loss: 0.0013 - val_mse: 9.2569e-04 - val_NMSE: 0.0084\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0010 - mse: 6.4441e-04 - NMSE: 0.0058 - tot_time: 0h 2m 17.5s\n",
      "\n",
      "Epoch 14: val_NMSE improved from 0.00624 to 0.00598, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 548ms/step - loss: 0.0010 - mse: 6.4441e-04 - NMSE: 0.0058 - val_loss: 0.0010 - val_mse: 6.6194e-04 - val_NMSE: 0.0060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 8.3338e-04 - mse: 4.5366e-04 - NMSE: 0.0041 - tot_time: 0h 2m 27.9s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00598 to 0.00507, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 582ms/step - loss: 8.3338e-04 - mse: 4.5366e-04 - NMSE: 0.0041 - val_loss: 9.3739e-04 - val_mse: 5.6119e-04 - val_NMSE: 0.0051\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.5846e-04 - mse: 3.8648e-04 - NMSE: 0.0035 - tot_time: 0h 2m 38.0s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00507 to 0.00465, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 561ms/step - loss: 7.5846e-04 - mse: 3.8648e-04 - NMSE: 0.0035 - val_loss: 8.8100e-04 - val_mse: 5.1414e-04 - val_NMSE: 0.0046\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.0765e-04 - mse: 3.4592e-04 - NMSE: 0.0031 - tot_time: 0h 2m 47.7s\n",
      "\n",
      "Epoch 17: val_NMSE improved from 0.00465 to 0.00429, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 534ms/step - loss: 7.0765e-04 - mse: 3.4592e-04 - NMSE: 0.0031 - val_loss: 8.3059e-04 - val_mse: 4.7474e-04 - val_NMSE: 0.0043\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.6721e-04 - mse: 3.1674e-04 - NMSE: 0.0029 - tot_time: 0h 2m 57.2s\n",
      "\n",
      "Epoch 18: val_NMSE improved from 0.00429 to 0.00415, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 520ms/step - loss: 6.6721e-04 - mse: 3.1674e-04 - NMSE: 0.0029 - val_loss: 8.0362e-04 - val_mse: 4.5915e-04 - val_NMSE: 0.0042\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.5114e-04 - mse: 4.1177e-04 - NMSE: 0.0037 - tot_time: 0h 3m 6.9s\n",
      "\n",
      "Epoch 19: val_NMSE did not improve from 0.00415\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 7.5114e-04 - mse: 4.1177e-04 - NMSE: 0.0037 - val_loss: 8.2387e-04 - val_mse: 4.8952e-04 - val_NMSE: 0.0044\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.2481e-04 - mse: 2.9427e-04 - NMSE: 0.0027 - tot_time: 0h 3m 17.0s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00415 to 0.00367, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 563ms/step - loss: 6.2481e-04 - mse: 2.9427e-04 - NMSE: 0.0027 - val_loss: 7.3217e-04 - val_mse: 4.0608e-04 - val_NMSE: 0.0037\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.1251e-04 - mse: 2.9069e-04 - NMSE: 0.0026 - tot_time: 0h 3m 27.6s\n",
      "\n",
      "Epoch 21: val_NMSE did not improve from 0.00367\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 11s 593ms/step - loss: 6.1251e-04 - mse: 2.9069e-04 - NMSE: 0.0026 - val_loss: 0.0011 - val_mse: 8.2999e-04 - val_NMSE: 0.0075\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.4930e-04 - mse: 4.3612e-04 - NMSE: 0.0039 - tot_time: 0h 3m 37.7s\n",
      "\n",
      "Epoch 22: val_NMSE did not improve from 0.00367\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 555ms/step - loss: 7.4930e-04 - mse: 4.3612e-04 - NMSE: 0.0039 - val_loss: 7.4071e-04 - val_mse: 4.3077e-04 - val_NMSE: 0.0039\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.6049e-04 - mse: 2.5326e-04 - NMSE: 0.0023 - tot_time: 0h 3m 46.9s\n",
      "\n",
      "Epoch 23: val_NMSE improved from 0.00367 to 0.00331, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 518ms/step - loss: 5.6049e-04 - mse: 2.5326e-04 - NMSE: 0.0023 - val_loss: 6.7048e-04 - val_mse: 3.6654e-04 - val_NMSE: 0.0033\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.2449e-04 - mse: 2.2407e-04 - NMSE: 0.0020 - tot_time: 0h 3m 56.0s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00331 to 0.00311, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 508ms/step - loss: 5.2449e-04 - mse: 2.2407e-04 - NMSE: 0.0020 - val_loss: 6.3981e-04 - val_mse: 3.4352e-04 - val_NMSE: 0.0031\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.4187e-04 - mse: 4.4937e-04 - NMSE: 0.0041 - tot_time: 0h 4m 5.4s\n",
      "\n",
      "Epoch 25: val_NMSE did not improve from 0.00311\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 519ms/step - loss: 7.4187e-04 - mse: 4.4937e-04 - NMSE: 0.0041 - val_loss: 0.0015 - val_mse: 0.0012 - val_NMSE: 0.0107\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.5084e-04 - mse: 4.6311e-04 - NMSE: 0.0042 - tot_time: 0h 4m 14.9s\n",
      "\n",
      "Epoch 26: val_NMSE did not improve from 0.00311\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 527ms/step - loss: 7.5084e-04 - mse: 4.6311e-04 - NMSE: 0.0042 - val_loss: 7.1911e-04 - val_mse: 4.3131e-04 - val_NMSE: 0.0039\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.3662e-04 - mse: 2.4958e-04 - NMSE: 0.0023 - tot_time: 0h 4m 24.3s\n",
      "\n",
      "Epoch 27: val_NMSE improved from 0.00311 to 0.00303, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 528ms/step - loss: 5.3662e-04 - mse: 2.4958e-04 - NMSE: 0.0023 - val_loss: 6.2052e-04 - val_mse: 3.3495e-04 - val_NMSE: 0.0030\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.9042e-04 - mse: 2.0727e-04 - NMSE: 0.0019 - tot_time: 0h 4m 33.2s\n",
      "\n",
      "Epoch 28: val_NMSE improved from 0.00303 to 0.00285, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 496ms/step - loss: 4.9042e-04 - mse: 2.0727e-04 - NMSE: 0.0019 - val_loss: 5.9492e-04 - val_mse: 3.1491e-04 - val_NMSE: 0.0028\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.6834e-04 - mse: 1.9141e-04 - NMSE: 0.0017 - tot_time: 0h 4m 42.4s\n",
      "\n",
      "Epoch 29: val_NMSE improved from 0.00285 to 0.00273, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 515ms/step - loss: 4.6834e-04 - mse: 1.9141e-04 - NMSE: 0.0017 - val_loss: 5.7498e-04 - val_mse: 3.0156e-04 - val_NMSE: 0.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.5139e-04 - mse: 1.8110e-04 - NMSE: 0.0016 - tot_time: 0h 4m 52.4s\n",
      "\n",
      "Epoch 30: val_NMSE improved from 0.00273 to 0.00265, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 556ms/step - loss: 4.5139e-04 - mse: 1.8110e-04 - NMSE: 0.0016 - val_loss: 5.5993e-04 - val_mse: 2.9311e-04 - val_NMSE: 0.0026\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.5643e-04 - mse: 1.9265e-04 - NMSE: 0.0017 - tot_time: 0h 5m 1.8s\n",
      "\n",
      "Epoch 31: val_NMSE did not improve from 0.00265\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 511ms/step - loss: 4.5643e-04 - mse: 1.9265e-04 - NMSE: 0.0017 - val_loss: 8.4689e-04 - val_mse: 5.8655e-04 - val_NMSE: 0.0053\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.2935e-04 - mse: 3.7094e-04 - NMSE: 0.0034 - tot_time: 0h 5m 10.9s\n",
      "\n",
      "Epoch 32: val_NMSE did not improve from 0.00265\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 513ms/step - loss: 6.2935e-04 - mse: 3.7094e-04 - NMSE: 0.0034 - val_loss: 5.5162e-04 - val_mse: 2.9461e-04 - val_NMSE: 0.0027\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.4220e-04 - mse: 1.8649e-04 - NMSE: 0.0017 - tot_time: 0h 5m 19.9s\n",
      "\n",
      "Epoch 33: val_NMSE improved from 0.00265 to 0.00249, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 496ms/step - loss: 4.4220e-04 - mse: 1.8649e-04 - NMSE: 0.0017 - val_loss: 5.2885e-04 - val_mse: 2.7496e-04 - val_NMSE: 0.0025\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.1324e-04 - mse: 1.6141e-04 - NMSE: 0.0015 - tot_time: 0h 5m 29.4s\n",
      "\n",
      "Epoch 34: val_NMSE improved from 0.00249 to 0.00243, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 526ms/step - loss: 4.1324e-04 - mse: 1.6141e-04 - NMSE: 0.0015 - val_loss: 5.1841e-04 - val_mse: 2.6908e-04 - val_NMSE: 0.0024\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.9859e-04 - mse: 1.5163e-04 - NMSE: 0.0014 - tot_time: 0h 5m 38.9s\n",
      "\n",
      "Epoch 35: val_NMSE improved from 0.00243 to 0.00232, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 531ms/step - loss: 3.9859e-04 - mse: 1.5163e-04 - NMSE: 0.0014 - val_loss: 5.0135e-04 - val_mse: 2.5709e-04 - val_NMSE: 0.0023\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.1300e-04 - mse: 2.7113e-04 - NMSE: 0.0025 - tot_time: 0h 5m 48.8s\n",
      "\n",
      "Epoch 36: val_NMSE did not improve from 0.00232\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 5.1300e-04 - mse: 2.7113e-04 - NMSE: 0.0025 - val_loss: 9.6355e-04 - val_mse: 7.2396e-04 - val_NMSE: 0.0065\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.2619e-04 - mse: 2.8721e-04 - NMSE: 0.0026 - tot_time: 0h 5m 58.3s\n",
      "\n",
      "Epoch 37: val_NMSE did not improve from 0.00232\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 524ms/step - loss: 5.2619e-04 - mse: 2.8721e-04 - NMSE: 0.0026 - val_loss: 5.1202e-04 - val_mse: 2.7343e-04 - val_NMSE: 0.0025\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.9916e-04 - mse: 1.6151e-04 - NMSE: 0.0015 - tot_time: 0h 6m 7.5s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00232 to 0.00224, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 516ms/step - loss: 3.9916e-04 - mse: 1.6151e-04 - NMSE: 0.0015 - val_loss: 4.8367e-04 - val_mse: 2.4746e-04 - val_NMSE: 0.0022\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.7809e-04 - mse: 1.4352e-04 - NMSE: 0.0013 - tot_time: 0h 6m 16.7s\n",
      "\n",
      "Epoch 39: val_NMSE improved from 0.00224 to 0.00216, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 508ms/step - loss: 3.7809e-04 - mse: 1.4352e-04 - NMSE: 0.0013 - val_loss: 4.7106e-04 - val_mse: 2.3850e-04 - val_NMSE: 0.0022\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6802e-04 - mse: 1.3743e-04 - NMSE: 0.0012 - tot_time: 0h 6m 26.2s\n",
      "\n",
      "Epoch 40: val_NMSE did not improve from 0.00216\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 3.6802e-04 - mse: 1.3743e-04 - NMSE: 0.0012 - val_loss: 4.7830e-04 - val_mse: 2.4991e-04 - val_NMSE: 0.0023\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.4236e-04 - mse: 3.1566e-04 - NMSE: 0.0029 - tot_time: 0h 6m 36.3s\n",
      "\n",
      "Epoch 41: val_NMSE did not improve from 0.00216\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 5.4236e-04 - mse: 3.1566e-04 - NMSE: 0.0029 - val_loss: 6.0964e-04 - val_mse: 3.8403e-04 - val_NMSE: 0.0035\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.1330e-04 - mse: 1.8827e-04 - NMSE: 0.0017 - tot_time: 0h 6m 45.1s\n",
      "\n",
      "Epoch 42: val_NMSE did not improve from 0.00216\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 492ms/step - loss: 4.1330e-04 - mse: 1.8827e-04 - NMSE: 0.0017 - val_loss: 4.7289e-04 - val_mse: 2.4863e-04 - val_NMSE: 0.0022\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5956e-04 - mse: 1.3648e-04 - NMSE: 0.0012 - tot_time: 0h 6m 53.8s\n",
      "\n",
      "Epoch 43: val_NMSE improved from 0.00216 to 0.00201, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 492ms/step - loss: 3.5956e-04 - mse: 1.3648e-04 - NMSE: 0.0012 - val_loss: 4.4390e-04 - val_mse: 2.2238e-04 - val_NMSE: 0.0020\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4503e-04 - mse: 1.2508e-04 - NMSE: 0.0011 - tot_time: 0h 7m 3.3s\n",
      "\n",
      "Epoch 44: val_NMSE did not improve from 0.00201\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 529ms/step - loss: 3.4503e-04 - mse: 1.2508e-04 - NMSE: 0.0011 - val_loss: 4.4372e-04 - val_mse: 2.2566e-04 - val_NMSE: 0.0020\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 4.0764e-04 - mse: 1.9130e-04 - NMSE: 0.0017 - tot_time: 0h 7m 12.7s\n",
      "\n",
      "Epoch 45: val_NMSE did not improve from 0.00201\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 526ms/step - loss: 4.0764e-04 - mse: 1.9130e-04 - NMSE: 0.0017 - val_loss: 4.6644e-04 - val_mse: 2.5182e-04 - val_NMSE: 0.0023\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6140e-04 - mse: 1.4773e-04 - NMSE: 0.0013 - tot_time: 0h 7m 22.6s\n",
      "\n",
      "Epoch 46: val_NMSE improved from 0.00201 to 0.00194, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 3.6140e-04 - mse: 1.4773e-04 - NMSE: 0.0013 - val_loss: 4.2742e-04 - val_mse: 2.1491e-04 - val_NMSE: 0.0019\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.0824e-04 - mse: 2.9691e-04 - NMSE: 0.0027 - tot_time: 0h 7m 32.2s\n",
      "\n",
      "Epoch 47: val_NMSE did not improve from 0.00194\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 528ms/step - loss: 5.0824e-04 - mse: 2.9691e-04 - NMSE: 0.0027 - val_loss: 6.2846e-04 - val_mse: 4.1817e-04 - val_NMSE: 0.0038\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.0925e-04 - mse: 1.9877e-04 - NMSE: 0.0018 - tot_time: 0h 7m 41.7s\n",
      "\n",
      "Epoch 48: val_NMSE did not improve from 0.00194\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 528ms/step - loss: 4.0925e-04 - mse: 1.9877e-04 - NMSE: 0.0018 - val_loss: 4.3013e-04 - val_mse: 2.1978e-04 - val_NMSE: 0.0020\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3810e-04 - mse: 1.2851e-04 - NMSE: 0.0012 - tot_time: 0h 7m 50.8s\n",
      "\n",
      "Epoch 49: val_NMSE improved from 0.00194 to 0.00187, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 509ms/step - loss: 3.3810e-04 - mse: 1.2851e-04 - NMSE: 0.0012 - val_loss: 4.1565e-04 - val_mse: 2.0722e-04 - val_NMSE: 0.0019\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2197e-04 - mse: 1.1479e-04 - NMSE: 0.0010 - tot_time: 0h 8m 0.6s\n",
      "\n",
      "Epoch 50: val_NMSE improved from 0.00187 to 0.00176, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 3.2197e-04 - mse: 1.1479e-04 - NMSE: 0.0010 - val_loss: 4.0038e-04 - val_mse: 1.9478e-04 - val_NMSE: 0.0018\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.4034e-04 - mse: 3.3625e-04 - NMSE: 0.0030 - tot_time: 0h 8m 9.9s\n",
      "\n",
      "Epoch 51: val_NMSE did not improve from 0.00176\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 515ms/step - loss: 5.4034e-04 - mse: 3.3625e-04 - NMSE: 0.0030 - val_loss: 6.5914e-04 - val_mse: 4.5625e-04 - val_NMSE: 0.0041\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.1279e-04 - mse: 2.0914e-04 - NMSE: 0.0019 - tot_time: 0h 8m 19.0s\n",
      "\n",
      "Epoch 52: val_NMSE did not improve from 0.00176\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 500ms/step - loss: 4.1279e-04 - mse: 2.0914e-04 - NMSE: 0.0019 - val_loss: 4.4956e-04 - val_mse: 2.4527e-04 - val_NMSE: 0.0022\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3703e-04 - mse: 1.3308e-04 - NMSE: 0.0012 - tot_time: 0h 8m 28.5s\n",
      "\n",
      "Epoch 53: val_NMSE did not improve from 0.00176\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 529ms/step - loss: 3.3703e-04 - mse: 1.3308e-04 - NMSE: 0.0012 - val_loss: 4.0552e-04 - val_mse: 2.0245e-04 - val_NMSE: 0.0018\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1696e-04 - mse: 1.1492e-04 - NMSE: 0.0010 - tot_time: 0h 8m 38.2s\n",
      "\n",
      "Epoch 54: val_NMSE improved from 0.00176 to 0.00175, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 3.1696e-04 - mse: 1.1492e-04 - NMSE: 0.0010 - val_loss: 3.9461e-04 - val_mse: 1.9394e-04 - val_NMSE: 0.0018\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0835e-04 - mse: 1.0907e-04 - NMSE: 9.8600e-04 - tot_time: 0h 8m 48.0s\n",
      "\n",
      "Epoch 55: val_NMSE improved from 0.00175 to 0.00170, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 548ms/step - loss: 3.0835e-04 - mse: 1.0907e-04 - NMSE: 9.8600e-04 - val_loss: 3.8536e-04 - val_mse: 1.8767e-04 - val_NMSE: 0.0017\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0217e-04 - mse: 1.0588e-04 - NMSE: 9.5719e-04 - tot_time: 0h 8m 57.6s\n",
      "\n",
      "Epoch 56: val_NMSE improved from 0.00170 to 0.00165, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 3.0217e-04 - mse: 1.0588e-04 - NMSE: 9.5719e-04 - val_loss: 3.7721e-04 - val_mse: 1.8250e-04 - val_NMSE: 0.0016\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.6758e-04 - mse: 4.7408e-04 - NMSE: 0.0043 - tot_time: 0h 9m 6.4s\n",
      "\n",
      "Epoch 57: val_NMSE did not improve from 0.00165\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 486ms/step - loss: 6.6758e-04 - mse: 4.7408e-04 - NMSE: 0.0043 - val_loss: 4.2209e-04 - val_mse: 2.2895e-04 - val_NMSE: 0.0021\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.3063e-04 - mse: 2.3566e-04 - NMSE: 0.0021 - tot_time: 0h 9m 15.7s\n",
      "\n",
      "Epoch 58: val_NMSE did not improve from 0.00165\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 518ms/step - loss: 4.3063e-04 - mse: 2.3566e-04 - NMSE: 0.0021 - val_loss: 4.3960e-04 - val_mse: 2.4313e-04 - val_NMSE: 0.0022\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2620e-04 - mse: 1.2980e-04 - NMSE: 0.0012 - tot_time: 0h 9m 24.8s\n",
      "\n",
      "Epoch 59: val_NMSE did not improve from 0.00165\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 503ms/step - loss: 3.2620e-04 - mse: 1.2980e-04 - NMSE: 0.0012 - val_loss: 3.8324e-04 - val_mse: 1.8739e-04 - val_NMSE: 0.0017\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0415e-04 - mse: 1.0919e-04 - NMSE: 9.8708e-04 - tot_time: 0h 9m 34.2s\n",
      "\n",
      "Epoch 60: val_NMSE improved from 0.00165 to 0.00158, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 522ms/step - loss: 3.0415e-04 - mse: 1.0919e-04 - NMSE: 9.8708e-04 - val_loss: 3.6851e-04 - val_mse: 1.7474e-04 - val_NMSE: 0.0016\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9477e-04 - mse: 1.0224e-04 - NMSE: 9.2427e-04 - tot_time: 0h 9m 43.4s\n",
      "\n",
      "Epoch 61: val_NMSE improved from 0.00158 to 0.00155, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 502ms/step - loss: 2.9477e-04 - mse: 1.0224e-04 - NMSE: 9.2427e-04 - val_loss: 3.6213e-04 - val_mse: 1.7104e-04 - val_NMSE: 0.0015\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8866e-04 - mse: 9.8854e-05 - NMSE: 8.9364e-04 - tot_time: 0h 9m 52.6s\n",
      "\n",
      "Epoch 62: val_NMSE improved from 0.00155 to 0.00153, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 514ms/step - loss: 2.8866e-04 - mse: 9.8854e-05 - NMSE: 8.9364e-04 - val_loss: 3.5786e-04 - val_mse: 1.6952e-04 - val_NMSE: 0.0015\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3546e-04 - mse: 1.4841e-04 - NMSE: 0.0013 - tot_time: 0h 10m 2.4s\n",
      "\n",
      "Epoch 63: val_NMSE did not improve from 0.00153\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 542ms/step - loss: 3.3546e-04 - mse: 1.4841e-04 - NMSE: 0.0013 - val_loss: 4.2466e-04 - val_mse: 2.3903e-04 - val_NMSE: 0.0022\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1370e-04 - mse: 1.2887e-04 - NMSE: 0.0012 - tot_time: 0h 10m 11.6s\n",
      "\n",
      "Epoch 64: val_NMSE did not improve from 0.00153\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 508ms/step - loss: 3.1370e-04 - mse: 1.2887e-04 - NMSE: 0.0012 - val_loss: 3.5519e-04 - val_mse: 1.7123e-04 - val_NMSE: 0.0015\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8059e-04 - mse: 9.7510e-05 - NMSE: 8.8149e-04 - tot_time: 0h 10m 21.0s\n",
      "\n",
      "Epoch 65: val_NMSE improved from 0.00153 to 0.00147, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 529ms/step - loss: 2.8059e-04 - mse: 9.7510e-05 - NMSE: 8.8149e-04 - val_loss: 3.4512e-04 - val_mse: 1.6312e-04 - val_NMSE: 0.0015\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7354e-04 - mse: 9.2563e-05 - NMSE: 8.3677e-04 - tot_time: 0h 10m 30.9s\n",
      "\n",
      "Epoch 66: val_NMSE improved from 0.00147 to 0.00145, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 2.7354e-04 - mse: 9.2563e-05 - NMSE: 8.3677e-04 - val_loss: 3.4044e-04 - val_mse: 1.6069e-04 - val_NMSE: 0.0015\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.6346e-04 - mse: 2.8483e-04 - NMSE: 0.0026 - tot_time: 0h 10m 40.5s\n",
      "\n",
      "Epoch 67: val_NMSE did not improve from 0.00145\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 4.6346e-04 - mse: 2.8483e-04 - NMSE: 0.0026 - val_loss: 4.1114e-04 - val_mse: 2.3350e-04 - val_NMSE: 0.0021\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5446e-04 - mse: 1.7669e-04 - NMSE: 0.0016 - tot_time: 0h 10m 50.2s\n",
      "\n",
      "Epoch 68: val_NMSE did not improve from 0.00145\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 3.5446e-04 - mse: 1.7669e-04 - NMSE: 0.0016 - val_loss: 3.4768e-04 - val_mse: 1.6983e-04 - val_NMSE: 0.0015\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8307e-04 - mse: 1.0552e-04 - NMSE: 9.5393e-04 - tot_time: 0h 10m 60.0s\n",
      "\n",
      "Epoch 69: val_NMSE did not improve from 0.00145\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 2.8307e-04 - mse: 1.0552e-04 - NMSE: 9.5393e-04 - val_loss: 3.4162e-04 - val_mse: 1.6465e-04 - val_NMSE: 0.0015\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6950e-04 - mse: 9.3332e-05 - NMSE: 8.4372e-04 - tot_time: 0h 11m 9.4s\n",
      "\n",
      "Epoch 70: val_NMSE improved from 0.00145 to 0.00142, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 528ms/step - loss: 2.6950e-04 - mse: 9.3332e-05 - NMSE: 8.4372e-04 - val_loss: 3.3187e-04 - val_mse: 1.5670e-04 - val_NMSE: 0.0014\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6596e-04 - mse: 9.1776e-05 - NMSE: 8.2965e-04 - tot_time: 0h 11m 18.9s\n",
      "\n",
      "Epoch 71: val_NMSE did not improve from 0.00142\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 524ms/step - loss: 2.6596e-04 - mse: 9.1776e-05 - NMSE: 8.2965e-04 - val_loss: 3.4768e-04 - val_mse: 1.7465e-04 - val_NMSE: 0.0016\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6843e-04 - mse: 1.9606e-04 - NMSE: 0.0018 - tot_time: 0h 11m 28.3s\n",
      "\n",
      "Epoch 72: val_NMSE did not improve from 0.00142\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 520ms/step - loss: 3.6843e-04 - mse: 1.9606e-04 - NMSE: 0.0018 - val_loss: 3.3589e-04 - val_mse: 1.6389e-04 - val_NMSE: 0.0015\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6935e-04 - mse: 9.7778e-05 - NMSE: 8.8390e-04 - tot_time: 0h 11m 37.9s\n",
      "\n",
      "Epoch 73: val_NMSE improved from 0.00142 to 0.00140, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 530ms/step - loss: 2.6935e-04 - mse: 9.7778e-05 - NMSE: 8.8390e-04 - val_loss: 3.2627e-04 - val_mse: 1.5541e-04 - val_NMSE: 0.0014\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6014e-04 - mse: 9.0090e-05 - NMSE: 8.1441e-04 - tot_time: 0h 11m 46.6s\n",
      "\n",
      "Epoch 74: val_NMSE improved from 0.00140 to 0.00135, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 489ms/step - loss: 2.6014e-04 - mse: 9.0090e-05 - NMSE: 8.1441e-04 - val_loss: 3.1823e-04 - val_mse: 1.4917e-04 - val_NMSE: 0.0013\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.1158e-04 - mse: 2.4347e-04 - NMSE: 0.0022 - tot_time: 0h 11m 56.4s\n",
      "\n",
      "Epoch 75: val_NMSE did not improve from 0.00135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 4.1158e-04 - mse: 2.4347e-04 - NMSE: 0.0022 - val_loss: 4.6305e-04 - val_mse: 2.9580e-04 - val_NMSE: 0.0027\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2356e-04 - mse: 1.5600e-04 - NMSE: 0.0014 - tot_time: 0h 12m 5.8s\n",
      "\n",
      "Epoch 76: val_NMSE did not improve from 0.00135\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 525ms/step - loss: 3.2356e-04 - mse: 1.5600e-04 - NMSE: 0.0014 - val_loss: 3.4868e-04 - val_mse: 1.8091e-04 - val_NMSE: 0.0016\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6587e-04 - mse: 9.8458e-05 - NMSE: 8.9007e-04 - tot_time: 0h 12m 15.1s\n",
      "\n",
      "Epoch 77: val_NMSE did not improve from 0.00135\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 521ms/step - loss: 2.6587e-04 - mse: 9.8458e-05 - NMSE: 8.9007e-04 - val_loss: 3.1639e-04 - val_mse: 1.4957e-04 - val_NMSE: 0.0014\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5240e-04 - mse: 8.6266e-05 - NMSE: 7.7984e-04 - tot_time: 0h 12m 24.5s\n",
      "\n",
      "Epoch 78: val_NMSE improved from 0.00135 to 0.00130, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 522ms/step - loss: 2.5240e-04 - mse: 8.6266e-05 - NMSE: 7.7984e-04 - val_loss: 3.0950e-04 - val_mse: 1.4425e-04 - val_NMSE: 0.0013\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4833e-04 - mse: 8.3957e-05 - NMSE: 7.5897e-04 - tot_time: 0h 12m 33.9s\n",
      "\n",
      "Epoch 79: val_NMSE improved from 0.00130 to 0.00130, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 528ms/step - loss: 2.4833e-04 - mse: 8.3957e-05 - NMSE: 7.5897e-04 - val_loss: 3.0675e-04 - val_mse: 1.4337e-04 - val_NMSE: 0.0013\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4743e-04 - mse: 8.4918e-05 - NMSE: 7.6766e-04 - tot_time: 0h 12m 43.4s\n",
      "\n",
      "Epoch 80: val_NMSE improved from 0.00130 to 0.00128, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 533ms/step - loss: 2.4743e-04 - mse: 8.4918e-05 - NMSE: 7.6766e-04 - val_loss: 3.0340e-04 - val_mse: 1.4188e-04 - val_NMSE: 0.0013\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.8734e-04 - mse: 3.2678e-04 - NMSE: 0.0030 - tot_time: 0h 12m 53.2s\n",
      "\n",
      "Epoch 81: val_NMSE did not improve from 0.00128\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 542ms/step - loss: 4.8734e-04 - mse: 3.2678e-04 - NMSE: 0.0030 - val_loss: 5.8001e-04 - val_mse: 4.2040e-04 - val_NMSE: 0.0038\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4432e-04 - mse: 1.8415e-04 - NMSE: 0.0017 - tot_time: 0h 13m 3.3s\n",
      "\n",
      "Epoch 82: val_NMSE did not improve from 0.00128\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 556ms/step - loss: 3.4432e-04 - mse: 1.8415e-04 - NMSE: 0.0017 - val_loss: 3.2383e-04 - val_mse: 1.6305e-04 - val_NMSE: 0.0015\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6160e-04 - mse: 1.0080e-04 - NMSE: 9.1127e-04 - tot_time: 0h 13m 13.0s\n",
      "\n",
      "Epoch 83: val_NMSE did not improve from 0.00128\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 2.6160e-04 - mse: 1.0080e-04 - NMSE: 9.1127e-04 - val_loss: 3.0538e-04 - val_mse: 1.4484e-04 - val_NMSE: 0.0013\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4774e-04 - mse: 8.7722e-05 - NMSE: 7.9301e-04 - tot_time: 0h 13m 22.0s\n",
      "\n",
      "Epoch 84: val_NMSE improved from 0.00128 to 0.00126, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 506ms/step - loss: 2.4774e-04 - mse: 8.7722e-05 - NMSE: 7.9301e-04 - val_loss: 2.9924e-04 - val_mse: 1.3993e-04 - val_NMSE: 0.0013\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2731e-04 - mse: 1.6866e-04 - NMSE: 0.0015 - tot_time: 0h 13m 31.1s\n",
      "\n",
      "Epoch 85: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 504ms/step - loss: 3.2731e-04 - mse: 1.6866e-04 - NMSE: 0.0015 - val_loss: 4.0345e-04 - val_mse: 2.4519e-04 - val_NMSE: 0.0022\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8010e-04 - mse: 1.2188e-04 - NMSE: 0.0011 - tot_time: 0h 13m 40.3s\n",
      "\n",
      "Epoch 86: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 518ms/step - loss: 2.8010e-04 - mse: 1.2188e-04 - NMSE: 0.0011 - val_loss: 3.2183e-04 - val_mse: 1.6380e-04 - val_NMSE: 0.0015\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4833e-04 - mse: 9.0793e-05 - NMSE: 8.2076e-04 - tot_time: 0h 13m 49.9s\n",
      "\n",
      "Epoch 87: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 536ms/step - loss: 2.4833e-04 - mse: 9.0793e-05 - NMSE: 8.2076e-04 - val_loss: 2.9836e-04 - val_mse: 1.4150e-04 - val_NMSE: 0.0013\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4056e-04 - mse: 8.4390e-05 - NMSE: 7.6289e-04 - tot_time: 0h 13m 59.1s\n",
      "\n",
      "Epoch 88: val_NMSE improved from 0.00126 to 0.00120, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 504ms/step - loss: 2.4056e-04 - mse: 8.4390e-05 - NMSE: 7.6289e-04 - val_loss: 2.8805e-04 - val_mse: 1.3270e-04 - val_NMSE: 0.0012\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4143e-04 - mse: 8.6851e-05 - NMSE: 7.8514e-04 - tot_time: 0h 14m 8.7s\n",
      "\n",
      "Epoch 89: val_NMSE did not improve from 0.00120\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 534ms/step - loss: 2.4143e-04 - mse: 8.6851e-05 - NMSE: 7.8514e-04 - val_loss: 4.4008e-04 - val_mse: 2.8641e-04 - val_NMSE: 0.0026\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.6019e-04 - mse: 3.0646e-04 - NMSE: 0.0028 - tot_time: 0h 14m 17.8s\n",
      "\n",
      "Epoch 90: val_NMSE did not improve from 0.00120\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 505ms/step - loss: 4.6019e-04 - mse: 3.0646e-04 - NMSE: 0.0028 - val_loss: 3.4970e-04 - val_mse: 1.9554e-04 - val_NMSE: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5378e-04 - mse: 9.9483e-05 - NMSE: 8.9932e-04 - tot_time: 0h 14m 27.1s\n",
      "\n",
      "Epoch 91: val_NMSE did not improve from 0.00120\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 521ms/step - loss: 2.5378e-04 - mse: 9.9483e-05 - NMSE: 8.9932e-04 - val_loss: 2.9034e-04 - val_mse: 1.3629e-04 - val_NMSE: 0.0012\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3487e-04 - mse: 8.1317e-05 - NMSE: 7.3510e-04 - tot_time: 0h 14m 36.5s\n",
      "\n",
      "Epoch 92: val_NMSE improved from 0.00120 to 0.00118, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 526ms/step - loss: 2.3487e-04 - mse: 8.1317e-05 - NMSE: 7.3510e-04 - val_loss: 2.8327e-04 - val_mse: 1.3037e-04 - val_NMSE: 0.0012\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2908e-04 - mse: 7.6865e-05 - NMSE: 6.9486e-04 - tot_time: 0h 14m 46.1s\n",
      "\n",
      "Epoch 93: val_NMSE improved from 0.00118 to 0.00116, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 2.2908e-04 - mse: 7.6865e-05 - NMSE: 6.9486e-04 - val_loss: 2.7929e-04 - val_mse: 1.2787e-04 - val_NMSE: 0.0012\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2553e-04 - mse: 7.4833e-05 - NMSE: 6.7649e-04 - tot_time: 0h 14m 55.6s\n",
      "\n",
      "Epoch 94: val_NMSE improved from 0.00116 to 0.00114, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 524ms/step - loss: 2.2553e-04 - mse: 7.4833e-05 - NMSE: 6.7649e-04 - val_loss: 2.7586e-04 - val_mse: 1.2600e-04 - val_NMSE: 0.0011\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2836e-04 - mse: 7.9229e-05 - NMSE: 7.1622e-04 - tot_time: 0h 15m 4.7s\n",
      "\n",
      "Epoch 95: val_NMSE did not improve from 0.00114\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 507ms/step - loss: 2.2836e-04 - mse: 7.9229e-05 - NMSE: 7.1622e-04 - val_loss: 3.3227e-04 - val_mse: 1.8398e-04 - val_NMSE: 0.0017\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0837e-04 - mse: 1.6060e-04 - NMSE: 0.0015 - tot_time: 0h 15m 14.1s\n",
      "\n",
      "Epoch 96: val_NMSE did not improve from 0.00114\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 522ms/step - loss: 3.0837e-04 - mse: 1.6060e-04 - NMSE: 0.0015 - val_loss: 2.9715e-04 - val_mse: 1.4966e-04 - val_NMSE: 0.0014\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3307e-04 - mse: 8.5877e-05 - NMSE: 7.7630e-04 - tot_time: 0h 15m 24.1s\n",
      "\n",
      "Epoch 97: val_NMSE did not improve from 0.00114\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 2.3307e-04 - mse: 8.5877e-05 - NMSE: 7.7630e-04 - val_loss: 2.7361e-04 - val_mse: 1.2691e-04 - val_NMSE: 0.0011\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3635e-04 - mse: 9.0205e-05 - NMSE: 8.1546e-04 - tot_time: 0h 15m 33.2s\n",
      "\n",
      "Epoch 98: val_NMSE did not improve from 0.00114\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 508ms/step - loss: 2.3635e-04 - mse: 9.0205e-05 - NMSE: 8.1546e-04 - val_loss: 4.7463e-04 - val_mse: 3.2915e-04 - val_NMSE: 0.0030\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.8255e-04 - mse: 2.3707e-04 - NMSE: 0.0021 - tot_time: 0h 15m 43.2s\n",
      "\n",
      "Epoch 99: val_NMSE did not improve from 0.00114\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 3.8255e-04 - mse: 2.3707e-04 - NMSE: 0.0021 - val_loss: 3.4319e-04 - val_mse: 1.9746e-04 - val_NMSE: 0.0018\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4091e-04 - mse: 9.5237e-05 - NMSE: 8.6094e-04 - tot_time: 0h 15m 52.1s\n",
      "\n",
      "Epoch 100: val_NMSE did not improve from 0.00114\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 496ms/step - loss: 2.4091e-04 - mse: 9.5237e-05 - NMSE: 8.6094e-04 - val_loss: 2.7711e-04 - val_mse: 1.3171e-04 - val_NMSE: 0.0012\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2290e-04 - mse: 7.7948e-05 - NMSE: 7.0465e-04 - tot_time: 0h 16m 1.9s\n",
      "\n",
      "Epoch 101: val_NMSE improved from 0.00114 to 0.00110, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 2.2290e-04 - mse: 7.7948e-05 - NMSE: 7.0465e-04 - val_loss: 2.6625e-04 - val_mse: 1.2190e-04 - val_NMSE: 0.0011\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0543e-04 - mse: 1.6168e-04 - NMSE: 0.0015   - tot_time: 0h 16m 11.6s\n",
      "\n",
      "Epoch 102: val_NMSE did not improve from 0.00110\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 536ms/step - loss: 3.0543e-04 - mse: 1.6168e-04 - NMSE: 0.0015 - val_loss: 0.0014 - val_mse: 0.0012 - val_NMSE: 0.0111\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.9647e-04 - mse: 3.5289e-04 - NMSE: 0.0032 - tot_time: 0h 16m 21.3s\n",
      "\n",
      "Epoch 103: val_NMSE did not improve from 0.00110\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 542ms/step - loss: 4.9647e-04 - mse: 3.5289e-04 - NMSE: 0.0032 - val_loss: 3.3095e-04 - val_mse: 1.8608e-04 - val_NMSE: 0.0017\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5881e-04 - mse: 1.1319e-04 - NMSE: 0.0010 - tot_time: 0h 16m 30.4s\n",
      "\n",
      "Epoch 104: val_NMSE did not improve from 0.00110\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 513ms/step - loss: 2.5881e-04 - mse: 1.1319e-04 - NMSE: 0.0010 - val_loss: 2.8781e-04 - val_mse: 1.4188e-04 - val_NMSE: 0.0013\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2521e-04 - mse: 7.9540e-05 - NMSE: 7.1903e-04 - tot_time: 0h 16m 39.6s\n",
      "\n",
      "Epoch 105: val_NMSE improved from 0.00110 to 0.00109, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 514ms/step - loss: 2.2521e-04 - mse: 7.9540e-05 - NMSE: 7.1903e-04 - val_loss: 2.6589e-04 - val_mse: 1.2067e-04 - val_NMSE: 0.0011\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1618e-04 - mse: 7.1564e-05 - NMSE: 6.4694e-04 - tot_time: 0h 16m 49.2s\n",
      "\n",
      "Epoch 106: val_NMSE improved from 0.00109 to 0.00106, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 526ms/step - loss: 2.1618e-04 - mse: 7.1564e-05 - NMSE: 6.4694e-04 - val_loss: 2.6150e-04 - val_mse: 1.1763e-04 - val_NMSE: 0.0011\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1283e-04 - mse: 6.9649e-05 - NMSE: 6.2962e-04 - tot_time: 0h 16m 58.3s\n",
      "\n",
      "Epoch 107: val_NMSE improved from 0.00106 to 0.00105, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 508ms/step - loss: 2.1283e-04 - mse: 6.9649e-05 - NMSE: 6.2962e-04 - val_loss: 2.5831e-04 - val_mse: 1.1592e-04 - val_NMSE: 0.0010\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1022e-04 - mse: 6.8533e-05 - NMSE: 6.1954e-04 - tot_time: 0h 17m 7.7s\n",
      "\n",
      "Epoch 108: val_NMSE improved from 0.00105 to 0.00103, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 524ms/step - loss: 2.1022e-04 - mse: 6.8533e-05 - NMSE: 6.1954e-04 - val_loss: 2.5522e-04 - val_mse: 1.1434e-04 - val_NMSE: 0.0010\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0824e-04 - mse: 6.8086e-05 - NMSE: 6.1549e-04 - tot_time: 0h 17m 17.7s\n",
      "\n",
      "Epoch 109: val_NMSE improved from 0.00103 to 0.00102, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 2.0824e-04 - mse: 6.8086e-05 - NMSE: 6.1549e-04 - val_loss: 2.5244e-04 - val_mse: 1.1310e-04 - val_NMSE: 0.0010\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0830e-04 - mse: 6.9660e-05 - NMSE: 6.2972e-04 - tot_time: 0h 17m 27.6s\n",
      "\n",
      "Epoch 110: val_NMSE did not improve from 0.00102\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 558ms/step - loss: 2.0830e-04 - mse: 6.9660e-05 - NMSE: 6.2972e-04 - val_loss: 3.1039e-04 - val_mse: 1.7254e-04 - val_NMSE: 0.0016\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.8131e-04 - mse: 2.4358e-04 - NMSE: 0.0022 - tot_time: 0h 17m 36.7s\n",
      "\n",
      "Epoch 111: val_NMSE did not improve from 0.00102\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 496ms/step - loss: 3.8131e-04 - mse: 2.4358e-04 - NMSE: 0.0022 - val_loss: 3.2058e-04 - val_mse: 1.8261e-04 - val_NMSE: 0.0017\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3976e-04 - mse: 1.0172e-04 - NMSE: 9.1952e-04 - tot_time: 0h 17m 46.6s\n",
      "\n",
      "Epoch 112: val_NMSE did not improve from 0.00102\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 2.3976e-04 - mse: 1.0172e-04 - NMSE: 9.1952e-04 - val_loss: 2.6828e-04 - val_mse: 1.3040e-04 - val_NMSE: 0.0012\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1291e-04 - mse: 7.5398e-05 - NMSE: 6.8159e-04 - tot_time: 0h 17m 56.4s\n",
      "\n",
      "Epoch 113: val_NMSE did not improve from 0.00102\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 2.1291e-04 - mse: 7.5398e-05 - NMSE: 6.8159e-04 - val_loss: 2.5295e-04 - val_mse: 1.1593e-04 - val_NMSE: 0.0010\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0368e-04 - mse: 6.7204e-05 - NMSE: 6.0752e-04 - tot_time: 0h 18m 6.0s\n",
      "\n",
      "Epoch 114: val_NMSE improved from 0.00102 to 0.00100, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 532ms/step - loss: 2.0368e-04 - mse: 6.7204e-05 - NMSE: 6.0752e-04 - val_loss: 2.4680e-04 - val_mse: 1.1098e-04 - val_NMSE: 0.0010\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0089e-04 - mse: 6.5662e-05 - NMSE: 5.9358e-04 - tot_time: 0h 18m 14.7s\n",
      "\n",
      "Epoch 115: val_NMSE did not improve from 0.00100\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 479ms/step - loss: 2.0089e-04 - mse: 6.5662e-05 - NMSE: 5.9358e-04 - val_loss: 2.4567e-04 - val_mse: 1.1113e-04 - val_NMSE: 0.0010\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.2667e-04 - mse: 4.9253e-04 - NMSE: 0.0045 - tot_time: 0h 18m 24.4s\n",
      "\n",
      "Epoch 116: val_NMSE did not improve from 0.00100\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 6.2667e-04 - mse: 4.9253e-04 - NMSE: 0.0045 - val_loss: 3.5894e-04 - val_mse: 2.2420e-04 - val_NMSE: 0.0020\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9559e-04 - mse: 1.5956e-04 - NMSE: 0.0014 - tot_time: 0h 18m 34.3s\n",
      "\n",
      "Epoch 117: val_NMSE did not improve from 0.00100\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 2.9559e-04 - mse: 1.5956e-04 - NMSE: 0.0014 - val_loss: 2.9465e-04 - val_mse: 1.5745e-04 - val_NMSE: 0.0014\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2398e-04 - mse: 8.6655e-05 - NMSE: 7.8335e-04 - tot_time: 0h 18m 44.1s\n",
      "\n",
      "Epoch 118: val_NMSE did not improve from 0.00100\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 2.2398e-04 - mse: 8.6655e-05 - NMSE: 7.8335e-04 - val_loss: 2.5629e-04 - val_mse: 1.1909e-04 - val_NMSE: 0.0011\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0793e-04 - mse: 7.1099e-05 - NMSE: 6.4273e-04 - tot_time: 0h 18m 53.8s\n",
      "\n",
      "Epoch 119: val_NMSE improved from 0.00100 to 0.00100, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 2.0793e-04 - mse: 7.1099e-05 - NMSE: 6.4273e-04 - val_loss: 2.4699e-04 - val_mse: 1.1066e-04 - val_NMSE: 0.0010\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0261e-04 - mse: 6.6844e-05 - NMSE: 6.0427e-04 - tot_time: 0h 19m 3.4s\n",
      "\n",
      "Epoch 120: val_NMSE improved from 0.00100 to 0.00098, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 2.0261e-04 - mse: 6.6844e-05 - NMSE: 6.0427e-04 - val_loss: 2.4300e-04 - val_mse: 1.0787e-04 - val_NMSE: 9.7515e-04\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9957e-04 - mse: 6.5023e-05 - NMSE: 5.8781e-04 - tot_time: 0h 19m 12.5s\n",
      "\n",
      "Epoch 121: val_NMSE improved from 0.00098 to 0.00096, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 502ms/step - loss: 1.9957e-04 - mse: 6.5023e-05 - NMSE: 5.8781e-04 - val_loss: 2.4049e-04 - val_mse: 1.0662e-04 - val_NMSE: 9.6380e-04\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9633e-04 - mse: 6.3074e-05 - NMSE: 5.7019e-04 - tot_time: 0h 19m 22.2s\n",
      "\n",
      "Epoch 122: val_NMSE improved from 0.00096 to 0.00095, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 542ms/step - loss: 1.9633e-04 - mse: 6.3074e-05 - NMSE: 5.7019e-04 - val_loss: 2.3729e-04 - val_mse: 1.0473e-04 - val_NMSE: 9.4678e-04\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9110e-04 - mse: 1.5913e-04 - NMSE: 0.0014 - tot_time: 0h 19m 31.2s\n",
      "\n",
      "Epoch 123: val_NMSE did not improve from 0.00095\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 504ms/step - loss: 2.9110e-04 - mse: 1.5913e-04 - NMSE: 0.0014 - val_loss: 2.5255e-04 - val_mse: 1.2096e-04 - val_NMSE: 0.0011\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3015e-04 - mse: 9.8437e-05 - NMSE: 8.8984e-04 - tot_time: 0h 19m 41.2s\n",
      "\n",
      "Epoch 124: val_NMSE did not improve from 0.00095\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 2.3015e-04 - mse: 9.8437e-05 - NMSE: 8.8984e-04 - val_loss: 2.4310e-04 - val_mse: 1.1137e-04 - val_NMSE: 0.0010\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9992e-04 - mse: 6.8517e-05 - NMSE: 6.1939e-04 - tot_time: 0h 19m 50.6s\n",
      "\n",
      "Epoch 125: val_NMSE improved from 0.00095 to 0.00094, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 533ms/step - loss: 1.9992e-04 - mse: 6.8517e-05 - NMSE: 6.1939e-04 - val_loss: 2.3485e-04 - val_mse: 1.0394e-04 - val_NMSE: 9.3963e-04\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9203e-04 - mse: 6.1634e-05 - NMSE: 5.5717e-04 - tot_time: 0h 20m 0.8s\n",
      "\n",
      "Epoch 126: val_NMSE improved from 0.00094 to 0.00092, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 567ms/step - loss: 1.9203e-04 - mse: 6.1634e-05 - NMSE: 5.5717e-04 - val_loss: 2.3198e-04 - val_mse: 1.0222e-04 - val_NMSE: 9.2403e-04\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8975e-04 - mse: 6.0572e-05 - NMSE: 5.4757e-04 - tot_time: 0h 20m 10.8s\n",
      "\n",
      "Epoch 127: val_NMSE improved from 0.00092 to 0.00090, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 557ms/step - loss: 1.8975e-04 - mse: 6.0572e-05 - NMSE: 5.4757e-04 - val_loss: 2.2843e-04 - val_mse: 9.9920e-05 - val_NMSE: 9.0327e-04\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8581e-04 - mse: 5.7889e-05 - NMSE: 5.2332e-04 - tot_time: 0h 20m 21.0s\n",
      "\n",
      "Epoch 128: val_NMSE improved from 0.00090 to 0.00090, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 558ms/step - loss: 1.8581e-04 - mse: 5.7889e-05 - NMSE: 5.2332e-04 - val_loss: 2.2662e-04 - val_mse: 9.9374e-05 - val_NMSE: 8.9832e-04\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0544e-04 - mse: 7.8790e-05 - NMSE: 7.1226e-04 - tot_time: 0h 20m 31.2s\n",
      "\n",
      "Epoch 129: val_NMSE did not improve from 0.00090\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 573ms/step - loss: 2.0544e-04 - mse: 7.8790e-05 - NMSE: 7.1226e-04 - val_loss: 9.0318e-04 - val_mse: 7.7725e-04 - val_NMSE: 0.0070\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.9717e-04 - mse: 5.7012e-04 - NMSE: 0.0052 - tot_time: 0h 20m 41.2s\n",
      "\n",
      "Epoch 130: val_NMSE did not improve from 0.00090\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 561ms/step - loss: 6.9717e-04 - mse: 5.7012e-04 - NMSE: 0.0052 - val_loss: 3.6148e-04 - val_mse: 2.3262e-04 - val_NMSE: 0.0021\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5283e-04 - mse: 1.2264e-04 - NMSE: 0.0011 - tot_time: 0h 20m 51.6s\n",
      "\n",
      "Epoch 131: val_NMSE did not improve from 0.00090\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 572ms/step - loss: 2.5283e-04 - mse: 1.2264e-04 - NMSE: 0.0011 - val_loss: 2.6730e-04 - val_mse: 1.3638e-04 - val_NMSE: 0.0012\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0674e-04 - mse: 7.5893e-05 - NMSE: 6.8607e-04 - tot_time: 0h 21m 1.6s\n",
      "\n",
      "Epoch 132: val_NMSE did not improve from 0.00090\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 2.0674e-04 - mse: 7.5893e-05 - NMSE: 6.8607e-04 - val_loss: 2.4356e-04 - val_mse: 1.1296e-04 - val_NMSE: 0.0010\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9556e-04 - mse: 6.5365e-05 - NMSE: 5.9090e-04 - tot_time: 0h 21m 11.3s\n",
      "\n",
      "Epoch 133: val_NMSE did not improve from 0.00090\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 1.9556e-04 - mse: 6.5365e-05 - NMSE: 5.9090e-04 - val_loss: 2.3584e-04 - val_mse: 1.0621e-04 - val_NMSE: 9.6011e-04\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9056e-04 - mse: 6.1478e-05 - NMSE: 5.5576e-04 - tot_time: 0h 21m 20.6s\n",
      "\n",
      "Epoch 134: val_NMSE did not improve from 0.00090\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 511ms/step - loss: 1.9056e-04 - mse: 6.1478e-05 - NMSE: 5.5576e-04 - val_loss: 2.3134e-04 - val_mse: 1.0290e-04 - val_NMSE: 9.3020e-04\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8671e-04 - mse: 5.8833e-05 - NMSE: 5.3185e-04 - tot_time: 0h 21m 30.4s\n",
      "\n",
      "Epoch 135: val_NMSE did not improve from 0.00090\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 538ms/step - loss: 1.8671e-04 - mse: 5.8833e-05 - NMSE: 5.3185e-04 - val_loss: 2.2804e-04 - val_mse: 1.0080e-04 - val_NMSE: 9.1121e-04\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8389e-04 - mse: 5.7238e-05 - NMSE: 5.1743e-04 - tot_time: 0h 21m 40.6s\n",
      "\n",
      "Epoch 136: val_NMSE improved from 0.00090 to 0.00090, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 572ms/step - loss: 1.8389e-04 - mse: 5.7238e-05 - NMSE: 5.1743e-04 - val_loss: 2.2514e-04 - val_mse: 9.9153e-05 - val_NMSE: 8.9634e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8164e-04 - mse: 5.6247e-05 - NMSE: 5.0847e-04 - tot_time: 0h 21m 50.4s\n",
      "\n",
      "Epoch 137: val_NMSE improved from 0.00090 to 0.00088, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 549ms/step - loss: 1.8164e-04 - mse: 5.6247e-05 - NMSE: 5.0847e-04 - val_loss: 2.2207e-04 - val_mse: 9.7349e-05 - val_NMSE: 8.8002e-04\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7954e-04 - mse: 5.5413e-05 - NMSE: 5.0093e-04 - tot_time: 0h 22m 0.4s\n",
      "\n",
      "Epoch 138: val_NMSE did not improve from 0.00088\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 563ms/step - loss: 1.7954e-04 - mse: 5.5413e-05 - NMSE: 5.0093e-04 - val_loss: 2.2168e-04 - val_mse: 9.8215e-05 - val_NMSE: 8.8785e-04\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7701e-04 - mse: 5.4137e-05 - NMSE: 4.8939e-04 - tot_time: 0h 22m 10.6s\n",
      "\n",
      "Epoch 139: val_NMSE improved from 0.00088 to 0.00086, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 568ms/step - loss: 1.7701e-04 - mse: 5.4137e-05 - NMSE: 4.8939e-04 - val_loss: 2.1766e-04 - val_mse: 9.5447e-05 - val_NMSE: 8.6282e-04\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5012e-04 - mse: 1.2848e-04 - NMSE: 0.0012     - tot_time: 0h 22m 20.5s\n",
      "\n",
      "Epoch 140: val_NMSE did not improve from 0.00086\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 2.5012e-04 - mse: 1.2848e-04 - NMSE: 0.0012 - val_loss: 6.4056e-04 - val_mse: 5.1959e-04 - val_NMSE: 0.0047\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5837e-04 - mse: 1.3746e-04 - NMSE: 0.0012 - tot_time: 0h 22m 30.0s\n",
      "\n",
      "Epoch 141: val_NMSE did not improve from 0.00086\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 533ms/step - loss: 2.5837e-04 - mse: 1.3746e-04 - NMSE: 0.0012 - val_loss: 2.5010e-04 - val_mse: 1.2909e-04 - val_NMSE: 0.0012\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8692e-04 - mse: 6.5929e-05 - NMSE: 5.9596e-04 - tot_time: 0h 22m 39.9s\n",
      "\n",
      "Epoch 142: val_NMSE did not improve from 0.00086\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 1.8692e-04 - mse: 6.5929e-05 - NMSE: 5.9596e-04 - val_loss: 2.1933e-04 - val_mse: 9.8605e-05 - val_NMSE: 8.9137e-04\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7546e-04 - mse: 5.5152e-05 - NMSE: 4.9857e-04 - tot_time: 0h 22m 48.9s\n",
      "\n",
      "Epoch 143: val_NMSE improved from 0.00086 to 0.00085, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 506ms/step - loss: 1.7546e-04 - mse: 5.5152e-05 - NMSE: 4.9857e-04 - val_loss: 2.1425e-04 - val_mse: 9.4444e-05 - val_NMSE: 8.5376e-04\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7287e-04 - mse: 5.3546e-05 - NMSE: 4.8404e-04 - tot_time: 0h 22m 57.9s\n",
      "\n",
      "Epoch 144: val_NMSE improved from 0.00085 to 0.00083, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 500ms/step - loss: 1.7287e-04 - mse: 5.3546e-05 - NMSE: 4.8404e-04 - val_loss: 2.1089e-04 - val_mse: 9.2126e-05 - val_NMSE: 8.3280e-04\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7084e-04 - mse: 5.2571e-05 - NMSE: 4.7524e-04 - tot_time: 0h 23m 7.2s\n",
      "\n",
      "Epoch 145: val_NMSE improved from 0.00083 to 0.00081, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 521ms/step - loss: 1.7084e-04 - mse: 5.2571e-05 - NMSE: 4.7524e-04 - val_loss: 2.0785e-04 - val_mse: 9.0147e-05 - val_NMSE: 8.1491e-04\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6772e-04 - mse: 1.5048e-04 - NMSE: 0.0014 - tot_time: 0h 23m 16.0s\n",
      "\n",
      "Epoch 146: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 492ms/step - loss: 2.6772e-04 - mse: 1.5048e-04 - NMSE: 0.0014 - val_loss: 4.2509e-04 - val_mse: 3.0821e-04 - val_NMSE: 0.0028\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4170e-04 - mse: 1.2468e-04 - NMSE: 0.0011 - tot_time: 0h 23m 25.6s\n",
      "\n",
      "Epoch 147: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 529ms/step - loss: 2.4170e-04 - mse: 1.2468e-04 - NMSE: 0.0011 - val_loss: 2.3388e-04 - val_mse: 1.1673e-04 - val_NMSE: 0.0011\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8580e-04 - mse: 6.8739e-05 - NMSE: 6.2139e-04 - tot_time: 0h 23m 35.3s\n",
      "\n",
      "Epoch 148: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 1.8580e-04 - mse: 6.8739e-05 - NMSE: 6.2139e-04 - val_loss: 2.1033e-04 - val_mse: 9.3516e-05 - val_NMSE: 8.4536e-04\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7108e-04 - mse: 5.4589e-05 - NMSE: 4.9348e-04 - tot_time: 0h 23m 45.3s\n",
      "\n",
      "Epoch 149: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 564ms/step - loss: 1.7108e-04 - mse: 5.4589e-05 - NMSE: 4.9348e-04 - val_loss: 2.0856e-04 - val_mse: 9.2515e-05 - val_NMSE: 8.3631e-04\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6765e-04 - mse: 5.2036e-05 - NMSE: 4.7039e-04 - tot_time: 0h 23m 55.1s\n",
      "\n",
      "Epoch 150: val_NMSE improved from 0.00081 to 0.00081, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 1.6765e-04 - mse: 5.2036e-05 - NMSE: 4.7039e-04 - val_loss: 2.0431e-04 - val_mse: 8.9207e-05 - val_NMSE: 8.0641e-04\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7016e-04 - mse: 5.5515e-05 - NMSE: 5.0185e-04 - tot_time: 0h 24m 4.1s\n",
      "\n",
      "Epoch 151: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 501ms/step - loss: 1.7016e-04 - mse: 5.5515e-05 - NMSE: 5.0185e-04 - val_loss: 2.8554e-04 - val_mse: 1.7143e-04 - val_NMSE: 0.0015\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 5.0080e-04 - mse: 3.8669e-04 - NMSE: 0.0035 - tot_time: 0h 24m 13.9s\n",
      "\n",
      "Epoch 152: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 5.0080e-04 - mse: 3.8669e-04 - NMSE: 0.0035 - val_loss: 3.1101e-04 - val_mse: 1.9606e-04 - val_NMSE: 0.0018\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0858e-04 - mse: 9.2676e-05 - NMSE: 8.3780e-04 - tot_time: 0h 24m 23.7s\n",
      "\n",
      "Epoch 153: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 2.0858e-04 - mse: 9.2676e-05 - NMSE: 8.3780e-04 - val_loss: 2.2128e-04 - val_mse: 1.0481e-04 - val_NMSE: 9.4744e-04\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7787e-04 - mse: 6.1444e-05 - NMSE: 5.5545e-04 - tot_time: 0h 24m 33.5s\n",
      "\n",
      "Epoch 154: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 549ms/step - loss: 1.7787e-04 - mse: 6.1444e-05 - NMSE: 5.5545e-04 - val_loss: 2.1106e-04 - val_mse: 9.4824e-05 - val_NMSE: 8.5719e-04\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1665e-04 - mse: 1.0071e-04 - NMSE: 9.1021e-04 - tot_time: 0h 24m 43.6s\n",
      "\n",
      "Epoch 155: val_NMSE did not improve from 0.00081\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 2.1665e-04 - mse: 1.0071e-04 - NMSE: 9.1021e-04 - val_loss: 2.0768e-04 - val_mse: 9.2054e-05 - val_NMSE: 8.3215e-04\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7678e-04 - mse: 6.1323e-05 - NMSE: 5.5432e-04 - tot_time: 0h 24m 53.4s\n",
      "\n",
      "Epoch 156: val_NMSE improved from 0.00081 to 0.00080, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 555ms/step - loss: 1.7678e-04 - mse: 6.1323e-05 - NMSE: 5.5432e-04 - val_loss: 2.0394e-04 - val_mse: 8.8731e-05 - val_NMSE: 8.0211e-04\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6516e-04 - mse: 5.0340e-05 - NMSE: 4.5507e-04 - tot_time: 0h 25m 3.1s\n",
      "\n",
      "Epoch 157: val_NMSE improved from 0.00080 to 0.00078, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 535ms/step - loss: 1.6516e-04 - mse: 5.0340e-05 - NMSE: 4.5507e-04 - val_loss: 2.0051e-04 - val_mse: 8.6163e-05 - val_NMSE: 7.7889e-04\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6369e-04 - mse: 4.9789e-05 - NMSE: 4.5009e-04 - tot_time: 0h 25m 12.8s\n",
      "\n",
      "Epoch 158: val_NMSE did not improve from 0.00078\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 1.6369e-04 - mse: 4.9789e-05 - NMSE: 4.5009e-04 - val_loss: 1.9961e-04 - val_mse: 8.6223e-05 - val_NMSE: 7.7943e-04\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6186e-04 - mse: 4.8944e-05 - NMSE: 4.4244e-04 - tot_time: 0h 25m 23.1s\n",
      "\n",
      "Epoch 159: val_NMSE did not improve from 0.00078\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 567ms/step - loss: 1.6186e-04 - mse: 4.8944e-05 - NMSE: 4.4244e-04 - val_loss: 2.0126e-04 - val_mse: 8.8867e-05 - val_NMSE: 8.0333e-04\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.0379e-04 - mse: 3.9152e-04 - NMSE: 0.0035 - tot_time: 0h 25m 33.0s\n",
      "\n",
      "Epoch 160: val_NMSE did not improve from 0.00078\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 5.0379e-04 - mse: 3.9152e-04 - NMSE: 0.0035 - val_loss: 3.1990e-04 - val_mse: 2.0696e-04 - val_NMSE: 0.0019\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3487e-04 - mse: 1.2075e-04 - NMSE: 0.0011 - tot_time: 0h 25m 43.1s\n",
      "\n",
      "Epoch 161: val_NMSE did not improve from 0.00078\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 562ms/step - loss: 2.3487e-04 - mse: 1.2075e-04 - NMSE: 0.0011 - val_loss: 2.1871e-04 - val_mse: 1.0368e-04 - val_NMSE: 9.3724e-04\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7759e-04 - mse: 6.2412e-05 - NMSE: 5.6419e-04 - tot_time: 0h 25m 52.5s\n",
      "\n",
      "Epoch 162: val_NMSE did not improve from 0.00078\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 516ms/step - loss: 1.7759e-04 - mse: 6.2412e-05 - NMSE: 5.6419e-04 - val_loss: 2.0614e-04 - val_mse: 9.1024e-05 - val_NMSE: 8.2283e-04\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6642e-04 - mse: 5.1627e-05 - NMSE: 4.6671e-04 - tot_time: 0h 26m 1.8s\n",
      "\n",
      "Epoch 163: val_NMSE did not improve from 0.00078\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 516ms/step - loss: 1.6642e-04 - mse: 5.1627e-05 - NMSE: 4.6671e-04 - val_loss: 2.0142e-04 - val_mse: 8.7054e-05 - val_NMSE: 7.8695e-04\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6244e-04 - mse: 4.8559e-05 - NMSE: 4.3896e-04 - tot_time: 0h 26m 10.8s\n",
      "\n",
      "Epoch 164: val_NMSE improved from 0.00078 to 0.00077, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 500ms/step - loss: 1.6244e-04 - mse: 4.8559e-05 - NMSE: 4.3896e-04 - val_loss: 1.9886e-04 - val_mse: 8.5533e-05 - val_NMSE: 7.7319e-04\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6012e-04 - mse: 4.7298e-05 - NMSE: 4.2757e-04 - tot_time: 0h 26m 20.4s\n",
      "\n",
      "Epoch 165: val_NMSE improved from 0.00077 to 0.00076, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 529ms/step - loss: 1.6012e-04 - mse: 4.7298e-05 - NMSE: 4.2757e-04 - val_loss: 1.9670e-04 - val_mse: 8.4451e-05 - val_NMSE: 7.6342e-04\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5877e-04 - mse: 4.7022e-05 - NMSE: 4.2507e-04 - tot_time: 0h 26m 29.4s\n",
      "\n",
      "Epoch 166: val_NMSE improved from 0.00076 to 0.00076, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 492ms/step - loss: 1.5877e-04 - mse: 4.7022e-05 - NMSE: 4.2507e-04 - val_loss: 1.9523e-04 - val_mse: 8.4058e-05 - val_NMSE: 7.5986e-04\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.5690e-04 - mse: 4.6217e-05 - NMSE: 4.1779e-04 - tot_time: 0h 26m 39.1s\n",
      "\n",
      "Epoch 167: val_NMSE improved from 0.00076 to 0.00075, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 1.5690e-04 - mse: 4.6217e-05 - NMSE: 4.1779e-04 - val_loss: 1.9266e-04 - val_mse: 8.2538e-05 - val_NMSE: 7.4612e-04\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5796e-04 - mse: 4.8319e-05 - NMSE: 4.3678e-04 - tot_time: 0h 26m 48.4s\n",
      "\n",
      "Epoch 168: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 510ms/step - loss: 1.5796e-04 - mse: 4.8319e-05 - NMSE: 4.3678e-04 - val_loss: 2.2290e-04 - val_mse: 1.1380e-04 - val_NMSE: 0.0010\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9648e-04 - mse: 8.7761e-05 - NMSE: 7.9319e-04 - tot_time: 0h 26m 57.6s\n",
      "\n",
      "Epoch 169: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 516ms/step - loss: 1.9648e-04 - mse: 8.7761e-05 - NMSE: 7.9319e-04 - val_loss: 2.0943e-04 - val_mse: 1.0103e-04 - val_NMSE: 9.1320e-04\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6117e-04 - mse: 5.3029e-05 - NMSE: 4.7934e-04 - tot_time: 0h 27m 7.6s\n",
      "\n",
      "Epoch 170: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 1.6117e-04 - mse: 5.3029e-05 - NMSE: 4.7934e-04 - val_loss: 1.9286e-04 - val_mse: 8.5106e-05 - val_NMSE: 7.6932e-04\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5411e-04 - mse: 4.6758e-05 - NMSE: 4.2267e-04 - tot_time: 0h 27m 16.9s\n",
      "\n",
      "Epoch 171: val_NMSE improved from 0.00075 to 0.00074, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 526ms/step - loss: 1.5411e-04 - mse: 4.6758e-05 - NMSE: 4.2267e-04 - val_loss: 1.8856e-04 - val_mse: 8.1685e-05 - val_NMSE: 7.3840e-04\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4478e-04 - mse: 1.3835e-04 - NMSE: 0.0013   - tot_time: 0h 27m 27.1s\n",
      "\n",
      "Epoch 172: val_NMSE did not improve from 0.00074\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 560ms/step - loss: 2.4478e-04 - mse: 1.3835e-04 - NMSE: 0.0013 - val_loss: 2.8190e-04 - val_mse: 1.7599e-04 - val_NMSE: 0.0016\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2799e-04 - mse: 1.2206e-04 - NMSE: 0.0011 - tot_time: 0h 27m 36.8s\n",
      "\n",
      "Epoch 173: val_NMSE did not improve from 0.00074\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 2.2799e-04 - mse: 1.2206e-04 - NMSE: 0.0011 - val_loss: 2.2794e-04 - val_mse: 1.2177e-04 - val_NMSE: 0.0011\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6530e-04 - mse: 5.9057e-05 - NMSE: 5.3383e-04 - tot_time: 0h 27m 46.5s\n",
      "\n",
      "Epoch 174: val_NMSE did not improve from 0.00074\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 1.6530e-04 - mse: 5.9057e-05 - NMSE: 5.3383e-04 - val_loss: 1.9282e-04 - val_mse: 8.6724e-05 - val_NMSE: 7.8393e-04\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5248e-04 - mse: 4.6646e-05 - NMSE: 4.2166e-04 - tot_time: 0h 27m 56.7s\n",
      "\n",
      "Epoch 175: val_NMSE did not improve from 0.00074\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 571ms/step - loss: 1.5248e-04 - mse: 4.6646e-05 - NMSE: 4.2166e-04 - val_loss: 1.8729e-04 - val_mse: 8.1822e-05 - val_NMSE: 7.3964e-04\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4915e-04 - mse: 4.4069e-05 - NMSE: 3.9837e-04 - tot_time: 0h 28m 6.5s\n",
      "\n",
      "Epoch 176: val_NMSE improved from 0.00074 to 0.00073, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 1.4915e-04 - mse: 4.4069e-05 - NMSE: 3.9837e-04 - val_loss: 1.8500e-04 - val_mse: 8.0368e-05 - val_NMSE: 7.2650e-04\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4878e-04 - mse: 4.4555e-05 - NMSE: 4.0276e-04 - tot_time: 0h 28m 16.1s\n",
      "\n",
      "Epoch 177: val_NMSE improved from 0.00073 to 0.00072, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 531ms/step - loss: 1.4878e-04 - mse: 4.4555e-05 - NMSE: 4.0276e-04 - val_loss: 1.8375e-04 - val_mse: 7.9980e-05 - val_NMSE: 7.2298e-04\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9147e-04 - mse: 1.8807e-04 - NMSE: 0.0017 - tot_time: 0h 28m 25.3s\n",
      "\n",
      "Epoch 178: val_NMSE did not improve from 0.00072\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 511ms/step - loss: 2.9147e-04 - mse: 1.8807e-04 - NMSE: 0.0017 - val_loss: 1.9678e-04 - val_mse: 9.3592e-05 - val_NMSE: 8.4605e-04\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0560e-04 - mse: 1.0195e-04 - NMSE: 9.2161e-04 - tot_time: 0h 28m 34.6s\n",
      "\n",
      "Epoch 179: val_NMSE did not improve from 0.00072\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 511ms/step - loss: 2.0560e-04 - mse: 1.0195e-04 - NMSE: 9.2161e-04 - val_loss: 2.0260e-04 - val_mse: 9.8531e-05 - val_NMSE: 8.9065e-04\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5689e-04 - mse: 5.2790e-05 - NMSE: 4.7720e-04 - tot_time: 0h 28m 43.1s\n",
      "\n",
      "Epoch 180: val_NMSE improved from 0.00072 to 0.00070, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 468ms/step - loss: 1.5689e-04 - mse: 5.2790e-05 - NMSE: 4.7720e-04 - val_loss: 1.8173e-04 - val_mse: 7.7754e-05 - val_NMSE: 7.0287e-04\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4874e-04 - mse: 4.5027e-05 - NMSE: 4.0703e-04 - tot_time: 0h 28m 52.4s\n",
      "\n",
      "Epoch 181: val_NMSE did not improve from 0.00070\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 524ms/step - loss: 1.4874e-04 - mse: 4.5027e-05 - NMSE: 4.0703e-04 - val_loss: 1.8148e-04 - val_mse: 7.8127e-05 - val_NMSE: 7.0624e-04\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4634e-04 - mse: 4.3367e-05 - NMSE: 3.9202e-04 - tot_time: 0h 29m 1.6s\n",
      "\n",
      "Epoch 182: val_NMSE did not improve from 0.00070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 510ms/step - loss: 1.4634e-04 - mse: 4.3367e-05 - NMSE: 3.9202e-04 - val_loss: 1.8416e-04 - val_mse: 8.1624e-05 - val_NMSE: 7.3783e-04\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1656e-04 - mse: 2.1441e-04 - NMSE: 0.0019 - tot_time: 0h 29m 10.3s\n",
      "\n",
      "Epoch 183: val_NMSE did not improve from 0.00070\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 485ms/step - loss: 3.1656e-04 - mse: 2.1441e-04 - NMSE: 0.0019 - val_loss: 4.7109e-04 - val_mse: 3.6927e-04 - val_NMSE: 0.0033\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7850e-04 - mse: 1.7600e-04 - NMSE: 0.0016 - tot_time: 0h 29m 19.9s\n",
      "\n",
      "Epoch 184: val_NMSE did not improve from 0.00070\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 536ms/step - loss: 2.7850e-04 - mse: 1.7600e-04 - NMSE: 0.0016 - val_loss: 2.3043e-04 - val_mse: 1.2704e-04 - val_NMSE: 0.0011\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6904e-04 - mse: 6.5263e-05 - NMSE: 5.8995e-04 - tot_time: 0h 29m 29.8s\n",
      "\n",
      "Epoch 185: val_NMSE did not improve from 0.00070\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 1.6904e-04 - mse: 6.5263e-05 - NMSE: 5.8995e-04 - val_loss: 1.8713e-04 - val_mse: 8.3210e-05 - val_NMSE: 7.5219e-04\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5100e-04 - mse: 4.7222e-05 - NMSE: 4.2687e-04 - tot_time: 0h 29m 39.5s\n",
      "\n",
      "Epoch 186: val_NMSE improved from 0.00070 to 0.00070, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 1.5100e-04 - mse: 4.7222e-05 - NMSE: 4.2687e-04 - val_loss: 1.8070e-04 - val_mse: 7.7184e-05 - val_NMSE: 6.9772e-04\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4543e-04 - mse: 4.2244e-05 - NMSE: 3.8188e-04 - tot_time: 0h 29m 49.5s\n",
      "\n",
      "Epoch 187: val_NMSE improved from 0.00070 to 0.00068, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 1.4543e-04 - mse: 4.2244e-05 - NMSE: 3.8188e-04 - val_loss: 1.7818e-04 - val_mse: 7.5402e-05 - val_NMSE: 6.8161e-04\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4410e-04 - mse: 4.1703e-05 - NMSE: 3.7698e-04 - tot_time: 0h 29m 59.7s\n",
      "\n",
      "Epoch 188: val_NMSE improved from 0.00068 to 0.00068, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 567ms/step - loss: 1.4410e-04 - mse: 4.1703e-05 - NMSE: 3.7698e-04 - val_loss: 1.7674e-04 - val_mse: 7.4790e-05 - val_NMSE: 6.7607e-04\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4292e-04 - mse: 4.1367e-05 - NMSE: 3.7394e-04 - tot_time: 0h 30m 9.6s\n",
      "\n",
      "Epoch 189: val_NMSE improved from 0.00068 to 0.00067, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 1.4292e-04 - mse: 4.1367e-05 - NMSE: 3.7394e-04 - val_loss: 1.7470e-04 - val_mse: 7.3597e-05 - val_NMSE: 6.6529e-04\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4093e-04 - mse: 4.0220e-05 - NMSE: 3.6358e-04 - tot_time: 0h 30m 19.3s\n",
      "\n",
      "Epoch 190: val_NMSE did not improve from 0.00067\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 1.4093e-04 - mse: 4.0220e-05 - NMSE: 3.6358e-04 - val_loss: 1.7519e-04 - val_mse: 7.4920e-05 - val_NMSE: 6.7724e-04\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0170e-04 - mse: 2.0178e-04 - NMSE: 0.0018 - tot_time: 0h 30m 28.7s\n",
      "\n",
      "Epoch 191: val_NMSE did not improve from 0.00067\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 525ms/step - loss: 3.0170e-04 - mse: 2.0178e-04 - NMSE: 0.0018 - val_loss: 3.5809e-04 - val_mse: 2.5829e-04 - val_NMSE: 0.0023\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0391e-04 - mse: 1.0368e-04 - NMSE: 9.3710e-04 - tot_time: 0h 30m 38.5s\n",
      "\n",
      "Epoch 192: val_NMSE did not improve from 0.00067\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 549ms/step - loss: 2.0391e-04 - mse: 1.0368e-04 - NMSE: 9.3710e-04 - val_loss: 1.8605e-04 - val_mse: 8.5460e-05 - val_NMSE: 7.7248e-04\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5457e-04 - mse: 5.3986e-05 - NMSE: 4.8799e-04 - tot_time: 0h 30m 48.7s\n",
      "\n",
      "Epoch 193: val_NMSE did not improve from 0.00067\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 562ms/step - loss: 1.5457e-04 - mse: 5.3986e-05 - NMSE: 4.8799e-04 - val_loss: 1.7574e-04 - val_mse: 7.5319e-05 - val_NMSE: 6.8083e-04\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4157e-04 - mse: 4.1435e-05 - NMSE: 3.7456e-04 - tot_time: 0h 30m 58.2s\n",
      "\n",
      "Epoch 194: val_NMSE improved from 0.00067 to 0.00066, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 525ms/step - loss: 1.4157e-04 - mse: 4.1435e-05 - NMSE: 3.7456e-04 - val_loss: 1.7243e-04 - val_mse: 7.2669e-05 - val_NMSE: 6.5689e-04\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3874e-04 - mse: 3.9356e-05 - NMSE: 3.5576e-04 - tot_time: 0h 31m 7.5s\n",
      "\n",
      "Epoch 195: val_NMSE improved from 0.00066 to 0.00065, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 517ms/step - loss: 1.3874e-04 - mse: 3.9356e-05 - NMSE: 3.5576e-04 - val_loss: 1.7069e-04 - val_mse: 7.1755e-05 - val_NMSE: 6.4862e-04\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3721e-04 - mse: 3.8650e-05 - NMSE: 3.4938e-04 - tot_time: 0h 31m 17.3s\n",
      "\n",
      "Epoch 196: val_NMSE improved from 0.00065 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 555ms/step - loss: 1.3721e-04 - mse: 3.8650e-05 - NMSE: 3.4938e-04 - val_loss: 1.6800e-04 - val_mse: 6.9889e-05 - val_NMSE: 6.3176e-04\n",
      "Epoch 197/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.3715e-04 - mse: 3.9414e-05 - NMSE: 3.5629e-04 - tot_time: 0h 31m 26.8s\n",
      "\n",
      "Epoch 197: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 524ms/step - loss: 1.3715e-04 - mse: 3.9414e-05 - NMSE: 3.5629e-04 - val_loss: 1.8429e-04 - val_mse: 8.6985e-05 - val_NMSE: 7.8632e-04\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1880e-04 - mse: 2.2159e-04 - NMSE: 0.0020 - tot_time: 0h 31m 36.2s\n",
      "\n",
      "Epoch 198: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 520ms/step - loss: 3.1880e-04 - mse: 2.2159e-04 - NMSE: 0.0020 - val_loss: 2.4422e-04 - val_mse: 1.4665e-04 - val_NMSE: 0.0013\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7001e-04 - mse: 7.2029e-05 - NMSE: 6.5113e-04 - tot_time: 0h 31m 45.9s\n",
      "\n",
      "Epoch 199: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 1.7001e-04 - mse: 7.2029e-05 - NMSE: 6.5113e-04 - val_loss: 1.7946e-04 - val_mse: 8.1195e-05 - val_NMSE: 7.3396e-04\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4293e-04 - mse: 4.4759e-05 - NMSE: 4.0460e-04 - tot_time: 0h 31m 54.7s\n",
      "\n",
      "Epoch 200: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 498ms/step - loss: 1.4293e-04 - mse: 4.4759e-05 - NMSE: 4.0460e-04 - val_loss: 1.7176e-04 - val_mse: 7.3822e-05 - val_NMSE: 6.6731e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.001 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3670e-04 - mse: 3.8790e-05 - NMSE: 3.5065e-04 - tot_time: 0h 32m 4.4s\n",
      "\n",
      "Epoch 1: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 535ms/step - loss: 1.3670e-04 - mse: 3.8790e-05 - NMSE: 3.5065e-04 - val_loss: 1.6918e-04 - val_mse: 7.1305e-05 - val_NMSE: 6.4456e-04\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3590e-04 - mse: 3.8060e-05 - NMSE: 3.4405e-04 - tot_time: 0h 32m 14.3s\n",
      "\n",
      "Epoch 2: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 549ms/step - loss: 1.3590e-04 - mse: 3.8060e-05 - NMSE: 3.4405e-04 - val_loss: 1.6888e-04 - val_mse: 7.1085e-05 - val_NMSE: 6.4258e-04\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3558e-04 - mse: 3.7820e-05 - NMSE: 3.4188e-04 - tot_time: 0h 32m 23.5s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 510ms/step - loss: 1.3558e-04 - mse: 3.7820e-05 - NMSE: 3.4188e-04 - val_loss: 1.6860e-04 - val_mse: 7.0890e-05 - val_NMSE: 6.4081e-04\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3537e-04 - mse: 3.7701e-05 - NMSE: 3.4080e-04 - tot_time: 0h 32m 32.9s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 521ms/step - loss: 1.3537e-04 - mse: 3.7701e-05 - NMSE: 3.4080e-04 - val_loss: 1.6840e-04 - val_mse: 7.0772e-05 - val_NMSE: 6.3975e-04\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3525e-04 - mse: 3.7655e-05 - NMSE: 3.4039e-04 - tot_time: 0h 32m 42.4s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 530ms/step - loss: 1.3525e-04 - mse: 3.7655e-05 - NMSE: 3.4039e-04 - val_loss: 1.6819e-04 - val_mse: 7.0646e-05 - val_NMSE: 6.3861e-04\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3508e-04 - mse: 3.7578e-05 - NMSE: 3.3969e-04 - tot_time: 0h 32m 51.1s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 487ms/step - loss: 1.3508e-04 - mse: 3.7578e-05 - NMSE: 3.3969e-04 - val_loss: 1.6800e-04 - val_mse: 7.0534e-05 - val_NMSE: 6.3759e-04\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3483e-04 - mse: 3.7405e-05 - NMSE: 3.3813e-04 - tot_time: 0h 33m 0.9s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 1.3483e-04 - mse: 3.7405e-05 - NMSE: 3.3813e-04 - val_loss: 1.6782e-04 - val_mse: 7.0440e-05 - val_NMSE: 6.3674e-04\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3461e-04 - mse: 3.7269e-05 - NMSE: 3.3690e-04 - tot_time: 0h 33m 10.1s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 512ms/step - loss: 1.3461e-04 - mse: 3.7269e-05 - NMSE: 3.3690e-04 - val_loss: 1.6760e-04 - val_mse: 7.0305e-05 - val_NMSE: 6.3552e-04\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3445e-04 - mse: 3.7202e-05 - NMSE: 3.3630e-04 - tot_time: 0h 33m 19.8s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 542ms/step - loss: 1.3445e-04 - mse: 3.7202e-05 - NMSE: 3.3630e-04 - val_loss: 1.6751e-04 - val_mse: 7.0303e-05 - val_NMSE: 6.3551e-04\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3423e-04 - mse: 3.7067e-05 - NMSE: 3.3508e-04 - tot_time: 0h 33m 29.5s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 1.3423e-04 - mse: 3.7067e-05 - NMSE: 3.3508e-04 - val_loss: 1.6724e-04 - val_mse: 7.0123e-05 - val_NMSE: 6.3388e-04\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3413e-04 - mse: 3.7058e-05 - NMSE: 3.3499e-04 - tot_time: 0h 33m 39.0s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 534ms/step - loss: 1.3413e-04 - mse: 3.7058e-05 - NMSE: 3.3499e-04 - val_loss: 1.6698e-04 - val_mse: 6.9956e-05 - val_NMSE: 6.3237e-04\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3389e-04 - mse: 3.6907e-05 - NMSE: 3.3363e-04 - tot_time: 0h 33m 48.4s\n",
      "\n",
      "Epoch 12: val_NMSE improved from 0.00063 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 530ms/step - loss: 1.3389e-04 - mse: 3.6907e-05 - NMSE: 3.3363e-04 - val_loss: 1.6681e-04 - val_mse: 6.9866e-05 - val_NMSE: 6.3156e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3366e-04 - mse: 3.6759e-05 - NMSE: 3.3229e-04 - tot_time: 0h 33m 58.0s\n",
      "\n",
      "Epoch 13: val_NMSE improved from 0.00063 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 531ms/step - loss: 1.3366e-04 - mse: 3.6759e-05 - NMSE: 3.3229e-04 - val_loss: 1.6668e-04 - val_mse: 6.9833e-05 - val_NMSE: 6.3126e-04\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3356e-04 - mse: 3.6751e-05 - NMSE: 3.3222e-04 - tot_time: 0h 34m 7.5s\n",
      "\n",
      "Epoch 14: val_NMSE improved from 0.00063 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 525ms/step - loss: 1.3356e-04 - mse: 3.6751e-05 - NMSE: 3.3222e-04 - val_loss: 1.6651e-04 - val_mse: 6.9748e-05 - val_NMSE: 6.3049e-04\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3335e-04 - mse: 3.6636e-05 - NMSE: 3.3118e-04 - tot_time: 0h 34m 17.4s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00063 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 1.3335e-04 - mse: 3.6636e-05 - NMSE: 3.3118e-04 - val_loss: 1.6633e-04 - val_mse: 6.9662e-05 - val_NMSE: 6.2971e-04\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3324e-04 - mse: 3.6613e-05 - NMSE: 3.3096e-04 - tot_time: 0h 34m 26.7s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00063 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 511ms/step - loss: 1.3324e-04 - mse: 3.6613e-05 - NMSE: 3.3096e-04 - val_loss: 1.6615e-04 - val_mse: 6.9570e-05 - val_NMSE: 6.2888e-04\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3307e-04 - mse: 3.6542e-05 - NMSE: 3.3033e-04 - tot_time: 0h 34m 36.9s\n",
      "\n",
      "Epoch 17: val_NMSE did not improve from 0.00063\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 563ms/step - loss: 1.3307e-04 - mse: 3.6542e-05 - NMSE: 3.3033e-04 - val_loss: 1.6612e-04 - val_mse: 6.9635e-05 - val_NMSE: 6.2946e-04\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3296e-04 - mse: 3.6516e-05 - NMSE: 3.3009e-04 - tot_time: 0h 34m 47.2s\n",
      "\n",
      "Epoch 18: val_NMSE improved from 0.00063 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 571ms/step - loss: 1.3296e-04 - mse: 3.6516e-05 - NMSE: 3.3009e-04 - val_loss: 1.6580e-04 - val_mse: 6.9410e-05 - val_NMSE: 6.2743e-04\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3268e-04 - mse: 3.6338e-05 - NMSE: 3.2848e-04 - tot_time: 0h 34m 57.2s\n",
      "\n",
      "Epoch 19: val_NMSE improved from 0.00063 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 1.3268e-04 - mse: 3.6338e-05 - NMSE: 3.2848e-04 - val_loss: 1.6562e-04 - val_mse: 6.9323e-05 - val_NMSE: 6.2665e-04\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3249e-04 - mse: 3.6234e-05 - NMSE: 3.2754e-04 - tot_time: 0h 35m 7.8s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00063 to 0.00063, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 11s 582ms/step - loss: 1.3249e-04 - mse: 3.6234e-05 - NMSE: 3.2754e-04 - val_loss: 1.6551e-04 - val_mse: 6.9301e-05 - val_NMSE: 6.2645e-04\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3235e-04 - mse: 3.6192e-05 - NMSE: 3.2716e-04 - tot_time: 0h 35m 17.4s\n",
      "\n",
      "Epoch 21: val_NMSE improved from 0.00063 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 534ms/step - loss: 1.3235e-04 - mse: 3.6192e-05 - NMSE: 3.2716e-04 - val_loss: 1.6512e-04 - val_mse: 6.9008e-05 - val_NMSE: 6.2380e-04\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3216e-04 - mse: 3.6096e-05 - NMSE: 3.2630e-04 - tot_time: 0h 35m 27.5s\n",
      "\n",
      "Epoch 22: val_NMSE did not improve from 0.00062\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 566ms/step - loss: 1.3216e-04 - mse: 3.6096e-05 - NMSE: 3.2630e-04 - val_loss: 1.6506e-04 - val_mse: 6.9043e-05 - val_NMSE: 6.2411e-04\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3197e-04 - mse: 3.6003e-05 - NMSE: 3.2545e-04 - tot_time: 0h 35m 36.9s\n",
      "\n",
      "Epoch 23: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 534ms/step - loss: 1.3197e-04 - mse: 3.6003e-05 - NMSE: 3.2545e-04 - val_loss: 1.6483e-04 - val_mse: 6.8915e-05 - val_NMSE: 6.2296e-04\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3174e-04 - mse: 3.5874e-05 - NMSE: 3.2429e-04 - tot_time: 0h 35m 46.8s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 1.3174e-04 - mse: 3.5874e-05 - NMSE: 3.2429e-04 - val_loss: 1.6473e-04 - val_mse: 6.8914e-05 - val_NMSE: 6.2295e-04\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3178e-04 - mse: 3.6007e-05 - NMSE: 3.2549e-04 - tot_time: 0h 35m 56.5s\n",
      "\n",
      "Epoch 25: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 1.3178e-04 - mse: 3.6007e-05 - NMSE: 3.2549e-04 - val_loss: 1.6449e-04 - val_mse: 6.8766e-05 - val_NMSE: 6.2161e-04\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3148e-04 - mse: 3.5807e-05 - NMSE: 3.2369e-04 - tot_time: 0h 36m 6.5s\n",
      "\n",
      "Epoch 26: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 1.3148e-04 - mse: 3.5807e-05 - NMSE: 3.2369e-04 - val_loss: 1.6433e-04 - val_mse: 6.8707e-05 - val_NMSE: 6.2107e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3129e-04 - mse: 3.5707e-05 - NMSE: 3.2278e-04 - tot_time: 0h 36m 15.5s\n",
      "\n",
      "Epoch 27: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 501ms/step - loss: 1.3129e-04 - mse: 3.5707e-05 - NMSE: 3.2278e-04 - val_loss: 1.6412e-04 - val_mse: 6.8588e-05 - val_NMSE: 6.2000e-04\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3104e-04 - mse: 3.5562e-05 - NMSE: 3.2146e-04 - tot_time: 0h 36m 24.7s\n",
      "\n",
      "Epoch 28: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 511ms/step - loss: 1.3104e-04 - mse: 3.5562e-05 - NMSE: 3.2146e-04 - val_loss: 1.6392e-04 - val_mse: 6.8493e-05 - val_NMSE: 6.1914e-04\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3090e-04 - mse: 3.5522e-05 - NMSE: 3.2110e-04 - tot_time: 0h 36m 34.0s\n",
      "\n",
      "Epoch 29: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 517ms/step - loss: 1.3090e-04 - mse: 3.5522e-05 - NMSE: 3.2110e-04 - val_loss: 1.6376e-04 - val_mse: 6.8427e-05 - val_NMSE: 6.1854e-04\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3091e-04 - mse: 3.5632e-05 - NMSE: 3.2210e-04 - tot_time: 0h 36m 43.7s\n",
      "\n",
      "Epoch 30: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 1.3091e-04 - mse: 3.5632e-05 - NMSE: 3.2210e-04 - val_loss: 1.6362e-04 - val_mse: 6.8389e-05 - val_NMSE: 6.1820e-04\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3063e-04 - mse: 3.5447e-05 - NMSE: 3.2042e-04 - tot_time: 0h 36m 53.3s\n",
      "\n",
      "Epoch 31: val_NMSE did not improve from 0.00062\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 1.3063e-04 - mse: 3.5447e-05 - NMSE: 3.2042e-04 - val_loss: 1.6354e-04 - val_mse: 6.8416e-05 - val_NMSE: 6.1845e-04\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3049e-04 - mse: 3.5413e-05 - NMSE: 3.2012e-04 - tot_time: 0h 37m 3.6s\n",
      "\n",
      "Epoch 32: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 577ms/step - loss: 1.3049e-04 - mse: 3.5413e-05 - NMSE: 3.2012e-04 - val_loss: 1.6320e-04 - val_mse: 6.8171e-05 - val_NMSE: 6.1623e-04\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3035e-04 - mse: 3.5376e-05 - NMSE: 3.1978e-04 - tot_time: 0h 37m 13.7s\n",
      "\n",
      "Epoch 33: val_NMSE improved from 0.00062 to 0.00062, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 555ms/step - loss: 1.3035e-04 - mse: 3.5376e-05 - NMSE: 3.1978e-04 - val_loss: 1.6302e-04 - val_mse: 6.8094e-05 - val_NMSE: 6.1553e-04\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3006e-04 - mse: 3.5185e-05 - NMSE: 3.1806e-04 - tot_time: 0h 37m 24.1s\n",
      "\n",
      "Epoch 34: val_NMSE improved from 0.00062 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 579ms/step - loss: 1.3006e-04 - mse: 3.5185e-05 - NMSE: 3.1806e-04 - val_loss: 1.6278e-04 - val_mse: 6.7960e-05 - val_NMSE: 6.1433e-04\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2992e-04 - mse: 3.5148e-05 - NMSE: 3.1772e-04 - tot_time: 0h 37m 34.5s\n",
      "\n",
      "Epoch 35: val_NMSE did not improve from 0.00061\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 575ms/step - loss: 1.2992e-04 - mse: 3.5148e-05 - NMSE: 3.1772e-04 - val_loss: 1.6271e-04 - val_mse: 6.7995e-05 - val_NMSE: 6.1464e-04\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2977e-04 - mse: 3.5097e-05 - NMSE: 3.1726e-04 - tot_time: 0h 37m 43.5s\n",
      "\n",
      "Epoch 36: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 497ms/step - loss: 1.2977e-04 - mse: 3.5097e-05 - NMSE: 3.1726e-04 - val_loss: 1.6243e-04 - val_mse: 6.7818e-05 - val_NMSE: 6.1304e-04\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2964e-04 - mse: 3.5070e-05 - NMSE: 3.1702e-04 - tot_time: 0h 37m 52.6s\n",
      "\n",
      "Epoch 37: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 504ms/step - loss: 1.2964e-04 - mse: 3.5070e-05 - NMSE: 3.1702e-04 - val_loss: 1.6223e-04 - val_mse: 6.7720e-05 - val_NMSE: 6.1216e-04\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2932e-04 - mse: 3.4859e-05 - NMSE: 3.1511e-04 - tot_time: 0h 38m 2.8s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 566ms/step - loss: 1.2932e-04 - mse: 3.4859e-05 - NMSE: 3.1511e-04 - val_loss: 1.6212e-04 - val_mse: 6.7710e-05 - val_NMSE: 6.1206e-04\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2920e-04 - mse: 3.4844e-05 - NMSE: 3.1497e-04 - tot_time: 0h 38m 12.7s\n",
      "\n",
      "Epoch 39: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 1.2920e-04 - mse: 3.4844e-05 - NMSE: 3.1497e-04 - val_loss: 1.6187e-04 - val_mse: 6.7565e-05 - val_NMSE: 6.1075e-04\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2911e-04 - mse: 3.4852e-05 - NMSE: 3.1505e-04 - tot_time: 0h 38m 21.9s\n",
      "\n",
      "Epoch 40: val_NMSE did not improve from 0.00061\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 512ms/step - loss: 1.2911e-04 - mse: 3.4852e-05 - NMSE: 3.1505e-04 - val_loss: 1.6185e-04 - val_mse: 6.7652e-05 - val_NMSE: 6.1153e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2887e-04 - mse: 3.4718e-05 - NMSE: 3.1384e-04 - tot_time: 0h 38m 32.1s\n",
      "\n",
      "Epoch 41: val_NMSE did not improve from 0.00061\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 570ms/step - loss: 1.2887e-04 - mse: 3.4718e-05 - NMSE: 3.1384e-04 - val_loss: 1.6170e-04 - val_mse: 6.7609e-05 - val_NMSE: 6.1115e-04\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2870e-04 - mse: 3.4663e-05 - NMSE: 3.1334e-04 - tot_time: 0h 38m 41.5s\n",
      "\n",
      "Epoch 42: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 525ms/step - loss: 1.2870e-04 - mse: 3.4663e-05 - NMSE: 3.1334e-04 - val_loss: 1.6133e-04 - val_mse: 6.7348e-05 - val_NMSE: 6.0879e-04\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2854e-04 - mse: 3.4599e-05 - NMSE: 3.1276e-04 - tot_time: 0h 38m 50.9s\n",
      "\n",
      "Epoch 43: val_NMSE did not improve from 0.00061\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 526ms/step - loss: 1.2854e-04 - mse: 3.4599e-05 - NMSE: 3.1276e-04 - val_loss: 1.6125e-04 - val_mse: 6.7369e-05 - val_NMSE: 6.0897e-04\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2842e-04 - mse: 3.4592e-05 - NMSE: 3.1270e-04 - tot_time: 0h 39m 0.3s\n",
      "\n",
      "Epoch 44: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 530ms/step - loss: 1.2842e-04 - mse: 3.4592e-05 - NMSE: 3.1270e-04 - val_loss: 1.6103e-04 - val_mse: 6.7261e-05 - val_NMSE: 6.0800e-04\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2816e-04 - mse: 3.4438e-05 - NMSE: 3.1130e-04 - tot_time: 0h 39m 9.9s\n",
      "\n",
      "Epoch 45: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 1.2816e-04 - mse: 3.4438e-05 - NMSE: 3.1130e-04 - val_loss: 1.6077e-04 - val_mse: 6.7108e-05 - val_NMSE: 6.0661e-04\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2797e-04 - mse: 3.4349e-05 - NMSE: 3.1050e-04 - tot_time: 0h 39m 20.1s\n",
      "\n",
      "Epoch 46: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 567ms/step - loss: 1.2797e-04 - mse: 3.4349e-05 - NMSE: 3.1050e-04 - val_loss: 1.6064e-04 - val_mse: 6.7082e-05 - val_NMSE: 6.0639e-04\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2783e-04 - mse: 3.4320e-05 - NMSE: 3.1024e-04 - tot_time: 0h 39m 29.1s\n",
      "\n",
      "Epoch 47: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 492ms/step - loss: 1.2783e-04 - mse: 3.4320e-05 - NMSE: 3.1024e-04 - val_loss: 1.6046e-04 - val_mse: 6.7007e-05 - val_NMSE: 6.0571e-04\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2761e-04 - mse: 3.4210e-05 - NMSE: 3.0925e-04 - tot_time: 0h 39m 39.1s\n",
      "\n",
      "Epoch 48: val_NMSE improved from 0.00061 to 0.00061, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 1.2761e-04 - mse: 3.4210e-05 - NMSE: 3.0925e-04 - val_loss: 1.6027e-04 - val_mse: 6.6930e-05 - val_NMSE: 6.0501e-04\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2746e-04 - mse: 3.4172e-05 - NMSE: 3.0890e-04 - tot_time: 0h 39m 48.9s\n",
      "\n",
      "Epoch 49: val_NMSE improved from 0.00061 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 1.2746e-04 - mse: 3.4172e-05 - NMSE: 3.0890e-04 - val_loss: 1.6013e-04 - val_mse: 6.6901e-05 - val_NMSE: 6.0475e-04\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2721e-04 - mse: 3.4026e-05 - NMSE: 3.0758e-04 - tot_time: 0h 39m 58.9s\n",
      "\n",
      "Epoch 50: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 1.2721e-04 - mse: 3.4026e-05 - NMSE: 3.0758e-04 - val_loss: 1.5989e-04 - val_mse: 6.6768e-05 - val_NMSE: 6.0354e-04\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2701e-04 - mse: 3.3933e-05 - NMSE: 3.0674e-04 - tot_time: 0h 40m 8.8s\n",
      "\n",
      "Epoch 51: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 560ms/step - loss: 1.2701e-04 - mse: 3.3933e-05 - NMSE: 3.0674e-04 - val_loss: 1.5967e-04 - val_mse: 6.6654e-05 - val_NMSE: 6.0251e-04\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2690e-04 - mse: 3.3937e-05 - NMSE: 3.0677e-04 - tot_time: 0h 40m 18.6s\n",
      "\n",
      "Epoch 52: val_NMSE did not improve from 0.00060\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 542ms/step - loss: 1.2690e-04 - mse: 3.3937e-05 - NMSE: 3.0677e-04 - val_loss: 1.5957e-04 - val_mse: 6.6662e-05 - val_NMSE: 6.0259e-04\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2675e-04 - mse: 3.3896e-05 - NMSE: 3.0641e-04 - tot_time: 0h 40m 29.0s\n",
      "\n",
      "Epoch 53: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 576ms/step - loss: 1.2675e-04 - mse: 3.3896e-05 - NMSE: 3.0641e-04 - val_loss: 1.5929e-04 - val_mse: 6.6494e-05 - val_NMSE: 6.0107e-04\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2655e-04 - mse: 3.3809e-05 - NMSE: 3.0562e-04 - tot_time: 0h 40m 38.7s\n",
      "\n",
      "Epoch 54: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 549ms/step - loss: 1.2655e-04 - mse: 3.3809e-05 - NMSE: 3.0562e-04 - val_loss: 1.5917e-04 - val_mse: 6.6486e-05 - val_NMSE: 6.0100e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2646e-04 - mse: 3.3822e-05 - NMSE: 3.0574e-04 - tot_time: 0h 40m 49.1s\n",
      "\n",
      "Epoch 55: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 573ms/step - loss: 1.2646e-04 - mse: 3.3822e-05 - NMSE: 3.0574e-04 - val_loss: 1.5890e-04 - val_mse: 6.6331e-05 - val_NMSE: 5.9959e-04\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2616e-04 - mse: 3.3641e-05 - NMSE: 3.0410e-04 - tot_time: 0h 40m 58.5s\n",
      "\n",
      "Epoch 56: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 521ms/step - loss: 1.2616e-04 - mse: 3.3641e-05 - NMSE: 3.0410e-04 - val_loss: 1.5870e-04 - val_mse: 6.6242e-05 - val_NMSE: 5.9879e-04\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2610e-04 - mse: 3.3692e-05 - NMSE: 3.0456e-04 - tot_time: 0h 41m 8.4s\n",
      "\n",
      "Epoch 57: val_NMSE did not improve from 0.00060\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 1.2610e-04 - mse: 3.3692e-05 - NMSE: 3.0456e-04 - val_loss: 1.5865e-04 - val_mse: 6.6299e-05 - val_NMSE: 5.9930e-04\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2579e-04 - mse: 3.3490e-05 - NMSE: 3.0273e-04 - tot_time: 0h 41m 18.4s\n",
      "\n",
      "Epoch 58: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 563ms/step - loss: 1.2579e-04 - mse: 3.3490e-05 - NMSE: 3.0273e-04 - val_loss: 1.5823e-04 - val_mse: 6.5992e-05 - val_NMSE: 5.9653e-04\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2573e-04 - mse: 3.3537e-05 - NMSE: 3.0316e-04 - tot_time: 0h 41m 28.5s\n",
      "\n",
      "Epoch 59: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 1.2573e-04 - mse: 3.3537e-05 - NMSE: 3.0316e-04 - val_loss: 1.5811e-04 - val_mse: 6.5980e-05 - val_NMSE: 5.9642e-04\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2550e-04 - mse: 3.3423e-05 - NMSE: 3.0213e-04 - tot_time: 0h 41m 38.3s\n",
      "\n",
      "Epoch 60: val_NMSE improved from 0.00060 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 1.2550e-04 - mse: 3.3423e-05 - NMSE: 3.0213e-04 - val_loss: 1.5790e-04 - val_mse: 6.5886e-05 - val_NMSE: 5.9557e-04\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2538e-04 - mse: 3.3411e-05 - NMSE: 3.0202e-04 - tot_time: 0h 41m 48.8s\n",
      "\n",
      "Epoch 61: val_NMSE improved from 0.00060 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 582ms/step - loss: 1.2538e-04 - mse: 3.3411e-05 - NMSE: 3.0202e-04 - val_loss: 1.5763e-04 - val_mse: 6.5725e-05 - val_NMSE: 5.9411e-04\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2505e-04 - mse: 3.3200e-05 - NMSE: 3.0012e-04 - tot_time: 0h 41m 58.7s\n",
      "\n",
      "Epoch 62: val_NMSE improved from 0.00059 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 1.2505e-04 - mse: 3.3200e-05 - NMSE: 3.0012e-04 - val_loss: 1.5737e-04 - val_mse: 6.5578e-05 - val_NMSE: 5.9278e-04\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2491e-04 - mse: 3.3165e-05 - NMSE: 2.9980e-04 - tot_time: 0h 42m 8.5s\n",
      "\n",
      "Epoch 63: val_NMSE did not improve from 0.00059\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 1.2491e-04 - mse: 3.3165e-05 - NMSE: 2.9980e-04 - val_loss: 1.5727e-04 - val_mse: 6.5586e-05 - val_NMSE: 5.9286e-04\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2471e-04 - mse: 3.3084e-05 - NMSE: 2.9907e-04 - tot_time: 0h 42m 18.2s\n",
      "\n",
      "Epoch 64: val_NMSE did not improve from 0.00059\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 536ms/step - loss: 1.2471e-04 - mse: 3.3084e-05 - NMSE: 2.9907e-04 - val_loss: 1.5723e-04 - val_mse: 6.5665e-05 - val_NMSE: 5.9357e-04\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2461e-04 - mse: 3.3087e-05 - NMSE: 2.9909e-04 - tot_time: 0h 42m 27.5s\n",
      "\n",
      "Epoch 65: val_NMSE improved from 0.00059 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 524ms/step - loss: 1.2461e-04 - mse: 3.3087e-05 - NMSE: 2.9909e-04 - val_loss: 1.5688e-04 - val_mse: 6.5421e-05 - val_NMSE: 5.9137e-04\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2437e-04 - mse: 3.2967e-05 - NMSE: 2.9800e-04 - tot_time: 0h 42m 37.1s\n",
      "\n",
      "Epoch 66: val_NMSE improved from 0.00059 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 529ms/step - loss: 1.2437e-04 - mse: 3.2967e-05 - NMSE: 2.9800e-04 - val_loss: 1.5670e-04 - val_mse: 6.5351e-05 - val_NMSE: 5.9073e-04\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2423e-04 - mse: 3.2941e-05 - NMSE: 2.9777e-04 - tot_time: 0h 42m 47.4s\n",
      "\n",
      "Epoch 67: val_NMSE improved from 0.00059 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 566ms/step - loss: 1.2423e-04 - mse: 3.2941e-05 - NMSE: 2.9777e-04 - val_loss: 1.5658e-04 - val_mse: 6.5348e-05 - val_NMSE: 5.9071e-04\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2411e-04 - mse: 3.2928e-05 - NMSE: 2.9765e-04 - tot_time: 0h 42m 57.6s\n",
      "\n",
      "Epoch 68: val_NMSE improved from 0.00059 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 570ms/step - loss: 1.2411e-04 - mse: 3.2928e-05 - NMSE: 2.9765e-04 - val_loss: 1.5631e-04 - val_mse: 6.5194e-05 - val_NMSE: 5.8931e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2385e-04 - mse: 3.2784e-05 - NMSE: 2.9635e-04 - tot_time: 0h 43m 7.5s\n",
      "\n",
      "Epoch 69: val_NMSE did not improve from 0.00059\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 543ms/step - loss: 1.2385e-04 - mse: 3.2784e-05 - NMSE: 2.9635e-04 - val_loss: 1.5628e-04 - val_mse: 6.5271e-05 - val_NMSE: 5.9001e-04\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2367e-04 - mse: 3.2707e-05 - NMSE: 2.9566e-04 - tot_time: 0h 43m 17.5s\n",
      "\n",
      "Epoch 70: val_NMSE improved from 0.00059 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 1.2367e-04 - mse: 3.2707e-05 - NMSE: 2.9566e-04 - val_loss: 1.5599e-04 - val_mse: 6.5096e-05 - val_NMSE: 5.8843e-04\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2353e-04 - mse: 3.2685e-05 - NMSE: 2.9546e-04 - tot_time: 0h 43m 27.5s\n",
      "\n",
      "Epoch 71: val_NMSE improved from 0.00059 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 1.2353e-04 - mse: 3.2685e-05 - NMSE: 2.9546e-04 - val_loss: 1.5583e-04 - val_mse: 6.5044e-05 - val_NMSE: 5.8795e-04\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2339e-04 - mse: 3.2656e-05 - NMSE: 2.9520e-04 - tot_time: 0h 43m 37.1s\n",
      "\n",
      "Epoch 72: val_NMSE improved from 0.00059 to 0.00059, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 1.2339e-04 - mse: 3.2656e-05 - NMSE: 2.9520e-04 - val_loss: 1.5547e-04 - val_mse: 6.4801e-05 - val_NMSE: 5.8576e-04\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2316e-04 - mse: 3.2539e-05 - NMSE: 2.9414e-04 - tot_time: 0h 43m 47.4s\n",
      "\n",
      "Epoch 73: val_NMSE improved from 0.00059 to 0.00058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 566ms/step - loss: 1.2316e-04 - mse: 3.2539e-05 - NMSE: 2.9414e-04 - val_loss: 1.5526e-04 - val_mse: 6.4702e-05 - val_NMSE: 5.8486e-04\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2293e-04 - mse: 3.2423e-05 - NMSE: 2.9308e-04 - tot_time: 0h 43m 57.4s\n",
      "\n",
      "Epoch 74: val_NMSE improved from 0.00058 to 0.00058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 1.2293e-04 - mse: 3.2423e-05 - NMSE: 2.9308e-04 - val_loss: 1.5498e-04 - val_mse: 6.4531e-05 - val_NMSE: 5.8332e-04\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2278e-04 - mse: 3.2383e-05 - NMSE: 2.9273e-04 - tot_time: 0h 44m 7.0s\n",
      "\n",
      "Epoch 75: val_NMSE improved from 0.00058 to 0.00058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 1.2278e-04 - mse: 3.2383e-05 - NMSE: 2.9273e-04 - val_loss: 1.5483e-04 - val_mse: 6.4489e-05 - val_NMSE: 5.8294e-04\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2262e-04 - mse: 3.2326e-05 - NMSE: 2.9221e-04 - tot_time: 0h 44m 16.9s\n",
      "\n",
      "Epoch 76: val_NMSE improved from 0.00058 to 0.00058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 1.2262e-04 - mse: 3.2326e-05 - NMSE: 2.9221e-04 - val_loss: 1.5466e-04 - val_mse: 6.4428e-05 - val_NMSE: 5.8239e-04\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2239e-04 - mse: 3.2209e-05 - NMSE: 2.9115e-04 - tot_time: 0h 44m 26.9s\n",
      "\n",
      "Epoch 77: val_NMSE improved from 0.00058 to 0.00058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 556ms/step - loss: 1.2239e-04 - mse: 3.2209e-05 - NMSE: 2.9115e-04 - val_loss: 1.5442e-04 - val_mse: 6.4302e-05 - val_NMSE: 5.8125e-04\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2236e-04 - mse: 3.2297e-05 - NMSE: 2.9195e-04 - tot_time: 0h 44m 36.9s\n",
      "\n",
      "Epoch 78: val_NMSE did not improve from 0.00058\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 548ms/step - loss: 1.2236e-04 - mse: 3.2297e-05 - NMSE: 2.9195e-04 - val_loss: 1.5433e-04 - val_mse: 6.4326e-05 - val_NMSE: 5.8147e-04\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2209e-04 - mse: 3.2131e-05 - NMSE: 2.9045e-04 - tot_time: 0h 44m 46.9s\n",
      "\n",
      "Epoch 79: val_NMSE did not improve from 0.00058\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 1.2209e-04 - mse: 3.2131e-05 - NMSE: 2.9045e-04 - val_loss: 1.5441e-04 - val_mse: 6.4510e-05 - val_NMSE: 5.8313e-04\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2192e-04 - mse: 3.2071e-05 - NMSE: 2.8991e-04 - tot_time: 0h 44m 57.0s\n",
      "\n",
      "Epoch 80: val_NMSE improved from 0.00058 to 0.00058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 565ms/step - loss: 1.2192e-04 - mse: 3.2071e-05 - NMSE: 2.8991e-04 - val_loss: 1.5377e-04 - val_mse: 6.3979e-05 - val_NMSE: 5.7833e-04\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2182e-04 - mse: 3.2078e-05 - NMSE: 2.8997e-04 - tot_time: 0h 45m 7.3s\n",
      "\n",
      "Epoch 81: val_NMSE did not improve from 0.00058\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 570ms/step - loss: 1.2182e-04 - mse: 3.2078e-05 - NMSE: 2.8997e-04 - val_loss: 1.5373e-04 - val_mse: 6.4049e-05 - val_NMSE: 5.7896e-04\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2160e-04 - mse: 3.1971e-05 - NMSE: 2.8901e-04 - tot_time: 0h 45m 17.0s\n",
      "\n",
      "Epoch 82: val_NMSE improved from 0.00058 to 0.00058, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 543ms/step - loss: 1.2160e-04 - mse: 3.1971e-05 - NMSE: 2.8901e-04 - val_loss: 1.5333e-04 - val_mse: 6.3752e-05 - val_NMSE: 5.7627e-04\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.2148e-04 - mse: 3.1958e-05 - NMSE: 2.8888e-04 - tot_time: 0h 45m 26.8s\n",
      "\n",
      "Epoch 83: val_NMSE improved from 0.00058 to 0.00057, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 557ms/step - loss: 1.2148e-04 - mse: 3.1958e-05 - NMSE: 2.8888e-04 - val_loss: 1.5307e-04 - val_mse: 6.3600e-05 - val_NMSE: 5.7490e-04\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2119e-04 - mse: 3.1767e-05 - NMSE: 2.8715e-04 - tot_time: 0h 45m 37.0s\n",
      "\n",
      "Epoch 84: val_NMSE improved from 0.00057 to 0.00057, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 568ms/step - loss: 1.2119e-04 - mse: 3.1767e-05 - NMSE: 2.8715e-04 - val_loss: 1.5290e-04 - val_mse: 6.3543e-05 - val_NMSE: 5.7439e-04\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2113e-04 - mse: 3.1815e-05 - NMSE: 2.8759e-04 - tot_time: 0h 45m 47.3s\n",
      "\n",
      "Epoch 85: val_NMSE improved from 0.00057 to 0.00057, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 573ms/step - loss: 1.2113e-04 - mse: 3.1815e-05 - NMSE: 2.8759e-04 - val_loss: 1.5258e-04 - val_mse: 6.3323e-05 - val_NMSE: 5.7240e-04\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2112e-04 - mse: 3.1911e-05 - NMSE: 2.8846e-04 - tot_time: 0h 45m 57.2s\n",
      "\n",
      "Epoch 86: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 1.2112e-04 - mse: 3.1911e-05 - NMSE: 2.8846e-04 - val_loss: 1.5258e-04 - val_mse: 6.3427e-05 - val_NMSE: 5.7334e-04\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2112e-04 - mse: 3.2013e-05 - NMSE: 2.8938e-04 - tot_time: 0h 46m 7.4s\n",
      "\n",
      "Epoch 87: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 569ms/step - loss: 1.2112e-04 - mse: 3.2013e-05 - NMSE: 2.8938e-04 - val_loss: 1.5295e-04 - val_mse: 6.3897e-05 - val_NMSE: 5.7759e-04\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2115e-04 - mse: 3.2155e-05 - NMSE: 2.9066e-04 - tot_time: 0h 46m 16.6s\n",
      "\n",
      "Epoch 88: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 508ms/step - loss: 1.2115e-04 - mse: 3.2155e-05 - NMSE: 2.9066e-04 - val_loss: 1.5423e-04 - val_mse: 6.5279e-05 - val_NMSE: 5.9008e-04\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2105e-04 - mse: 3.2149e-05 - NMSE: 2.9061e-04 - tot_time: 0h 46m 26.4s\n",
      "\n",
      "Epoch 89: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 1.2105e-04 - mse: 3.2149e-05 - NMSE: 2.9061e-04 - val_loss: 1.5290e-04 - val_mse: 6.4063e-05 - val_NMSE: 5.7908e-04\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2364e-04 - mse: 3.4854e-05 - NMSE: 3.1506e-04 - tot_time: 0h 46m 36.5s\n",
      "\n",
      "Epoch 90: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 1.2364e-04 - mse: 3.4854e-05 - NMSE: 3.1506e-04 - val_loss: 1.5318e-04 - val_mse: 6.4446e-05 - val_NMSE: 5.8255e-04\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2116e-04 - mse: 3.2477e-05 - NMSE: 2.9358e-04 - tot_time: 0h 46m 46.2s\n",
      "\n",
      "Epoch 91: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 1.2116e-04 - mse: 3.2477e-05 - NMSE: 2.9358e-04 - val_loss: 1.5342e-04 - val_mse: 6.4795e-05 - val_NMSE: 5.8570e-04\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2051e-04 - mse: 3.1931e-05 - NMSE: 2.8864e-04 - tot_time: 0h 46m 55.9s\n",
      "\n",
      "Epoch 92: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 1.2051e-04 - mse: 3.1931e-05 - NMSE: 2.8864e-04 - val_loss: 1.5410e-04 - val_mse: 6.5569e-05 - val_NMSE: 5.9270e-04\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2443e-04 - mse: 3.5950e-05 - NMSE: 3.2497e-04 - tot_time: 0h 47m 5.9s\n",
      "\n",
      "Epoch 93: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 1.2443e-04 - mse: 3.5950e-05 - NMSE: 3.2497e-04 - val_loss: 1.5319e-04 - val_mse: 6.4770e-05 - val_NMSE: 5.8547e-04\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2077e-04 - mse: 3.2406e-05 - NMSE: 2.9293e-04 - tot_time: 0h 47m 15.8s\n",
      "\n",
      "Epoch 94: val_NMSE improved from 0.00057 to 0.00057, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 1.2077e-04 - mse: 3.2406e-05 - NMSE: 2.9293e-04 - val_loss: 1.5094e-04 - val_mse: 6.2626e-05 - val_NMSE: 5.6610e-04\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2007e-04 - mse: 3.1798e-05 - NMSE: 2.8744e-04 - tot_time: 0h 47m 25.8s\n",
      "\n",
      "Epoch 95: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 1.2007e-04 - mse: 3.1798e-05 - NMSE: 2.8744e-04 - val_loss: 1.5110e-04 - val_mse: 6.2881e-05 - val_NMSE: 5.6841e-04\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2453e-04 - mse: 3.6358e-05 - NMSE: 3.2866e-04 - tot_time: 0h 47m 35.7s\n",
      "\n",
      "Epoch 96: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 1.2453e-04 - mse: 3.6358e-05 - NMSE: 3.2866e-04 - val_loss: 1.6071e-04 - val_mse: 7.2587e-05 - val_NMSE: 6.5615e-04\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2114e-04 - mse: 3.3081e-05 - NMSE: 2.9903e-04 - tot_time: 0h 47m 45.7s\n",
      "\n",
      "Epoch 97: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 1.2114e-04 - mse: 3.3081e-05 - NMSE: 2.9903e-04 - val_loss: 1.5159e-04 - val_mse: 6.3590e-05 - val_NMSE: 5.7480e-04\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1938e-04 - mse: 3.1414e-05 - NMSE: 2.8397e-04 - tot_time: 0h 47m 55.6s\n",
      "\n",
      "Epoch 98: val_NMSE improved from 0.00057 to 0.00056, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 1.1938e-04 - mse: 3.1414e-05 - NMSE: 2.8397e-04 - val_loss: 1.5041e-04 - val_mse: 6.2498e-05 - val_NMSE: 5.6494e-04\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2076e-04 - mse: 3.2881e-05 - NMSE: 2.9722e-04 - tot_time: 0h 48m 5.6s\n",
      "\n",
      "Epoch 99: val_NMSE did not improve from 0.00056\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 1.2076e-04 - mse: 3.2881e-05 - NMSE: 2.9722e-04 - val_loss: 1.5619e-04 - val_mse: 6.8363e-05 - val_NMSE: 6.1797e-04\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2210e-04 - mse: 3.4327e-05 - NMSE: 3.1030e-04 - tot_time: 0h 48m 15.4s\n",
      "\n",
      "Epoch 100: val_NMSE improved from 0.00056 to 0.00056, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 1.2210e-04 - mse: 3.4327e-05 - NMSE: 3.1030e-04 - val_loss: 1.4963e-04 - val_mse: 6.1915e-05 - val_NMSE: 5.5967e-04\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1871e-04 - mse: 3.1039e-05 - NMSE: 2.8058e-04 - tot_time: 0h 48m 24.6s\n",
      "\n",
      "Epoch 101: val_NMSE did not improve from 0.00056\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 514ms/step - loss: 1.1871e-04 - mse: 3.1039e-05 - NMSE: 2.8058e-04 - val_loss: 1.4986e-04 - val_mse: 6.2235e-05 - val_NMSE: 5.6256e-04\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2383e-04 - mse: 3.6242e-05 - NMSE: 3.2760e-04 - tot_time: 0h 48m 34.7s\n",
      "\n",
      "Epoch 102: val_NMSE did not improve from 0.00056\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 567ms/step - loss: 1.2383e-04 - mse: 3.6242e-05 - NMSE: 3.2760e-04 - val_loss: 1.5854e-04 - val_mse: 7.1014e-05 - val_NMSE: 6.4191e-04\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2170e-04 - mse: 3.4221e-05 - NMSE: 3.0934e-04 - tot_time: 0h 48m 43.9s\n",
      "\n",
      "Epoch 103: val_NMSE improved from 0.00056 to 0.00056, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 519ms/step - loss: 1.2170e-04 - mse: 3.4221e-05 - NMSE: 3.0934e-04 - val_loss: 1.4884e-04 - val_mse: 6.1412e-05 - val_NMSE: 5.5512e-04\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1923e-04 - mse: 3.1850e-05 - NMSE: 2.8791e-04 - tot_time: 0h 48m 53.6s\n",
      "\n",
      "Epoch 104: val_NMSE did not improve from 0.00056\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 1.1923e-04 - mse: 3.1850e-05 - NMSE: 2.8791e-04 - val_loss: 1.5148e-04 - val_mse: 6.4146e-05 - val_NMSE: 5.7984e-04\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1879e-04 - mse: 3.1497e-05 - NMSE: 2.8472e-04 - tot_time: 0h 49m 3.2s\n",
      "\n",
      "Epoch 105: val_NMSE did not improve from 0.00056\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 1.1879e-04 - mse: 3.1497e-05 - NMSE: 2.8472e-04 - val_loss: 1.4902e-04 - val_mse: 6.1773e-05 - val_NMSE: 5.5838e-04\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2326e-04 - mse: 3.6058e-05 - NMSE: 3.2594e-04 - tot_time: 0h 49m 12.1s\n",
      "\n",
      "Epoch 106: val_NMSE did not improve from 0.00056\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 496ms/step - loss: 1.2326e-04 - mse: 3.6058e-05 - NMSE: 3.2594e-04 - val_loss: 1.5095e-04 - val_mse: 6.3805e-05 - val_NMSE: 5.7675e-04\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1843e-04 - mse: 3.1332e-05 - NMSE: 2.8322e-04 - tot_time: 0h 49m 21.8s\n",
      "\n",
      "Epoch 107: val_NMSE improved from 0.00056 to 0.00055, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 1.1843e-04 - mse: 3.1332e-05 - NMSE: 2.8322e-04 - val_loss: 1.4754e-04 - val_mse: 6.0485e-05 - val_NMSE: 5.4674e-04\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2095e-04 - mse: 3.3938e-05 - NMSE: 3.0678e-04 - tot_time: 0h 49m 31.7s\n",
      "\n",
      "Epoch 108: val_NMSE did not improve from 0.00055\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 1.2095e-04 - mse: 3.3938e-05 - NMSE: 3.0678e-04 - val_loss: 1.4882e-04 - val_mse: 6.1855e-05 - val_NMSE: 5.5912e-04\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1855e-04 - mse: 3.1632e-05 - NMSE: 2.8593e-04 - tot_time: 0h 49m 41.6s\n",
      "\n",
      "Epoch 109: val_NMSE did not improve from 0.00055\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 560ms/step - loss: 1.1855e-04 - mse: 3.1632e-05 - NMSE: 2.8593e-04 - val_loss: 1.5537e-04 - val_mse: 6.8501e-05 - val_NMSE: 6.1922e-04\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2695e-04 - mse: 4.0134e-05 - NMSE: 3.6280e-04 - tot_time: 0h 49m 52.1s\n",
      "\n",
      "Epoch 110: val_NMSE did not improve from 0.00055\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 582ms/step - loss: 1.2695e-04 - mse: 4.0134e-05 - NMSE: 3.6280e-04 - val_loss: 1.4806e-04 - val_mse: 6.1303e-05 - val_NMSE: 5.5414e-04\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1784e-04 - mse: 3.1122e-05 - NMSE: 2.8132e-04 - tot_time: 0h 50m 2.0s\n",
      "\n",
      "Epoch 111: val_NMSE improved from 0.00055 to 0.00055, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 1.1784e-04 - mse: 3.1122e-05 - NMSE: 2.8132e-04 - val_loss: 1.4712e-04 - val_mse: 6.0441e-05 - val_NMSE: 5.4634e-04\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1660e-04 - mse: 2.9960e-05 - NMSE: 2.7083e-04 - tot_time: 0h 50m 11.5s\n",
      "\n",
      "Epoch 112: val_NMSE improved from 0.00055 to 0.00054, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 522ms/step - loss: 1.1660e-04 - mse: 2.9960e-05 - NMSE: 2.7083e-04 - val_loss: 1.4625e-04 - val_mse: 5.9653e-05 - val_NMSE: 5.3923e-04\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1708e-04 - mse: 3.0522e-05 - NMSE: 2.7590e-04 - tot_time: 0h 50m 21.7s\n",
      "\n",
      "Epoch 113: val_NMSE did not improve from 0.00054\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 566ms/step - loss: 1.1708e-04 - mse: 3.0522e-05 - NMSE: 2.7590e-04 - val_loss: 1.4778e-04 - val_mse: 6.1267e-05 - val_NMSE: 5.5381e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3490e-04 - mse: 4.8440e-05 - NMSE: 4.3785e-04 - tot_time: 0h 50m 31.5s\n",
      "\n",
      "Epoch 114: val_NMSE did not improve from 0.00054\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 1.3490e-04 - mse: 4.8440e-05 - NMSE: 4.3785e-04 - val_loss: 1.5207e-04 - val_mse: 6.5674e-05 - val_NMSE: 5.9363e-04\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1914e-04 - mse: 3.2803e-05 - NMSE: 2.9651e-04 - tot_time: 0h 50m 41.4s\n",
      "\n",
      "Epoch 115: val_NMSE improved from 0.00054 to 0.00054, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 549ms/step - loss: 1.1914e-04 - mse: 3.2803e-05 - NMSE: 2.9651e-04 - val_loss: 1.4584e-04 - val_mse: 5.9551e-05 - val_NMSE: 5.3830e-04\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1583e-04 - mse: 2.9574e-05 - NMSE: 2.6733e-04 - tot_time: 0h 50m 51.3s\n",
      "\n",
      "Epoch 116: val_NMSE improved from 0.00054 to 0.00053, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 1.1583e-04 - mse: 2.9574e-05 - NMSE: 2.6733e-04 - val_loss: 1.4535e-04 - val_mse: 5.9136e-05 - val_NMSE: 5.3455e-04\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1575e-04 - mse: 2.9572e-05 - NMSE: 2.6732e-04 - tot_time: 0h 51m 1.7s\n",
      "\n",
      "Epoch 117: val_NMSE improved from 0.00053 to 0.00053, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 579ms/step - loss: 1.1575e-04 - mse: 2.9572e-05 - NMSE: 2.6732e-04 - val_loss: 1.4512e-04 - val_mse: 5.8982e-05 - val_NMSE: 5.3316e-04\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1579e-04 - mse: 2.9693e-05 - NMSE: 2.6841e-04 - tot_time: 0h 51m 11.5s\n",
      "\n",
      "Epoch 118: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 548ms/step - loss: 1.1579e-04 - mse: 2.9693e-05 - NMSE: 2.6841e-04 - val_loss: 1.4545e-04 - val_mse: 5.9390e-05 - val_NMSE: 5.3684e-04\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2171e-04 - mse: 3.5696e-05 - NMSE: 3.2268e-04 - tot_time: 0h 51m 21.1s\n",
      "\n",
      "Epoch 119: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 538ms/step - loss: 1.2171e-04 - mse: 3.5696e-05 - NMSE: 3.2268e-04 - val_loss: 1.4670e-04 - val_mse: 6.0730e-05 - val_NMSE: 5.4896e-04\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1631e-04 - mse: 3.0379e-05 - NMSE: 2.7461e-04 - tot_time: 0h 51m 30.1s\n",
      "\n",
      "Epoch 120: val_NMSE improved from 0.00053 to 0.00053, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 504ms/step - loss: 1.1631e-04 - mse: 3.0379e-05 - NMSE: 2.7461e-04 - val_loss: 1.4481e-04 - val_mse: 5.8924e-05 - val_NMSE: 5.3263e-04\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1913e-04 - mse: 3.3281e-05 - NMSE: 3.0084e-04 - tot_time: 0h 51m 39.4s\n",
      "\n",
      "Epoch 121: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 512ms/step - loss: 1.1913e-04 - mse: 3.3281e-05 - NMSE: 3.0084e-04 - val_loss: 1.4566e-04 - val_mse: 5.9860e-05 - val_NMSE: 5.4109e-04\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1766e-04 - mse: 3.1901e-05 - NMSE: 2.8837e-04 - tot_time: 0h 51m 49.0s\n",
      "\n",
      "Epoch 122: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 529ms/step - loss: 1.1766e-04 - mse: 3.1901e-05 - NMSE: 2.8837e-04 - val_loss: 1.4491e-04 - val_mse: 5.9195e-05 - val_NMSE: 5.3508e-04\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1800e-04 - mse: 3.2321e-05 - NMSE: 2.9217e-04 - tot_time: 0h 51m 58.2s\n",
      "\n",
      "Epoch 123: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 9s 511ms/step - loss: 1.1800e-04 - mse: 3.2321e-05 - NMSE: 2.9217e-04 - val_loss: 1.4522e-04 - val_mse: 5.9587e-05 - val_NMSE: 5.3862e-04\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2117e-04 - mse: 3.5569e-05 - NMSE: 3.2153e-04 - tot_time: 0h 52m 7.8s\n",
      "\n",
      "Epoch 124: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 536ms/step - loss: 1.2117e-04 - mse: 3.5569e-05 - NMSE: 3.2153e-04 - val_loss: 1.6283e-04 - val_mse: 7.7273e-05 - val_NMSE: 6.9852e-04\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1904e-04 - mse: 3.3531e-05 - NMSE: 3.0311e-04 - tot_time: 0h 52m 18.4s\n",
      "\n",
      "Epoch 125: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 11s 589ms/step - loss: 1.1904e-04 - mse: 3.3531e-05 - NMSE: 3.0311e-04 - val_loss: 1.4704e-04 - val_mse: 6.1588e-05 - val_NMSE: 5.5671e-04\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1553e-04 - mse: 3.0110e-05 - NMSE: 2.7218e-04 - tot_time: 0h 52m 28.2s\n",
      "\n",
      "Epoch 126: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 1.1553e-04 - mse: 3.0110e-05 - NMSE: 2.7218e-04 - val_loss: 1.4642e-04 - val_mse: 6.1045e-05 - val_NMSE: 5.5179e-04\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2407e-04 - mse: 3.8734e-05 - NMSE: 3.5011e-04Restoring model weights from the end of the best epoch: 117.\n",
      " - tot_time: 0h 52m 38.3s\n",
      "\n",
      "Epoch 127: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 571ms/step - loss: 1.2407e-04 - mse: 3.8734e-05 - NMSE: 3.5011e-04 - val_loss: 1.4583e-04 - val_mse: 6.0545e-05 - val_NMSE: 5.4727e-04\n",
      "Epoch 127: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.0001 ----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1574e-04 - mse: 2.9615e-05 - NMSE: 2.6770e-04 - tot_time: 0h 52m 48.1s\n",
      "\n",
      "Epoch 1: val_NMSE improved from 0.00053 to 0.00053, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 1.1574e-04 - mse: 2.9615e-05 - NMSE: 2.6770e-04 - val_loss: 1.4483e-04 - val_mse: 5.8708e-05 - val_NMSE: 5.3068e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1507e-04 - mse: 2.8954e-05 - NMSE: 2.6173e-04 - tot_time: 0h 52m 58.4s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.00053 to 0.00053, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 579ms/step - loss: 1.1507e-04 - mse: 2.8954e-05 - NMSE: 2.6173e-04 - val_loss: 1.4471e-04 - val_mse: 5.8597e-05 - val_NMSE: 5.2968e-04\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1510e-04 - mse: 2.8986e-05 - NMSE: 2.6202e-04 - tot_time: 0h 53m 8.1s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 1.1510e-04 - mse: 2.8986e-05 - NMSE: 2.6202e-04 - val_loss: 1.4491e-04 - val_mse: 5.8806e-05 - val_NMSE: 5.3156e-04\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1507e-04 - mse: 2.8965e-05 - NMSE: 2.6183e-04 - tot_time: 0h 53m 18.0s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 560ms/step - loss: 1.1507e-04 - mse: 2.8965e-05 - NMSE: 2.6183e-04 - val_loss: 1.4472e-04 - val_mse: 5.8616e-05 - val_NMSE: 5.2985e-04\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1502e-04 - mse: 2.8925e-05 - NMSE: 2.6147e-04 - tot_time: 0h 53m 28.1s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.00053 to 0.00053, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 568ms/step - loss: 1.1502e-04 - mse: 2.8925e-05 - NMSE: 2.6147e-04 - val_loss: 1.4468e-04 - val_mse: 5.8583e-05 - val_NMSE: 5.2955e-04\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1502e-04 - mse: 2.8932e-05 - NMSE: 2.6153e-04 - tot_time: 0h 53m 38.1s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 555ms/step - loss: 1.1502e-04 - mse: 2.8932e-05 - NMSE: 2.6153e-04 - val_loss: 1.4473e-04 - val_mse: 5.8645e-05 - val_NMSE: 5.3011e-04\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1504e-04 - mse: 2.8955e-05 - NMSE: 2.6174e-04 - tot_time: 0h 53m 48.0s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 1.1504e-04 - mse: 2.8955e-05 - NMSE: 2.6174e-04 - val_loss: 1.4473e-04 - val_mse: 5.8658e-05 - val_NMSE: 5.3022e-04\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1500e-04 - mse: 2.8926e-05 - NMSE: 2.6148e-04 - tot_time: 0h 53m 58.2s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 565ms/step - loss: 1.1500e-04 - mse: 2.8926e-05 - NMSE: 2.6148e-04 - val_loss: 1.4469e-04 - val_mse: 5.8618e-05 - val_NMSE: 5.2986e-04\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1500e-04 - mse: 2.8933e-05 - NMSE: 2.6154e-04 - tot_time: 0h 54m 8.3s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.00053 to 0.00053, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 560ms/step - loss: 1.1500e-04 - mse: 2.8933e-05 - NMSE: 2.6154e-04 - val_loss: 1.4463e-04 - val_mse: 5.8572e-05 - val_NMSE: 5.2945e-04\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1500e-04 - mse: 2.8946e-05 - NMSE: 2.6166e-04 - tot_time: 0h 54m 18.4s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 555ms/step - loss: 1.1500e-04 - mse: 2.8946e-05 - NMSE: 2.6166e-04 - val_loss: 1.4476e-04 - val_mse: 5.8703e-05 - val_NMSE: 5.3063e-04\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1498e-04 - mse: 2.8932e-05 - NMSE: 2.6153e-04 - tot_time: 0h 54m 28.5s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 567ms/step - loss: 1.1498e-04 - mse: 2.8932e-05 - NMSE: 2.6153e-04 - val_loss: 1.4466e-04 - val_mse: 5.8618e-05 - val_NMSE: 5.2987e-04\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1489e-04 - mse: 2.8850e-05 - NMSE: 2.6079e-04Restoring model weights from the end of the best epoch: 2.\n",
      " - tot_time: 0h 54m 38.6s\n",
      "\n",
      "Epoch 12: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 560ms/step - loss: 1.1489e-04 - mse: 2.8850e-05 - NMSE: 2.6079e-04 - val_loss: 1.4473e-04 - val_mse: 5.8690e-05 - val_NMSE: 5.3052e-04\n",
      "Epoch 12: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-05 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1516e-04 - mse: 2.9045e-05 - NMSE: 2.6255e-04 - tot_time: 0h 54m 48.6s\n",
      "\n",
      "Epoch 1: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 548ms/step - loss: 1.1516e-04 - mse: 2.9045e-05 - NMSE: 2.6255e-04 - val_loss: 1.4480e-04 - val_mse: 5.8682e-05 - val_NMSE: 5.3045e-04\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1505e-04 - mse: 2.8940e-05 - NMSE: 2.6160e-04 - tot_time: 0h 54m 58.7s\n",
      "\n",
      "Epoch 2: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 565ms/step - loss: 1.1505e-04 - mse: 2.8940e-05 - NMSE: 2.6160e-04 - val_loss: 1.4473e-04 - val_mse: 5.8618e-05 - val_NMSE: 5.2986e-04\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1500e-04 - mse: 2.8887e-05 - NMSE: 2.6113e-04 - tot_time: 0h 55m 8.5s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 546ms/step - loss: 1.1500e-04 - mse: 2.8887e-05 - NMSE: 2.6113e-04 - val_loss: 1.4473e-04 - val_mse: 5.8620e-05 - val_NMSE: 5.2988e-04\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1510e-04 - mse: 2.8987e-05 - NMSE: 2.6203e-04 - tot_time: 0h 55m 18.6s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 565ms/step - loss: 1.1510e-04 - mse: 2.8987e-05 - NMSE: 2.6203e-04 - val_loss: 1.4485e-04 - val_mse: 5.8742e-05 - val_NMSE: 5.3098e-04\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.1506e-04 - mse: 2.8943e-05 - NMSE: 2.6163e-04 - tot_time: 0h 55m 28.7s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 562ms/step - loss: 1.1506e-04 - mse: 2.8943e-05 - NMSE: 2.6163e-04 - val_loss: 1.4484e-04 - val_mse: 5.8729e-05 - val_NMSE: 5.3087e-04\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1506e-04 - mse: 2.8947e-05 - NMSE: 2.6166e-04 - tot_time: 0h 55m 39.2s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 11s 588ms/step - loss: 1.1506e-04 - mse: 2.8947e-05 - NMSE: 2.6166e-04 - val_loss: 1.4486e-04 - val_mse: 5.8754e-05 - val_NMSE: 5.3109e-04\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1507e-04 - mse: 2.8962e-05 - NMSE: 2.6180e-04 - tot_time: 0h 55m 49.7s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 580ms/step - loss: 1.1507e-04 - mse: 2.8962e-05 - NMSE: 2.6180e-04 - val_loss: 1.4475e-04 - val_mse: 5.8637e-05 - val_NMSE: 5.3003e-04\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1510e-04 - mse: 2.8991e-05 - NMSE: 2.6207e-04 - tot_time: 0h 55m 59.9s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 567ms/step - loss: 1.1510e-04 - mse: 2.8991e-05 - NMSE: 2.6207e-04 - val_loss: 1.4481e-04 - val_mse: 5.8702e-05 - val_NMSE: 5.3062e-04\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1506e-04 - mse: 2.8949e-05 - NMSE: 2.6169e-04 - tot_time: 0h 56m 10.0s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 566ms/step - loss: 1.1506e-04 - mse: 2.8949e-05 - NMSE: 2.6169e-04 - val_loss: 1.4479e-04 - val_mse: 5.8687e-05 - val_NMSE: 5.3049e-04\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1507e-04 - mse: 2.8964e-05 - NMSE: 2.6182e-04 - tot_time: 0h 56m 19.8s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 543ms/step - loss: 1.1507e-04 - mse: 2.8964e-05 - NMSE: 2.6182e-04 - val_loss: 1.4476e-04 - val_mse: 5.8657e-05 - val_NMSE: 5.3022e-04\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1502e-04 - mse: 2.8918e-05 - NMSE: 2.6140e-04Restoring model weights from the end of the best epoch: 1.\n",
      " - tot_time: 0h 56m 29.6s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00053\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_008/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 10s 556ms/step - loss: 1.1502e-04 - mse: 2.8918e-05 - NMSE: 2.6140e-04 - val_loss: 1.4480e-04 - val_mse: 5.8696e-05 - val_NMSE: 5.3057e-04\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "# compiling the network\n",
    "rnn_net.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=['mse', NMSE(divisor_arr=time_stddev)],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # this loads the weights/attributes of the optimizer as well\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net.load_weights(wt_file)\n",
    "    else:\n",
    "        rnn_net.load_weights(wt_file)\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # implementing early stopping\n",
    "    baseline = None\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        baseline = np.min(val_loss_hist)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_NMSE',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=True,\n",
    "        min_delta=min_delta,\n",
    "        baseline=baseline\n",
    "    )\n",
    "    #** the two lines below are useless because wait is set to 0 in on_train_begin\n",
    "    # early_stopping_cb.wait = earlystopping_wait\n",
    "    # print('early_stopping_cb.wait : {}\\n'.format(early_stopping_cb.wait))\n",
    "\n",
    "    # time callback for each epoch\n",
    "    timekeeper_cb = mytimecallback()\n",
    "\n",
    "    # model checkpoint callback\n",
    "    dir_name_ckpt = dir_name_rnn+dir_sep+'checkpoints'\n",
    "    if not os.path.isdir(dir_name_ckpt):\n",
    "        os.makedirs(dir_name_ckpt)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dir_name_ckpt+dir_sep+'checkpoint',#+'/checkpoint--loss={loss:.4f}--vall_loss={val_loss:.4f}',\n",
    "        monitor='val_NMSE',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=2,\n",
    "        initial_value_threshold=baseline,\n",
    "        period=1  # saves every `period` epochs\n",
    "    )\n",
    "\n",
    "    # save losses callback\n",
    "    savelosses_cb = SaveLosses(\n",
    "        filepath=dir_name_ckpt+dir_sep+'LossHistoriesCheckpoint',\n",
    "        val_loss_arr=savelosses_cb_vallossarr,\n",
    "        train_loss_arr=savelosses_cb_trainlossarr,\n",
    "        total_epochs=epochs,\n",
    "        period=1)\n",
    "\n",
    "    for i in range(starting_lr_idx, len(learning_rate_list)):\n",
    "        learning_rate = learning_rate_list[i]\n",
    "        K.set_value(rnn_net.optimizer.lr, learning_rate)\n",
    "\n",
    "        savelosses_cb.update_lr_idx(i)\n",
    "\n",
    "        if i == starting_lr_idx:\n",
    "            EPOCHS = num_epochs_left\n",
    "            savelosses_cb.update_offset(epochs-num_epochs_left)\n",
    "        else:\n",
    "            EPOCHS = epochs\n",
    "            savelosses_cb.update_offset(0)\n",
    "\n",
    "        total_s_len = 80\n",
    "        sep_lr_s = ' LEARNING RATE : {} '.format(learning_rate)\n",
    "        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n",
    "        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n",
    "        print('\\n\\n' + '-'*len(sep_lr_s))\n",
    "        print('\\n' + sep_lr_s+'\\n')\n",
    "        print('-'*len(sep_lr_s) + '\\n\\n')\n",
    "        \n",
    "        history = rnn_net.fit(training_data_rnn_input, training_data_rnn_output,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=batch_size,\n",
    "#             validation_split=val_split/train_split,\n",
    "            validation_data=(val_data_rnn_input, val_data_rnn_output),\n",
    "            callbacks=[early_stopping_cb, timekeeper_cb, checkpoint_cb, savelosses_cb],\n",
    "            verbose=1,\n",
    "            shuffle=not stateful,\n",
    "        )\n",
    "\n",
    "        val_loss_hist.extend(history.history['val_loss'])\n",
    "        train_loss_hist.extend(history.history['loss'])\n",
    "        \n",
    "        val_NMSE_hist.extend(history.history['val_NMSE'])\n",
    "        train_NMSE_hist.extend(history.history['NMSE'])\n",
    "\n",
    "        val_MSE_hist.extend(history.history['val_mse'])\n",
    "        train_MSE_hist.extend(history.history['mse'])\n",
    "        \n",
    "        if i == starting_lr_idx:\n",
    "            lr_change[i+1] += len(history.history['val_loss'])\n",
    "        else:\n",
    "            lr_change.append(lr_change[i]+len(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 552, 2) (96, 552, 2)\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 1.5016e-04 - mse: 6.4044e-05 - NMSE: 5.7892e-04\n"
     ]
    }
   ],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    for layer in rnn_net.rnn_list:\n",
    "        if layer.stateful == True:\n",
    "            layer.reset_states()\n",
    "    print(testing_data_rnn_input.shape, testing_data_rnn_output.shape)\n",
    "    eval_dict = rnn_net.evaluate(\n",
    "        testing_data_rnn_input, testing_data_rnn_output,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "\n",
    "    with open(save_path+dir_sep+'losses.txt', 'w') as f:\n",
    "        f.write(str({\n",
    "            'val_loss_hist':val_loss_hist,\n",
    "            'train_loss_hist':train_loss_hist,\n",
    "            'val_MSE_hist':val_MSE_hist,\n",
    "            'train_MSE_hist':train_MSE_hist,\n",
    "            'val_NMSE_hist':val_NMSE_hist,\n",
    "            'train_NMSE_hist':train_NMSE_hist,\n",
    "            'lr_change':lr_change,\n",
    "            'test_loss':eval_dict[0],\n",
    "            'test_MSE':eval_dict[1],\n",
    "            'test_NMSE':eval_dict[2],\n",
    "        }))\n",
    "        \n",
    "    if normalize_dataset == True:\n",
    "        with open(save_path+dir_sep+'rnn_normalization.txt', 'w') as f:\n",
    "            f.write(str({\n",
    "                'normalization_arr':normalization_arr\n",
    "            }))\n",
    "\n",
    "    rnn_net.save_everything(\n",
    "        file_name=save_path+dir_sep+'final_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_kwargs = {'fontsize':15}\n",
    "ylabel_kwargs = {'fontsize':15}\n",
    "legend_kwargs = {'fontsize':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting losses\n",
    "dir_name_plot = dir_name_rnn + '/plots'\n",
    "if not os.path.isdir(dir_name_plot):\n",
    "    os.makedirs(dir_name_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": [
    "# Visualize loss history\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_loss_hist,\n",
    "    val_loss=val_loss_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "\n",
    "plt.savefig(dir_name_plot + '{ds}loss_history.pdf'.format(ds=dir_sep), dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_MSE_hist,\n",
    "    val_loss=val_MSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training MSE', 'Validation MSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='MSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/MSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_NMSE_hist,\n",
    "    val_loss=val_NMSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training NMSE', 'Validation NMSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='NMSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/NMSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": [
    "def rescale_data(data, normalization_arr):\n",
    "    '''\n",
    "    data - [num_batches x num_timesteps x num_states]\n",
    "    normalization_arr = [2 x num_states]\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(data.shape[-1]):\n",
    "        new_data[:, i] -= normalization_arr[0, i]\n",
    "        new_data[:, i] /= normalization_arr[1, i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def norm_sq_time_average(data):\n",
    "    data_norm_sq = np.zeros(shape=data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_norm_sq[:] += data[:, i]**2\n",
    "    # integrating using the trapezoidal rule\n",
    "    norm_sq_time_avg = np.sum(data_norm_sq) - 0.5*(data_norm_sq[0]+data_norm_sq[-1])\n",
    "    norm_sq_time_avg /= data_norm_sq.shape[0]\n",
    "    return norm_sq_time_avg\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = np.empty_like(data)\n",
    "    shape = new_data.shape\n",
    "    # print(shape)\n",
    "    for i in range(shape[-1]):\n",
    "        if len(shape) == 2:\n",
    "            new_data[:, i] = data[:, i]\n",
    "            new_data[:, i] *= normalization_arr[1, i]\n",
    "            new_data[:, i] += normalization_arr[0, i]\n",
    "        elif len(shape) == 3:\n",
    "            new_data[:, :, i] = data[:, :, i]\n",
    "            new_data[:, :, i] *= normalization_arr[1, i]\n",
    "            new_data[:, :, i] += normalization_arr[0, i]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_rnn/rnn_008\n",
      "num_runs : 100\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 0.9212007482583902, median : 0.724641689878773\n",
      "ph_min : 0.09058021123484662, ph_max : 4.438430350507485\n",
      "stddev : 0.7090822356167917, IQR : 0.9963823235833129\n",
      "1st quartile : 0.3623208449393865, 3rd quartile : 1.3587031685226993\n",
      "analysis time : 12.376651287078857 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = AR_testing_data_rnn_input.shape[0]\n",
    "\n",
    "analysis_time = time.time()\n",
    "\n",
    "AR_rnn_net = AR_RNN(\n",
    "    load_file=save_path+'/final_net_class_dict.txt',\n",
    "    T_input=T_sample_input_AR,\n",
    "    T_output=T_sample_output_AR,\n",
    "    stddev=0.0,\n",
    "    batch_size=num_runs,\n",
    "    lambda_reg=lambda_reg,\n",
    ")\n",
    "AR_rnn_net.build(input_shape=tuple(AR_testing_data_rnn_input.shape[0:2]) + tuple(testing_data_rnn_input.shape[2:]))\n",
    "AR_rnn_net.load_weights_from_file(save_path+'/final_net_gru_weights.h5')\n",
    "\n",
    "AR_AERNN_net = AR_AERNN(\n",
    "    ae_net,\n",
    "    AR_rnn_net,\n",
    "    normalization_arr,\n",
    "    normalization_constant_arr_aedata,\n",
    "    covmat_lmda=0.0,\n",
    "    time_stddev_ogdata=time_stddev_ogdata,\n",
    "    time_mean_ogdata=time_mean_ogdata,\n",
    "    loss_weights=None,\n",
    "    clipnorm=None,\n",
    "    global_clipnorm=None\n",
    ")\n",
    "\n",
    "savefig_fname = 'pre_ARtraining-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "npsavedata_fname = '/prediction_horizons-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "plot_dir = '/plots'\n",
    "\n",
    "sidx1 = dir_name_rnn[::-1].index('/')\n",
    "sidx2 = dir_name_rnn[-sidx1-2::-1].index('/')\n",
    "print(dir_name_rnn[-(sidx1+sidx2+1):])\n",
    "print('num_runs :', num_runs)\n",
    "\n",
    "prediction_horizon_arr = np.empty(shape=num_runs)\n",
    "prediction = np.array(AR_AERNN_net(AR_testing_data_rnn_input, training=False))\n",
    "prediction = invert_normalization(prediction, normalization_constant_arr_aedata)\n",
    "\n",
    "data_in_og = AR_testing_data_rnn_input\n",
    "data_out_og = AR_testing_data_rnn_output\n",
    "\n",
    "energySpectrum_dataout = 0.0\n",
    "energySpectrum_pred = 0.0\n",
    "\n",
    "avg_time = 0.\n",
    "for i in range(num_runs):\n",
    "    run_time = time.time()\n",
    "    lyap_time = lyapunov_time_arr[0]\n",
    "\n",
    "    data_out = data_out_og[i]\n",
    "    data_out = invert_normalization(data_out, normalization_constant_arr_aedata)\n",
    "\n",
    "    ### Error and prediction horizon\n",
    "    # error = np.linalg.norm(data_out[:, :] - prediction[i, :, :], axis=1)\n",
    "    error = (data_out[:, :] - prediction[i, :, :])**2\n",
    "    # error /= norm_sq_time_average(data_out)**0.5\n",
    "    error = np.mean(np.divide(error, time_stddev_ogdata**2), axis=1)**0.5\n",
    "\n",
    "    predhor_idx = np.where(error >= error_threshold)[0]\n",
    "    if predhor_idx.shape[0] == 0:\n",
    "        predhor_idx = error.shape[0]\n",
    "    else:\n",
    "        predhor_idx = predhor_idx[0]\n",
    "\n",
    "    prediction_horizon_arr[i] = predhor_idx*dt_rnn/lyap_time\n",
    "\n",
    "    run_time = time.time() - run_time\n",
    "    avg_time = (avg_time*i + run_time)/(i+1)\n",
    "    eta = avg_time * (num_runs-1 - i)\n",
    "    # print('    {} / {} -- run_time : {:.2f} s -- eta : {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "    #     i+1,\n",
    "    #     num_runs,\n",
    "    #     run_time,\n",
    "    #     float(eta // 3600),\n",
    "    #     float((eta%3600)//60),\n",
    "    #     float((eta%3600)%60),\n",
    "    # ))\n",
    "\n",
    "median_idx = int(np.round(0.5*num_runs-1))\n",
    "quartile_1_idx = int(np.round(0.25*num_runs-1))\n",
    "quartile_3_idx = int(np.round(0.75*num_runs-1))\n",
    "\n",
    "prediction_horizon_arr.sort()\n",
    "\n",
    "median = prediction_horizon_arr[median_idx]\n",
    "quartile_1 = prediction_horizon_arr[quartile_1_idx]\n",
    "quartile_3 = prediction_horizon_arr[quartile_3_idx]\n",
    "IQR = quartile_3 - quartile_1\n",
    "\n",
    "prediction_horizon = np.mean(prediction_horizon_arr)\n",
    "stddev_ph = np.std(prediction_horizon_arr)\n",
    "\n",
    "s = 'error_threshold = {}\\n'.format(error_threshold)\n",
    "s += 'prediction_horizon : {}, median : {}\\n'.format(prediction_horizon, median)\n",
    "s += 'ph_min : {}, ph_max : {}\\n'.format(prediction_horizon_arr.min(), prediction_horizon_arr.max())\n",
    "s += 'stddev : {}, IQR : {}\\n'.format(stddev_ph, IQR)\n",
    "s += '1st quartile : {}, 3rd quartile : {}'.format(quartile_1, quartile_3)\n",
    "\n",
    "print('\\n'+s)\n",
    "\n",
    "plot_histogram_and_save(\n",
    "    prediction_horizon_arr, median,\n",
    "    save_dir=dir_name_rnn+plot_dir,\n",
    "    savefig_fname=savefig_fname,\n",
    ")\n",
    "\n",
    "np.savez(\n",
    "    dir_name_rnn+npsavedata_fname,\n",
    "    prediction_horizon_arr=prediction_horizon_arr,\n",
    "    error_threshold=error_threshold,\n",
    ")\n",
    "\n",
    "with open(dir_name_rnn+npsavedata_fname+'--statistics.txt', 'w') as fl:\n",
    "    fl.write(s)\n",
    "\n",
    "print('analysis time : {} s\\n'.format(time.time() - analysis_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'use_trainable_weights_with_reslayers' in rnn_net.__dict__.keys():\n",
    "    if use_trainable_weights_with_reslayers == True:\n",
    "        for i in range(rnn_net.num_skip_connections):\n",
    "            print('reslayer_factor_{} : {}'.format(i, rnn_net.reslayer_factor[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
