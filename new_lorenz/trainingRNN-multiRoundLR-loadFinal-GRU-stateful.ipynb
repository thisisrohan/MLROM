{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868739487,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4xhxMpe_r-Y5"
   },
   "outputs": [],
   "source": [
    "# enabling 3rd party widgets\n",
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()\n",
    "# output.disable_custom_widget_manager()\n",
    "\n",
    "# interactive 3D plot\n",
    "# !pip install ipympl\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1667868743047,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a5qPupCDsjSz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "import time as time\n",
    "import platform as platform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import h5py\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\":True,\n",
    "    \"font.family\":\"serif\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "h_qXhHdbCgoj",
    "outputId": "3473a883-d145-4778-9be7-7d44e0c6ea67"
   },
   "outputs": [],
   "source": [
    "colab_flag = False\n",
    "FTYPE = np.float32\n",
    "ITYPE = np.int32\n",
    "\n",
    "array = np.array\n",
    "float32 = np.float32\n",
    "int32 = np.int32\n",
    "float64 = np.float64\n",
    "int64 = np.int64\n",
    "\n",
    "strategy = None\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868743048,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BiLIUmBPneQR"
   },
   "outputs": [],
   "source": [
    "current_sys = platform.system()\n",
    "\n",
    "if current_sys == 'Windows':\n",
    "    dir_sep = '\\\\'\n",
    "else:\n",
    "    dir_sep = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18870,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fnTV6Anhni6O",
    "outputId": "bf1d11f8-667f-4cb5-d8d5-b9d860b44d99"
   },
   "outputs": [],
   "source": [
    "if colab_flag == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Github/MLROM/KS/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868761912,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "paDfPOrjnkAS",
    "outputId": "58054510-4476-49b4-f8ba-e2978a028b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rkaushik/Documents/Thesis/MLROM/new_lorenz\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "P6K2YWlR6ZPD"
   },
   "outputs": [],
   "source": [
    "from tools.misc_tools import create_data_for_RNN, mytimecallback, SaveLosses, plot_losses, plot_reconstructed_data_KS, plot_latent_states_KS , readAndReturnLossHistories, plot_histogram_and_save\n",
    "from tools.ae_v2 import Autoencoder\n",
    "from tools.GRU_SingleStep_v1 import RNN_GRU\n",
    "# from tools.LSTM_SingleStep_v2 import RNN_GRU\n",
    "# from tools.SimpleRNN_SingleStep_v2 import RNN_GRU\n",
    "from tools.GRU_AR_v1 import AR_RNN_GRU as AR_RNN\n",
    "from tools.AEGRU_AR_v1 import AR_AERNN_GRU as AR_AERNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766483,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_xtkwXE2tGTP"
   },
   "outputs": [],
   "source": [
    "behaviour = 'initialiseAndTrainFromScratch'\n",
    "# behaviour = 'loadCheckpointAndContinueTraining'\n",
    "# behaviour = 'loadFinalNetAndPlot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8S1AHEkl48bn"
   },
   "outputs": [],
   "source": [
    "# setting seed for PRNGs\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    prng_seed = 42\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "qvA9oeCHCTVM",
    "outputId": "0f2de849-59ee-4ed9-b65d-c5952e0dcb55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 14:37:40.691704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:40.692386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:40.800384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:40.800669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:40.800908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:40.801160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:40.802350: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-09 14:37:40.803013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:40.803486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:40.803756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:41.412100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:41.412313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:41.412493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-09 14:37:41.412643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3067 MB memory:  -> device: 1, name: Quadro K2200, pci bus id: 0000:03:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if colab_flag == False:\n",
    "    if strategy is None:\n",
    "        if gpus:\n",
    "            gpu_to_use = 1\n",
    "            tf.config.set_visible_devices(gpus[gpu_to_use], 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868766484,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "tc3zO9xL_tNl",
    "outputId": "c9786b4c-8510-47d0-801d-181e3b12239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "2.8.1\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices())\n",
    "print('')\n",
    "print(tf.config.list_logical_devices())\n",
    "print('')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UbdnOtc4_z9"
   },
   "source": [
    "# KS System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868766485,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8aNkoXfyGq52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1667868772247,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "O7sl7i5H5Dqz",
    "outputId": "419ef0e0-4d58-454e-d0af-17af3b846b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_name_rnn: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005\n",
      "dir_name_ae: /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_ae/ae_024\n",
      "data_dir_idx: 010\n",
      "normalize_flag_ogdata: False\n"
     ]
    }
   ],
   "source": [
    "# setting up params (and saving, if applicable)\n",
    "from numpy import *\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # making RNN save directory\n",
    "    dir_name_rnn = os.getcwd() + dir_sep + 'saved_rnn'\n",
    "    if not os.path.isdir(dir_name_rnn):\n",
    "        os.makedirs(dir_name_rnn)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        dir_check = 'rnn_' + str(counter).zfill(3)\n",
    "        if os.path.isdir(dir_name_rnn + dir_sep + dir_check):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dir_name_rnn = dir_name_rnn + dir_sep + dir_check\n",
    "    os.makedirs(dir_name_rnn)\n",
    "    os.makedirs(dir_name_rnn+dir_sep+'plots')\n",
    "\n",
    "    # whether to use AE data or just work on raw data\n",
    "    use_ae_data = True # if false, specifying ae_idx will only show which dataset to use\n",
    "\n",
    "    # autoencoder directory\n",
    "    ae_idx = '024'\n",
    "    dir_name_ae = os.getcwd()+'{ds}saved_ae{ds}ae_'.format(ds=dir_sep)+ae_idx\n",
    "else:\n",
    "    # RNN directory\n",
    "    dir_name_rnn = os.getcwd()+'/saved_rnn/rnn_015'\n",
    "\n",
    "    # reading AE directory\n",
    "    with open(dir_name_rnn + '/sim_data_AE_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_dict = eval(''.join(lines))\n",
    "\n",
    "    try:\n",
    "        use_ae_data = params_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in sim_data_AE_params, set to True.\")\n",
    "        normalize_dataset = True\n",
    "    \n",
    "    dir_name_ae = params_dict['dir_name_ae']\n",
    "    ae_idx = dir_name_ae[-3:]\n",
    "    dir_name_ae = os.getcwd()+'/saved_ae/ae_'+ae_idx\n",
    "\n",
    "    # reading RNN paramaters\n",
    "    with open(dir_name_rnn + '/RNN_specific_data.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    params_rnn_dict = eval(''.join(lines))\n",
    "\n",
    "    dt_rnn = params_rnn_dict['dt_rnn']\n",
    "    T_sample_input = params_rnn_dict['T_sample_input']\n",
    "    T_sample_output = params_rnn_dict['T_sample_output']\n",
    "    T_offset = params_rnn_dict['T_offset']\n",
    "    return_params_arr = params_rnn_dict['return_params_arr']\n",
    "    params = params_rnn_dict['params']\n",
    "    try:\n",
    "        normalize_dataset = params_rnn_dict['normalize_dataset']\n",
    "    except:\n",
    "        print(\"'normalize_dataset' not present in RNN_specific_data, set to False.\")\n",
    "        normalize_dataset = False\n",
    "    try:\n",
    "        stddev_multiplier = params_rnn_dict['stddev_multiplier']\n",
    "    except:\n",
    "        print(\"'stddev_multiplier' not present in RNN_specific_data, set to None.\")\n",
    "        stddev_multiplier = None\n",
    "    try:\n",
    "        skip_intermediate = params_rnn_dict['skip_intermediate']\n",
    "    except:\n",
    "        print(\"'skip_intermediate' not present in RNN_specific_data, set to 1.\")\n",
    "        skip_intermediate = 1\n",
    "    try:\n",
    "        normalization_type = params_rnn_dict['normalization_type']\n",
    "    except:\n",
    "        print(\"'normalization_type' not present in RNN_specific_data, set to 'stddev'.\")\n",
    "        normalization_type = 'stddev'\n",
    "    try:\n",
    "        dense_layer_act_func = params_rnn_dict['dense_layer_act_func']\n",
    "    except:\n",
    "        print(\"'dense_layer_act_func' not present in RNN_specific_data, set to 'linear'.\")\n",
    "        dense_layer_act_func = 'linear'\n",
    "    try:\n",
    "        stateful = params_rnn_dict['stateful']\n",
    "    except:\n",
    "        print(\"'stateful' not present in RNN_specific_data, set to True.\")\n",
    "        stateful = True\n",
    "    try:\n",
    "        use_learnable_state = params_rnn_dict['use_learnable_state']\n",
    "    except:\n",
    "        print(\"'use_learnable_state' not present in RNN_specific_data, set to False.\")\n",
    "        use_learnable_state = False\n",
    "    try:\n",
    "        use_weights_post_dense = params_rnn_dict['use_weights_post_dense']\n",
    "    except:\n",
    "        print(\"'use_weights_post_dense' not present in RNN_specific_data, set to False.\")\n",
    "        use_weights_post_dense = False\n",
    "    try:\n",
    "        use_ae_data = params_rnn_dict['use_ae_data']\n",
    "    except:\n",
    "        print(\"'use_ae_data' not present in RNN_specific_data, set to True.\")\n",
    "        use_ae_data = True\n",
    "\n",
    "    \n",
    "\n",
    "    normalization_arr = None\n",
    "    try:\n",
    "        with open(dir_name_rnn + '/final_net/rnn_normalization.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        rnn_norm_arr_dict = eval(lines)\n",
    "        normalization_arr = rnn_norm_arr_dict['normalization_arr']\n",
    "    except:\n",
    "        pass\n",
    "    if os.path.exists(dir_name_rnn+dir_sep+'normalization_data.npz'):\n",
    "        with np.load(dir_name_rnn+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "            normalization_arr = fl['normalization_arr'][0]\n",
    "\n",
    "# reading simulation parameters\n",
    "with open(dir_name_ae + dir_sep + 'ae_data.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "data_dir_idx = params_dict['data_dir_idx']\n",
    "normalizeforae_flag = params_dict['normalizeforae_flag']\n",
    "normalization_constant_arr_aedata = params_dict['normalization_constant_arr_aedata']\n",
    "try:\n",
    "    ae_data_with_params = params_dict['ae_data_with_params']\n",
    "except:\n",
    "    print(\"'ae_data_with_params' not present in ae_data, set to 'True'.\")\n",
    "    ae_data_with_params = True\n",
    "\n",
    "if os.path.exists(dir_name_ae+dir_sep+'normalization_data.npz'):\n",
    "    with np.load(dir_name_ae+dir_sep+'normalization_data.npz', allow_pickle=True) as fl:\n",
    "        normalization_constant_arr_aedata = fl['normalization_constant_arr_aedata'][0]\n",
    "\n",
    "print('dir_name_rnn:', dir_name_rnn)\n",
    "print('dir_name_ae:', dir_name_ae)\n",
    "print('data_dir_idx:', data_dir_idx)\n",
    "\n",
    "# loading data\n",
    "dir_name_data = os.getcwd() + dir_sep + 'saved_data' + dir_sep + 'data_' + data_dir_idx\n",
    "    \n",
    "with open(dir_name_data + dir_sep + 'sim_data_params.txt') as f:\n",
    "    lines = f.readlines()\n",
    "params_dict = eval(''.join(lines))\n",
    "params_mat = params_dict['params_mat']\n",
    "# init_state = params_dict['init_state']\n",
    "t0 = params_dict['t0']\n",
    "T = params_dict['T']\n",
    "delta_t = params_dict['delta_t']\n",
    "return_params_arr = params_dict['return_params_arr']\n",
    "normalize_flag_ogdata = params_dict['normalize_flag']\n",
    "print('normalize_flag_ogdata:', normalize_flag_ogdata)\n",
    "alldata_withparams_flag = params_dict['alldata_withparams_flag']\n",
    "\n",
    "with np.load(dir_name_data+dir_sep+'data.npz', allow_pickle=True) as fl:\n",
    "    all_data = fl['all_data'].astype(FTYPE)\n",
    "    boundary_idx_arr = fl['boundary_idx_arr']\n",
    "    normalization_constant_arr_ogdata = fl['normalization_constant_arr'][0]\n",
    "    initial_t0 = fl['initial_t0']\n",
    "    init_state_mat = fl['init_state_mat']\n",
    "\n",
    "    lyapunov_spectrum_mat = fl['lyapunov_spectrum_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1667868772777,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ySVDz_2U5FH5",
    "outputId": "c57be82f-527d-4e83-a605-aac85c39088e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case : 1, lyapunov exponent : 0.9058021372262592, lyapunov time : 1.1039938926696777s\n"
     ]
    }
   ],
   "source": [
    "lyapunov_time_arr = np.empty(shape=lyapunov_spectrum_mat.shape[0], dtype=FTYPE)\n",
    "for i in range(lyapunov_spectrum_mat.shape[0]):\n",
    "    lyapunov_time_arr[i] = 1/lyapunov_spectrum_mat[i, 0]\n",
    "    print('Case : {}, lyapunov exponent : {}, lyapunov time : {}s'.format(i+1, lyapunov_spectrum_mat[i, 0], lyapunov_time_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868772778,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "bkQx9q_p5Gro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1667868774262,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uDhfYHU45IS8",
    "outputId": "5307dc6a-17c5-4c77-dac5-fcb96116ac44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868774263,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-MJa7P5t5KiC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delaing with normalizing the data before feeding into autoencoder\n",
    "num_params = params_mat.shape[1]\n",
    "og_vars = all_data.shape[1]\n",
    "if alldata_withparams_flag == True:\n",
    "    og_vars -= num_params\n",
    "\n",
    "time_stddev_ogdata = np.std(all_data[:, 0:og_vars], axis=0)\n",
    "time_mean_ogdata = np.mean(all_data[:, 0:og_vars], axis=0)\n",
    "    \n",
    "if use_ae_data == True:\n",
    "    if ae_data_with_params == True and alldata_withparams_flag == False:\n",
    "        new_all_data = np.empty(shape=(all_data.shape[0], og_vars+num_params), dtype=FTYPE)\n",
    "        new_all_data[:, 0:og_vars] = all_data[:, 0:og_vars]\n",
    "        del(all_data)\n",
    "        all_data = new_all_data\n",
    "        prev_idx = 0\n",
    "        for i in range(boundary_idx_arr.shape[0]):\n",
    "            all_data[prev_idx:boundary_idx_arr[i], num_params:] = params_mat[i]\n",
    "            prev_idx = boundary_idx_arr[i]\n",
    "\n",
    "    if normalizeforae_flag == True:\n",
    "        for i in range(all_data.shape[1]):\n",
    "            all_data[:, i] -= normalization_constant_arr_aedata[0, i]\n",
    "            all_data[:, i] /= normalization_constant_arr_aedata[1, i]\n",
    "\n",
    "    if ae_data_with_params == False:\n",
    "        all_data = all_data[:, 0:og_vars]\n",
    "else:\n",
    "    # using raw data, neglecting the params attached (if any)\n",
    "    all_data = all_data[:, 0:og_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1667868775190,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "sMENXULAGFPm",
    "outputId": "dbf2c14d-2e8a-42c9-b6c5-f5f7c7a6092f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v6KQEjR5LkK"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868775191,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ZBTJl9PeneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    load_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_class_dict.txt'\n",
    "    wt_file = dir_name_ae+dir_sep+'final_net'+dir_sep+'final_net_ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1667868776552,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "a3Pq-qorneQb"
   },
   "outputs": [],
   "source": [
    "if use_ae_data == True:\n",
    "    ae_net = Autoencoder(all_data.shape[1], load_file=load_file)\n",
    "    ae_net.load_weights_from_file(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wwt4brHcOaXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868776553,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Zl6ZvgtNtA_u",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868776554,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lXpoaKRIneQc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1667868777509,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Q3a8HHyvneQc",
    "outputId": "51084913-6faf-4bb5-db69-2cbea705dd28"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "if use_ae_data == True:\n",
    "    latent_states_all = ae_net.encoder_net.predict(all_data)\n",
    "    # del(all_data)\n",
    "else:\n",
    "    latent_states_all = all_data\n",
    "num_latent_states = latent_states_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1667868778304,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wjgPNitSrt5p",
    "outputId": "0c916524-33ec-47bf-a16a-51e53d2e25f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667868778305,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "wnLnqg0Jrt5t"
   },
   "outputs": [],
   "source": [
    "# for i in range(ae_net.layers):\n",
    "#     tf.keras.utils.plot_model(\n",
    "#         ae_net.layers[i],\n",
    "#         to_file=dir_name_ae+'/plots/netlayer_{}.png'.format(i),\n",
    "#         show_shapes=True,\n",
    "#         dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "BOJE8vREtque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "fwjcsAxKneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778788,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "aFd7XgwVneQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAcFjRRn_IQ"
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "lPVqWNwjoAGP"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    # RNN data parameters\n",
    "    num_lyaptimesteps_totrain = 50 # int(5000/np.mean(lyapunov_time_arr))#\n",
    "    dt_rnn = 0.1\n",
    "    T_sample_input = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_sample_output = num_lyaptimesteps_totrain*np.mean(lyapunov_time_arr)\n",
    "    T_offset = dt_rnn\n",
    "    normalize_dataset = True # whether the data for the RNN should be normalized by the dataset's mean and std\n",
    "    normalization_arr = None\n",
    "    skip_intermediate = 'full sample'\n",
    "    noise_type = 'normal' # can be 'uniform' or 'normal'\n",
    "\n",
    "    # can be 'minmax', 'minmax2', 'stddev', or a list with\n",
    "    # sequential order of any of these; if it is 'minmax'\n",
    "    # then stddev_multiplier has no effect\n",
    "    normalization_type = 'stddev'\n",
    "    stddev_multiplier = 3\n",
    "\n",
    "    dense_layer_act_func = ['tanh']\n",
    "    use_weights_post_dense = True\n",
    "    stateful = True\n",
    "    use_learnable_state = False\n",
    "    use_trainable_weights_with_reslayers = False\n",
    "        \n",
    "    if return_params_arr != False:\n",
    "        params = params_arr\n",
    "    else:\n",
    "        params = None\n",
    "        \n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "\n",
    "    # saving simulation data\n",
    "    sim_data = {\n",
    "        'params_mat':params_mat,\n",
    "        'init_state_mat':init_state_mat,\n",
    "        't0':t0,\n",
    "        'T':T,\n",
    "        'delta_t':delta_t,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'dir_name_ae':dir_name_ae,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'use_ae_data':use_ae_data,\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'sim_data_AE_params.txt', 'w') as f:\n",
    "        f.write(str(sim_data))\n",
    "        \n",
    "    # saving RNN specific data\n",
    "    RNN_specific_data = {\n",
    "        'dt_rnn':dt_rnn,\n",
    "        'T_sample_input':T_sample_input,\n",
    "        'T_sample_output':T_sample_output,\n",
    "        'T_offset':T_offset,\n",
    "        'boundary_idx_arr':boundary_idx_arr,\n",
    "        'delta_t':delta_t,\n",
    "        'params':params,\n",
    "        'return_params_arr':return_params_arr,\n",
    "        'normalize_dataset':normalize_dataset,\n",
    "        'num_lyaptimesteps_totrain':num_lyaptimesteps_totrain,\n",
    "        'stddev_multiplier':stddev_multiplier,\n",
    "        'skip_intermediate':skip_intermediate,\n",
    "        'module':RNN_GRU.__module__,\n",
    "        'noise_type':noise_type,\n",
    "        'normalization_type':normalization_type,\n",
    "        'dense_layer_act_func':dense_layer_act_func,\n",
    "        'stateful':stateful,\n",
    "        'use_learnable_state':use_learnable_state,\n",
    "        'use_weights_post_dense':use_weights_post_dense,\n",
    "        'use_trainable_weights_with_reslayers':use_trainable_weights_with_reslayers,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'RNN_specific_data.txt', 'w') as f:\n",
    "        f.write(str(RNN_specific_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "S21-VEUYrkk-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778789,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "UGnj8uQQ83-y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "0t2_8mzI1fhX"
   },
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    latent_states_all,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=normalize_dataset,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type)\n",
    "    \n",
    "data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "num_samples = rnn_res_dict['num_samples']\n",
    "normalization_arr = rnn_res_dict['normalization_arr']\n",
    "rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "pIsWCXkbr7ws"
   },
   "outputs": [],
   "source": [
    "temp = np.divide(latent_states_all-normalization_arr[0], normalization_arr[1])\n",
    "time_stddev = np.std(temp, axis=0)\n",
    "timeMeanofSpaceRMS = np.mean(np.mean(temp**2, axis=1)**0.5)\n",
    "del(org_data_idx_arr_input)\n",
    "del(org_data_idx_arr_output)\n",
    "del(latent_states_all)\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_res_dict = create_data_for_RNN(\n",
    "    all_data,\n",
    "    dt_rnn,\n",
    "    T_sample_input,\n",
    "    T_sample_output,\n",
    "    T_offset,\n",
    "    None,\n",
    "    boundary_idx_arr,\n",
    "    delta_t,\n",
    "    params=params,\n",
    "    return_numsamples=True,\n",
    "    normalize_dataset=False,\n",
    "    stddev_multiplier=stddev_multiplier,\n",
    "    skip_intermediate=skip_intermediate,\n",
    "    return_OrgDataIdxArr=False,\n",
    "    normalization_arr_external=normalization_arr,\n",
    "    normalization_type=normalization_type,\n",
    "    FTYPE=FTYPE,\n",
    "    ITYPE=ITYPE)\n",
    "    \n",
    "AR_data_rnn_input = rnn_res_dict['data_rnn_input']\n",
    "AR_data_rnn_output = rnn_res_dict['data_rnn_output']\n",
    "AR_org_data_idx_arr_input = rnn_res_dict['org_data_idx_arr_input']\n",
    "AR_org_data_idx_arr_output = rnn_res_dict['org_data_idx_arr_output']\n",
    "AR_num_samples = rnn_res_dict['num_samples']\n",
    "AR_normalization_arr = rnn_res_dict['normalization_arr']\n",
    "AR_rnn_data_boundary_idx_arr = rnn_res_dict['rnn_data_boundary_idx_arr']\n",
    "\n",
    "del(all_data)\n",
    "del(AR_org_data_idx_arr_input)\n",
    "del(AR_org_data_idx_arr_output)\n",
    "del(AR_rnn_data_boundary_idx_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1667868778790,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Hem_9PUqneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667868778791,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "uskBAAXpneQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1667868779211,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "-1uL-GomneQi"
   },
   "outputs": [],
   "source": [
    "# setting up training params\n",
    "\n",
    "# ph computation parameters\n",
    "num_runs = 100\n",
    "T_sample_input_AR_ratio = 1\n",
    "T_sample_output_AR_ratio = 3\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    learning_rate_list = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    epochs = 200\n",
    "    patience = 10 # parameter for early stopping\n",
    "    min_delta = 1e-6  # parameter for early stopping\n",
    "    lambda_reg = 3.72759372e-07  # weight for regularizer\n",
    "    train_split = 0.8\n",
    "    val_split = 0.1\n",
    "    test_split = 1 - train_split - val_split\n",
    "    batch_size = 32\n",
    "    fRMS = 5.17947468e-03\n",
    "    zoneout_rate = 0.0\n",
    "    rnncell_dropout_rate = 0.0\n",
    "    denselayer_dropout_rate = 0.0\n",
    "    \n",
    "\n",
    "    stddev = fRMS*np.mean(time_stddev[0:og_vars])\n",
    "    \n",
    "    # saving training params\n",
    "    training_specific_params = {\n",
    "        'learning_rate_list':learning_rate_list,\n",
    "        'epochs':epochs,\n",
    "        'patience':patience,\n",
    "        'min_delta':min_delta,\n",
    "        'prng_seed':prng_seed,\n",
    "        'train_split':train_split,\n",
    "        'val_split':val_split,\n",
    "        'batch_size':batch_size,\n",
    "        'fRMS':fRMS,\n",
    "        'timeMeanofSpaceRMS':timeMeanofSpaceRMS,\n",
    "        'stddev':stddev,\n",
    "        'zoneout_rate':zoneout_rate,\n",
    "        'rnncell_dropout_rate':rnncell_dropout_rate,\n",
    "        'denselayer_dropout_rate':denselayer_dropout_rate,\n",
    "    }\n",
    "\n",
    "    with open(dir_name_rnn+dir_sep+'training_specific_params.txt', 'w') as f:\n",
    "        f.write(str(training_specific_params))\n",
    "    \n",
    "    np.savez(\n",
    "        dir_name_rnn+dir_sep+'normalization_data',\n",
    "        normalization_arr=[normalization_arr],\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    # dir_name_rnn_og = dir_name_rnn\n",
    "    # dir_name_rnn_temp = '/home/rkaushik/Documents/Thesis/MLROM/CDV/saved_rnn/rnn_'+dir_name_rnn_og[-3:]\n",
    "    # dir_name_rnn = dir_name_rnn_temp\n",
    "\n",
    "    with open(dir_name_rnn + dir_sep + 'training_specific_params.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "\n",
    "    tparams_dict = eval(''.join(lines))\n",
    "\n",
    "    learning_rate_list = tparams_dict['learning_rate_list']\n",
    "    epochs = tparams_dict['epochs']\n",
    "    patience = tparams_dict['patience']\n",
    "    min_delta = tparams_dict['min_delta']\n",
    "    prng_seed = tparams_dict['prng_seed']\n",
    "    train_split = tparams_dict['train_split']\n",
    "    val_split = tparams_dict['val_split']\n",
    "    batch_size = tparams_dict['batch_size']\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # setting seed for PRNGs\n",
    "    np.random.seed(prng_seed)\n",
    "    tf.random.set_seed(prng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667868779212,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "4hx9ZaSpEMmv"
   },
   "outputs": [],
   "source": [
    "# idx = np.arange(data_rnn_input.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# boundary = int(np.round(train_split*data_rnn_input.shape[0]))\n",
    "\n",
    "# training_data_rnn_input = data_rnn_input[idx[0:boundary]]\n",
    "# training_data_rnn_output = data_rnn_output[idx[0:boundary]]\n",
    "\n",
    "# testing_data_rnn_input = data_rnn_input[idx[boundary:]]\n",
    "# testing_data_rnn_output = data_rnn_output[idx[boundary:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1667868779601,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "EENXaWqcKW7j"
   },
   "outputs": [],
   "source": [
    "cum_samples = rnn_data_boundary_idx_arr[-1]\n",
    "# idx = np.arange(cum_samples)\n",
    "# np.random.shuffle(idx)\n",
    "num_train_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_val_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_test_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "num_samples_arr = np.zeros(shape=rnn_data_boundary_idx_arr.shape[0], dtype='int32')\n",
    "begin_idx = 0\n",
    "for i in range(len(rnn_data_boundary_idx_arr)):\n",
    "    num_samples = batch_size * int((rnn_data_boundary_idx_arr[i] - begin_idx) // batch_size)\n",
    "    num_train_arr[i] = batch_size * int( np.round(train_split*num_samples/batch_size) )\n",
    "    num_val_arr[i] = batch_size * int( np.round(val_split*num_samples/batch_size) )\n",
    "    num_test_arr[i] = batch_size * int( np.round((num_samples - num_train_arr[i] - num_val_arr[i])/batch_size) )\n",
    "    num_samples_arr[i] = num_train_arr[i] + num_val_arr[i] + num_test_arr[i]\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# defining shapes\n",
    "training_input_shape = [np.sum(num_train_arr)]\n",
    "training_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "training_output_shape = [np.sum(num_train_arr)]\n",
    "training_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "val_input_shape = [np.sum(num_val_arr)]\n",
    "val_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "val_output_shape = [np.sum(num_val_arr)]\n",
    "val_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "testing_input_shape = [np.sum(num_test_arr)]\n",
    "testing_input_shape.extend(data_rnn_input.shape[1:])\n",
    "\n",
    "testing_output_shape = [np.sum(num_test_arr)]\n",
    "testing_output_shape.extend(data_rnn_output.shape[1:])\n",
    "\n",
    "# defining required arrays\n",
    "training_data_rnn_input = np.empty(shape=training_input_shape, dtype=FTYPE)\n",
    "training_data_rnn_output = np.empty(shape=training_output_shape, dtype=FTYPE)\n",
    "\n",
    "val_data_rnn_input = np.empty(shape=val_input_shape, dtype=FTYPE)\n",
    "val_data_rnn_output = np.empty(shape=val_output_shape, dtype=FTYPE)\n",
    "\n",
    "testing_data_rnn_input = np.empty(shape=testing_input_shape, dtype=FTYPE)\n",
    "testing_data_rnn_output = np.empty(shape=testing_output_shape, dtype=FTYPE)\n",
    "\n",
    "AR_testing_data_rnn_input = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "AR_testing_data_rnn_output = np.empty(shape=tuple(testing_input_shape[0:2])+tuple(AR_data_rnn_input.shape[2:]), dtype=FTYPE)\n",
    "\n",
    "begin_idx = 0\n",
    "training_data_rolling_count = 0\n",
    "val_data_rolling_count = 0\n",
    "testing_data_rolling_count = 0\n",
    "for i in range(len(boundary_idx_arr)):\n",
    "    idx = np.arange(begin_idx, rnn_data_boundary_idx_arr[i])\n",
    "    # np.random.shuffle(idx)\n",
    "    # num_samples = idx.shape[0]\n",
    "    # num_train = int( np.round(train_split*num_samples/batch_size) )*batch_size\n",
    "    # num_val = int( np.round(val_split*num_samples/batch_size) )*batch_size\n",
    "    \n",
    "    num_samples = num_samples_arr[i]\n",
    "    num_train = num_train_arr[i]\n",
    "    num_val = num_val_arr[i]\n",
    "    num_test = num_test_arr[i]\n",
    "    \n",
    "    nbatches_train = num_train // batch_size\n",
    "    nbatches_val = num_val // batch_size\n",
    "    nbatches_test = num_test // batch_size\n",
    "\n",
    "    for j in range(batch_size):\n",
    "        training_data_rnn_input[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_input[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        training_data_rnn_output[training_data_rolling_count+j:training_data_rolling_count+num_train:batch_size] = data_rnn_output[idx[0:num_train]][j*nbatches_train:(j+1)*nbatches_train]\n",
    "        \n",
    "        val_data_rnn_input[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_input[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "        val_data_rnn_output[val_data_rolling_count+j:val_data_rolling_count+num_val:batch_size] = data_rnn_output[idx[num_train:num_train+num_val]][j*nbatches_val:(j+1)*nbatches_val]\n",
    "\n",
    "        testing_data_rnn_input[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_input[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "        testing_data_rnn_output[testing_data_rolling_count+j:testing_data_rolling_count+num_test:batch_size] = data_rnn_output[idx[num_train+num_val:num_samples]][j*nbatches_test:(j+1)*nbatches_test]\n",
    "\n",
    "    AR_testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = AR_data_rnn_input[idx[num_train+num_val:num_samples]]\n",
    "    AR_testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = AR_data_rnn_output[idx[num_train+num_val:num_samples]]\n",
    "\n",
    "    # training_data_rnn_input[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_input[idx[0:num_train]]\n",
    "    # training_data_rnn_output[training_data_rolling_count:training_data_rolling_count+num_train] = data_rnn_output[idx[0:num_train]]\n",
    "    training_data_rolling_count += num_train\n",
    "\n",
    "    # val_data_rnn_input[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_input[idx[num_train:num_train+num_val]]\n",
    "    # val_data_rnn_output[val_data_rolling_count:val_data_rolling_count+num_val] = data_rnn_output[idx[num_train:num_train+num_val]]\n",
    "    val_data_rolling_count += num_val\n",
    "\n",
    "    # num_test = num_samples-num_train-num_val+1\n",
    "    # testing_data_rnn_input[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_input[idx[num_train+num_val:]]\n",
    "    # testing_data_rnn_output[testing_data_rolling_count:testing_data_rolling_count+num_test] = data_rnn_output[idx[num_train+num_val:]]\n",
    "    testing_data_rolling_count += num_test\n",
    "\n",
    "    begin_idx = rnn_data_boundary_idx_arr[i]\n",
    "\n",
    "# cleaning up\n",
    "del(data_rnn_input)\n",
    "del(data_rnn_output)\n",
    "del(AR_data_rnn_input)\n",
    "del(AR_data_rnn_output)\n",
    "\n",
    "# further shuffling\n",
    "if stateful == False:\n",
    "    idx = np.arange(0, training_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    training_data_rnn_input = training_data_rnn_input[idx]\n",
    "    training_data_rnn_output = training_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, val_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    val_data_rnn_input = val_data_rnn_input[idx]\n",
    "    val_data_rnn_output = val_data_rnn_output[idx]\n",
    "\n",
    "    idx = np.arange(0, testing_data_rnn_input.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    testing_data_rnn_input = testing_data_rnn_input[idx]\n",
    "    testing_data_rnn_output = testing_data_rnn_output[idx]\n",
    "\n",
    "    del(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667868779603,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "8isZN1tYBifp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_runs :  100\n"
     ]
    }
   ],
   "source": [
    "s_in = AR_testing_data_rnn_input.shape\n",
    "AR_testing_data_rnn_input = AR_testing_data_rnn_input.reshape((1, s_in[0]*s_in[1]) + s_in[2:])\n",
    "\n",
    "s_out = AR_testing_data_rnn_output.shape\n",
    "AR_testing_data_rnn_output = AR_testing_data_rnn_output.reshape((1, s_out[0]*s_out[1]) + s_out[2:])\n",
    "\n",
    "T_sample_input_AR = T_sample_input_AR_ratio*np.mean(lyapunov_time_arr)#50.1*dt_rnn\n",
    "num_sample_input_AR = int((T_sample_input_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "T_sample_output_AR = T_sample_output_AR_ratio*np.mean(lyapunov_time_arr)\n",
    "num_sample_output_AR = int((T_sample_output_AR+0.5*dt_rnn)//dt_rnn)\n",
    "\n",
    "num_offset_AR = num_sample_input_AR\n",
    "T_offset_AR = num_offset_AR*dt_rnn\n",
    "\n",
    "batch_idx = np.random.randint(low=0, high=AR_testing_data_rnn_input.shape[0])\n",
    "maxpossible_num_runs = AR_testing_data_rnn_input.shape[1]-(num_sample_input_AR+num_sample_output_AR)\n",
    "\n",
    "num_runs = np.min([num_runs, maxpossible_num_runs])\n",
    "\n",
    "print('num_runs : ', num_runs)\n",
    "\n",
    "data_idx_arr = np.linspace(0, maxpossible_num_runs-1, num_runs, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779605,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "x3KglJsgneQj"
   },
   "outputs": [],
   "source": [
    "AR_data_in = np.empty(shape=(num_runs, num_sample_input_AR)+tuple(s_in[2:]))\n",
    "AR_data_out = np.empty(shape=(num_runs, num_sample_output_AR)+tuple(s_out[2:]))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    d_idx = data_idx_arr[i]\n",
    "    AR_data_in[i] = AR_testing_data_rnn_input[0, d_idx:d_idx+num_sample_input_AR]\n",
    "    AR_data_out[i] = AR_testing_data_rnn_input[0, d_idx+num_sample_input_AR:d_idx+num_sample_input_AR+num_sample_output_AR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "ixetsZHjCMKO"
   },
   "outputs": [],
   "source": [
    "del(AR_testing_data_rnn_input)\n",
    "del(AR_testing_data_rnn_output)\n",
    "AR_testing_data_rnn_input = AR_data_in\n",
    "AR_testing_data_rnn_output = AR_data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   training_data_rnn_input.shape :  (576, 552, 2)\n",
      "  training_data_rnn_output.shape :  (576, 552, 2)\n",
      "    testing_data_rnn_input.shape :  (96, 552, 2)\n",
      "   testing_data_rnn_output.shape :  (96, 552, 2)\n",
      "        val_data_rnn_input.shape :  (64, 552, 2)\n",
      "       val_data_rnn_output.shape :  (64, 552, 2)\n",
      "\n",
      " AR_testing_data_rnn_input.shape :  (100, 11, 3)\n",
      "AR_testing_data_rnn_output.shape :  (100, 33, 3)\n"
     ]
    }
   ],
   "source": [
    "print('   training_data_rnn_input.shape : ', training_data_rnn_input.shape)\n",
    "print('  training_data_rnn_output.shape : ', training_data_rnn_output.shape)\n",
    "print('    testing_data_rnn_input.shape : ', testing_data_rnn_input.shape)\n",
    "print('   testing_data_rnn_output.shape : ', testing_data_rnn_output.shape)\n",
    "print('        val_data_rnn_input.shape : ', val_data_rnn_input.shape)\n",
    "print('       val_data_rnn_output.shape : ', val_data_rnn_output.shape)\n",
    "print('')\n",
    "print(' AR_testing_data_rnn_input.shape : ', AR_testing_data_rnn_input.shape)\n",
    "print('AR_testing_data_rnn_output.shape : ', AR_testing_data_rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667868779606,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "_NSTtZuyneQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1667868783230,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "Py-Jg0QKneQk",
    "outputId": "1b768270-9013-4d53-8b5e-63e69776e3ac",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeMeanofSpaceRMS : 0.2933353\n",
      "stddev : 0.0017228189150698256\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "#     rnn_layers_units = [500]*3\n",
    "#     scalar_weights = None\n",
    "#     scalar_weights = [\n",
    "#         1.0,\n",
    "#     ] # Euler\n",
    "    scalar_weights = [\n",
    "        0.5, \n",
    "        0.0, 0.5,\n",
    "        0.0, 0.0, 1.0,\n",
    "        1/6, 1/3, 1/3, 1/6\n",
    "    ] # RK4\n",
    "    # scalar_weights = [\n",
    "    #     1.0,\n",
    "    #     0.25, 0.25,\n",
    "    #     1/6, 1/6, 2/3\n",
    "    # ] # TVD RK3\n",
    "#     scalar_weights = [\n",
    "#         1.0,\n",
    "#         0.5, 0.5\n",
    "#     ] # TVD RK2\n",
    "    num_rnn_layers = 1\n",
    "    if not isinstance(scalar_weights, type(None)):\n",
    "        num_rnn_layers += int( ((8*len(scalar_weights)+1)**0.5 - 1)/2 )\n",
    "    rnn_layers_units = [80*num_latent_states]*num_rnn_layers\n",
    "    # timeMeanofSpaceRMS = np.mean(np.mean(latent_states_all**2, axis=1)**0.5)\n",
    "    print('timeMeanofSpaceRMS :', timeMeanofSpaceRMS)\n",
    "    print('stddev :', stddev)\n",
    "    if return_params_arr != False:\n",
    "        data_dim = num_latent_states + 3\n",
    "    else:\n",
    "        data_dim = num_latent_states\n",
    "\n",
    "    dense_dim = [rnn_layers_units[-1]]*(len(dense_layer_act_func)-1)\n",
    "    dense_dim.append(data_dim)\n",
    "        \n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_GRU(\n",
    "                data_dim=data_dim,\n",
    "            #     in_steps=int(T_sample_input // dt_rnn),\n",
    "            #     out_steps=int(T_sample_output // dt_rnn),\n",
    "                dt_rnn=dt_rnn,\n",
    "                lambda_reg=lambda_reg,\n",
    "                reg_name='L2',\n",
    "                rnn_layers_units=rnn_layers_units,\n",
    "                dense_layer_act_func=dense_layer_act_func,\n",
    "                load_file=None,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                stddev=stddev,\n",
    "                noise_type=noise_type,\n",
    "                dense_dim=dense_dim,\n",
    "                use_learnable_state=use_learnable_state,\n",
    "                stateful=stateful,\n",
    "                zoneout_rate=zoneout_rate,\n",
    "                batch_size=batch_size,\n",
    "                use_weights_post_dense=use_weights_post_dense,\n",
    "                rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "                denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "                scalar_weights=scalar_weights, # corresponding to RK4\n",
    "                use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_GRU(\n",
    "            data_dim=data_dim,\n",
    "        #     in_steps=int(T_sample_input // dt_rnn),\n",
    "        #     out_steps=int(T_sample_output // dt_rnn),\n",
    "            dt_rnn=dt_rnn,\n",
    "            lambda_reg=lambda_reg,\n",
    "            reg_name='L2',\n",
    "            rnn_layers_units=rnn_layers_units,\n",
    "            dense_layer_act_func=dense_layer_act_func,\n",
    "            load_file=None,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            stddev=stddev,\n",
    "            noise_type=noise_type,\n",
    "            dense_dim=dense_dim,\n",
    "            use_learnable_state=use_learnable_state,\n",
    "            stateful=stateful,\n",
    "            zoneout_rate=zoneout_rate,\n",
    "            batch_size=batch_size,\n",
    "            use_weights_post_dense=use_weights_post_dense,\n",
    "            rnncell_dropout_rate=rnncell_dropout_rate,\n",
    "            denselayer_dropout_rate=denselayer_dropout_rate,\n",
    "            scalar_weights=scalar_weights, # corresponding to RK4\n",
    "            use_trainable_weights_with_reslayers=use_trainable_weights_with_reslayers,\n",
    "        )\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    rnn_net.save_class_dict(save_path+dir_sep+'final_net_class_dict.txt')\n",
    "else:\n",
    "    load_file = dir_name_rnn + dir_sep + 'final_net' + dir_sep + 'final_net_class_dict.txt'\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net = RNN_GRU(\n",
    "                load_file=load_file,\n",
    "                # T_input=T_sample_input,\n",
    "                # T_output=T_sample_output,\n",
    "                batch_size=batch_size,\n",
    "                \n",
    "            )\n",
    "    else:\n",
    "        rnn_net = RNN_GRU(\n",
    "            load_file=load_file,\n",
    "            # T_input=T_sample_input,\n",
    "            # T_output=T_sample_output,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    rnn_net.build(input_shape=(batch_size, None, num_latent_states))\n",
    "    \n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        wt_file = tf.train.latest_checkpoint(dir_name_rnn+dir_sep+'checkpoints')\n",
    "    elif behaviour == 'loadFinalNetAndPlot':\n",
    "        wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'final_net_gru_weights.h5'\n",
    "        # wt_file = dir_name_rnn+dir_sep+'final_net'+dir_sep+'f2'#+dir_sep+'saved_model.pb'\n",
    "        rnn_net.load_weights_from_file(wt_file)\n",
    "    \n",
    "    # this forces the model to initialize its kernel weights/biases\n",
    "    # temp = rnn_net.predict(tf.ones(shape=[batch_size, int(T_sample_input//dt_rnn), rnn_net.data_dim]))\n",
    "    # this loads just the kernel wieghts and biases of the model\n",
    "#     rnn_net.load_weights_from_file(wt_file)\n",
    "\n",
    "    # rnn_net = tf.keras.models.load_model(wt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667868783568,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "7ASCopnIH6nl"
   },
   "outputs": [],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch':\n",
    "    val_loss_hist = []\n",
    "    train_loss_hist = []\n",
    "    lr_change=[0, 0]\n",
    "    savelosses_cb_vallossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    savelosses_cb_trainlossarr = np.ones(shape=epochs*len(learning_rate_list))*np.NaN\n",
    "    starting_lr_idx = 0\n",
    "    num_epochs_left = epochs\n",
    "    earlystopping_wait = 0\n",
    "elif behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    val_loss_hist, train_loss_hist, lr_change, starting_lr_idx, num_epochs_left, val_loss_arr_fromckpt, train_loss_arr_fromckpt, earlystopping_wait = readAndReturnLossHistories(\n",
    "        dir_name_ae=dir_name_rnn,\n",
    "        dir_sep=dir_sep,\n",
    "        epochs=epochs,\n",
    "        learning_rate_list=learning_rate_list,\n",
    "        return_earlystopping_wait=True)\n",
    "    savelosses_cb_vallossarr = val_loss_arr_fromckpt\n",
    "    savelosses_cb_trainlossarr = train_loss_arr_fromckpt\n",
    "elif behaviour == 'loadFinalNetAndPlot':\n",
    "    with open(dir_name_rnn+'{ds}final_net{ds}losses.txt'.format(ds=dir_sep), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    losses_dict = eval(''.join(lines))\n",
    "\n",
    "    val_loss_hist = losses_dict['val_loss_hist']\n",
    "    train_loss_hist = losses_dict['train_loss_hist']\n",
    "    lr_change = losses_dict['lr_change']\n",
    "    test_loss = losses_dict['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NMSE_hist = []\n",
    "val_NMSE_hist = []\n",
    "\n",
    "train_MSE_hist = []\n",
    "val_MSE_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMSE(tf.keras.metrics.MeanSquaredError):\n",
    "    def __init__(self, divisor_arr, name='NMSE', **kwargs):\n",
    "        super(NMSE, self).__init__(name, **kwargs)\n",
    "        self.divisor_arr = divisor_arr\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = y_true / self.divisor_arr\n",
    "        y_pred = y_pred / self.divisor_arr\n",
    "        return super(NMSE, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4769220,
     "status": "ok",
     "timestamp": 1667873552785,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "6hh1pbKjCcO4",
    "outputId": "e594f4de-ec70-465e-eef7-bdef301361fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------- LEARNING RATE : 0.01 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0696 - mse: 0.0693 - NMSE: 0.6267 - tot_time: 0h 0m 38.9s\n",
      "\n",
      "Epoch 1: val_NMSE improved from inf to 0.17230, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 39s 2s/step - loss: 0.0696 - mse: 0.0693 - NMSE: 0.6267 - val_loss: 0.0194 - val_mse: 0.0191 - val_NMSE: 0.1723\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0108 - mse: 0.0104 - NMSE: 0.0938 - tot_time: 0h 1m 12.1s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.17230 to 0.03905, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.0108 - mse: 0.0104 - NMSE: 0.0938 - val_loss: 0.0049 - val_mse: 0.0043 - val_NMSE: 0.0391\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0027 - mse: 0.0020 - NMSE: 0.0184 - tot_time: 0h 1m 45.7s\n",
      "\n",
      "Epoch 3: val_NMSE improved from 0.03905 to 0.00968, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 0.0027 - mse: 0.0020 - NMSE: 0.0184 - val_loss: 0.0017 - val_mse: 0.0011 - val_NMSE: 0.0097\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0013 - mse: 6.4584e-04 - NMSE: 0.0058 - tot_time: 0h 2m 19.1s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.00968 to 0.00524, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.0013 - mse: 6.4584e-04 - NMSE: 0.0058 - val_loss: 0.0012 - val_mse: 5.7998e-04 - val_NMSE: 0.0052\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0010 - mse: 3.9100e-04 - NMSE: 0.0035 - tot_time: 0h 2m 52.4s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.00524 to 0.00388, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 0.0010 - mse: 3.9100e-04 - NMSE: 0.0035 - val_loss: 0.0011 - val_mse: 4.2869e-04 - val_NMSE: 0.0039\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 9.1335e-04 - mse: 3.0546e-04 - NMSE: 0.0028 - tot_time: 0h 3m 25.8s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.00388 to 0.00334, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 9.1335e-04 - mse: 3.0546e-04 - NMSE: 0.0028 - val_loss: 9.5536e-04 - val_mse: 3.6894e-04 - val_NMSE: 0.0033\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 8.3092e-04 - mse: 2.6325e-04 - NMSE: 0.0024 - tot_time: 0h 3m 59.1s\n",
      "\n",
      "Epoch 7: val_NMSE improved from 0.00334 to 0.00300, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 8.3092e-04 - mse: 2.6325e-04 - NMSE: 0.0024 - val_loss: 8.7898e-04 - val_mse: 3.3228e-04 - val_NMSE: 0.0030\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.6856e-04 - mse: 2.3989e-04 - NMSE: 0.0022 - tot_time: 0h 4m 31.7s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.00300 to 0.00294, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 7.6856e-04 - mse: 2.3989e-04 - NMSE: 0.0022 - val_loss: 8.3455e-04 - val_mse: 3.2563e-04 - val_NMSE: 0.0029\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.7204e-04 - mse: 2.7955e-04 - NMSE: 0.0025 - tot_time: 0h 5m 4.2s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.00294 to 0.00264, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 7.7204e-04 - mse: 2.7955e-04 - NMSE: 0.0025 - val_loss: 7.6741e-04 - val_mse: 2.9234e-04 - val_NMSE: 0.0026\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.8062e-04 - mse: 2.1967e-04 - NMSE: 0.0020 - tot_time: 0h 5m 37.4s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.00264 to 0.00247, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 6.8062e-04 - mse: 2.1967e-04 - NMSE: 0.0020 - val_loss: 7.1883e-04 - val_mse: 2.7323e-04 - val_NMSE: 0.0025\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.2594e-04 - mse: 1.9356e-04 - NMSE: 0.0017 - tot_time: 0h 6m 11.0s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00247 to 0.00233, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 6.2594e-04 - mse: 1.9356e-04 - NMSE: 0.0017 - val_loss: 6.7570e-04 - val_mse: 2.5765e-04 - val_NMSE: 0.0023\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 7.1068e-04 - mse: 3.0476e-04 - NMSE: 0.0028 - tot_time: 0h 6m 44.0s\n",
      "\n",
      "Epoch 12: val_NMSE did not improve from 0.00233\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 7.1068e-04 - mse: 3.0476e-04 - NMSE: 0.0028 - val_loss: 9.4731e-04 - val_mse: 5.5416e-04 - val_NMSE: 0.0050\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.2728e-04 - mse: 2.4239e-04 - NMSE: 0.0022 - tot_time: 0h 7m 17.3s\n",
      "\n",
      "Epoch 13: val_NMSE improved from 0.00233 to 0.00222, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 6.2728e-04 - mse: 2.4239e-04 - NMSE: 0.0022 - val_loss: 6.2119e-04 - val_mse: 2.4546e-04 - val_NMSE: 0.0022\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.4663e-04 - mse: 1.7961e-04 - NMSE: 0.0016 - tot_time: 0h 7m 50.8s\n",
      "\n",
      "Epoch 14: val_NMSE improved from 0.00222 to 0.00217, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 5.4663e-04 - mse: 1.7961e-04 - NMSE: 0.0016 - val_loss: 5.9727e-04 - val_mse: 2.3989e-04 - val_NMSE: 0.0022\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.1858e-04 - mse: 1.6954e-04 - NMSE: 0.0015 - tot_time: 0h 8m 24.1s\n",
      "\n",
      "Epoch 15: val_NMSE did not improve from 0.00217\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 5.1858e-04 - mse: 1.6954e-04 - NMSE: 0.0015 - val_loss: 6.7188e-04 - val_mse: 3.3203e-04 - val_NMSE: 0.0030\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.1697e-04 - mse: 2.8320e-04 - NMSE: 0.0026 - tot_time: 0h 8m 58.2s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00217 to 0.00205, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 6.1697e-04 - mse: 2.8320e-04 - NMSE: 0.0026 - val_loss: 5.5498e-04 - val_mse: 2.2655e-04 - val_NMSE: 0.0020\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.8300e-04 - mse: 1.6028e-04 - NMSE: 0.0014 - tot_time: 0h 9m 31.4s\n",
      "\n",
      "Epoch 17: val_NMSE improved from 0.00205 to 0.00189, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.8300e-04 - mse: 1.6028e-04 - NMSE: 0.0014 - val_loss: 5.2451e-04 - val_mse: 2.0876e-04 - val_NMSE: 0.0019\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.5384e-04 - mse: 1.4439e-04 - NMSE: 0.0013 - tot_time: 0h 10m 4.4s\n",
      "\n",
      "Epoch 18: val_NMSE improved from 0.00189 to 0.00181, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.5384e-04 - mse: 1.4439e-04 - NMSE: 0.0013 - val_loss: 5.0323e-04 - val_mse: 2.0075e-04 - val_NMSE: 0.0018\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.3462e-04 - mse: 1.3792e-04 - NMSE: 0.0012 - tot_time: 0h 10m 37.5s\n",
      "\n",
      "Epoch 19: val_NMSE improved from 0.00181 to 0.00177, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.3462e-04 - mse: 1.3792e-04 - NMSE: 0.0012 - val_loss: 4.8615e-04 - val_mse: 1.9569e-04 - val_NMSE: 0.0018\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.9196e-04 - mse: 4.0618e-04 - NMSE: 0.0037 - tot_time: 0h 11m 11.0s\n",
      "\n",
      "Epoch 20: val_NMSE did not improve from 0.00177\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 6.9196e-04 - mse: 4.0618e-04 - NMSE: 0.0037 - val_loss: 6.3813e-04 - val_mse: 3.5513e-04 - val_NMSE: 0.0032\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.9893e-04 - mse: 2.1707e-04 - NMSE: 0.0020 - tot_time: 0h 11m 43.9s\n",
      "\n",
      "Epoch 21: val_NMSE did not improve from 0.00177\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.9893e-04 - mse: 2.1707e-04 - NMSE: 0.0020 - val_loss: 4.9255e-04 - val_mse: 2.1299e-04 - val_NMSE: 0.0019\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.2201e-04 - mse: 1.4587e-04 - NMSE: 0.0013 - tot_time: 0h 12m 15.9s\n",
      "\n",
      "Epoch 22: val_NMSE improved from 0.00177 to 0.00173, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 4.2201e-04 - mse: 1.4587e-04 - NMSE: 0.0013 - val_loss: 4.6393e-04 - val_mse: 1.9190e-04 - val_NMSE: 0.0017\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.9741e-04 - mse: 1.2922e-04 - NMSE: 0.0012 - tot_time: 0h 12m 48.9s\n",
      "\n",
      "Epoch 23: val_NMSE improved from 0.00173 to 0.00164, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.9741e-04 - mse: 1.2922e-04 - NMSE: 0.0012 - val_loss: 4.4488e-04 - val_mse: 1.8095e-04 - val_NMSE: 0.0016\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.9288e-04 - mse: 1.3265e-04 - NMSE: 0.0012 - tot_time: 0h 13m 22.0s\n",
      "\n",
      "Epoch 24: val_NMSE did not improve from 0.00164\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.9288e-04 - mse: 1.3265e-04 - NMSE: 0.0012 - val_loss: 5.1944e-04 - val_mse: 2.6313e-04 - val_NMSE: 0.0024\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.7794e-04 - mse: 3.2347e-04 - NMSE: 0.0029 - tot_time: 0h 13m 56.0s\n",
      "\n",
      "Epoch 25: val_NMSE did not improve from 0.00164\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 5.7794e-04 - mse: 3.2347e-04 - NMSE: 0.0029 - val_loss: 4.5368e-04 - val_mse: 1.9964e-04 - val_NMSE: 0.0018\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.0483e-04 - mse: 1.5182e-04 - NMSE: 0.0014 - tot_time: 0h 14m 30.1s\n",
      "\n",
      "Epoch 26: val_NMSE did not improve from 0.00164\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 4.0483e-04 - mse: 1.5182e-04 - NMSE: 0.0014 - val_loss: 4.5478e-04 - val_mse: 2.0379e-04 - val_NMSE: 0.0018\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.0706e-04 - mse: 1.5863e-04 - NMSE: 0.0014 - tot_time: 0h 15m 2.8s\n",
      "\n",
      "Epoch 27: val_NMSE improved from 0.00164 to 0.00157, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.0706e-04 - mse: 1.5863e-04 - NMSE: 0.0014 - val_loss: 4.1964e-04 - val_mse: 1.7372e-04 - val_NMSE: 0.0016\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6254e-04 - mse: 1.1898e-04 - NMSE: 0.0011 - tot_time: 0h 15m 35.9s\n",
      "\n",
      "Epoch 28: val_NMSE improved from 0.00157 to 0.00154, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.6254e-04 - mse: 1.1898e-04 - NMSE: 0.0011 - val_loss: 4.1108e-04 - val_mse: 1.7041e-04 - val_NMSE: 0.0015\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.5226e-04 - mse: 4.1301e-04 - NMSE: 0.0037 - tot_time: 0h 16m 9.2s\n",
      "\n",
      "Epoch 29: val_NMSE did not improve from 0.00154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 6.5226e-04 - mse: 4.1301e-04 - NMSE: 0.0037 - val_loss: 6.1577e-04 - val_mse: 3.7502e-04 - val_NMSE: 0.0034\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.5010e-04 - mse: 2.0746e-04 - NMSE: 0.0019 - tot_time: 0h 16m 41.9s\n",
      "\n",
      "Epoch 30: val_NMSE did not improve from 0.00154\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.5010e-04 - mse: 2.0746e-04 - NMSE: 0.0019 - val_loss: 4.1678e-04 - val_mse: 1.7360e-04 - val_NMSE: 0.0016\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6783e-04 - mse: 1.2627e-04 - NMSE: 0.0011 - tot_time: 0h 17m 14.9s\n",
      "\n",
      "Epoch 31: val_NMSE improved from 0.00154 to 0.00142, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.6783e-04 - mse: 1.2627e-04 - NMSE: 0.0011 - val_loss: 3.9580e-04 - val_mse: 1.5676e-04 - val_NMSE: 0.0014\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4762e-04 - mse: 1.1109e-04 - NMSE: 0.0010 - tot_time: 0h 17m 48.7s\n",
      "\n",
      "Epoch 32: val_NMSE improved from 0.00142 to 0.00140, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.4762e-04 - mse: 1.1109e-04 - NMSE: 0.0010 - val_loss: 3.8878e-04 - val_mse: 1.5517e-04 - val_NMSE: 0.0014\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4481e-04 - mse: 1.1358e-04 - NMSE: 0.0010 - tot_time: 0h 18m 22.1s\n",
      "\n",
      "Epoch 33: val_NMSE did not improve from 0.00140\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.4481e-04 - mse: 1.1358e-04 - NMSE: 0.0010 - val_loss: 4.4270e-04 - val_mse: 2.1411e-04 - val_NMSE: 0.0019\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.8067e-04 - mse: 2.5334e-04 - NMSE: 0.0023 - tot_time: 0h 18m 55.3s\n",
      "\n",
      "Epoch 34: val_NMSE did not improve from 0.00140\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.8067e-04 - mse: 2.5334e-04 - NMSE: 0.0023 - val_loss: 4.3258e-04 - val_mse: 2.0627e-04 - val_NMSE: 0.0019\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4251e-04 - mse: 1.1714e-04 - NMSE: 0.0011 - tot_time: 0h 19m 29.6s\n",
      "\n",
      "Epoch 35: val_NMSE improved from 0.00140 to 0.00134, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.4251e-04 - mse: 1.1714e-04 - NMSE: 0.0011 - val_loss: 3.7181e-04 - val_mse: 1.4811e-04 - val_NMSE: 0.0013\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2271e-04 - mse: 1.0079e-04 - NMSE: 9.1114e-04 - tot_time: 0h 20m 3.7s\n",
      "\n",
      "Epoch 36: val_NMSE improved from 0.00134 to 0.00126, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.2271e-04 - mse: 1.0079e-04 - NMSE: 9.1114e-04 - val_loss: 3.5949e-04 - val_mse: 1.3974e-04 - val_NMSE: 0.0013\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.5867e-04 - mse: 4.4006e-04 - NMSE: 0.0040 - tot_time: 0h 20m 36.6s\n",
      "\n",
      "Epoch 37: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 6.5867e-04 - mse: 4.4006e-04 - NMSE: 0.0040 - val_loss: 5.8854e-04 - val_mse: 3.6934e-04 - val_NMSE: 0.0033\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.0642e-04 - mse: 1.8530e-04 - NMSE: 0.0017 - tot_time: 0h 21m 9.5s\n",
      "\n",
      "Epoch 38: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.0642e-04 - mse: 1.8530e-04 - NMSE: 0.0017 - val_loss: 4.0744e-04 - val_mse: 1.8607e-04 - val_NMSE: 0.0017\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3065e-04 - mse: 1.1065e-04 - NMSE: 0.0010 - tot_time: 0h 21m 43.0s\n",
      "\n",
      "Epoch 39: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.3065e-04 - mse: 1.1065e-04 - NMSE: 0.0010 - val_loss: 3.6320e-04 - val_mse: 1.4541e-04 - val_NMSE: 0.0013\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.2496e-04 - mse: 2.0929e-04 - NMSE: 0.0019 - tot_time: 0h 22m 14.7s\n",
      "\n",
      "Epoch 40: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 4.2496e-04 - mse: 2.0929e-04 - NMSE: 0.0019 - val_loss: 0.0011 - val_mse: 8.7247e-04 - val_NMSE: 0.0079\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.7719e-04 - mse: 2.6171e-04 - NMSE: 0.0024 - tot_time: 0h 22m 47.7s\n",
      "\n",
      "Epoch 41: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.7719e-04 - mse: 2.6171e-04 - NMSE: 0.0024 - val_loss: 4.0799e-04 - val_mse: 1.9118e-04 - val_NMSE: 0.0017\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.4400e-04 - mse: 1.2752e-04 - NMSE: 0.0012 - tot_time: 0h 23m 21.6s\n",
      "\n",
      "Epoch 42: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.4400e-04 - mse: 1.2752e-04 - NMSE: 0.0012 - val_loss: 3.7917e-04 - val_mse: 1.6395e-04 - val_NMSE: 0.0015\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2362e-04 - mse: 1.1002e-04 - NMSE: 9.9461e-04 - tot_time: 0h 23m 55.6s\n",
      "\n",
      "Epoch 43: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.2362e-04 - mse: 1.1002e-04 - NMSE: 9.9461e-04 - val_loss: 4.1272e-04 - val_mse: 2.0107e-04 - val_NMSE: 0.0018\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.2624e-04 - mse: 2.1589e-04 - NMSE: 0.0020 - tot_time: 0h 24m 29.0s\n",
      "\n",
      "Epoch 44: val_NMSE did not improve from 0.00126\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.2624e-04 - mse: 2.1589e-04 - NMSE: 0.0020 - val_loss: 3.5925e-04 - val_mse: 1.4938e-04 - val_NMSE: 0.0014\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.1347e-04 - mse: 1.0416e-04 - NMSE: 9.4160e-04 - tot_time: 0h 25m 3.4s\n",
      "\n",
      "Epoch 45: val_NMSE improved from 0.00126 to 0.00123, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.1347e-04 - mse: 1.0416e-04 - NMSE: 9.4160e-04 - val_loss: 3.4366e-04 - val_mse: 1.3566e-04 - val_NMSE: 0.0012\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9636e-04 - mse: 8.9800e-05 - NMSE: 8.1181e-04 - tot_time: 0h 25m 37.3s\n",
      "\n",
      "Epoch 46: val_NMSE improved from 0.00123 to 0.00119, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.9636e-04 - mse: 8.9800e-05 - NMSE: 8.1181e-04 - val_loss: 3.3683e-04 - val_mse: 1.3198e-04 - val_NMSE: 0.0012\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.9599e-04 - mse: 2.9244e-04 - NMSE: 0.0026 - tot_time: 0h 26m 10.7s\n",
      "\n",
      "Epoch 47: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 4.9599e-04 - mse: 2.9244e-04 - NMSE: 0.0026 - val_loss: 0.0011 - val_mse: 8.8433e-04 - val_NMSE: 0.0080\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.5078e-04 - mse: 3.4446e-04 - NMSE: 0.0031 - tot_time: 0h 26m 45.3s\n",
      "\n",
      "Epoch 48: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 5.5078e-04 - mse: 3.4446e-04 - NMSE: 0.0031 - val_loss: 4.1064e-04 - val_mse: 2.0138e-04 - val_NMSE: 0.0018\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2686e-04 - mse: 1.1778e-04 - NMSE: 0.0011 - tot_time: 0h 27m 19.9s\n",
      "\n",
      "Epoch 49: val_NMSE did not improve from 0.00119\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 3.2686e-04 - mse: 1.1778e-04 - NMSE: 0.0011 - val_loss: 3.4743e-04 - val_mse: 1.3957e-04 - val_NMSE: 0.0013\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.9883e-04 - mse: 9.2503e-05 - NMSE: 8.3624e-04 - tot_time: 0h 27m 53.8s\n",
      "\n",
      "Epoch 50: val_NMSE improved from 0.00119 to 0.00119, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.9883e-04 - mse: 9.2503e-05 - NMSE: 8.3624e-04 - val_loss: 3.3566e-04 - val_mse: 1.3132e-04 - val_NMSE: 0.0012\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8763e-04 - mse: 8.4990e-05 - NMSE: 7.6833e-04 - tot_time: 0h 28m 27.6s\n",
      "\n",
      "Epoch 51: val_NMSE improved from 0.00119 to 0.00115, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.8763e-04 - mse: 8.4990e-05 - NMSE: 7.6833e-04 - val_loss: 3.2778e-04 - val_mse: 1.2704e-04 - val_NMSE: 0.0011\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8254e-04 - mse: 8.3250e-05 - NMSE: 7.5260e-04 - tot_time: 0h 29m 1.5s\n",
      "\n",
      "Epoch 52: val_NMSE did not improve from 0.00115\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.8254e-04 - mse: 8.3250e-05 - NMSE: 7.5260e-04 - val_loss: 3.2615e-04 - val_mse: 1.2846e-04 - val_NMSE: 0.0012\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 5.6382e-04 - mse: 3.6686e-04 - NMSE: 0.0033 - tot_time: 0h 29m 35.7s\n",
      "\n",
      "Epoch 53: val_NMSE did not improve from 0.00115\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 5.6382e-04 - mse: 3.6686e-04 - NMSE: 0.0033 - val_loss: 5.6295e-04 - val_mse: 3.6488e-04 - val_NMSE: 0.0033\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5479e-04 - mse: 1.5539e-04 - NMSE: 0.0014 - tot_time: 0h 30m 9.0s\n",
      "\n",
      "Epoch 54: val_NMSE did not improve from 0.00115\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.5479e-04 - mse: 1.5539e-04 - NMSE: 0.0014 - val_loss: 3.4440e-04 - val_mse: 1.4454e-04 - val_NMSE: 0.0013\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8970e-04 - mse: 9.0715e-05 - NMSE: 8.2005e-04 - tot_time: 0h 30m 43.4s\n",
      "\n",
      "Epoch 55: val_NMSE improved from 0.00115 to 0.00110, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.8970e-04 - mse: 9.0715e-05 - NMSE: 8.2005e-04 - val_loss: 3.1904e-04 - val_mse: 1.2135e-04 - val_NMSE: 0.0011\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2468e-04 - mse: 1.2830e-04 - NMSE: 0.0012 - tot_time: 0h 31m 17.1s\n",
      "\n",
      "Epoch 56: val_NMSE did not improve from 0.00110\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.2468e-04 - mse: 1.2830e-04 - NMSE: 0.0012 - val_loss: 3.4115e-04 - val_mse: 1.4589e-04 - val_NMSE: 0.0013\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8750e-04 - mse: 9.2857e-05 - NMSE: 8.3945e-04 - tot_time: 0h 31m 51.4s\n",
      "\n",
      "Epoch 57: val_NMSE improved from 0.00110 to 0.00104, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.8750e-04 - mse: 9.2857e-05 - NMSE: 8.3945e-04 - val_loss: 3.0877e-04 - val_mse: 1.1502e-04 - val_NMSE: 0.0010\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7011e-04 - mse: 7.7474e-05 - NMSE: 7.0037e-04 - tot_time: 0h 32m 25.7s\n",
      "\n",
      "Epoch 58: val_NMSE did not improve from 0.00104\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.7011e-04 - mse: 7.7474e-05 - NMSE: 7.0037e-04 - val_loss: 3.1887e-04 - val_mse: 1.2755e-04 - val_NMSE: 0.0012\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5798e-04 - mse: 1.6723e-04 - NMSE: 0.0015 - tot_time: 0h 32m 59.7s\n",
      "\n",
      "Epoch 59: val_NMSE did not improve from 0.00104\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.5798e-04 - mse: 1.6723e-04 - NMSE: 0.0015 - val_loss: 3.1413e-04 - val_mse: 1.2370e-04 - val_NMSE: 0.0011\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7881e-04 - mse: 8.8818e-05 - NMSE: 8.0289e-04 - tot_time: 0h 33m 33.2s\n",
      "\n",
      "Epoch 60: val_NMSE did not improve from 0.00104\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.7881e-04 - mse: 8.8818e-05 - NMSE: 8.0289e-04 - val_loss: 3.2952e-04 - val_mse: 1.4028e-04 - val_NMSE: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.8002e-04 - mse: 1.9135e-04 - NMSE: 0.0017 - tot_time: 0h 34m 7.2s\n",
      "\n",
      "Epoch 61: val_NMSE did not improve from 0.00104\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.8002e-04 - mse: 1.9135e-04 - NMSE: 0.0017 - val_loss: 3.5787e-04 - val_mse: 1.6958e-04 - val_NMSE: 0.0015\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8504e-04 - mse: 9.7126e-05 - NMSE: 8.7801e-04 - tot_time: 0h 34m 40.6s\n",
      "\n",
      "Epoch 62: val_NMSE did not improve from 0.00104\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.8504e-04 - mse: 9.7126e-05 - NMSE: 8.7801e-04 - val_loss: 3.0813e-04 - val_mse: 1.2078e-04 - val_NMSE: 0.0011\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.4447e-04 - mse: 2.5771e-04 - NMSE: 0.0023 - tot_time: 0h 35m 14.3s\n",
      "\n",
      "Epoch 63: val_NMSE did not improve from 0.00104\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 4.4447e-04 - mse: 2.5771e-04 - NMSE: 0.0023 - val_loss: 4.4272e-04 - val_mse: 2.5576e-04 - val_NMSE: 0.0023\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.7726e-04 - mse: 1.8901e-04 - NMSE: 0.0017 - tot_time: 0h 35m 47.9s\n",
      "\n",
      "Epoch 64: val_NMSE did not improve from 0.00104\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.7726e-04 - mse: 1.8901e-04 - NMSE: 0.0017 - val_loss: 3.5101e-04 - val_mse: 1.6193e-04 - val_NMSE: 0.0015\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7117e-04 - mse: 8.2807e-05 - NMSE: 7.4857e-04 - tot_time: 0h 36m 20.9s\n",
      "\n",
      "Epoch 65: val_NMSE improved from 0.00104 to 0.00103, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.7117e-04 - mse: 8.2807e-05 - NMSE: 7.4857e-04 - val_loss: 3.0133e-04 - val_mse: 1.1407e-04 - val_NMSE: 0.0010\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5179e-04 - mse: 6.5679e-05 - NMSE: 5.9374e-04 - tot_time: 0h 36m 53.4s\n",
      "\n",
      "Epoch 66: val_NMSE improved from 0.00103 to 0.00096, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 2.5179e-04 - mse: 6.5679e-05 - NMSE: 5.9374e-04 - val_loss: 2.9136e-04 - val_mse: 1.0660e-04 - val_NMSE: 9.6369e-04\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.0415e-04 - mse: 2.2021e-04 - NMSE: 0.0020 - tot_time: 0h 37m 24.2s\n",
      "\n",
      "Epoch 67: val_NMSE did not improve from 0.00096\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 31s 2s/step - loss: 4.0415e-04 - mse: 2.2021e-04 - NMSE: 0.0020 - val_loss: 8.3396e-04 - val_mse: 6.4992e-04 - val_NMSE: 0.0059\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.0365e-04 - mse: 2.1781e-04 - NMSE: 0.0020 - tot_time: 0h 37m 56.3s\n",
      "\n",
      "Epoch 68: val_NMSE did not improve from 0.00096\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 4.0365e-04 - mse: 2.1781e-04 - NMSE: 0.0020 - val_loss: 3.0376e-04 - val_mse: 1.1702e-04 - val_NMSE: 0.0011\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6337e-04 - mse: 7.7010e-05 - NMSE: 6.9615e-04 - tot_time: 0h 38m 30.4s\n",
      "\n",
      "Epoch 69: val_NMSE did not improve from 0.00096\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.6337e-04 - mse: 7.7010e-05 - NMSE: 6.9615e-04 - val_loss: 2.9372e-04 - val_mse: 1.0842e-04 - val_NMSE: 9.8013e-04\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4139e-04 - mse: 5.7366e-05 - NMSE: 5.1858e-04 - tot_time: 0h 39m 2.8s\n",
      "\n",
      "Epoch 70: val_NMSE improved from 0.00096 to 0.00086, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 2.4139e-04 - mse: 5.7366e-05 - NMSE: 5.1858e-04 - val_loss: 2.7728e-04 - val_mse: 9.4798e-05 - val_NMSE: 8.5696e-04\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.5950e-04 - mse: 2.7802e-04 - NMSE: 0.0025 - tot_time: 0h 39m 36.4s\n",
      "\n",
      "Epoch 71: val_NMSE did not improve from 0.00086\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 4.5950e-04 - mse: 2.7802e-04 - NMSE: 0.0025 - val_loss: 5.1125e-04 - val_mse: 3.2948e-04 - val_NMSE: 0.0030\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.5169e-04 - mse: 1.6928e-04 - NMSE: 0.0015 - tot_time: 0h 40m 9.7s\n",
      "\n",
      "Epoch 72: val_NMSE did not improve from 0.00086\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.5169e-04 - mse: 1.6928e-04 - NMSE: 0.0015 - val_loss: 3.0102e-04 - val_mse: 1.1811e-04 - val_NMSE: 0.0011\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5089e-04 - mse: 6.8637e-05 - NMSE: 6.2046e-04 - tot_time: 0h 40m 42.8s\n",
      "\n",
      "Epoch 73: val_NMSE did not improve from 0.00086\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.5089e-04 - mse: 6.8637e-05 - NMSE: 6.2046e-04 - val_loss: 2.8118e-04 - val_mse: 9.9929e-05 - val_NMSE: 9.0335e-04\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3331e-04 - mse: 5.3221e-05 - NMSE: 4.8111e-04 - tot_time: 0h 41m 16.7s\n",
      "\n",
      "Epoch 74: val_NMSE improved from 0.00086 to 0.00080, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.3331e-04 - mse: 5.3221e-05 - NMSE: 4.8111e-04 - val_loss: 2.6681e-04 - val_mse: 8.8148e-05 - val_NMSE: 7.9686e-04\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7739e-04 - mse: 9.9909e-05 - NMSE: 9.0315e-04 - tot_time: 0h 41m 49.9s\n",
      "\n",
      "Epoch 75: val_NMSE did not improve from 0.00080\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.7739e-04 - mse: 9.9909e-05 - NMSE: 9.0315e-04 - val_loss: 5.4194e-04 - val_mse: 3.6563e-04 - val_NMSE: 0.0033\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0486e-04 - mse: 1.2871e-04 - NMSE: 0.0012 - tot_time: 0h 42m 23.8s\n",
      "\n",
      "Epoch 76: val_NMSE did not improve from 0.00080\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.0486e-04 - mse: 1.2871e-04 - NMSE: 0.0012 - val_loss: 2.7190e-04 - val_mse: 9.6102e-05 - val_NMSE: 8.6875e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3016e-04 - mse: 5.5113e-05 - NMSE: 4.9819e-04 - tot_time: 0h 42m 57.3s\n",
      "\n",
      "Epoch 77: val_NMSE improved from 0.00080 to 0.00076, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.3016e-04 - mse: 5.5113e-05 - NMSE: 4.9819e-04 - val_loss: 2.5850e-04 - val_mse: 8.4576e-05 - val_NMSE: 7.6455e-04\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.7411e-04 - mse: 2.0119e-04 - NMSE: 0.0018 - tot_time: 0h 43m 30.7s\n",
      "\n",
      "Epoch 78: val_NMSE did not improve from 0.00076\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.7411e-04 - mse: 2.0119e-04 - NMSE: 0.0018 - val_loss: 5.7389e-04 - val_mse: 4.0118e-04 - val_NMSE: 0.0036\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.8142e-04 - mse: 2.0767e-04 - NMSE: 0.0019 - tot_time: 0h 44m 3.9s\n",
      "\n",
      "Epoch 79: val_NMSE did not improve from 0.00076\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 3.8142e-04 - mse: 2.0767e-04 - NMSE: 0.0019 - val_loss: 2.9506e-04 - val_mse: 1.2039e-04 - val_NMSE: 0.0011\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.4504e-04 - mse: 7.0960e-05 - NMSE: 6.4143e-04 - tot_time: 0h 44m 39.2s\n",
      "\n",
      "Epoch 80: val_NMSE did not improve from 0.00076\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.4504e-04 - mse: 7.0960e-05 - NMSE: 6.4143e-04 - val_loss: 2.6430e-04 - val_mse: 9.1075e-05 - val_NMSE: 8.2331e-04\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1943e-04 - mse: 4.7365e-05 - NMSE: 4.2816e-04 - tot_time: 0h 45m 13.7s\n",
      "\n",
      "Epoch 81: val_NMSE improved from 0.00076 to 0.00075, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.1943e-04 - mse: 4.7365e-05 - NMSE: 4.2816e-04 - val_loss: 2.5394e-04 - val_mse: 8.3303e-05 - val_NMSE: 7.5305e-04\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.7485e-04 - mse: 2.0537e-04 - NMSE: 0.0019 - tot_time: 0h 45m 47.0s\n",
      "\n",
      "Epoch 82: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.7485e-04 - mse: 2.0537e-04 - NMSE: 0.0019 - val_loss: 3.4427e-04 - val_mse: 1.7551e-04 - val_NMSE: 0.0016\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3717e-04 - mse: 1.6648e-04 - NMSE: 0.0015 - tot_time: 0h 46m 21.2s\n",
      "\n",
      "Epoch 83: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.3717e-04 - mse: 1.6648e-04 - NMSE: 0.0015 - val_loss: 2.9353e-04 - val_mse: 1.2122e-04 - val_NMSE: 0.0011\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3962e-04 - mse: 6.7528e-05 - NMSE: 6.1041e-04 - tot_time: 0h 46m 55.0s\n",
      "\n",
      "Epoch 84: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.3962e-04 - mse: 6.7528e-05 - NMSE: 6.1041e-04 - val_loss: 2.6216e-04 - val_mse: 9.0880e-05 - val_NMSE: 8.2156e-04\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1752e-04 - mse: 4.7522e-05 - NMSE: 4.2958e-04 - tot_time: 0h 47m 28.0s\n",
      "\n",
      "Epoch 85: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.1752e-04 - mse: 4.7522e-05 - NMSE: 4.2958e-04 - val_loss: 2.5646e-04 - val_mse: 8.7941e-05 - val_NMSE: 7.9496e-04\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.2913e-04 - mse: 1.6170e-04 - NMSE: 0.0015 - tot_time: 0h 48m 1.6s\n",
      "\n",
      "Epoch 86: val_NMSE did not improve from 0.00075\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.2913e-04 - mse: 1.6170e-04 - NMSE: 0.0015 - val_loss: 2.7887e-04 - val_mse: 1.1185e-04 - val_NMSE: 0.0010\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3317e-04 - mse: 6.6422e-05 - NMSE: 6.0042e-04 - tot_time: 0h 48m 35.1s\n",
      "\n",
      "Epoch 87: val_NMSE improved from 0.00075 to 0.00074, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.3317e-04 - mse: 6.6422e-05 - NMSE: 6.0042e-04 - val_loss: 2.4825e-04 - val_mse: 8.2074e-05 - val_NMSE: 7.4192e-04\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0653e-04 - mse: 4.1401e-05 - NMSE: 3.7425e-04 - tot_time: 0h 49m 8.9s\n",
      "\n",
      "Epoch 88: val_NMSE improved from 0.00074 to 0.00066, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.0653e-04 - mse: 4.1401e-05 - NMSE: 3.7425e-04 - val_loss: 2.3703e-04 - val_mse: 7.3239e-05 - val_NMSE: 6.6207e-04\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.7717e-04 - mse: 2.1457e-04 - NMSE: 0.0019 - tot_time: 0h 49m 42.9s\n",
      "\n",
      "Epoch 89: val_NMSE did not improve from 0.00066\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.7717e-04 - mse: 2.1457e-04 - NMSE: 0.0019 - val_loss: 7.0410e-04 - val_mse: 5.4217e-04 - val_NMSE: 0.0049\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 4.5329e-04 - mse: 2.8953e-04 - NMSE: 0.0026 - tot_time: 0h 50m 16.9s\n",
      "\n",
      "Epoch 90: val_NMSE did not improve from 0.00066\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 4.5329e-04 - mse: 2.8953e-04 - NMSE: 0.0026 - val_loss: 3.0467e-04 - val_mse: 1.3894e-04 - val_NMSE: 0.0013\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3902e-04 - mse: 7.3134e-05 - NMSE: 6.6107e-04 - tot_time: 0h 50m 50.9s\n",
      "\n",
      "Epoch 91: val_NMSE did not improve from 0.00066\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.3902e-04 - mse: 7.3134e-05 - NMSE: 6.6107e-04 - val_loss: 2.4883e-04 - val_mse: 8.3400e-05 - val_NMSE: 7.5391e-04\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0718e-04 - mse: 4.3041e-05 - NMSE: 3.8908e-04 - tot_time: 0h 51m 24.1s\n",
      "\n",
      "Epoch 92: val_NMSE improved from 0.00066 to 0.00066, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.0718e-04 - mse: 4.3041e-05 - NMSE: 3.8908e-04 - val_loss: 2.3503e-04 - val_mse: 7.2481e-05 - val_NMSE: 6.5522e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9685e-04 - mse: 3.5648e-05 - NMSE: 3.2224e-04 - tot_time: 0h 51m 58.0s\n",
      "\n",
      "Epoch 93: val_NMSE improved from 0.00066 to 0.00060, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.9685e-04 - mse: 3.5648e-05 - NMSE: 3.2224e-04 - val_loss: 2.2555e-04 - val_mse: 6.5877e-05 - val_NMSE: 5.9551e-04\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9152e-04 - mse: 3.3124e-05 - NMSE: 2.9942e-04 - tot_time: 0h 52m 32.5s\n",
      "\n",
      "Epoch 94: val_NMSE improved from 0.00060 to 0.00057, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 1.9152e-04 - mse: 3.3124e-05 - NMSE: 2.9942e-04 - val_loss: 2.1954e-04 - val_mse: 6.2560e-05 - val_NMSE: 5.6552e-04\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.3236e-04 - mse: 1.7621e-04 - NMSE: 0.0016 - tot_time: 0h 53m 6.4s\n",
      "\n",
      "Epoch 95: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 3.3236e-04 - mse: 1.7621e-04 - NMSE: 0.0016 - val_loss: 3.4897e-04 - val_mse: 1.9293e-04 - val_NMSE: 0.0017\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.3491e-04 - mse: 7.8369e-05 - NMSE: 7.0838e-04 - tot_time: 0h 53m 40.5s\n",
      "\n",
      "Epoch 96: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.3491e-04 - mse: 7.8369e-05 - NMSE: 7.0838e-04 - val_loss: 2.4013e-04 - val_mse: 8.3789e-05 - val_NMSE: 7.5741e-04\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9565e-04 - mse: 4.0069e-05 - NMSE: 3.6220e-04 - tot_time: 0h 54m 14.0s\n",
      "\n",
      "Epoch 97: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.9565e-04 - mse: 4.0069e-05 - NMSE: 3.6220e-04 - val_loss: 2.2102e-04 - val_mse: 6.6541e-05 - val_NMSE: 6.0152e-04\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8780e-04 - mse: 3.4398e-05 - NMSE: 3.1094e-04 - tot_time: 0h 54m 47.8s\n",
      "\n",
      "Epoch 98: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.8780e-04 - mse: 3.4398e-05 - NMSE: 3.1094e-04 - val_loss: 2.1808e-04 - val_mse: 6.5896e-05 - val_NMSE: 5.9568e-04\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9382e-04 - mse: 4.2710e-05 - NMSE: 3.8607e-04 - tot_time: 0h 55m 21.0s\n",
      "\n",
      "Epoch 99: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.9382e-04 - mse: 4.2710e-05 - NMSE: 3.8607e-04 - val_loss: 4.4168e-04 - val_mse: 2.9186e-04 - val_NMSE: 0.0026\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.9854e-04 - mse: 5.4662e-04 - NMSE: 0.0049 - tot_time: 0h 55m 54.8s\n",
      "\n",
      "Epoch 100: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 6.9854e-04 - mse: 5.4662e-04 - NMSE: 0.0049 - val_loss: 3.6105e-04 - val_mse: 2.0449e-04 - val_NMSE: 0.0018\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7302e-04 - mse: 1.1396e-04 - NMSE: 0.0010 - tot_time: 0h 56m 28.3s\n",
      "\n",
      "Epoch 101: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.7302e-04 - mse: 1.1396e-04 - NMSE: 0.0010 - val_loss: 2.7059e-04 - val_mse: 1.1042e-04 - val_NMSE: 9.9810e-04\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.1085e-04 - mse: 5.1551e-05 - NMSE: 4.6599e-04 - tot_time: 0h 57m 2.5s\n",
      "\n",
      "Epoch 102: val_NMSE did not improve from 0.00057\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.1085e-04 - mse: 5.1551e-05 - NMSE: 4.6599e-04 - val_loss: 2.2547e-04 - val_mse: 6.7893e-05 - val_NMSE: 6.1373e-04\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9293e-04 - mse: 3.7042e-05 - NMSE: 3.3484e-04 - tot_time: 0h 57m 36.1s\n",
      "\n",
      "Epoch 103: val_NMSE improved from 0.00057 to 0.00055, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.9293e-04 - mse: 3.7042e-05 - NMSE: 3.3484e-04 - val_loss: 2.1482e-04 - val_mse: 6.0758e-05 - val_NMSE: 5.4924e-04\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8478e-04 - mse: 3.2168e-05 - NMSE: 2.9078e-04 - tot_time: 0h 58m 9.2s\n",
      "\n",
      "Epoch 104: val_NMSE improved from 0.00055 to 0.00051, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.8478e-04 - mse: 3.2168e-05 - NMSE: 2.9078e-04 - val_loss: 2.0733e-04 - val_mse: 5.6264e-05 - val_NMSE: 5.0861e-04\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8050e-04 - mse: 3.0746e-05 - NMSE: 2.7793e-04 - tot_time: 0h 58m 43.7s\n",
      "\n",
      "Epoch 105: val_NMSE improved from 0.00051 to 0.00050, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.8050e-04 - mse: 3.0746e-05 - NMSE: 2.7793e-04 - val_loss: 2.0331e-04 - val_mse: 5.4990e-05 - val_NMSE: 4.9709e-04\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.7171e-04 - mse: 1.2449e-04 - NMSE: 0.0011 - tot_time: 0h 59m 17.5s\n",
      "\n",
      "Epoch 106: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.7171e-04 - mse: 1.2449e-04 - NMSE: 0.0011 - val_loss: 3.0186e-04 - val_mse: 1.5550e-04 - val_NMSE: 0.0014\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.0885e-04 - mse: 6.2978e-05 - NMSE: 5.6930e-04 - tot_time: 0h 59m 51.0s\n",
      "\n",
      "Epoch 107: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 2.0885e-04 - mse: 6.2978e-05 - NMSE: 5.6930e-04 - val_loss: 2.0805e-04 - val_mse: 6.2937e-05 - val_NMSE: 5.6891e-04\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7874e-04 - mse: 3.4512e-05 - NMSE: 3.1196e-04 - tot_time: 1h 0m 24.5s\n",
      "\n",
      "Epoch 108: val_NMSE did not improve from 0.00050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.7874e-04 - mse: 3.4512e-05 - NMSE: 3.1196e-04 - val_loss: 2.0287e-04 - val_mse: 5.9711e-05 - val_NMSE: 5.3977e-04\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8577e-04 - mse: 4.3589e-05 - NMSE: 3.9402e-04 - tot_time: 1h 0m 58.5s\n",
      "\n",
      "Epoch 109: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.8577e-04 - mse: 4.3589e-05 - NMSE: 3.9402e-04 - val_loss: 3.8127e-04 - val_mse: 2.4017e-04 - val_NMSE: 0.0022\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.6286e-04 - mse: 2.2107e-04 - NMSE: 0.0020 - tot_time: 1h 1m 31.3s\n",
      "\n",
      "Epoch 110: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.6286e-04 - mse: 2.2107e-04 - NMSE: 0.0020 - val_loss: 2.3949e-04 - val_mse: 9.6649e-05 - val_NMSE: 8.7360e-04\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.9238e-04 - mse: 4.9548e-05 - NMSE: 4.4787e-04 - tot_time: 1h 2m 4.4s\n",
      "\n",
      "Epoch 111: val_NMSE did not improve from 0.00050\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.9238e-04 - mse: 4.9548e-05 - NMSE: 4.4787e-04 - val_loss: 2.0694e-04 - val_mse: 6.4713e-05 - val_NMSE: 5.8497e-04\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7252e-04 - mse: 3.1228e-05 - NMSE: 2.8228e-04 - tot_time: 1h 2m 38.0s\n",
      "\n",
      "Epoch 112: val_NMSE improved from 0.00050 to 0.00049, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.7252e-04 - mse: 3.1228e-05 - NMSE: 2.8228e-04 - val_loss: 1.9473e-04 - val_mse: 5.4549e-05 - val_NMSE: 4.9311e-04\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6703e-04 - mse: 2.7846e-05 - NMSE: 2.5171e-04 - tot_time: 1h 3m 11.1s\n",
      "\n",
      "Epoch 113: val_NMSE improved from 0.00049 to 0.00047, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.6703e-04 - mse: 2.7846e-05 - NMSE: 2.5171e-04 - val_loss: 1.8958e-04 - val_mse: 5.1514e-05 - val_NMSE: 4.6567e-04\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6477e-04 - mse: 2.7661e-05 - NMSE: 2.5004e-04 - tot_time: 1h 3m 44.2s\n",
      "\n",
      "Epoch 114: val_NMSE did not improve from 0.00047\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.6477e-04 - mse: 2.7661e-05 - NMSE: 2.5004e-04 - val_loss: 1.9462e-04 - val_mse: 5.8612e-05 - val_NMSE: 5.2984e-04\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8280e-04 - mse: 1.4726e-04 - NMSE: 0.0013 - tot_time: 1h 4m 17.3s\n",
      "\n",
      "Epoch 115: val_NMSE did not improve from 0.00047\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.8280e-04 - mse: 1.4726e-04 - NMSE: 0.0013 - val_loss: 4.1022e-04 - val_mse: 2.7454e-04 - val_NMSE: 0.0025\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5033e-04 - mse: 1.1413e-04 - NMSE: 0.0010 - tot_time: 1h 4m 50.5s\n",
      "\n",
      "Epoch 116: val_NMSE did not improve from 0.00047\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.5033e-04 - mse: 1.1413e-04 - NMSE: 0.0010 - val_loss: 2.0100e-04 - val_mse: 6.4562e-05 - val_NMSE: 5.8358e-04\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7138e-04 - mse: 3.5454e-05 - NMSE: 3.2048e-04 - tot_time: 1h 5m 21.6s\n",
      "\n",
      "Epoch 117: val_NMSE improved from 0.00047 to 0.00045, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 31s 2s/step - loss: 1.7138e-04 - mse: 3.5454e-05 - NMSE: 3.2048e-04 - val_loss: 1.8444e-04 - val_mse: 4.9363e-05 - val_NMSE: 4.4622e-04\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6111e-04 - mse: 2.6992e-05 - NMSE: 2.4399e-04 - tot_time: 1h 5m 52.1s\n",
      "\n",
      "Epoch 118: val_NMSE improved from 0.00045 to 0.00044, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 31s 2s/step - loss: 1.6111e-04 - mse: 2.6992e-05 - NMSE: 2.4399e-04 - val_loss: 1.8187e-04 - val_mse: 4.8852e-05 - val_NMSE: 4.4160e-04\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.5523e-04 - mse: 1.2289e-04 - NMSE: 0.0011 - tot_time: 1h 6m 24.4s\n",
      "\n",
      "Epoch 119: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 2.5523e-04 - mse: 1.2289e-04 - NMSE: 0.0011 - val_loss: 2.4425e-04 - val_mse: 1.1205e-04 - val_NMSE: 0.0010\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8049e-04 - mse: 4.8429e-05 - NMSE: 4.3774e-04 - tot_time: 1h 6m 57.2s\n",
      "\n",
      "Epoch 120: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.8049e-04 - mse: 4.8429e-05 - NMSE: 4.3774e-04 - val_loss: 1.9021e-04 - val_mse: 5.8567e-05 - val_NMSE: 5.2942e-04\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6384e-04 - mse: 3.2993e-05 - NMSE: 2.9823e-04 - tot_time: 1h 7m 29.9s\n",
      "\n",
      "Epoch 121: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.6384e-04 - mse: 3.2993e-05 - NMSE: 2.9823e-04 - val_loss: 2.0141e-04 - val_mse: 7.1536e-05 - val_NMSE: 6.4661e-04\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.8097e-04 - mse: 1.5157e-04 - NMSE: 0.0014 - tot_time: 1h 8m 2.6s\n",
      "\n",
      "Epoch 122: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 2.8097e-04 - mse: 1.5157e-04 - NMSE: 0.0014 - val_loss: 2.3102e-04 - val_mse: 1.0152e-04 - val_NMSE: 9.1768e-04\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.7982e-04 - mse: 5.0569e-05 - NMSE: 4.5709e-04 - tot_time: 1h 8m 35.6s\n",
      "\n",
      "Epoch 123: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.7982e-04 - mse: 5.0569e-05 - NMSE: 4.5709e-04 - val_loss: 1.8653e-04 - val_mse: 5.7714e-05 - val_NMSE: 5.2170e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5748e-04 - mse: 2.9434e-05 - NMSE: 2.6606e-04 - tot_time: 1h 9m 8.9s\n",
      "\n",
      "Epoch 124: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5748e-04 - mse: 2.9434e-05 - NMSE: 2.6606e-04 - val_loss: 1.7702e-04 - val_mse: 4.9889e-05 - val_NMSE: 4.5098e-04\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.9287e-04 - mse: 2.6621e-04 - NMSE: 0.0024 - tot_time: 1h 9m 41.6s\n",
      "\n",
      "Epoch 125: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.9287e-04 - mse: 2.6621e-04 - NMSE: 0.0024 - val_loss: 4.9483e-04 - val_mse: 3.6747e-04 - val_NMSE: 0.0033\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 3.0372e-04 - mse: 1.7435e-04 - NMSE: 0.0016 - tot_time: 1h 10m 14.5s\n",
      "\n",
      "Epoch 126: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 3.0372e-04 - mse: 1.7435e-04 - NMSE: 0.0016 - val_loss: 2.1525e-04 - val_mse: 8.3937e-05 - val_NMSE: 7.5869e-04\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8464e-04 - mse: 5.3237e-05 - NMSE: 4.8121e-04 - tot_time: 1h 10m 47.2s\n",
      "\n",
      "Epoch 127: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.8464e-04 - mse: 5.3237e-05 - NMSE: 4.8121e-04 - val_loss: 1.9016e-04 - val_mse: 5.9152e-05 - val_NMSE: 5.3471e-04\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6121e-04 - mse: 3.1157e-05 - NMSE: 2.8165e-04Restoring model weights from the end of the best epoch: 118.\n",
      " - tot_time: 1h 11m 20.5s\n",
      "\n",
      "Epoch 128: val_NMSE did not improve from 0.00044\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.6121e-04 - mse: 3.1157e-05 - NMSE: 2.8165e-04 - val_loss: 1.7777e-04 - val_mse: 4.8945e-05 - val_NMSE: 4.4245e-04\n",
      "Epoch 128: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.001 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5714e-04 - mse: 2.4215e-05 - NMSE: 2.1889e-04 - tot_time: 1h 11m 53.1s\n",
      "\n",
      "Epoch 1: val_NMSE improved from 0.00044 to 0.00042, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.5714e-04 - mse: 2.4215e-05 - NMSE: 2.1889e-04 - val_loss: 1.7915e-04 - val_mse: 4.6343e-05 - val_NMSE: 4.1892e-04\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5629e-04 - mse: 2.3577e-05 - NMSE: 2.1312e-04 - tot_time: 1h 12m 25.7s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.00042 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5629e-04 - mse: 2.3577e-05 - NMSE: 2.1312e-04 - val_loss: 1.7849e-04 - val_mse: 4.5892e-05 - val_NMSE: 4.1485e-04\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5578e-04 - mse: 2.3290e-05 - NMSE: 2.1053e-04 - tot_time: 1h 12m 58.2s\n",
      "\n",
      "Epoch 3: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.5578e-04 - mse: 2.3290e-05 - NMSE: 2.1053e-04 - val_loss: 1.7807e-04 - val_mse: 4.5694e-05 - val_NMSE: 4.1306e-04\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5542e-04 - mse: 2.3149e-05 - NMSE: 2.0926e-04 - tot_time: 1h 13m 31.0s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5542e-04 - mse: 2.3149e-05 - NMSE: 2.0926e-04 - val_loss: 1.7772e-04 - val_mse: 4.5566e-05 - val_NMSE: 4.1191e-04\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5508e-04 - mse: 2.3030e-05 - NMSE: 2.0818e-04 - tot_time: 1h 14m 3.6s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5508e-04 - mse: 2.3030e-05 - NMSE: 2.0818e-04 - val_loss: 1.7722e-04 - val_mse: 4.5296e-05 - val_NMSE: 4.0946e-04\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5479e-04 - mse: 2.2964e-05 - NMSE: 2.0758e-04 - tot_time: 1h 14m 36.5s\n",
      "\n",
      "Epoch 6: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5479e-04 - mse: 2.2964e-05 - NMSE: 2.0758e-04 - val_loss: 1.7687e-04 - val_mse: 4.5163e-05 - val_NMSE: 4.0825e-04\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5441e-04 - mse: 2.2813e-05 - NMSE: 2.0622e-04 - tot_time: 1h 15m 9.1s\n",
      "\n",
      "Epoch 7: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5441e-04 - mse: 2.2813e-05 - NMSE: 2.0622e-04 - val_loss: 1.7658e-04 - val_mse: 4.5100e-05 - val_NMSE: 4.0769e-04\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5405e-04 - mse: 2.2670e-05 - NMSE: 2.0492e-04 - tot_time: 1h 15m 42.2s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.00041 to 0.00041, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5405e-04 - mse: 2.2670e-05 - NMSE: 2.0492e-04 - val_loss: 1.7617e-04 - val_mse: 4.4913e-05 - val_NMSE: 4.0600e-04\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5374e-04 - mse: 2.2583e-05 - NMSE: 2.0414e-04 - tot_time: 1h 16m 14.8s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.00041 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5374e-04 - mse: 2.2583e-05 - NMSE: 2.0414e-04 - val_loss: 1.7578e-04 - val_mse: 4.4745e-05 - val_NMSE: 4.0448e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5335e-04 - mse: 2.2419e-05 - NMSE: 2.0265e-04 - tot_time: 1h 16m 47.6s\n",
      "\n",
      "Epoch 10: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5335e-04 - mse: 2.2419e-05 - NMSE: 2.0265e-04 - val_loss: 1.7542e-04 - val_mse: 4.4607e-05 - val_NMSE: 4.0323e-04\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5302e-04 - mse: 2.2317e-05 - NMSE: 2.0173e-04 - tot_time: 1h 17m 20.6s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5302e-04 - mse: 2.2317e-05 - NMSE: 2.0173e-04 - val_loss: 1.7503e-04 - val_mse: 4.4449e-05 - val_NMSE: 4.0180e-04\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5275e-04 - mse: 2.2267e-05 - NMSE: 2.0129e-04 - tot_time: 1h 17m 53.9s\n",
      "\n",
      "Epoch 12: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5275e-04 - mse: 2.2267e-05 - NMSE: 2.0129e-04 - val_loss: 1.7457e-04 - val_mse: 4.4210e-05 - val_NMSE: 3.9964e-04\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5236e-04 - mse: 2.2109e-05 - NMSE: 1.9985e-04 - tot_time: 1h 18m 27.1s\n",
      "\n",
      "Epoch 13: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.5236e-04 - mse: 2.2109e-05 - NMSE: 1.9985e-04 - val_loss: 1.7424e-04 - val_mse: 4.4105e-05 - val_NMSE: 3.9870e-04\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5207e-04 - mse: 2.2038e-05 - NMSE: 1.9921e-04 - tot_time: 1h 19m 0.9s\n",
      "\n",
      "Epoch 14: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.5207e-04 - mse: 2.2038e-05 - NMSE: 1.9921e-04 - val_loss: 1.7386e-04 - val_mse: 4.3946e-05 - val_NMSE: 3.9726e-04\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5174e-04 - mse: 2.1932e-05 - NMSE: 1.9825e-04 - tot_time: 1h 19m 34.9s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.5174e-04 - mse: 2.1932e-05 - NMSE: 1.9825e-04 - val_loss: 1.7359e-04 - val_mse: 4.3912e-05 - val_NMSE: 3.9695e-04\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5140e-04 - mse: 2.1823e-05 - NMSE: 1.9727e-04 - tot_time: 1h 20m 8.6s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00040 to 0.00040, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.5140e-04 - mse: 2.1823e-05 - NMSE: 1.9727e-04 - val_loss: 1.7320e-04 - val_mse: 4.3744e-05 - val_NMSE: 3.9543e-04\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5106e-04 - mse: 2.1708e-05 - NMSE: 1.9623e-04 - tot_time: 1h 20m 42.2s\n",
      "\n",
      "Epoch 17: val_NMSE improved from 0.00040 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.5106e-04 - mse: 2.1708e-05 - NMSE: 1.9623e-04 - val_loss: 1.7282e-04 - val_mse: 4.3596e-05 - val_NMSE: 3.9409e-04\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5078e-04 - mse: 2.1658e-05 - NMSE: 1.9578e-04 - tot_time: 1h 21m 15.7s\n",
      "\n",
      "Epoch 18: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.5078e-04 - mse: 2.1658e-05 - NMSE: 1.9578e-04 - val_loss: 1.7247e-04 - val_mse: 4.3469e-05 - val_NMSE: 3.9294e-04\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5041e-04 - mse: 2.1517e-05 - NMSE: 1.9450e-04 - tot_time: 1h 21m 50.1s\n",
      "\n",
      "Epoch 19: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.5041e-04 - mse: 2.1517e-05 - NMSE: 1.9450e-04 - val_loss: 1.7201e-04 - val_mse: 4.3236e-05 - val_NMSE: 3.9084e-04\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5008e-04 - mse: 2.1409e-05 - NMSE: 1.9353e-04 - tot_time: 1h 22m 23.8s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.5008e-04 - mse: 2.1409e-05 - NMSE: 1.9353e-04 - val_loss: 1.7166e-04 - val_mse: 4.3119e-05 - val_NMSE: 3.8978e-04\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4980e-04 - mse: 2.1358e-05 - NMSE: 1.9307e-04 - tot_time: 1h 22m 57.5s\n",
      "\n",
      "Epoch 21: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.4980e-04 - mse: 2.1358e-05 - NMSE: 1.9307e-04 - val_loss: 1.7133e-04 - val_mse: 4.3013e-05 - val_NMSE: 3.8882e-04\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4944e-04 - mse: 2.1229e-05 - NMSE: 1.9190e-04 - tot_time: 1h 23m 31.2s\n",
      "\n",
      "Epoch 22: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.4944e-04 - mse: 2.1229e-05 - NMSE: 1.9190e-04 - val_loss: 1.7098e-04 - val_mse: 4.2896e-05 - val_NMSE: 3.8776e-04\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4918e-04 - mse: 2.1197e-05 - NMSE: 1.9161e-04 - tot_time: 1h 24m 4.0s\n",
      "\n",
      "Epoch 23: val_NMSE improved from 0.00039 to 0.00039, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4918e-04 - mse: 2.1197e-05 - NMSE: 1.9161e-04 - val_loss: 1.7056e-04 - val_mse: 4.2703e-05 - val_NMSE: 3.8602e-04\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4884e-04 - mse: 2.1084e-05 - NMSE: 1.9059e-04 - tot_time: 1h 24m 37.1s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00039 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4884e-04 - mse: 2.1084e-05 - NMSE: 1.9059e-04 - val_loss: 1.7019e-04 - val_mse: 4.2559e-05 - val_NMSE: 3.8471e-04\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4852e-04 - mse: 2.0998e-05 - NMSE: 1.8981e-04 - tot_time: 1h 25m 10.3s\n",
      "\n",
      "Epoch 25: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4852e-04 - mse: 2.0998e-05 - NMSE: 1.8981e-04 - val_loss: 1.6987e-04 - val_mse: 4.2475e-05 - val_NMSE: 3.8396e-04\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4821e-04 - mse: 2.0920e-05 - NMSE: 1.8910e-04 - tot_time: 1h 25m 43.3s\n",
      "\n",
      "Epoch 26: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4821e-04 - mse: 2.0920e-05 - NMSE: 1.8910e-04 - val_loss: 1.6948e-04 - val_mse: 4.2314e-05 - val_NMSE: 3.8250e-04\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4784e-04 - mse: 2.0785e-05 - NMSE: 1.8789e-04 - tot_time: 1h 26m 16.0s\n",
      "\n",
      "Epoch 27: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4784e-04 - mse: 2.0785e-05 - NMSE: 1.8789e-04 - val_loss: 1.6909e-04 - val_mse: 4.2155e-05 - val_NMSE: 3.8106e-04\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4757e-04 - mse: 2.0741e-05 - NMSE: 1.8749e-04 - tot_time: 1h 26m 48.9s\n",
      "\n",
      "Epoch 28: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4757e-04 - mse: 2.0741e-05 - NMSE: 1.8749e-04 - val_loss: 1.6874e-04 - val_mse: 4.2045e-05 - val_NMSE: 3.8007e-04\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4720e-04 - mse: 2.0611e-05 - NMSE: 1.8631e-04 - tot_time: 1h 27m 21.9s\n",
      "\n",
      "Epoch 29: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4720e-04 - mse: 2.0611e-05 - NMSE: 1.8631e-04 - val_loss: 1.6838e-04 - val_mse: 4.1916e-05 - val_NMSE: 3.7891e-04\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4689e-04 - mse: 2.0533e-05 - NMSE: 1.8561e-04 - tot_time: 1h 27m 54.7s\n",
      "\n",
      "Epoch 30: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4689e-04 - mse: 2.0533e-05 - NMSE: 1.8561e-04 - val_loss: 1.6800e-04 - val_mse: 4.1769e-05 - val_NMSE: 3.7758e-04\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4653e-04 - mse: 2.0407e-05 - NMSE: 1.8447e-04 - tot_time: 1h 28m 27.2s\n",
      "\n",
      "Epoch 31: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4653e-04 - mse: 2.0407e-05 - NMSE: 1.8447e-04 - val_loss: 1.6770e-04 - val_mse: 4.1705e-05 - val_NMSE: 3.7700e-04\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4624e-04 - mse: 2.0355e-05 - NMSE: 1.8400e-04 - tot_time: 1h 28m 58.4s\n",
      "\n",
      "Epoch 32: val_NMSE improved from 0.00038 to 0.00038, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 31s 2s/step - loss: 1.4624e-04 - mse: 2.0355e-05 - NMSE: 1.8400e-04 - val_loss: 1.6732e-04 - val_mse: 4.1563e-05 - val_NMSE: 3.7571e-04\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4599e-04 - mse: 2.0339e-05 - NMSE: 1.8386e-04 - tot_time: 1h 29m 28.8s\n",
      "\n",
      "Epoch 33: val_NMSE improved from 0.00038 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 30s 2s/step - loss: 1.4599e-04 - mse: 2.0339e-05 - NMSE: 1.8386e-04 - val_loss: 1.6699e-04 - val_mse: 4.1465e-05 - val_NMSE: 3.7483e-04\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4563e-04 - mse: 2.0212e-05 - NMSE: 1.8270e-04 - tot_time: 1h 29m 58.9s\n",
      "\n",
      "Epoch 34: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 30s 2s/step - loss: 1.4563e-04 - mse: 2.0212e-05 - NMSE: 1.8270e-04 - val_loss: 1.6648e-04 - val_mse: 4.1196e-05 - val_NMSE: 3.7239e-04\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4535e-04 - mse: 2.0177e-05 - NMSE: 1.8239e-04 - tot_time: 1h 30m 31.3s\n",
      "\n",
      "Epoch 35: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4535e-04 - mse: 2.0177e-05 - NMSE: 1.8239e-04 - val_loss: 1.6618e-04 - val_mse: 4.1129e-05 - val_NMSE: 3.7179e-04\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4498e-04 - mse: 2.0038e-05 - NMSE: 1.8113e-04 - tot_time: 1h 31m 3.8s\n",
      "\n",
      "Epoch 36: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4498e-04 - mse: 2.0038e-05 - NMSE: 1.8113e-04 - val_loss: 1.6575e-04 - val_mse: 4.0938e-05 - val_NMSE: 3.7006e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4464e-04 - mse: 1.9945e-05 - NMSE: 1.8029e-04 - tot_time: 1h 31m 36.6s\n",
      "\n",
      "Epoch 37: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4464e-04 - mse: 1.9945e-05 - NMSE: 1.8029e-04 - val_loss: 1.6547e-04 - val_mse: 4.0895e-05 - val_NMSE: 3.6967e-04\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4442e-04 - mse: 1.9957e-05 - NMSE: 1.8040e-04 - tot_time: 1h 32m 9.6s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4442e-04 - mse: 1.9957e-05 - NMSE: 1.8040e-04 - val_loss: 1.6511e-04 - val_mse: 4.0777e-05 - val_NMSE: 3.6860e-04\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4406e-04 - mse: 1.9843e-05 - NMSE: 1.7937e-04 - tot_time: 1h 32m 42.1s\n",
      "\n",
      "Epoch 39: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4406e-04 - mse: 1.9843e-05 - NMSE: 1.7937e-04 - val_loss: 1.6469e-04 - val_mse: 4.0601e-05 - val_NMSE: 3.6702e-04\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4376e-04 - mse: 1.9778e-05 - NMSE: 1.7878e-04 - tot_time: 1h 33m 15.6s\n",
      "\n",
      "Epoch 40: val_NMSE improved from 0.00037 to 0.00037, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4376e-04 - mse: 1.9778e-05 - NMSE: 1.7878e-04 - val_loss: 1.6433e-04 - val_mse: 4.0473e-05 - val_NMSE: 3.6586e-04\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4339e-04 - mse: 1.9655e-05 - NMSE: 1.7767e-04 - tot_time: 1h 33m 47.9s\n",
      "\n",
      "Epoch 41: val_NMSE improved from 0.00037 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4339e-04 - mse: 1.9655e-05 - NMSE: 1.7767e-04 - val_loss: 1.6386e-04 - val_mse: 4.0253e-05 - val_NMSE: 3.6387e-04\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4313e-04 - mse: 1.9628e-05 - NMSE: 1.7743e-04 - tot_time: 1h 34m 20.9s\n",
      "\n",
      "Epoch 42: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4313e-04 - mse: 1.9628e-05 - NMSE: 1.7743e-04 - val_loss: 1.6346e-04 - val_mse: 4.0087e-05 - val_NMSE: 3.6237e-04\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4282e-04 - mse: 1.9560e-05 - NMSE: 1.7681e-04 - tot_time: 1h 34m 54.1s\n",
      "\n",
      "Epoch 43: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4282e-04 - mse: 1.9560e-05 - NMSE: 1.7681e-04 - val_loss: 1.6312e-04 - val_mse: 3.9993e-05 - val_NMSE: 3.6152e-04\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4244e-04 - mse: 1.9431e-05 - NMSE: 1.7565e-04 - tot_time: 1h 35m 26.2s\n",
      "\n",
      "Epoch 44: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4244e-04 - mse: 1.9431e-05 - NMSE: 1.7565e-04 - val_loss: 1.6274e-04 - val_mse: 3.9855e-05 - val_NMSE: 3.6027e-04\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4212e-04 - mse: 1.9350e-05 - NMSE: 1.7491e-04 - tot_time: 1h 35m 58.5s\n",
      "\n",
      "Epoch 45: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4212e-04 - mse: 1.9350e-05 - NMSE: 1.7491e-04 - val_loss: 1.6243e-04 - val_mse: 3.9788e-05 - val_NMSE: 3.5967e-04\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4179e-04 - mse: 1.9260e-05 - NMSE: 1.7410e-04 - tot_time: 1h 36m 31.3s\n",
      "\n",
      "Epoch 46: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4179e-04 - mse: 1.9260e-05 - NMSE: 1.7410e-04 - val_loss: 1.6199e-04 - val_mse: 3.9592e-05 - val_NMSE: 3.5789e-04\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4146e-04 - mse: 1.9185e-05 - NMSE: 1.7342e-04 - tot_time: 1h 37m 4.2s\n",
      "\n",
      "Epoch 47: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4146e-04 - mse: 1.9185e-05 - NMSE: 1.7342e-04 - val_loss: 1.6152e-04 - val_mse: 3.9369e-05 - val_NMSE: 3.5588e-04\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4114e-04 - mse: 1.9103e-05 - NMSE: 1.7268e-04 - tot_time: 1h 37m 36.7s\n",
      "\n",
      "Epoch 48: val_NMSE improved from 0.00036 to 0.00036, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4114e-04 - mse: 1.9103e-05 - NMSE: 1.7268e-04 - val_loss: 1.6119e-04 - val_mse: 3.9289e-05 - val_NMSE: 3.5515e-04\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4087e-04 - mse: 1.9083e-05 - NMSE: 1.7250e-04 - tot_time: 1h 38m 9.2s\n",
      "\n",
      "Epoch 49: val_NMSE improved from 0.00036 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4087e-04 - mse: 1.9083e-05 - NMSE: 1.7250e-04 - val_loss: 1.6082e-04 - val_mse: 3.9163e-05 - val_NMSE: 3.5402e-04\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4049e-04 - mse: 1.8952e-05 - NMSE: 1.7132e-04 - tot_time: 1h 38m 41.5s\n",
      "\n",
      "Epoch 50: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4049e-04 - mse: 1.8952e-05 - NMSE: 1.7132e-04 - val_loss: 1.6045e-04 - val_mse: 3.9035e-05 - val_NMSE: 3.5286e-04\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4018e-04 - mse: 1.8887e-05 - NMSE: 1.7072e-04 - tot_time: 1h 39m 13.3s\n",
      "\n",
      "Epoch 51: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.4018e-04 - mse: 1.8887e-05 - NMSE: 1.7072e-04 - val_loss: 1.5995e-04 - val_mse: 3.8786e-05 - val_NMSE: 3.5060e-04\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3987e-04 - mse: 1.8820e-05 - NMSE: 1.7012e-04 - tot_time: 1h 39m 45.8s\n",
      "\n",
      "Epoch 52: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3987e-04 - mse: 1.8820e-05 - NMSE: 1.7012e-04 - val_loss: 1.5957e-04 - val_mse: 3.8655e-05 - val_NMSE: 3.4942e-04\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3950e-04 - mse: 1.8700e-05 - NMSE: 1.6903e-04 - tot_time: 1h 40m 18.3s\n",
      "\n",
      "Epoch 53: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3950e-04 - mse: 1.8700e-05 - NMSE: 1.6903e-04 - val_loss: 1.5920e-04 - val_mse: 3.8532e-05 - val_NMSE: 3.4831e-04\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3925e-04 - mse: 1.8700e-05 - NMSE: 1.6903e-04 - tot_time: 1h 40m 50.9s\n",
      "\n",
      "Epoch 54: val_NMSE improved from 0.00035 to 0.00035, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3925e-04 - mse: 1.8700e-05 - NMSE: 1.6903e-04 - val_loss: 1.5881e-04 - val_mse: 3.8391e-05 - val_NMSE: 3.4703e-04\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3886e-04 - mse: 1.8561e-05 - NMSE: 1.6778e-04 - tot_time: 1h 41m 24.0s\n",
      "\n",
      "Epoch 55: val_NMSE improved from 0.00035 to 0.00034, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3886e-04 - mse: 1.8561e-05 - NMSE: 1.6778e-04 - val_loss: 1.5832e-04 - val_mse: 3.8154e-05 - val_NMSE: 3.4490e-04\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3856e-04 - mse: 1.8511e-05 - NMSE: 1.6733e-04 - tot_time: 1h 41m 56.3s\n",
      "\n",
      "Epoch 56: val_NMSE improved from 0.00034 to 0.00034, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3856e-04 - mse: 1.8511e-05 - NMSE: 1.6733e-04 - val_loss: 1.5805e-04 - val_mse: 3.8132e-05 - val_NMSE: 3.4470e-04\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3820e-04 - mse: 1.8406e-05 - NMSE: 1.6638e-04 - tot_time: 1h 42m 28.0s\n",
      "\n",
      "Epoch 57: val_NMSE improved from 0.00034 to 0.00034, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3820e-04 - mse: 1.8406e-05 - NMSE: 1.6638e-04 - val_loss: 1.5751e-04 - val_mse: 3.7850e-05 - val_NMSE: 3.4215e-04\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3790e-04 - mse: 1.8359e-05 - NMSE: 1.6596e-04 - tot_time: 1h 42m 59.9s\n",
      "\n",
      "Epoch 58: val_NMSE improved from 0.00034 to 0.00034, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3790e-04 - mse: 1.8359e-05 - NMSE: 1.6596e-04 - val_loss: 1.5712e-04 - val_mse: 3.7711e-05 - val_NMSE: 3.4089e-04\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3761e-04 - mse: 1.8316e-05 - NMSE: 1.6556e-04 - tot_time: 1h 43m 32.5s\n",
      "\n",
      "Epoch 59: val_NMSE improved from 0.00034 to 0.00034, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3761e-04 - mse: 1.8316e-05 - NMSE: 1.6556e-04 - val_loss: 1.5675e-04 - val_mse: 3.7586e-05 - val_NMSE: 3.3976e-04\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3732e-04 - mse: 1.8276e-05 - NMSE: 1.6520e-04 - tot_time: 1h 44m 4.8s\n",
      "\n",
      "Epoch 60: val_NMSE improved from 0.00034 to 0.00034, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3732e-04 - mse: 1.8276e-05 - NMSE: 1.6520e-04 - val_loss: 1.5633e-04 - val_mse: 3.7421e-05 - val_NMSE: 3.3827e-04\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3691e-04 - mse: 1.8124e-05 - NMSE: 1.6383e-04 - tot_time: 1h 44m 37.3s\n",
      "\n",
      "Epoch 61: val_NMSE improved from 0.00034 to 0.00034, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3691e-04 - mse: 1.8124e-05 - NMSE: 1.6383e-04 - val_loss: 1.5589e-04 - val_mse: 3.7237e-05 - val_NMSE: 3.3660e-04\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3657e-04 - mse: 1.8037e-05 - NMSE: 1.6304e-04 - tot_time: 1h 45m 9.5s\n",
      "\n",
      "Epoch 62: val_NMSE improved from 0.00034 to 0.00033, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3657e-04 - mse: 1.8037e-05 - NMSE: 1.6304e-04 - val_loss: 1.5546e-04 - val_mse: 3.7058e-05 - val_NMSE: 3.3498e-04\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3624e-04 - mse: 1.7967e-05 - NMSE: 1.6241e-04 - tot_time: 1h 45m 41.9s\n",
      "\n",
      "Epoch 63: val_NMSE improved from 0.00033 to 0.00033, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3624e-04 - mse: 1.7967e-05 - NMSE: 1.6241e-04 - val_loss: 1.5507e-04 - val_mse: 3.6925e-05 - val_NMSE: 3.3378e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3589e-04 - mse: 1.7867e-05 - NMSE: 1.6151e-04 - tot_time: 1h 46m 14.4s\n",
      "\n",
      "Epoch 64: val_NMSE improved from 0.00033 to 0.00033, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3589e-04 - mse: 1.7867e-05 - NMSE: 1.6151e-04 - val_loss: 1.5467e-04 - val_mse: 3.6777e-05 - val_NMSE: 3.3244e-04\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3557e-04 - mse: 1.7800e-05 - NMSE: 1.6090e-04 - tot_time: 1h 46m 46.8s\n",
      "\n",
      "Epoch 65: val_NMSE improved from 0.00033 to 0.00033, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3557e-04 - mse: 1.7800e-05 - NMSE: 1.6090e-04 - val_loss: 1.5425e-04 - val_mse: 3.6621e-05 - val_NMSE: 3.3104e-04\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3526e-04 - mse: 1.7748e-05 - NMSE: 1.6043e-04 - tot_time: 1h 47m 19.3s\n",
      "\n",
      "Epoch 66: val_NMSE improved from 0.00033 to 0.00033, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3526e-04 - mse: 1.7748e-05 - NMSE: 1.6043e-04 - val_loss: 1.5382e-04 - val_mse: 3.6443e-05 - val_NMSE: 3.2942e-04\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3496e-04 - mse: 1.7702e-05 - NMSE: 1.6001e-04 - tot_time: 1h 47m 51.4s\n",
      "\n",
      "Epoch 67: val_NMSE improved from 0.00033 to 0.00033, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3496e-04 - mse: 1.7702e-05 - NMSE: 1.6001e-04 - val_loss: 1.5335e-04 - val_mse: 3.6231e-05 - val_NMSE: 3.2751e-04\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3459e-04 - mse: 1.7593e-05 - NMSE: 1.5903e-04 - tot_time: 1h 48m 24.0s\n",
      "\n",
      "Epoch 68: val_NMSE improved from 0.00033 to 0.00033, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3459e-04 - mse: 1.7593e-05 - NMSE: 1.5903e-04 - val_loss: 1.5296e-04 - val_mse: 3.6097e-05 - val_NMSE: 3.2629e-04\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3435e-04 - mse: 1.7603e-05 - NMSE: 1.5912e-04 - tot_time: 1h 48m 56.6s\n",
      "\n",
      "Epoch 69: val_NMSE improved from 0.00033 to 0.00032, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3435e-04 - mse: 1.7603e-05 - NMSE: 1.5912e-04 - val_loss: 1.5255e-04 - val_mse: 3.5938e-05 - val_NMSE: 3.2486e-04\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3398e-04 - mse: 1.7499e-05 - NMSE: 1.5818e-04 - tot_time: 1h 49m 28.7s\n",
      "\n",
      "Epoch 70: val_NMSE improved from 0.00032 to 0.00032, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3398e-04 - mse: 1.7499e-05 - NMSE: 1.5818e-04 - val_loss: 1.5209e-04 - val_mse: 3.5741e-05 - val_NMSE: 3.2308e-04\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3359e-04 - mse: 1.7364e-05 - NMSE: 1.5696e-04 - tot_time: 1h 50m 0.9s\n",
      "\n",
      "Epoch 71: val_NMSE improved from 0.00032 to 0.00032, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3359e-04 - mse: 1.7364e-05 - NMSE: 1.5696e-04 - val_loss: 1.5170e-04 - val_mse: 3.5613e-05 - val_NMSE: 3.2192e-04\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3324e-04 - mse: 1.7275e-05 - NMSE: 1.5615e-04 - tot_time: 1h 50m 32.9s\n",
      "\n",
      "Epoch 72: val_NMSE improved from 0.00032 to 0.00032, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3324e-04 - mse: 1.7275e-05 - NMSE: 1.5615e-04 - val_loss: 1.5126e-04 - val_mse: 3.5432e-05 - val_NMSE: 3.2029e-04\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3292e-04 - mse: 1.7209e-05 - NMSE: 1.5555e-04 - tot_time: 1h 51m 5.5s\n",
      "\n",
      "Epoch 73: val_NMSE improved from 0.00032 to 0.00032, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3292e-04 - mse: 1.7209e-05 - NMSE: 1.5555e-04 - val_loss: 1.5088e-04 - val_mse: 3.5310e-05 - val_NMSE: 3.1918e-04\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3262e-04 - mse: 1.7169e-05 - NMSE: 1.5520e-04 - tot_time: 1h 51m 37.8s\n",
      "\n",
      "Epoch 74: val_NMSE did not improve from 0.00032\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3262e-04 - mse: 1.7169e-05 - NMSE: 1.5520e-04 - val_loss: 1.5068e-04 - val_mse: 3.5367e-05 - val_NMSE: 3.1970e-04\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3246e-04 - mse: 1.7271e-05 - NMSE: 1.5612e-04 - tot_time: 1h 52m 10.1s\n",
      "\n",
      "Epoch 75: val_NMSE did not improve from 0.00032\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3246e-04 - mse: 1.7271e-05 - NMSE: 1.5612e-04 - val_loss: 1.5038e-04 - val_mse: 3.5328e-05 - val_NMSE: 3.1935e-04\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3221e-04 - mse: 1.7275e-05 - NMSE: 1.5616e-04 - tot_time: 1h 52m 42.2s\n",
      "\n",
      "Epoch 76: val_NMSE improved from 0.00032 to 0.00032, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3221e-04 - mse: 1.7275e-05 - NMSE: 1.5616e-04 - val_loss: 1.4968e-04 - val_mse: 3.4879e-05 - val_NMSE: 3.1529e-04\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3177e-04 - mse: 1.7091e-05 - NMSE: 1.5450e-04 - tot_time: 1h 53m 14.6s\n",
      "\n",
      "Epoch 77: val_NMSE improved from 0.00032 to 0.00031, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3177e-04 - mse: 1.7091e-05 - NMSE: 1.5450e-04 - val_loss: 1.4913e-04 - val_mse: 3.4590e-05 - val_NMSE: 3.1268e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3130e-04 - mse: 1.6879e-05 - NMSE: 1.5257e-04 - tot_time: 1h 53m 47.4s\n",
      "\n",
      "Epoch 78: val_NMSE improved from 0.00031 to 0.00031, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3130e-04 - mse: 1.6879e-05 - NMSE: 1.5257e-04 - val_loss: 1.4869e-04 - val_mse: 3.4406e-05 - val_NMSE: 3.1101e-04\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3103e-04 - mse: 1.6868e-05 - NMSE: 1.5248e-04 - tot_time: 1h 54m 20.2s\n",
      "\n",
      "Epoch 79: val_NMSE improved from 0.00031 to 0.00031, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3103e-04 - mse: 1.6868e-05 - NMSE: 1.5248e-04 - val_loss: 1.4821e-04 - val_mse: 3.4189e-05 - val_NMSE: 3.0905e-04\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3100e-04 - mse: 1.7097e-05 - NMSE: 1.5455e-04 - tot_time: 1h 54m 52.6s\n",
      "\n",
      "Epoch 80: val_NMSE did not improve from 0.00031\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3100e-04 - mse: 1.7097e-05 - NMSE: 1.5455e-04 - val_loss: 1.4842e-04 - val_mse: 3.4654e-05 - val_NMSE: 3.1325e-04\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3054e-04 - mse: 1.6902e-05 - NMSE: 1.5279e-04 - tot_time: 1h 55m 25.3s\n",
      "\n",
      "Epoch 81: val_NMSE improved from 0.00031 to 0.00031, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3054e-04 - mse: 1.6902e-05 - NMSE: 1.5279e-04 - val_loss: 1.4751e-04 - val_mse: 3.4009e-05 - val_NMSE: 3.0742e-04\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3010e-04 - mse: 1.6717e-05 - NMSE: 1.5111e-04 - tot_time: 1h 55m 58.4s\n",
      "\n",
      "Epoch 82: val_NMSE did not improve from 0.00031\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3010e-04 - mse: 1.6717e-05 - NMSE: 1.5111e-04 - val_loss: 1.4792e-04 - val_mse: 3.4686e-05 - val_NMSE: 3.1354e-04\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3155e-04 - mse: 1.8432e-05 - NMSE: 1.6662e-04 - tot_time: 1h 56m 31.5s\n",
      "\n",
      "Epoch 83: val_NMSE did not improve from 0.00031\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3155e-04 - mse: 1.8432e-05 - NMSE: 1.6662e-04 - val_loss: 1.4792e-04 - val_mse: 3.4940e-05 - val_NMSE: 3.1584e-04\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3104e-04 - mse: 1.8182e-05 - NMSE: 1.6435e-04 - tot_time: 1h 57m 4.2s\n",
      "\n",
      "Epoch 84: val_NMSE did not improve from 0.00031\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3104e-04 - mse: 1.8182e-05 - NMSE: 1.6435e-04 - val_loss: 1.4871e-04 - val_mse: 3.5993e-05 - val_NMSE: 3.2536e-04\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3304e-04 - mse: 2.0448e-05 - NMSE: 1.8484e-04 - tot_time: 1h 57m 37.4s\n",
      "\n",
      "Epoch 85: val_NMSE did not improve from 0.00031\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3304e-04 - mse: 2.0448e-05 - NMSE: 1.8484e-04 - val_loss: 1.4702e-04 - val_mse: 3.4564e-05 - val_NMSE: 3.1243e-04\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3054e-04 - mse: 1.8209e-05 - NMSE: 1.6459e-04 - tot_time: 1h 58m 10.3s\n",
      "\n",
      "Epoch 86: val_NMSE improved from 0.00031 to 0.00030, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3054e-04 - mse: 1.8209e-05 - NMSE: 1.6459e-04 - val_loss: 1.4577e-04 - val_mse: 3.3577e-05 - val_NMSE: 3.0352e-04\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3095e-04 - mse: 1.8887e-05 - NMSE: 1.7073e-04 - tot_time: 1h 58m 42.4s\n",
      "\n",
      "Epoch 87: val_NMSE did not improve from 0.00030\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3095e-04 - mse: 1.8887e-05 - NMSE: 1.7073e-04 - val_loss: 1.4551e-04 - val_mse: 3.3580e-05 - val_NMSE: 3.0354e-04\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3077e-04 - mse: 1.8967e-05 - NMSE: 1.7145e-04 - tot_time: 1h 59m 14.6s\n",
      "\n",
      "Epoch 88: val_NMSE did not improve from 0.00030\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3077e-04 - mse: 1.8967e-05 - NMSE: 1.7145e-04 - val_loss: 1.4624e-04 - val_mse: 3.4568e-05 - val_NMSE: 3.1248e-04\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3132e-04 - mse: 1.9778e-05 - NMSE: 1.7877e-04 - tot_time: 1h 59m 47.1s\n",
      "\n",
      "Epoch 89: val_NMSE did not improve from 0.00030\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3132e-04 - mse: 1.9778e-05 - NMSE: 1.7877e-04 - val_loss: 1.4906e-04 - val_mse: 3.7662e-05 - val_NMSE: 3.4043e-04\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3043e-04 - mse: 1.9141e-05 - NMSE: 1.7301e-04 - tot_time: 2h 0m 19.4s\n",
      "\n",
      "Epoch 90: val_NMSE improved from 0.00030 to 0.00030, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3043e-04 - mse: 1.9141e-05 - NMSE: 1.7301e-04 - val_loss: 1.4419e-04 - val_mse: 3.3037e-05 - val_NMSE: 2.9864e-04\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2933e-04 - mse: 1.8306e-05 - NMSE: 1.6547e-04 - tot_time: 2h 0m 52.3s\n",
      "\n",
      "Epoch 91: val_NMSE did not improve from 0.00030\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2933e-04 - mse: 1.8306e-05 - NMSE: 1.6547e-04 - val_loss: 1.4469e-04 - val_mse: 3.3806e-05 - val_NMSE: 3.0558e-04\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3654e-04 - mse: 2.5777e-05 - NMSE: 2.3302e-04 - tot_time: 2h 1m 25.1s\n",
      "\n",
      "Epoch 92: val_NMSE did not improve from 0.00030\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3654e-04 - mse: 2.5777e-05 - NMSE: 2.3302e-04 - val_loss: 1.4695e-04 - val_mse: 3.6325e-05 - val_NMSE: 3.2836e-04\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2981e-04 - mse: 1.9313e-05 - NMSE: 1.7457e-04 - tot_time: 2h 1m 58.2s\n",
      "\n",
      "Epoch 93: val_NMSE did not improve from 0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2981e-04 - mse: 1.9313e-05 - NMSE: 1.7457e-04 - val_loss: 1.4395e-04 - val_mse: 3.3590e-05 - val_NMSE: 3.0363e-04\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2682e-04 - mse: 1.6581e-05 - NMSE: 1.4988e-04 - tot_time: 2h 2m 31.4s\n",
      "\n",
      "Epoch 94: val_NMSE did not improve from 0.00030\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2682e-04 - mse: 1.6581e-05 - NMSE: 1.4988e-04 - val_loss: 1.4369e-04 - val_mse: 3.3590e-05 - val_NMSE: 3.0363e-04\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2703e-04 - mse: 1.7046e-05 - NMSE: 1.5408e-04 - tot_time: 2h 3m 3.4s\n",
      "\n",
      "Epoch 95: val_NMSE improved from 0.00030 to 0.00030, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.2703e-04 - mse: 1.7046e-05 - NMSE: 1.5408e-04 - val_loss: 1.4280e-04 - val_mse: 3.2951e-05 - val_NMSE: 2.9786e-04\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2849e-04 - mse: 1.8764e-05 - NMSE: 1.6962e-04 - tot_time: 2h 3m 36.0s\n",
      "\n",
      "Epoch 96: val_NMSE did not improve from 0.00030\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2849e-04 - mse: 1.8764e-05 - NMSE: 1.6962e-04 - val_loss: 1.4386e-04 - val_mse: 3.4267e-05 - val_NMSE: 3.0976e-04\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4903e-04 - mse: 3.9570e-05 - NMSE: 3.5762e-04 - tot_time: 2h 4m 9.1s\n",
      "\n",
      "Epoch 97: val_NMSE did not improve from 0.00030\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.4903e-04 - mse: 3.9570e-05 - NMSE: 3.5762e-04 - val_loss: 1.5024e-04 - val_mse: 4.0925e-05 - val_NMSE: 3.6991e-04\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2918e-04 - mse: 1.9961e-05 - NMSE: 1.8042e-04 - tot_time: 2h 4m 41.5s\n",
      "\n",
      "Epoch 98: val_NMSE improved from 0.00030 to 0.00029, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.2918e-04 - mse: 1.9961e-05 - NMSE: 1.8042e-04 - val_loss: 1.4146e-04 - val_mse: 3.2385e-05 - val_NMSE: 2.9274e-04\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2570e-04 - mse: 1.6754e-05 - NMSE: 1.5144e-04 - tot_time: 2h 5m 14.1s\n",
      "\n",
      "Epoch 99: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2570e-04 - mse: 1.6754e-05 - NMSE: 1.5144e-04 - val_loss: 1.4132e-04 - val_mse: 3.2510e-05 - val_NMSE: 2.9388e-04\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2460e-04 - mse: 1.5910e-05 - NMSE: 1.4381e-04 - tot_time: 2h 5m 46.9s\n",
      "\n",
      "Epoch 100: val_NMSE improved from 0.00029 to 0.00029, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2460e-04 - mse: 1.5910e-05 - NMSE: 1.4381e-04 - val_loss: 1.4039e-04 - val_mse: 3.1830e-05 - val_NMSE: 2.8773e-04\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2522e-04 - mse: 1.6771e-05 - NMSE: 1.5160e-04 - tot_time: 2h 6m 20.0s\n",
      "\n",
      "Epoch 101: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2522e-04 - mse: 1.6771e-05 - NMSE: 1.5160e-04 - val_loss: 1.4452e-04 - val_mse: 3.6210e-05 - val_NMSE: 3.2732e-04\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3292e-04 - mse: 2.4727e-05 - NMSE: 2.2353e-04 - tot_time: 2h 6m 53.2s\n",
      "\n",
      "Epoch 102: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3292e-04 - mse: 2.4727e-05 - NMSE: 2.2353e-04 - val_loss: 1.4343e-04 - val_mse: 3.5373e-05 - val_NMSE: 3.1975e-04\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2816e-04 - mse: 2.0225e-05 - NMSE: 1.8283e-04 - tot_time: 2h 7m 26.2s\n",
      "\n",
      "Epoch 103: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2816e-04 - mse: 2.0225e-05 - NMSE: 1.8283e-04 - val_loss: 1.4019e-04 - val_mse: 3.2392e-05 - val_NMSE: 2.9280e-04\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2353e-04 - mse: 1.5849e-05 - NMSE: 1.4327e-04 - tot_time: 2h 7m 58.9s\n",
      "\n",
      "Epoch 104: val_NMSE improved from 0.00029 to 0.00029, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2353e-04 - mse: 1.5849e-05 - NMSE: 1.4327e-04 - val_loss: 1.3918e-04 - val_mse: 3.1632e-05 - val_NMSE: 2.8594e-04\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2341e-04 - mse: 1.5975e-05 - NMSE: 1.4440e-04 - tot_time: 2h 8m 31.8s\n",
      "\n",
      "Epoch 105: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2341e-04 - mse: 1.5975e-05 - NMSE: 1.4440e-04 - val_loss: 1.3923e-04 - val_mse: 3.1925e-05 - val_NMSE: 2.8859e-04\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3379e-04 - mse: 2.6602e-05 - NMSE: 2.4047e-04 - tot_time: 2h 9m 4.0s\n",
      "\n",
      "Epoch 106: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3379e-04 - mse: 2.6602e-05 - NMSE: 2.4047e-04 - val_loss: 1.4665e-04 - val_mse: 3.9606e-05 - val_NMSE: 3.5802e-04\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2561e-04 - mse: 1.8677e-05 - NMSE: 1.6883e-04 - tot_time: 2h 9m 36.7s\n",
      "\n",
      "Epoch 107: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2561e-04 - mse: 1.8677e-05 - NMSE: 1.6883e-04 - val_loss: 1.3859e-04 - val_mse: 3.1786e-05 - val_NMSE: 2.8733e-04\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2335e-04 - mse: 1.6659e-05 - NMSE: 1.5059e-04 - tot_time: 2h 10m 9.7s\n",
      "\n",
      "Epoch 108: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2335e-04 - mse: 1.6659e-05 - NMSE: 1.5059e-04 - val_loss: 1.3878e-04 - val_mse: 3.2224e-05 - val_NMSE: 2.9129e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2214e-04 - mse: 1.5702e-05 - NMSE: 1.4193e-04 - tot_time: 2h 10m 41.8s\n",
      "\n",
      "Epoch 109: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.2214e-04 - mse: 1.5702e-05 - NMSE: 1.4193e-04 - val_loss: 1.3954e-04 - val_mse: 3.3230e-05 - val_NMSE: 3.0037e-04\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3569e-04 - mse: 2.9494e-05 - NMSE: 2.6658e-04 - tot_time: 2h 11m 14.4s\n",
      "\n",
      "Epoch 110: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3569e-04 - mse: 2.9494e-05 - NMSE: 2.6658e-04 - val_loss: 1.4499e-04 - val_mse: 3.8937e-05 - val_NMSE: 3.5197e-04\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2610e-04 - mse: 2.0157e-05 - NMSE: 1.8220e-04 - tot_time: 2h 11m 47.6s\n",
      "\n",
      "Epoch 111: val_NMSE did not improve from 0.00029\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2610e-04 - mse: 2.0157e-05 - NMSE: 1.8220e-04 - val_loss: 1.3819e-04 - val_mse: 3.2386e-05 - val_NMSE: 2.9275e-04\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2186e-04 - mse: 1.6169e-05 - NMSE: 1.4615e-04 - tot_time: 2h 12m 20.4s\n",
      "\n",
      "Epoch 112: val_NMSE improved from 0.00029 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2186e-04 - mse: 1.6169e-05 - NMSE: 1.4615e-04 - val_loss: 1.3678e-04 - val_mse: 3.1220e-05 - val_NMSE: 2.8221e-04\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2181e-04 - mse: 1.6359e-05 - NMSE: 1.4787e-04 - tot_time: 2h 12m 52.7s\n",
      "\n",
      "Epoch 113: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.2181e-04 - mse: 1.6359e-05 - NMSE: 1.4787e-04 - val_loss: 1.4107e-04 - val_mse: 3.5743e-05 - val_NMSE: 3.2310e-04\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3085e-04 - mse: 2.5648e-05 - NMSE: 2.3186e-04 - tot_time: 2h 13m 25.1s\n",
      "\n",
      "Epoch 114: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.3085e-04 - mse: 2.5648e-05 - NMSE: 2.3186e-04 - val_loss: 1.3872e-04 - val_mse: 3.3660e-05 - val_NMSE: 3.0427e-04\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2712e-04 - mse: 2.2175e-05 - NMSE: 2.0045e-04 - tot_time: 2h 13m 57.9s\n",
      "\n",
      "Epoch 115: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2712e-04 - mse: 2.2175e-05 - NMSE: 2.0045e-04 - val_loss: 1.3700e-04 - val_mse: 3.2194e-05 - val_NMSE: 2.9101e-04\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2104e-04 - mse: 1.6335e-05 - NMSE: 1.4766e-04 - tot_time: 2h 14m 30.5s\n",
      "\n",
      "Epoch 116: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2104e-04 - mse: 1.6335e-05 - NMSE: 1.4766e-04 - val_loss: 1.3658e-04 - val_mse: 3.2015e-05 - val_NMSE: 2.8940e-04\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2117e-04 - mse: 1.6704e-05 - NMSE: 1.5099e-04 - tot_time: 2h 15m 3.0s\n",
      "\n",
      "Epoch 117: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2117e-04 - mse: 1.6704e-05 - NMSE: 1.5099e-04 - val_loss: 1.4161e-04 - val_mse: 3.7280e-05 - val_NMSE: 3.3697e-04\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2647e-04 - mse: 2.2256e-05 - NMSE: 2.0116e-04 - tot_time: 2h 15m 35.5s\n",
      "\n",
      "Epoch 118: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2647e-04 - mse: 2.2256e-05 - NMSE: 2.0116e-04 - val_loss: 1.3833e-04 - val_mse: 3.4246e-05 - val_NMSE: 3.0957e-04\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2234e-04 - mse: 1.8364e-05 - NMSE: 1.6600e-04 - tot_time: 2h 16m 8.2s\n",
      "\n",
      "Epoch 119: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2234e-04 - mse: 1.8364e-05 - NMSE: 1.6600e-04 - val_loss: 1.3905e-04 - val_mse: 3.5211e-05 - val_NMSE: 3.1827e-04\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2369e-04 - mse: 1.9964e-05 - NMSE: 1.8045e-04 - tot_time: 2h 16m 41.1s\n",
      "\n",
      "Epoch 120: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2369e-04 - mse: 1.9964e-05 - NMSE: 1.8045e-04 - val_loss: 1.3627e-04 - val_mse: 3.2680e-05 - val_NMSE: 2.9540e-04\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2236e-04 - mse: 1.8881e-05 - NMSE: 1.7066e-04 - tot_time: 2h 17m 12.7s\n",
      "\n",
      "Epoch 121: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.2236e-04 - mse: 1.8881e-05 - NMSE: 1.7066e-04 - val_loss: 1.3853e-04 - val_mse: 3.5174e-05 - val_NMSE: 3.1793e-04\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3137e-04 - mse: 2.8130e-05 - NMSE: 2.5428e-04Restoring model weights from the end of the best epoch: 112.\n",
      " - tot_time: 2h 17m 45.9s\n",
      "\n",
      "Epoch 122: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.3137e-04 - mse: 2.8130e-05 - NMSE: 2.5428e-04 - val_loss: 1.4254e-04 - val_mse: 3.9447e-05 - val_NMSE: 3.5659e-04\n",
      "Epoch 122: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 0.0001 ----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2097e-04 - mse: 1.5426e-05 - NMSE: 1.3944e-04 - tot_time: 2h 18m 20.5s\n",
      "\n",
      "Epoch 1: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 1.2097e-04 - mse: 1.5426e-05 - NMSE: 1.3944e-04 - val_loss: 1.3643e-04 - val_mse: 3.0889e-05 - val_NMSE: 2.7922e-04\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.2063e-04 - mse: 1.5106e-05 - NMSE: 1.3655e-04 - tot_time: 2h 18m 53.7s\n",
      "\n",
      "Epoch 2: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2063e-04 - mse: 1.5106e-05 - NMSE: 1.3655e-04 - val_loss: 1.3637e-04 - val_mse: 3.0859e-05 - val_NMSE: 2.7895e-04\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2057e-04 - mse: 1.5066e-05 - NMSE: 1.3619e-04 - tot_time: 2h 19m 26.9s\n",
      "\n",
      "Epoch 3: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2057e-04 - mse: 1.5066e-05 - NMSE: 1.3619e-04 - val_loss: 1.3627e-04 - val_mse: 3.0788e-05 - val_NMSE: 2.7831e-04\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2047e-04 - mse: 1.4996e-05 - NMSE: 1.3555e-04 - tot_time: 2h 19m 59.3s\n",
      "\n",
      "Epoch 4: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.2047e-04 - mse: 1.4996e-05 - NMSE: 1.3555e-04 - val_loss: 1.3620e-04 - val_mse: 3.0740e-05 - val_NMSE: 2.7787e-04\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2044e-04 - mse: 1.4990e-05 - NMSE: 1.3550e-04 - tot_time: 2h 20m 32.4s\n",
      "\n",
      "Epoch 5: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2044e-04 - mse: 1.4990e-05 - NMSE: 1.3550e-04 - val_loss: 1.3615e-04 - val_mse: 3.0719e-05 - val_NMSE: 2.7768e-04\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2043e-04 - mse: 1.5004e-05 - NMSE: 1.3563e-04 - tot_time: 2h 21m 6.7s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.2043e-04 - mse: 1.5004e-05 - NMSE: 1.3563e-04 - val_loss: 1.3615e-04 - val_mse: 3.0736e-05 - val_NMSE: 2.7784e-04\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2041e-04 - mse: 1.5011e-05 - NMSE: 1.3569e-04 - tot_time: 2h 21m 39.9s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2041e-04 - mse: 1.5011e-05 - NMSE: 1.3569e-04 - val_loss: 1.3610e-04 - val_mse: 3.0720e-05 - val_NMSE: 2.7769e-04\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2035e-04 - mse: 1.4976e-05 - NMSE: 1.3537e-04 - tot_time: 2h 22m 12.8s\n",
      "\n",
      "Epoch 8: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2035e-04 - mse: 1.4976e-05 - NMSE: 1.3537e-04 - val_loss: 1.3602e-04 - val_mse: 3.0667e-05 - val_NMSE: 2.7722e-04\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2031e-04 - mse: 1.4969e-05 - NMSE: 1.3531e-04 - tot_time: 2h 22m 45.5s\n",
      "\n",
      "Epoch 9: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2031e-04 - mse: 1.4969e-05 - NMSE: 1.3531e-04 - val_loss: 1.3596e-04 - val_mse: 3.0633e-05 - val_NMSE: 2.7691e-04\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2029e-04 - mse: 1.4977e-05 - NMSE: 1.3538e-04 - tot_time: 2h 23m 18.8s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2029e-04 - mse: 1.4977e-05 - NMSE: 1.3538e-04 - val_loss: 1.3597e-04 - val_mse: 3.0670e-05 - val_NMSE: 2.7724e-04\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2027e-04 - mse: 1.4981e-05 - NMSE: 1.3542e-04 - tot_time: 2h 23m 52.8s\n",
      "\n",
      "Epoch 11: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.2027e-04 - mse: 1.4981e-05 - NMSE: 1.3542e-04 - val_loss: 1.3589e-04 - val_mse: 3.0618e-05 - val_NMSE: 2.7677e-04\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2023e-04 - mse: 1.4973e-05 - NMSE: 1.3534e-04 - tot_time: 2h 24m 25.3s\n",
      "\n",
      "Epoch 12: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.2023e-04 - mse: 1.4973e-05 - NMSE: 1.3534e-04 - val_loss: 1.3588e-04 - val_mse: 3.0638e-05 - val_NMSE: 2.7695e-04\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2012e-04 - mse: 1.4881e-05 - NMSE: 1.3451e-04 - tot_time: 2h 24m 58.9s\n",
      "\n",
      "Epoch 13: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.2012e-04 - mse: 1.4881e-05 - NMSE: 1.3451e-04 - val_loss: 1.3578e-04 - val_mse: 3.0556e-05 - val_NMSE: 2.7621e-04\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2014e-04 - mse: 1.4935e-05 - NMSE: 1.3500e-04 - tot_time: 2h 25m 31.6s\n",
      "\n",
      "Epoch 14: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2014e-04 - mse: 1.4935e-05 - NMSE: 1.3500e-04 - val_loss: 1.3579e-04 - val_mse: 3.0603e-05 - val_NMSE: 2.7664e-04\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2013e-04 - mse: 1.4948e-05 - NMSE: 1.3512e-04 - tot_time: 2h 26m 4.6s\n",
      "\n",
      "Epoch 15: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.2013e-04 - mse: 1.4948e-05 - NMSE: 1.3512e-04 - val_loss: 1.3572e-04 - val_mse: 3.0555e-05 - val_NMSE: 2.7620e-04\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2004e-04 - mse: 1.4890e-05 - NMSE: 1.3459e-04 - tot_time: 2h 26m 38.4s\n",
      "\n",
      "Epoch 16: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.2004e-04 - mse: 1.4890e-05 - NMSE: 1.3459e-04 - val_loss: 1.3566e-04 - val_mse: 3.0524e-05 - val_NMSE: 2.7592e-04\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2005e-04 - mse: 1.4932e-05 - NMSE: 1.3497e-04 - tot_time: 2h 27m 12.5s\n",
      "\n",
      "Epoch 17: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.2005e-04 - mse: 1.4932e-05 - NMSE: 1.3497e-04 - val_loss: 1.3563e-04 - val_mse: 3.0523e-05 - val_NMSE: 2.7591e-04\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2001e-04 - mse: 1.4920e-05 - NMSE: 1.3487e-04 - tot_time: 2h 27m 46.3s\n",
      "\n",
      "Epoch 18: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.2001e-04 - mse: 1.4920e-05 - NMSE: 1.3487e-04 - val_loss: 1.3561e-04 - val_mse: 3.0534e-05 - val_NMSE: 2.7601e-04\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1995e-04 - mse: 1.4883e-05 - NMSE: 1.3453e-04 - tot_time: 2h 28m 19.9s\n",
      "\n",
      "Epoch 19: val_NMSE did not improve from 0.00028\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1995e-04 - mse: 1.4883e-05 - NMSE: 1.3453e-04 - val_loss: 1.3559e-04 - val_mse: 3.0539e-05 - val_NMSE: 2.7605e-04\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1990e-04 - mse: 1.4869e-05 - NMSE: 1.3441e-04 - tot_time: 2h 28m 52.2s\n",
      "\n",
      "Epoch 20: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1990e-04 - mse: 1.4869e-05 - NMSE: 1.3441e-04 - val_loss: 1.3553e-04 - val_mse: 3.0507e-05 - val_NMSE: 2.7576e-04\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1987e-04 - mse: 1.4861e-05 - NMSE: 1.3433e-04 - tot_time: 2h 29m 24.8s\n",
      "\n",
      "Epoch 21: val_NMSE improved from 0.00028 to 0.00028, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1987e-04 - mse: 1.4861e-05 - NMSE: 1.3433e-04 - val_loss: 1.3544e-04 - val_mse: 3.0446e-05 - val_NMSE: 2.7522e-04\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1986e-04 - mse: 1.4884e-05 - NMSE: 1.3454e-04 - tot_time: 2h 29m 58.3s\n",
      "\n",
      "Epoch 22: val_NMSE improved from 0.00028 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1986e-04 - mse: 1.4884e-05 - NMSE: 1.3454e-04 - val_loss: 1.3537e-04 - val_mse: 3.0406e-05 - val_NMSE: 2.7486e-04\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1975e-04 - mse: 1.4802e-05 - NMSE: 1.3380e-04 - tot_time: 2h 30m 31.0s\n",
      "\n",
      "Epoch 23: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1975e-04 - mse: 1.4802e-05 - NMSE: 1.3380e-04 - val_loss: 1.3534e-04 - val_mse: 3.0407e-05 - val_NMSE: 2.7486e-04\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1971e-04 - mse: 1.4796e-05 - NMSE: 1.3374e-04 - tot_time: 2h 31m 4.1s\n",
      "\n",
      "Epoch 24: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1971e-04 - mse: 1.4796e-05 - NMSE: 1.3374e-04 - val_loss: 1.3529e-04 - val_mse: 3.0392e-05 - val_NMSE: 2.7473e-04\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1971e-04 - mse: 1.4820e-05 - NMSE: 1.3397e-04 - tot_time: 2h 31m 37.0s\n",
      "\n",
      "Epoch 25: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1971e-04 - mse: 1.4820e-05 - NMSE: 1.3397e-04 - val_loss: 1.3517e-04 - val_mse: 3.0304e-05 - val_NMSE: 2.7393e-04\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1966e-04 - mse: 1.4806e-05 - NMSE: 1.3384e-04 - tot_time: 2h 32m 10.3s\n",
      "\n",
      "Epoch 26: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1966e-04 - mse: 1.4806e-05 - NMSE: 1.3384e-04 - val_loss: 1.3521e-04 - val_mse: 3.0369e-05 - val_NMSE: 2.7452e-04\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1962e-04 - mse: 1.4794e-05 - NMSE: 1.3372e-04 - tot_time: 2h 32m 43.2s\n",
      "\n",
      "Epoch 27: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1962e-04 - mse: 1.4794e-05 - NMSE: 1.3372e-04 - val_loss: 1.3513e-04 - val_mse: 3.0322e-05 - val_NMSE: 2.7410e-04\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1963e-04 - mse: 1.4837e-05 - NMSE: 1.3412e-04 - tot_time: 2h 33m 16.5s\n",
      "\n",
      "Epoch 28: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1963e-04 - mse: 1.4837e-05 - NMSE: 1.3412e-04 - val_loss: 1.3508e-04 - val_mse: 3.0305e-05 - val_NMSE: 2.7394e-04\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1956e-04 - mse: 1.4794e-05 - NMSE: 1.3373e-04 - tot_time: 2h 33m 49.9s\n",
      "\n",
      "Epoch 29: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1956e-04 - mse: 1.4794e-05 - NMSE: 1.3373e-04 - val_loss: 1.3506e-04 - val_mse: 3.0320e-05 - val_NMSE: 2.7407e-04\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1949e-04 - mse: 1.4761e-05 - NMSE: 1.3343e-04 - tot_time: 2h 34m 22.6s\n",
      "\n",
      "Epoch 30: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1949e-04 - mse: 1.4761e-05 - NMSE: 1.3343e-04 - val_loss: 1.3502e-04 - val_mse: 3.0311e-05 - val_NMSE: 2.7400e-04\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1951e-04 - mse: 1.4808e-05 - NMSE: 1.3385e-04 - tot_time: 2h 34m 57.2s\n",
      "\n",
      "Epoch 31: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 1.1951e-04 - mse: 1.4808e-05 - NMSE: 1.3385e-04 - val_loss: 1.3489e-04 - val_mse: 3.0212e-05 - val_NMSE: 2.7310e-04\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1940e-04 - mse: 1.4740e-05 - NMSE: 1.3324e-04 - tot_time: 2h 35m 31.2s\n",
      "\n",
      "Epoch 32: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1940e-04 - mse: 1.4740e-05 - NMSE: 1.3324e-04 - val_loss: 1.3484e-04 - val_mse: 3.0195e-05 - val_NMSE: 2.7295e-04\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1938e-04 - mse: 1.4748e-05 - NMSE: 1.3331e-04 - tot_time: 2h 36m 4.4s\n",
      "\n",
      "Epoch 33: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1938e-04 - mse: 1.4748e-05 - NMSE: 1.3331e-04 - val_loss: 1.3482e-04 - val_mse: 3.0208e-05 - val_NMSE: 2.7306e-04\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1931e-04 - mse: 1.4710e-05 - NMSE: 1.3297e-04 - tot_time: 2h 36m 37.9s\n",
      "\n",
      "Epoch 34: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1931e-04 - mse: 1.4710e-05 - NMSE: 1.3297e-04 - val_loss: 1.3479e-04 - val_mse: 3.0208e-05 - val_NMSE: 2.7306e-04\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1934e-04 - mse: 1.4770e-05 - NMSE: 1.3351e-04 - tot_time: 2h 37m 10.9s\n",
      "\n",
      "Epoch 35: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1934e-04 - mse: 1.4770e-05 - NMSE: 1.3351e-04 - val_loss: 1.3473e-04 - val_mse: 3.0185e-05 - val_NMSE: 2.7285e-04\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1924e-04 - mse: 1.4705e-05 - NMSE: 1.3292e-04 - tot_time: 2h 37m 44.0s\n",
      "\n",
      "Epoch 36: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1924e-04 - mse: 1.4705e-05 - NMSE: 1.3292e-04 - val_loss: 1.3466e-04 - val_mse: 3.0140e-05 - val_NMSE: 2.7245e-04\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1923e-04 - mse: 1.4731e-05 - NMSE: 1.3315e-04 - tot_time: 2h 38m 17.2s\n",
      "\n",
      "Epoch 37: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1923e-04 - mse: 1.4731e-05 - NMSE: 1.3315e-04 - val_loss: 1.3465e-04 - val_mse: 3.0165e-05 - val_NMSE: 2.7267e-04\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1914e-04 - mse: 1.4679e-05 - NMSE: 1.3269e-04 - tot_time: 2h 38m 51.2s\n",
      "\n",
      "Epoch 38: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1914e-04 - mse: 1.4679e-05 - NMSE: 1.3269e-04 - val_loss: 1.3457e-04 - val_mse: 3.0125e-05 - val_NMSE: 2.7231e-04\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1914e-04 - mse: 1.4706e-05 - NMSE: 1.3293e-04 - tot_time: 2h 39m 24.7s\n",
      "\n",
      "Epoch 39: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1914e-04 - mse: 1.4706e-05 - NMSE: 1.3293e-04 - val_loss: 1.3455e-04 - val_mse: 3.0141e-05 - val_NMSE: 2.7246e-04\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1910e-04 - mse: 1.4701e-05 - NMSE: 1.3288e-04 - tot_time: 2h 39m 58.4s\n",
      "\n",
      "Epoch 40: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1910e-04 - mse: 1.4701e-05 - NMSE: 1.3288e-04 - val_loss: 1.3448e-04 - val_mse: 3.0103e-05 - val_NMSE: 2.7212e-04\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1903e-04 - mse: 1.4668e-05 - NMSE: 1.3259e-04 - tot_time: 2h 40m 31.7s\n",
      "\n",
      "Epoch 41: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1903e-04 - mse: 1.4668e-05 - NMSE: 1.3259e-04 - val_loss: 1.3438e-04 - val_mse: 3.0034e-05 - val_NMSE: 2.7149e-04\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1900e-04 - mse: 1.4678e-05 - NMSE: 1.3268e-04 - tot_time: 2h 41m 5.5s\n",
      "\n",
      "Epoch 42: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1900e-04 - mse: 1.4678e-05 - NMSE: 1.3268e-04 - val_loss: 1.3439e-04 - val_mse: 3.0083e-05 - val_NMSE: 2.7194e-04\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1896e-04 - mse: 1.4665e-05 - NMSE: 1.3256e-04 - tot_time: 2h 41m 39.6s\n",
      "\n",
      "Epoch 43: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1896e-04 - mse: 1.4665e-05 - NMSE: 1.3256e-04 - val_loss: 1.3427e-04 - val_mse: 2.9997e-05 - val_NMSE: 2.7116e-04\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1889e-04 - mse: 1.4635e-05 - NMSE: 1.3229e-04 - tot_time: 2h 42m 12.5s\n",
      "\n",
      "Epoch 44: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1889e-04 - mse: 1.4635e-05 - NMSE: 1.3229e-04 - val_loss: 1.3428e-04 - val_mse: 3.0040e-05 - val_NMSE: 2.7154e-04\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1887e-04 - mse: 1.4654e-05 - NMSE: 1.3246e-04 - tot_time: 2h 42m 46.8s\n",
      "\n",
      "Epoch 45: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1887e-04 - mse: 1.4654e-05 - NMSE: 1.3246e-04 - val_loss: 1.3420e-04 - val_mse: 3.0002e-05 - val_NMSE: 2.7120e-04\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.1878e-04 - mse: 1.4595e-05 - NMSE: 1.3193e-04 - tot_time: 2h 43m 21.0s\n",
      "\n",
      "Epoch 46: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1878e-04 - mse: 1.4595e-05 - NMSE: 1.3193e-04 - val_loss: 1.3412e-04 - val_mse: 2.9956e-05 - val_NMSE: 2.7078e-04\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1876e-04 - mse: 1.4607e-05 - NMSE: 1.3203e-04 - tot_time: 2h 43m 53.9s\n",
      "\n",
      "Epoch 47: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1876e-04 - mse: 1.4607e-05 - NMSE: 1.3203e-04 - val_loss: 1.3409e-04 - val_mse: 2.9965e-05 - val_NMSE: 2.7087e-04\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1877e-04 - mse: 1.4658e-05 - NMSE: 1.3249e-04 - tot_time: 2h 44m 28.6s\n",
      "\n",
      "Epoch 48: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 1.1877e-04 - mse: 1.4658e-05 - NMSE: 1.3249e-04 - val_loss: 1.3403e-04 - val_mse: 2.9937e-05 - val_NMSE: 2.7061e-04\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1870e-04 - mse: 1.4623e-05 - NMSE: 1.3218e-04 - tot_time: 2h 45m 2.3s\n",
      "\n",
      "Epoch 49: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1870e-04 - mse: 1.4623e-05 - NMSE: 1.3218e-04 - val_loss: 1.3395e-04 - val_mse: 2.9900e-05 - val_NMSE: 2.7028e-04\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1863e-04 - mse: 1.4598e-05 - NMSE: 1.3195e-04 - tot_time: 2h 45m 35.5s\n",
      "\n",
      "Epoch 50: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1863e-04 - mse: 1.4598e-05 - NMSE: 1.3195e-04 - val_loss: 1.3390e-04 - val_mse: 2.9883e-05 - val_NMSE: 2.7012e-04\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1858e-04 - mse: 1.4586e-05 - NMSE: 1.3184e-04 - tot_time: 2h 46m 8.4s\n",
      "\n",
      "Epoch 51: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1858e-04 - mse: 1.4586e-05 - NMSE: 1.3184e-04 - val_loss: 1.3383e-04 - val_mse: 2.9856e-05 - val_NMSE: 2.6988e-04\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1856e-04 - mse: 1.4601e-05 - NMSE: 1.3198e-04 - tot_time: 2h 46m 41.8s\n",
      "\n",
      "Epoch 52: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1856e-04 - mse: 1.4601e-05 - NMSE: 1.3198e-04 - val_loss: 1.3377e-04 - val_mse: 2.9826e-05 - val_NMSE: 2.6961e-04\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1852e-04 - mse: 1.4592e-05 - NMSE: 1.3190e-04 - tot_time: 2h 47m 14.7s\n",
      "\n",
      "Epoch 53: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1852e-04 - mse: 1.4592e-05 - NMSE: 1.3190e-04 - val_loss: 1.3374e-04 - val_mse: 2.9836e-05 - val_NMSE: 2.6970e-04\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1843e-04 - mse: 1.4545e-05 - NMSE: 1.3147e-04 - tot_time: 2h 47m 47.9s\n",
      "\n",
      "Epoch 54: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1843e-04 - mse: 1.4545e-05 - NMSE: 1.3147e-04 - val_loss: 1.3366e-04 - val_mse: 2.9797e-05 - val_NMSE: 2.6935e-04\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1844e-04 - mse: 1.4592e-05 - NMSE: 1.3190e-04 - tot_time: 2h 48m 21.0s\n",
      "\n",
      "Epoch 55: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1844e-04 - mse: 1.4592e-05 - NMSE: 1.3190e-04 - val_loss: 1.3362e-04 - val_mse: 2.9790e-05 - val_NMSE: 2.6929e-04\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1837e-04 - mse: 1.4558e-05 - NMSE: 1.3160e-04 - tot_time: 2h 48m 54.6s\n",
      "\n",
      "Epoch 56: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1837e-04 - mse: 1.4558e-05 - NMSE: 1.3160e-04 - val_loss: 1.3358e-04 - val_mse: 2.9793e-05 - val_NMSE: 2.6931e-04\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1832e-04 - mse: 1.4556e-05 - NMSE: 1.3158e-04 - tot_time: 2h 49m 27.8s\n",
      "\n",
      "Epoch 57: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1832e-04 - mse: 1.4556e-05 - NMSE: 1.3158e-04 - val_loss: 1.3353e-04 - val_mse: 2.9786e-05 - val_NMSE: 2.6925e-04\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1825e-04 - mse: 1.4517e-05 - NMSE: 1.3122e-04 - tot_time: 2h 50m 1.0s\n",
      "\n",
      "Epoch 58: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1825e-04 - mse: 1.4517e-05 - NMSE: 1.3122e-04 - val_loss: 1.3339e-04 - val_mse: 2.9681e-05 - val_NMSE: 2.6830e-04\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1822e-04 - mse: 1.4528e-05 - NMSE: 1.3132e-04 - tot_time: 2h 50m 33.9s\n",
      "\n",
      "Epoch 59: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1822e-04 - mse: 1.4528e-05 - NMSE: 1.3132e-04 - val_loss: 1.3334e-04 - val_mse: 2.9668e-05 - val_NMSE: 2.6818e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1818e-04 - mse: 1.4534e-05 - NMSE: 1.3138e-04 - tot_time: 2h 51m 8.1s\n",
      "\n",
      "Epoch 60: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1818e-04 - mse: 1.4534e-05 - NMSE: 1.3138e-04 - val_loss: 1.3336e-04 - val_mse: 2.9735e-05 - val_NMSE: 2.6879e-04\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1813e-04 - mse: 1.4521e-05 - NMSE: 1.3126e-04 - tot_time: 2h 51m 40.5s\n",
      "\n",
      "Epoch 61: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1813e-04 - mse: 1.4521e-05 - NMSE: 1.3126e-04 - val_loss: 1.3325e-04 - val_mse: 2.9666e-05 - val_NMSE: 2.6817e-04\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1802e-04 - mse: 1.4451e-05 - NMSE: 1.3062e-04 - tot_time: 2h 52m 14.1s\n",
      "\n",
      "Epoch 62: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1802e-04 - mse: 1.4451e-05 - NMSE: 1.3062e-04 - val_loss: 1.3321e-04 - val_mse: 2.9661e-05 - val_NMSE: 2.6812e-04\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1801e-04 - mse: 1.4482e-05 - NMSE: 1.3091e-04 - tot_time: 2h 52m 47.4s\n",
      "\n",
      "Epoch 63: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1801e-04 - mse: 1.4482e-05 - NMSE: 1.3091e-04 - val_loss: 1.3312e-04 - val_mse: 2.9620e-05 - val_NMSE: 2.6775e-04\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1797e-04 - mse: 1.4483e-05 - NMSE: 1.3092e-04 - tot_time: 2h 53m 21.1s\n",
      "\n",
      "Epoch 64: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1797e-04 - mse: 1.4483e-05 - NMSE: 1.3092e-04 - val_loss: 1.3306e-04 - val_mse: 2.9593e-05 - val_NMSE: 2.6751e-04\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1795e-04 - mse: 1.4507e-05 - NMSE: 1.3113e-04 - tot_time: 2h 53m 55.2s\n",
      "\n",
      "Epoch 65: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1795e-04 - mse: 1.4507e-05 - NMSE: 1.3113e-04 - val_loss: 1.3302e-04 - val_mse: 2.9604e-05 - val_NMSE: 2.6760e-04\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1786e-04 - mse: 1.4464e-05 - NMSE: 1.3074e-04 - tot_time: 2h 54m 28.6s\n",
      "\n",
      "Epoch 66: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1786e-04 - mse: 1.4464e-05 - NMSE: 1.3074e-04 - val_loss: 1.3295e-04 - val_mse: 2.9570e-05 - val_NMSE: 2.6729e-04\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1784e-04 - mse: 1.4482e-05 - NMSE: 1.3091e-04 - tot_time: 2h 55m 1.6s\n",
      "\n",
      "Epoch 67: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1784e-04 - mse: 1.4482e-05 - NMSE: 1.3091e-04 - val_loss: 1.3293e-04 - val_mse: 2.9592e-05 - val_NMSE: 2.6749e-04\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1775e-04 - mse: 1.4431e-05 - NMSE: 1.3045e-04 - tot_time: 2h 55m 34.1s\n",
      "\n",
      "Epoch 68: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1775e-04 - mse: 1.4431e-05 - NMSE: 1.3045e-04 - val_loss: 1.3286e-04 - val_mse: 2.9565e-05 - val_NMSE: 2.6726e-04\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1772e-04 - mse: 1.4445e-05 - NMSE: 1.3057e-04 - tot_time: 2h 56m 6.9s\n",
      "\n",
      "Epoch 69: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1772e-04 - mse: 1.4445e-05 - NMSE: 1.3057e-04 - val_loss: 1.3270e-04 - val_mse: 2.9453e-05 - val_NMSE: 2.6624e-04\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1769e-04 - mse: 1.4456e-05 - NMSE: 1.3067e-04 - tot_time: 2h 56m 39.8s\n",
      "\n",
      "Epoch 70: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1769e-04 - mse: 1.4456e-05 - NMSE: 1.3067e-04 - val_loss: 1.3264e-04 - val_mse: 2.9437e-05 - val_NMSE: 2.6610e-04\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1766e-04 - mse: 1.4469e-05 - NMSE: 1.3079e-04 - tot_time: 2h 57m 13.5s\n",
      "\n",
      "Epoch 71: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1766e-04 - mse: 1.4469e-05 - NMSE: 1.3079e-04 - val_loss: 1.3263e-04 - val_mse: 2.9466e-05 - val_NMSE: 2.6635e-04\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1759e-04 - mse: 1.4452e-05 - NMSE: 1.3064e-04 - tot_time: 2h 57m 47.2s\n",
      "\n",
      "Epoch 72: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1759e-04 - mse: 1.4452e-05 - NMSE: 1.3064e-04 - val_loss: 1.3254e-04 - val_mse: 2.9426e-05 - val_NMSE: 2.6599e-04\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1748e-04 - mse: 1.4381e-05 - NMSE: 1.3000e-04 - tot_time: 2h 58m 20.2s\n",
      "\n",
      "Epoch 73: val_NMSE did not improve from 0.00027\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1748e-04 - mse: 1.4381e-05 - NMSE: 1.3000e-04 - val_loss: 1.3250e-04 - val_mse: 2.9428e-05 - val_NMSE: 2.6601e-04\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1743e-04 - mse: 1.4374e-05 - NMSE: 1.2993e-04 - tot_time: 2h 58m 53.7s\n",
      "\n",
      "Epoch 74: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1743e-04 - mse: 1.4374e-05 - NMSE: 1.2993e-04 - val_loss: 1.3244e-04 - val_mse: 2.9407e-05 - val_NMSE: 2.6582e-04\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1742e-04 - mse: 1.4410e-05 - NMSE: 1.3026e-04 - tot_time: 2h 59m 27.3s\n",
      "\n",
      "Epoch 75: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1742e-04 - mse: 1.4410e-05 - NMSE: 1.3026e-04 - val_loss: 1.3238e-04 - val_mse: 2.9392e-05 - val_NMSE: 2.6569e-04\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1733e-04 - mse: 1.4371e-05 - NMSE: 1.2990e-04 - tot_time: 3h 0m 0.3s\n",
      "\n",
      "Epoch 76: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1733e-04 - mse: 1.4371e-05 - NMSE: 1.2990e-04 - val_loss: 1.3231e-04 - val_mse: 2.9373e-05 - val_NMSE: 2.6552e-04\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1732e-04 - mse: 1.4402e-05 - NMSE: 1.3018e-04 - tot_time: 3h 0m 33.7s\n",
      "\n",
      "Epoch 77: val_NMSE improved from 0.00027 to 0.00027, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1732e-04 - mse: 1.4402e-05 - NMSE: 1.3018e-04 - val_loss: 1.3225e-04 - val_mse: 2.9359e-05 - val_NMSE: 2.6539e-04\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1723e-04 - mse: 1.4362e-05 - NMSE: 1.2982e-04 - tot_time: 3h 1m 6.3s\n",
      "\n",
      "Epoch 78: val_NMSE improved from 0.00027 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1723e-04 - mse: 1.4362e-05 - NMSE: 1.2982e-04 - val_loss: 1.3214e-04 - val_mse: 2.9291e-05 - val_NMSE: 2.6477e-04\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1715e-04 - mse: 1.4324e-05 - NMSE: 1.2948e-04 - tot_time: 3h 1m 39.7s\n",
      "\n",
      "Epoch 79: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1715e-04 - mse: 1.4324e-05 - NMSE: 1.2948e-04 - val_loss: 1.3204e-04 - val_mse: 2.9238e-05 - val_NMSE: 2.6430e-04\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1711e-04 - mse: 1.4333e-05 - NMSE: 1.2956e-04 - tot_time: 3h 2m 13.4s\n",
      "\n",
      "Epoch 80: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1711e-04 - mse: 1.4333e-05 - NMSE: 1.2956e-04 - val_loss: 1.3199e-04 - val_mse: 2.9239e-05 - val_NMSE: 2.6431e-04\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1704e-04 - mse: 1.4311e-05 - NMSE: 1.2936e-04 - tot_time: 3h 2m 46.2s\n",
      "\n",
      "Epoch 81: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1704e-04 - mse: 1.4311e-05 - NMSE: 1.2936e-04 - val_loss: 1.3190e-04 - val_mse: 2.9193e-05 - val_NMSE: 2.6388e-04\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1696e-04 - mse: 1.4276e-05 - NMSE: 1.2904e-04 - tot_time: 3h 3m 19.2s\n",
      "\n",
      "Epoch 82: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1696e-04 - mse: 1.4276e-05 - NMSE: 1.2904e-04 - val_loss: 1.3188e-04 - val_mse: 2.9221e-05 - val_NMSE: 2.6414e-04\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1691e-04 - mse: 1.4276e-05 - NMSE: 1.2904e-04 - tot_time: 3h 3m 52.6s\n",
      "\n",
      "Epoch 83: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1691e-04 - mse: 1.4276e-05 - NMSE: 1.2904e-04 - val_loss: 1.3179e-04 - val_mse: 2.9182e-05 - val_NMSE: 2.6379e-04\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1686e-04 - mse: 1.4267e-05 - NMSE: 1.2896e-04 - tot_time: 3h 4m 27.0s\n",
      "\n",
      "Epoch 84: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1686e-04 - mse: 1.4267e-05 - NMSE: 1.2896e-04 - val_loss: 1.3173e-04 - val_mse: 2.9169e-05 - val_NMSE: 2.6367e-04\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1679e-04 - mse: 1.4249e-05 - NMSE: 1.2880e-04 - tot_time: 3h 4m 59.3s\n",
      "\n",
      "Epoch 85: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1679e-04 - mse: 1.4249e-05 - NMSE: 1.2880e-04 - val_loss: 1.3163e-04 - val_mse: 2.9118e-05 - val_NMSE: 2.6321e-04\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1673e-04 - mse: 1.4235e-05 - NMSE: 1.2867e-04 - tot_time: 3h 5m 32.9s\n",
      "\n",
      "Epoch 86: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1673e-04 - mse: 1.4235e-05 - NMSE: 1.2867e-04 - val_loss: 1.3159e-04 - val_mse: 2.9118e-05 - val_NMSE: 2.6321e-04\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1665e-04 - mse: 1.4209e-05 - NMSE: 1.2844e-04 - tot_time: 3h 6m 6.7s\n",
      "\n",
      "Epoch 87: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1665e-04 - mse: 1.4209e-05 - NMSE: 1.2844e-04 - val_loss: 1.3152e-04 - val_mse: 2.9100e-05 - val_NMSE: 2.6304e-04\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1659e-04 - mse: 1.4195e-05 - NMSE: 1.2831e-04 - tot_time: 3h 6m 39.2s\n",
      "\n",
      "Epoch 88: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1659e-04 - mse: 1.4195e-05 - NMSE: 1.2831e-04 - val_loss: 1.3146e-04 - val_mse: 2.9086e-05 - val_NMSE: 2.6292e-04\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1654e-04 - mse: 1.4190e-05 - NMSE: 1.2827e-04 - tot_time: 3h 7m 12.6s\n",
      "\n",
      "Epoch 89: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1654e-04 - mse: 1.4190e-05 - NMSE: 1.2827e-04 - val_loss: 1.3136e-04 - val_mse: 2.9045e-05 - val_NMSE: 2.6255e-04\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1650e-04 - mse: 1.4208e-05 - NMSE: 1.2843e-04 - tot_time: 3h 7m 44.6s\n",
      "\n",
      "Epoch 90: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1650e-04 - mse: 1.4208e-05 - NMSE: 1.2843e-04 - val_loss: 1.3128e-04 - val_mse: 2.9013e-05 - val_NMSE: 2.6226e-04\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1643e-04 - mse: 1.4187e-05 - NMSE: 1.2824e-04 - tot_time: 3h 8m 18.6s\n",
      "\n",
      "Epoch 91: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1643e-04 - mse: 1.4187e-05 - NMSE: 1.2824e-04 - val_loss: 1.3122e-04 - val_mse: 2.9002e-05 - val_NMSE: 2.6216e-04\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1634e-04 - mse: 1.4146e-05 - NMSE: 1.2787e-04 - tot_time: 3h 8m 51.8s\n",
      "\n",
      "Epoch 92: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1634e-04 - mse: 1.4146e-05 - NMSE: 1.2787e-04 - val_loss: 1.3113e-04 - val_mse: 2.8963e-05 - val_NMSE: 2.6181e-04\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1633e-04 - mse: 1.4187e-05 - NMSE: 1.2824e-04 - tot_time: 3h 9m 25.4s\n",
      "\n",
      "Epoch 93: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1633e-04 - mse: 1.4187e-05 - NMSE: 1.2824e-04 - val_loss: 1.3109e-04 - val_mse: 2.8976e-05 - val_NMSE: 2.6193e-04\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1620e-04 - mse: 1.4110e-05 - NMSE: 1.2755e-04 - tot_time: 3h 9m 58.9s\n",
      "\n",
      "Epoch 94: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1620e-04 - mse: 1.4110e-05 - NMSE: 1.2755e-04 - val_loss: 1.3097e-04 - val_mse: 2.8904e-05 - val_NMSE: 2.6128e-04\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1615e-04 - mse: 1.4107e-05 - NMSE: 1.2751e-04 - tot_time: 3h 10m 32.4s\n",
      "\n",
      "Epoch 95: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1615e-04 - mse: 1.4107e-05 - NMSE: 1.2751e-04 - val_loss: 1.3091e-04 - val_mse: 2.8891e-05 - val_NMSE: 2.6116e-04\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1615e-04 - mse: 1.4155e-05 - NMSE: 1.2795e-04 - tot_time: 3h 11m 4.8s\n",
      "\n",
      "Epoch 96: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1615e-04 - mse: 1.4155e-05 - NMSE: 1.2795e-04 - val_loss: 1.3083e-04 - val_mse: 2.8869e-05 - val_NMSE: 2.6096e-04\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1607e-04 - mse: 1.4135e-05 - NMSE: 1.2777e-04 - tot_time: 3h 11m 36.7s\n",
      "\n",
      "Epoch 97: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1607e-04 - mse: 1.4135e-05 - NMSE: 1.2777e-04 - val_loss: 1.3077e-04 - val_mse: 2.8862e-05 - val_NMSE: 2.6090e-04\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1599e-04 - mse: 1.4105e-05 - NMSE: 1.2750e-04 - tot_time: 3h 12m 10.3s\n",
      "\n",
      "Epoch 98: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1599e-04 - mse: 1.4105e-05 - NMSE: 1.2750e-04 - val_loss: 1.3067e-04 - val_mse: 2.8817e-05 - val_NMSE: 2.6049e-04\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1594e-04 - mse: 1.4108e-05 - NMSE: 1.2753e-04 - tot_time: 3h 12m 43.1s\n",
      "\n",
      "Epoch 99: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1594e-04 - mse: 1.4108e-05 - NMSE: 1.2753e-04 - val_loss: 1.3057e-04 - val_mse: 2.8766e-05 - val_NMSE: 2.6002e-04\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1587e-04 - mse: 1.4086e-05 - NMSE: 1.2732e-04 - tot_time: 3h 13m 15.6s\n",
      "\n",
      "Epoch 100: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1587e-04 - mse: 1.4086e-05 - NMSE: 1.2732e-04 - val_loss: 1.3050e-04 - val_mse: 2.8754e-05 - val_NMSE: 2.5992e-04\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1578e-04 - mse: 1.4050e-05 - NMSE: 1.2700e-04 - tot_time: 3h 13m 48.4s\n",
      "\n",
      "Epoch 101: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1578e-04 - mse: 1.4050e-05 - NMSE: 1.2700e-04 - val_loss: 1.3047e-04 - val_mse: 2.8774e-05 - val_NMSE: 2.6010e-04\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1572e-04 - mse: 1.4049e-05 - NMSE: 1.2699e-04 - tot_time: 3h 14m 20.3s\n",
      "\n",
      "Epoch 102: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1572e-04 - mse: 1.4049e-05 - NMSE: 1.2699e-04 - val_loss: 1.3036e-04 - val_mse: 2.8722e-05 - val_NMSE: 2.5963e-04\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1564e-04 - mse: 1.4021e-05 - NMSE: 1.2674e-04 - tot_time: 3h 14m 53.1s\n",
      "\n",
      "Epoch 103: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1564e-04 - mse: 1.4021e-05 - NMSE: 1.2674e-04 - val_loss: 1.3031e-04 - val_mse: 2.8721e-05 - val_NMSE: 2.5962e-04\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1562e-04 - mse: 1.4053e-05 - NMSE: 1.2703e-04 - tot_time: 3h 15m 26.4s\n",
      "\n",
      "Epoch 104: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1562e-04 - mse: 1.4053e-05 - NMSE: 1.2703e-04 - val_loss: 1.3020e-04 - val_mse: 2.8665e-05 - val_NMSE: 2.5912e-04\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1555e-04 - mse: 1.4040e-05 - NMSE: 1.2691e-04 - tot_time: 3h 15m 59.1s\n",
      "\n",
      "Epoch 105: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1555e-04 - mse: 1.4040e-05 - NMSE: 1.2691e-04 - val_loss: 1.3010e-04 - val_mse: 2.8622e-05 - val_NMSE: 2.5872e-04\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1548e-04 - mse: 1.4027e-05 - NMSE: 1.2679e-04 - tot_time: 3h 16m 32.5s\n",
      "\n",
      "Epoch 106: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1548e-04 - mse: 1.4027e-05 - NMSE: 1.2679e-04 - val_loss: 1.3001e-04 - val_mse: 2.8592e-05 - val_NMSE: 2.5845e-04\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1537e-04 - mse: 1.3973e-05 - NMSE: 1.2631e-04 - tot_time: 3h 17m 5.5s\n",
      "\n",
      "Epoch 107: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1537e-04 - mse: 1.3973e-05 - NMSE: 1.2631e-04 - val_loss: 1.2997e-04 - val_mse: 2.8607e-05 - val_NMSE: 2.5859e-04\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1532e-04 - mse: 1.3982e-05 - NMSE: 1.2639e-04 - tot_time: 3h 17m 37.8s\n",
      "\n",
      "Epoch 108: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1532e-04 - mse: 1.3982e-05 - NMSE: 1.2639e-04 - val_loss: 1.2986e-04 - val_mse: 2.8546e-05 - val_NMSE: 2.5804e-04\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1527e-04 - mse: 1.3984e-05 - NMSE: 1.2641e-04 - tot_time: 3h 18m 12.3s\n",
      "\n",
      "Epoch 109: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 1.1527e-04 - mse: 1.3984e-05 - NMSE: 1.2641e-04 - val_loss: 1.2980e-04 - val_mse: 2.8549e-05 - val_NMSE: 2.5807e-04\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1520e-04 - mse: 1.3973e-05 - NMSE: 1.2631e-04 - tot_time: 3h 18m 44.8s\n",
      "\n",
      "Epoch 110: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1520e-04 - mse: 1.3973e-05 - NMSE: 1.2631e-04 - val_loss: 1.2978e-04 - val_mse: 2.8581e-05 - val_NMSE: 2.5836e-04\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1510e-04 - mse: 1.3929e-05 - NMSE: 1.2591e-04 - tot_time: 3h 19m 18.7s\n",
      "\n",
      "Epoch 111: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1510e-04 - mse: 1.3929e-05 - NMSE: 1.2591e-04 - val_loss: 1.2967e-04 - val_mse: 2.8527e-05 - val_NMSE: 2.5787e-04\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1505e-04 - mse: 1.3939e-05 - NMSE: 1.2600e-04 - tot_time: 3h 19m 52.3s\n",
      "\n",
      "Epoch 112: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1505e-04 - mse: 1.3939e-05 - NMSE: 1.2600e-04 - val_loss: 1.2951e-04 - val_mse: 2.8433e-05 - val_NMSE: 2.5701e-04\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1498e-04 - mse: 1.3928e-05 - NMSE: 1.2589e-04 - tot_time: 3h 20m 25.6s\n",
      "\n",
      "Epoch 113: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1498e-04 - mse: 1.3928e-05 - NMSE: 1.2589e-04 - val_loss: 1.2946e-04 - val_mse: 2.8432e-05 - val_NMSE: 2.5701e-04\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1487e-04 - mse: 1.3868e-05 - NMSE: 1.2535e-04 - tot_time: 3h 20m 58.5s\n",
      "\n",
      "Epoch 114: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1487e-04 - mse: 1.3868e-05 - NMSE: 1.2535e-04 - val_loss: 1.2941e-04 - val_mse: 2.8446e-05 - val_NMSE: 2.5714e-04\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1486e-04 - mse: 1.3917e-05 - NMSE: 1.2580e-04 - tot_time: 3h 21m 31.5s\n",
      "\n",
      "Epoch 115: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1486e-04 - mse: 1.3917e-05 - NMSE: 1.2580e-04 - val_loss: 1.2928e-04 - val_mse: 2.8379e-05 - val_NMSE: 2.5653e-04\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1478e-04 - mse: 1.3902e-05 - NMSE: 1.2566e-04 - tot_time: 3h 22m 4.0s\n",
      "\n",
      "Epoch 116: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1478e-04 - mse: 1.3902e-05 - NMSE: 1.2566e-04 - val_loss: 1.2918e-04 - val_mse: 2.8335e-05 - val_NMSE: 2.5613e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1469e-04 - mse: 1.3874e-05 - NMSE: 1.2541e-04 - tot_time: 3h 22m 36.8s\n",
      "\n",
      "Epoch 117: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1469e-04 - mse: 1.3874e-05 - NMSE: 1.2541e-04 - val_loss: 1.2911e-04 - val_mse: 2.8325e-05 - val_NMSE: 2.5604e-04\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1462e-04 - mse: 1.3856e-05 - NMSE: 1.2525e-04 - tot_time: 3h 23m 10.4s\n",
      "\n",
      "Epoch 118: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1462e-04 - mse: 1.3856e-05 - NMSE: 1.2525e-04 - val_loss: 1.2900e-04 - val_mse: 2.8270e-05 - val_NMSE: 2.5554e-04\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1457e-04 - mse: 1.3865e-05 - NMSE: 1.2533e-04 - tot_time: 3h 23m 43.9s\n",
      "\n",
      "Epoch 119: val_NMSE improved from 0.00026 to 0.00026, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1457e-04 - mse: 1.3865e-05 - NMSE: 1.2533e-04 - val_loss: 1.2892e-04 - val_mse: 2.8248e-05 - val_NMSE: 2.5535e-04\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1445e-04 - mse: 1.3810e-05 - NMSE: 1.2483e-04 - tot_time: 3h 24m 17.7s\n",
      "\n",
      "Epoch 120: val_NMSE did not improve from 0.00026\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1445e-04 - mse: 1.3810e-05 - NMSE: 1.2483e-04 - val_loss: 1.2889e-04 - val_mse: 2.8284e-05 - val_NMSE: 2.5567e-04\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1444e-04 - mse: 1.3862e-05 - NMSE: 1.2530e-04 - tot_time: 3h 24m 51.2s\n",
      "\n",
      "Epoch 121: val_NMSE improved from 0.00026 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1444e-04 - mse: 1.3862e-05 - NMSE: 1.2530e-04 - val_loss: 1.2869e-04 - val_mse: 2.8146e-05 - val_NMSE: 2.5443e-04\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1435e-04 - mse: 1.3831e-05 - NMSE: 1.2502e-04 - tot_time: 3h 25m 24.4s\n",
      "\n",
      "Epoch 122: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1435e-04 - mse: 1.3831e-05 - NMSE: 1.2502e-04 - val_loss: 1.2866e-04 - val_mse: 2.8177e-05 - val_NMSE: 2.5471e-04\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1426e-04 - mse: 1.3800e-05 - NMSE: 1.2474e-04 - tot_time: 3h 25m 57.0s\n",
      "\n",
      "Epoch 123: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1426e-04 - mse: 1.3800e-05 - NMSE: 1.2474e-04 - val_loss: 1.2856e-04 - val_mse: 2.8142e-05 - val_NMSE: 2.5439e-04\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1424e-04 - mse: 1.3849e-05 - NMSE: 1.2518e-04 - tot_time: 3h 26m 31.6s\n",
      "\n",
      "Epoch 124: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 35s 2s/step - loss: 1.1424e-04 - mse: 1.3849e-05 - NMSE: 1.2518e-04 - val_loss: 1.2848e-04 - val_mse: 2.8123e-05 - val_NMSE: 2.5422e-04\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1417e-04 - mse: 1.3841e-05 - NMSE: 1.2511e-04 - tot_time: 3h 27m 5.2s\n",
      "\n",
      "Epoch 125: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1417e-04 - mse: 1.3841e-05 - NMSE: 1.2511e-04 - val_loss: 1.2836e-04 - val_mse: 2.8067e-05 - val_NMSE: 2.5371e-04\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1405e-04 - mse: 1.3783e-05 - NMSE: 1.2459e-04 - tot_time: 3h 27m 37.0s\n",
      "\n",
      "Epoch 126: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1405e-04 - mse: 1.3783e-05 - NMSE: 1.2459e-04 - val_loss: 1.2827e-04 - val_mse: 2.8041e-05 - val_NMSE: 2.5347e-04\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1397e-04 - mse: 1.3761e-05 - NMSE: 1.2439e-04 - tot_time: 3h 28m 10.3s\n",
      "\n",
      "Epoch 127: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1397e-04 - mse: 1.3761e-05 - NMSE: 1.2439e-04 - val_loss: 1.2822e-04 - val_mse: 2.8048e-05 - val_NMSE: 2.5354e-04\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1391e-04 - mse: 1.3765e-05 - NMSE: 1.2442e-04 - tot_time: 3h 28m 44.2s\n",
      "\n",
      "Epoch 128: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1391e-04 - mse: 1.3765e-05 - NMSE: 1.2442e-04 - val_loss: 1.2807e-04 - val_mse: 2.7967e-05 - val_NMSE: 2.5281e-04\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1386e-04 - mse: 1.3782e-05 - NMSE: 1.2458e-04 - tot_time: 3h 29m 17.0s\n",
      "\n",
      "Epoch 129: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1386e-04 - mse: 1.3782e-05 - NMSE: 1.2458e-04 - val_loss: 1.2801e-04 - val_mse: 2.7969e-05 - val_NMSE: 2.5282e-04\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1375e-04 - mse: 1.3734e-05 - NMSE: 1.2415e-04 - tot_time: 3h 29m 49.3s\n",
      "\n",
      "Epoch 130: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1375e-04 - mse: 1.3734e-05 - NMSE: 1.2415e-04 - val_loss: 1.2792e-04 - val_mse: 2.7944e-05 - val_NMSE: 2.5260e-04\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.1373e-04 - mse: 1.3778e-05 - NMSE: 1.2454e-04 - tot_time: 3h 30m 23.3s\n",
      "\n",
      "Epoch 131: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1373e-04 - mse: 1.3778e-05 - NMSE: 1.2454e-04 - val_loss: 1.2781e-04 - val_mse: 2.7896e-05 - val_NMSE: 2.5217e-04\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1360e-04 - mse: 1.3714e-05 - NMSE: 1.2396e-04 - tot_time: 3h 30m 55.8s\n",
      "\n",
      "Epoch 132: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1360e-04 - mse: 1.3714e-05 - NMSE: 1.2396e-04 - val_loss: 1.2773e-04 - val_mse: 2.7887e-05 - val_NMSE: 2.5208e-04\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1352e-04 - mse: 1.3705e-05 - NMSE: 1.2388e-04 - tot_time: 3h 31m 30.1s\n",
      "\n",
      "Epoch 133: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1352e-04 - mse: 1.3705e-05 - NMSE: 1.2388e-04 - val_loss: 1.2762e-04 - val_mse: 2.7839e-05 - val_NMSE: 2.5165e-04\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1338e-04 - mse: 1.3621e-05 - NMSE: 1.2313e-04 - tot_time: 3h 32m 3.5s\n",
      "\n",
      "Epoch 134: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1338e-04 - mse: 1.3621e-05 - NMSE: 1.2313e-04 - val_loss: 1.2754e-04 - val_mse: 2.7826e-05 - val_NMSE: 2.5153e-04\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1336e-04 - mse: 1.3670e-05 - NMSE: 1.2357e-04 - tot_time: 3h 32m 37.8s\n",
      "\n",
      "Epoch 135: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1336e-04 - mse: 1.3670e-05 - NMSE: 1.2357e-04 - val_loss: 1.2739e-04 - val_mse: 2.7738e-05 - val_NMSE: 2.5074e-04\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1324e-04 - mse: 1.3620e-05 - NMSE: 1.2311e-04 - tot_time: 3h 33m 10.8s\n",
      "\n",
      "Epoch 136: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1324e-04 - mse: 1.3620e-05 - NMSE: 1.2311e-04 - val_loss: 1.2733e-04 - val_mse: 2.7748e-05 - val_NMSE: 2.5082e-04\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1315e-04 - mse: 1.3601e-05 - NMSE: 1.2294e-04 - tot_time: 3h 33m 43.7s\n",
      "\n",
      "Epoch 137: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1315e-04 - mse: 1.3601e-05 - NMSE: 1.2294e-04 - val_loss: 1.2723e-04 - val_mse: 2.7715e-05 - val_NMSE: 2.5053e-04\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1310e-04 - mse: 1.3617e-05 - NMSE: 1.2309e-04 - tot_time: 3h 34m 18.0s\n",
      "\n",
      "Epoch 138: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1310e-04 - mse: 1.3617e-05 - NMSE: 1.2309e-04 - val_loss: 1.2726e-04 - val_mse: 2.7807e-05 - val_NMSE: 2.5136e-04\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1302e-04 - mse: 1.3602e-05 - NMSE: 1.2295e-04 - tot_time: 3h 34m 51.6s\n",
      "\n",
      "Epoch 139: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1302e-04 - mse: 1.3602e-05 - NMSE: 1.2295e-04 - val_loss: 1.2719e-04 - val_mse: 2.7804e-05 - val_NMSE: 2.5133e-04\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1291e-04 - mse: 1.3558e-05 - NMSE: 1.2255e-04 - tot_time: 3h 35m 25.3s\n",
      "\n",
      "Epoch 140: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1291e-04 - mse: 1.3558e-05 - NMSE: 1.2255e-04 - val_loss: 1.2702e-04 - val_mse: 2.7707e-05 - val_NMSE: 2.5046e-04\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1291e-04 - mse: 1.3624e-05 - NMSE: 1.2315e-04 - tot_time: 3h 35m 58.8s\n",
      "\n",
      "Epoch 141: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1291e-04 - mse: 1.3624e-05 - NMSE: 1.2315e-04 - val_loss: 1.2704e-04 - val_mse: 2.7799e-05 - val_NMSE: 2.5129e-04\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1285e-04 - mse: 1.3639e-05 - NMSE: 1.2328e-04 - tot_time: 3h 36m 31.4s\n",
      "\n",
      "Epoch 142: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1285e-04 - mse: 1.3639e-05 - NMSE: 1.2328e-04 - val_loss: 1.2697e-04 - val_mse: 2.7797e-05 - val_NMSE: 2.5127e-04\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1276e-04 - mse: 1.3619e-05 - NMSE: 1.2310e-04 - tot_time: 3h 37m 4.5s\n",
      "\n",
      "Epoch 143: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1276e-04 - mse: 1.3619e-05 - NMSE: 1.2310e-04 - val_loss: 1.2683e-04 - val_mse: 2.7720e-05 - val_NMSE: 2.5057e-04\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1261e-04 - mse: 1.3532e-05 - NMSE: 1.2232e-04 - tot_time: 3h 37m 38.3s\n",
      "\n",
      "Epoch 144: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1261e-04 - mse: 1.3532e-05 - NMSE: 1.2232e-04 - val_loss: 1.2674e-04 - val_mse: 2.7704e-05 - val_NMSE: 2.5043e-04\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1256e-04 - mse: 1.3551e-05 - NMSE: 1.2249e-04 - tot_time: 3h 38m 12.5s\n",
      "\n",
      "Epoch 145: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1256e-04 - mse: 1.3551e-05 - NMSE: 1.2249e-04 - val_loss: 1.2683e-04 - val_mse: 2.7861e-05 - val_NMSE: 2.5185e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1258e-04 - mse: 1.3646e-05 - NMSE: 1.2335e-04 - tot_time: 3h 38m 46.5s\n",
      "\n",
      "Epoch 146: val_NMSE improved from 0.00025 to 0.00025, saving model to /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/checkpoint\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1258e-04 - mse: 1.3646e-05 - NMSE: 1.2335e-04 - val_loss: 1.2657e-04 - val_mse: 2.7672e-05 - val_NMSE: 2.5014e-04\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1245e-04 - mse: 1.3584e-05 - NMSE: 1.2279e-04Restoring model weights from the end of the best epoch: 137.\n",
      " - tot_time: 3h 39m 20.2s\n",
      "\n",
      "Epoch 147: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1245e-04 - mse: 1.3584e-05 - NMSE: 1.2279e-04 - val_loss: 1.2670e-04 - val_mse: 2.7875e-05 - val_NMSE: 2.5198e-04\n",
      "Epoch 147: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------- LEARNING RATE : 1e-05 -----------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1315e-04 - mse: 1.3635e-05 - NMSE: 1.2325e-04 - tot_time: 3h 39m 53.5s\n",
      "\n",
      "Epoch 1: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1315e-04 - mse: 1.3635e-05 - NMSE: 1.2325e-04 - val_loss: 1.2728e-04 - val_mse: 2.7768e-05 - val_NMSE: 2.5101e-04\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1307e-04 - mse: 1.3566e-05 - NMSE: 1.2263e-04 - tot_time: 3h 40m 27.1s\n",
      "\n",
      "Epoch 2: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1307e-04 - mse: 1.3566e-05 - NMSE: 1.2263e-04 - val_loss: 1.2724e-04 - val_mse: 2.7741e-05 - val_NMSE: 2.5076e-04\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1307e-04 - mse: 1.3566e-05 - NMSE: 1.2262e-04 - tot_time: 3h 40m 59.8s\n",
      "\n",
      "Epoch 3: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1307e-04 - mse: 1.3566e-05 - NMSE: 1.2262e-04 - val_loss: 1.2723e-04 - val_mse: 2.7734e-05 - val_NMSE: 2.5069e-04\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1305e-04 - mse: 1.3557e-05 - NMSE: 1.2254e-04 - tot_time: 3h 41m 32.8s\n",
      "\n",
      "Epoch 4: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1305e-04 - mse: 1.3557e-05 - NMSE: 1.2254e-04 - val_loss: 1.2721e-04 - val_mse: 2.7719e-05 - val_NMSE: 2.5056e-04\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1297e-04 - mse: 1.3488e-05 - NMSE: 1.2192e-04 - tot_time: 3h 42m 5.8s\n",
      "\n",
      "Epoch 5: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1297e-04 - mse: 1.3488e-05 - NMSE: 1.2192e-04 - val_loss: 1.2724e-04 - val_mse: 2.7754e-05 - val_NMSE: 2.5088e-04\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1304e-04 - mse: 1.3559e-05 - NMSE: 1.2256e-04 - tot_time: 3h 42m 39.0s\n",
      "\n",
      "Epoch 6: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1304e-04 - mse: 1.3559e-05 - NMSE: 1.2256e-04 - val_loss: 1.2718e-04 - val_mse: 2.7707e-05 - val_NMSE: 2.5045e-04\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1300e-04 - mse: 1.3526e-05 - NMSE: 1.2226e-04 - tot_time: 3h 43m 13.0s\n",
      "\n",
      "Epoch 7: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1300e-04 - mse: 1.3526e-05 - NMSE: 1.2226e-04 - val_loss: 1.2716e-04 - val_mse: 2.7689e-05 - val_NMSE: 2.5029e-04\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1299e-04 - mse: 1.3530e-05 - NMSE: 1.2230e-04 - tot_time: 3h 43m 45.0s\n",
      "\n",
      "Epoch 8: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 32s 2s/step - loss: 1.1299e-04 - mse: 1.3530e-05 - NMSE: 1.2230e-04 - val_loss: 1.2716e-04 - val_mse: 2.7703e-05 - val_NMSE: 2.5042e-04\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1300e-04 - mse: 1.3543e-05 - NMSE: 1.2242e-04 - tot_time: 3h 44m 18.7s\n",
      "\n",
      "Epoch 9: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1300e-04 - mse: 1.3543e-05 - NMSE: 1.2242e-04 - val_loss: 1.2717e-04 - val_mse: 2.7721e-05 - val_NMSE: 2.5058e-04\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1297e-04 - mse: 1.3526e-05 - NMSE: 1.2227e-04 - tot_time: 3h 44m 52.6s\n",
      "\n",
      "Epoch 10: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 34s 2s/step - loss: 1.1297e-04 - mse: 1.3526e-05 - NMSE: 1.2227e-04 - val_loss: 1.2717e-04 - val_mse: 2.7727e-05 - val_NMSE: 2.5064e-04\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1297e-04 - mse: 1.3533e-05 - NMSE: 1.2233e-04Restoring model weights from the end of the best epoch: 1.\n",
      " - tot_time: 3h 45m 25.5s\n",
      "\n",
      "Epoch 11: val_NMSE did not improve from 0.00025\n",
      " - saving loss histories at /home/rkaushik/Documents/Thesis/MLROM/new_lorenz/saved_rnn/rnn_005/checkpoints/LossHistoriesCheckpoint\n",
      "18/18 [==============================] - 33s 2s/step - loss: 1.1297e-04 - mse: 1.3533e-05 - NMSE: 1.2233e-04 - val_loss: 1.2715e-04 - val_mse: 2.7713e-05 - val_NMSE: 2.5051e-04\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "# compiling the network\n",
    "rnn_net.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_list[0]),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=['mse', NMSE(divisor_arr=time_stddev)],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # this loads the weights/attributes of the optimizer as well\n",
    "    if strategy is not None:\n",
    "        with strategy.scope():\n",
    "            rnn_net.load_weights(wt_file)\n",
    "    else:\n",
    "        rnn_net.load_weights(wt_file)\n",
    "\n",
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    # implementing early stopping\n",
    "    baseline = None\n",
    "    if behaviour == 'loadCheckpointAndContinueTraining':\n",
    "        baseline = np.min(val_loss_hist)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_NMSE',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=True,\n",
    "        min_delta=min_delta,\n",
    "        baseline=baseline\n",
    "    )\n",
    "    #** the two lines below are useless because wait is set to 0 in on_train_begin\n",
    "    # early_stopping_cb.wait = earlystopping_wait\n",
    "    # print('early_stopping_cb.wait : {}\\n'.format(early_stopping_cb.wait))\n",
    "\n",
    "    # time callback for each epoch\n",
    "    timekeeper_cb = mytimecallback()\n",
    "\n",
    "    # model checkpoint callback\n",
    "    dir_name_ckpt = dir_name_rnn+dir_sep+'checkpoints'\n",
    "    if not os.path.isdir(dir_name_ckpt):\n",
    "        os.makedirs(dir_name_ckpt)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=dir_name_ckpt+dir_sep+'checkpoint',#+'/checkpoint--loss={loss:.4f}--vall_loss={val_loss:.4f}',\n",
    "        monitor='val_NMSE',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=2,\n",
    "        initial_value_threshold=baseline,\n",
    "        period=1  # saves every `period` epochs\n",
    "    )\n",
    "\n",
    "    # save losses callback\n",
    "    savelosses_cb = SaveLosses(\n",
    "        filepath=dir_name_ckpt+dir_sep+'LossHistoriesCheckpoint',\n",
    "        val_loss_arr=savelosses_cb_vallossarr,\n",
    "        train_loss_arr=savelosses_cb_trainlossarr,\n",
    "        total_epochs=epochs,\n",
    "        period=1)\n",
    "\n",
    "    for i in range(starting_lr_idx, len(learning_rate_list)):\n",
    "        learning_rate = learning_rate_list[i]\n",
    "        K.set_value(rnn_net.optimizer.lr, learning_rate)\n",
    "\n",
    "        savelosses_cb.update_lr_idx(i)\n",
    "\n",
    "        if i == starting_lr_idx:\n",
    "            EPOCHS = num_epochs_left\n",
    "            savelosses_cb.update_offset(epochs-num_epochs_left)\n",
    "        else:\n",
    "            EPOCHS = epochs\n",
    "            savelosses_cb.update_offset(0)\n",
    "\n",
    "        total_s_len = 80\n",
    "        sep_lr_s = ' LEARNING RATE : {} '.format(learning_rate)\n",
    "        sep_lr_s = int((total_s_len - len(sep_lr_s))//2)*'-' + sep_lr_s\n",
    "        sep_lr_s = sep_lr_s + (total_s_len-len(sep_lr_s))*'-'\n",
    "        print('\\n\\n' + '-'*len(sep_lr_s))\n",
    "        print('\\n' + sep_lr_s+'\\n')\n",
    "        print('-'*len(sep_lr_s) + '\\n\\n')\n",
    "        \n",
    "        history = rnn_net.fit(training_data_rnn_input, training_data_rnn_output,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=batch_size,\n",
    "#             validation_split=val_split/train_split,\n",
    "            validation_data=(val_data_rnn_input, val_data_rnn_output),\n",
    "            callbacks=[early_stopping_cb, timekeeper_cb, checkpoint_cb, savelosses_cb],\n",
    "            verbose=1,\n",
    "            shuffle=not stateful,\n",
    "        )\n",
    "\n",
    "        val_loss_hist.extend(history.history['val_loss'])\n",
    "        train_loss_hist.extend(history.history['loss'])\n",
    "        \n",
    "        val_NMSE_hist.extend(history.history['val_NMSE'])\n",
    "        train_NMSE_hist.extend(history.history['NMSE'])\n",
    "\n",
    "        val_MSE_hist.extend(history.history['val_mse'])\n",
    "        train_MSE_hist.extend(history.history['mse'])\n",
    "        \n",
    "        if i == starting_lr_idx:\n",
    "            lr_change[i+1] += len(history.history['val_loss'])\n",
    "        else:\n",
    "            lr_change.append(lr_change[i]+len(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10543,
     "status": "ok",
     "timestamp": 1667873563321,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "SO7iK4mbneQm",
    "outputId": "48110900-962a-49c1-c532-718999590884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 552, 2) (96, 552, 2)\n",
      "3/3 [==============================] - 1s 309ms/step - loss: 1.8092e-04 - mse: 8.1408e-05 - NMSE: 7.3596e-04\n"
     ]
    }
   ],
   "source": [
    "if behaviour == 'initialiseAndTrainFromScratch' or behaviour == 'loadCheckpointAndContinueTraining':\n",
    "    for layer in rnn_net.rnn_list:\n",
    "        if layer.stateful == True:\n",
    "            layer.reset_states()\n",
    "    print(testing_data_rnn_input.shape, testing_data_rnn_output.shape)\n",
    "    eval_dict = rnn_net.evaluate(\n",
    "        testing_data_rnn_input, testing_data_rnn_output,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    save_path = dir_name_rnn+dir_sep+'final_net'\n",
    "\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "\n",
    "    with open(save_path+dir_sep+'losses.txt', 'w') as f:\n",
    "        f.write(str({\n",
    "            'val_loss_hist':val_loss_hist,\n",
    "            'train_loss_hist':train_loss_hist,\n",
    "            'val_MSE_hist':val_MSE_hist,\n",
    "            'train_MSE_hist':train_MSE_hist,\n",
    "            'val_NMSE_hist':val_NMSE_hist,\n",
    "            'train_NMSE_hist':train_NMSE_hist,\n",
    "            'lr_change':lr_change,\n",
    "            'test_loss':eval_dict[0],\n",
    "            'test_MSE':eval_dict[1],\n",
    "            'test_NMSE':eval_dict[2],\n",
    "        }))\n",
    "        \n",
    "    if normalize_dataset == True:\n",
    "        with open(save_path+dir_sep+'rnn_normalization.txt', 'w') as f:\n",
    "            f.write(str({\n",
    "                'normalization_arr':normalization_arr\n",
    "            }))\n",
    "\n",
    "    rnn_net.save_everything(\n",
    "        file_name=save_path+dir_sep+'final_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_kwargs = {'fontsize':15}\n",
    "ylabel_kwargs = {'fontsize':15}\n",
    "legend_kwargs = {'fontsize':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting losses\n",
    "dir_name_plot = dir_name_rnn + '/plots'\n",
    "if not os.path.isdir(dir_name_plot):\n",
    "    os.makedirs(dir_name_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1667873564544,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "nDv5D8APneQm",
    "outputId": "ee911dc8-4d36-48af-8ad0-07cef0dbaf81"
   },
   "outputs": [],
   "source": [
    "# Visualize loss history\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_loss_hist,\n",
    "    val_loss=val_loss_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "\n",
    "plt.savefig(dir_name_plot + '{ds}loss_history.pdf'.format(ds=dir_sep), dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_MSE_hist,\n",
    "    val_loss=val_MSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training MSE', 'Validation MSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='MSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/MSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "fig, ax = plot_losses(\n",
    "    training_loss=train_NMSE_hist,\n",
    "    val_loss=val_NMSE_hist,\n",
    "    lr_change=lr_change,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    legend_list=['Training NMSE', 'Validation NMSE'],\n",
    "    xlabel='Epoch',\n",
    "    ylabel='NMSE',\n",
    "    xlabel_kwargs=xlabel_kwargs,\n",
    "    ylabel_kwargs=ylabel_kwargs,\n",
    "    legend_kwargs=legend_kwargs,\n",
    ")\n",
    "plt.savefig(dir_name_plot+'/NMSE_history.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1667873575637,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "dbLa0AwlDBWh",
    "outputId": "d3f93f58-9ce7-4994-8d68-29520477e02d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667873575638,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "MDopQ4JMhRPV",
    "outputId": "f6480bb7-5837-4a80-9333-f9acd175b27a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576097,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "2_fAlJz2Vdev"
   },
   "outputs": [],
   "source": [
    "def rescale_data(data, normalization_arr):\n",
    "    '''\n",
    "    data - [num_batches x num_timesteps x num_states]\n",
    "    normalization_arr = [2 x num_states]\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    shape = new_data.shape\n",
    "    for i in range(data.shape[-1]):\n",
    "        new_data[:, i] -= normalization_arr[0, i]\n",
    "        new_data[:, i] /= normalization_arr[1, i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def norm_sq_time_average(data):\n",
    "    data_norm_sq = np.zeros(shape=data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_norm_sq[:] += data[:, i]**2\n",
    "    # integrating using the trapezoidal rule\n",
    "    norm_sq_time_avg = np.sum(data_norm_sq) - 0.5*(data_norm_sq[0]+data_norm_sq[-1])\n",
    "    norm_sq_time_avg /= data_norm_sq.shape[0]\n",
    "    return norm_sq_time_avg\n",
    "\n",
    "def invert_normalization(data, normalization_arr):\n",
    "    new_data = np.empty_like(data)\n",
    "    shape = new_data.shape\n",
    "    # print(shape)\n",
    "    for i in range(shape[-1]):\n",
    "        if len(shape) == 2:\n",
    "            new_data[:, i] = data[:, i]\n",
    "            new_data[:, i] *= normalization_arr[1, i]\n",
    "            new_data[:, i] += normalization_arr[0, i]\n",
    "        elif len(shape) == 3:\n",
    "            new_data[:, :, i] = data[:, :, i]\n",
    "            new_data[:, :, i] *= normalization_arr[1, i]\n",
    "            new_data[:, :, i] += normalization_arr[0, i]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667873576098,
     "user": {
      "displayName": "Rohan Kaushik",
      "userId": "13918477614376051685"
     },
     "user_tz": -60
    },
    "id": "s5BNteRC7COC",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_rnn/rnn_005\n",
      "num_runs : 100\n",
      "\n",
      "error_threshold = 0.5\n",
      "prediction_horizon : 1.037143418638994, median : 0.9058021123484662\n",
      "ph_min : 0.09058021123484662, ph_max : 2.989146970749939\n",
      "stddev : 0.7864715190232928, IQR : 1.0869625348181595\n",
      "1st quartile : 0.3623208449393865, 3rd quartile : 1.449283379757546\n",
      "analysis time : 19.11406946182251 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = AR_testing_data_rnn_input.shape[0]\n",
    "\n",
    "analysis_time = time.time()\n",
    "\n",
    "AR_rnn_net = AR_RNN(\n",
    "    load_file=save_path+'/final_net_class_dict.txt',\n",
    "    T_input=T_sample_input_AR,\n",
    "    T_output=T_sample_output_AR,\n",
    "    stddev=0.0,\n",
    "    batch_size=num_runs,\n",
    "    lambda_reg=lambda_reg,\n",
    ")\n",
    "AR_rnn_net.build(input_shape=tuple(AR_testing_data_rnn_input.shape[0:2]) + tuple(testing_data_rnn_input.shape[2:]))\n",
    "AR_rnn_net.load_weights_from_file(save_path+'/final_net_gru_weights.h5')\n",
    "\n",
    "AR_AERNN_net = AR_AERNN(\n",
    "    ae_net,\n",
    "    AR_rnn_net,\n",
    "    normalization_arr,\n",
    "    normalization_constant_arr_aedata,\n",
    "    covmat_lmda=0.0,\n",
    "    time_stddev_ogdata=time_stddev_ogdata,\n",
    "    time_mean_ogdata=time_mean_ogdata,\n",
    "    loss_weights=None,\n",
    "    clipnorm=None,\n",
    "    global_clipnorm=None\n",
    ")\n",
    "\n",
    "savefig_fname = 'pre_ARtraining-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "npsavedata_fname = '/prediction_horizons-testingdata--combinedAERNN--ZEROoutsteps'\n",
    "plot_dir = '/plots'\n",
    "\n",
    "sidx1 = dir_name_rnn[::-1].index('/')\n",
    "sidx2 = dir_name_rnn[-sidx1-2::-1].index('/')\n",
    "print(dir_name_rnn[-(sidx1+sidx2+1):])\n",
    "print('num_runs :', num_runs)\n",
    "\n",
    "prediction_horizon_arr = np.empty(shape=num_runs)\n",
    "prediction = np.array(AR_AERNN_net(AR_testing_data_rnn_input, training=False))\n",
    "prediction = invert_normalization(prediction, normalization_constant_arr_aedata)\n",
    "\n",
    "data_in_og = AR_testing_data_rnn_input\n",
    "data_out_og = AR_testing_data_rnn_output\n",
    "\n",
    "energySpectrum_dataout = 0.0\n",
    "energySpectrum_pred = 0.0\n",
    "\n",
    "avg_time = 0.\n",
    "for i in range(num_runs):\n",
    "    run_time = time.time()\n",
    "    lyap_time = lyapunov_time_arr[0]\n",
    "\n",
    "    data_out = data_out_og[i]\n",
    "    data_out = invert_normalization(data_out, normalization_constant_arr_aedata)\n",
    "\n",
    "    ### Error and prediction horizon\n",
    "    # error = np.linalg.norm(data_out[:, :] - prediction[i, :, :], axis=1)\n",
    "    error = (data_out[:, :] - prediction[i, :, :])**2\n",
    "    # error /= norm_sq_time_average(data_out)**0.5\n",
    "    error = np.mean(np.divide(error, time_stddev_ogdata**2), axis=1)**0.5\n",
    "\n",
    "    predhor_idx = np.where(error >= error_threshold)[0]\n",
    "    if predhor_idx.shape[0] == 0:\n",
    "        predhor_idx = error.shape[0]\n",
    "    else:\n",
    "        predhor_idx = predhor_idx[0]\n",
    "\n",
    "    prediction_horizon_arr[i] = predhor_idx*dt_rnn/lyap_time\n",
    "\n",
    "    run_time = time.time() - run_time\n",
    "    avg_time = (avg_time*i + run_time)/(i+1)\n",
    "    eta = avg_time * (num_runs-1 - i)\n",
    "    # print('    {} / {} -- run_time : {:.2f} s -- eta : {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "    #     i+1,\n",
    "    #     num_runs,\n",
    "    #     run_time,\n",
    "    #     float(eta // 3600),\n",
    "    #     float((eta%3600)//60),\n",
    "    #     float((eta%3600)%60),\n",
    "    # ))\n",
    "\n",
    "median_idx = int(np.round(0.5*num_runs-1))\n",
    "quartile_1_idx = int(np.round(0.25*num_runs-1))\n",
    "quartile_3_idx = int(np.round(0.75*num_runs-1))\n",
    "\n",
    "prediction_horizon_arr.sort()\n",
    "\n",
    "median = prediction_horizon_arr[median_idx]\n",
    "quartile_1 = prediction_horizon_arr[quartile_1_idx]\n",
    "quartile_3 = prediction_horizon_arr[quartile_3_idx]\n",
    "IQR = quartile_3 - quartile_1\n",
    "\n",
    "prediction_horizon = np.mean(prediction_horizon_arr)\n",
    "stddev_ph = np.std(prediction_horizon_arr)\n",
    "\n",
    "s = 'error_threshold = {}\\n'.format(error_threshold)\n",
    "s += 'prediction_horizon : {}, median : {}\\n'.format(prediction_horizon, median)\n",
    "s += 'ph_min : {}, ph_max : {}\\n'.format(prediction_horizon_arr.min(), prediction_horizon_arr.max())\n",
    "s += 'stddev : {}, IQR : {}\\n'.format(stddev_ph, IQR)\n",
    "s += '1st quartile : {}, 3rd quartile : {}'.format(quartile_1, quartile_3)\n",
    "\n",
    "print('\\n'+s)\n",
    "\n",
    "plot_histogram_and_save(\n",
    "    prediction_horizon_arr, median,\n",
    "    save_dir=dir_name_rnn+plot_dir,\n",
    "    savefig_fname=savefig_fname,\n",
    ")\n",
    "\n",
    "np.savez(\n",
    "    dir_name_rnn+npsavedata_fname,\n",
    "    prediction_horizon_arr=prediction_horizon_arr,\n",
    "    error_threshold=error_threshold,\n",
    ")\n",
    "\n",
    "with open(dir_name_rnn+npsavedata_fname+'--statistics.txt', 'w') as fl:\n",
    "    fl.write(s)\n",
    "\n",
    "print('analysis time : {} s\\n'.format(time.time() - analysis_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'use_trainable_weights_with_reslayers' in rnn_net.__dict__.keys():\n",
    "    if use_trainable_weights_with_reslayers == True:\n",
    "        for i in range(rnn_net.num_skip_connections):\n",
    "            print('reslayer_factor_{} : {}'.format(i, rnn_net.reslayer_factor[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
